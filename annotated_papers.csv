Title,Abstract,Category,PDF File,Year
CONNECTIVITY VERSUS ENTROPY,"How does the connectivity of a neural network (number of synapses per neuron) relate to the complexity of the problems it can handle (measured by the entropy)? Switching theory would suggest no relation at all, since all Boolean functions can be implemented using a circuit with very low connectivity (e.g., using two-input NAND gates). However, for a network that learns a problem from examples using a local learning rule, we prove that the entropy of the problem becomes a lower bound for the connectivity of the network. INTRODUCTION The most distinguishing feature of neural networks is their ability to spon taneously learn the desired function from 'training' samples, i.e., their ability to program themselves. Clearly, a given neural network cannot just learn any function, there must be some restrictions on which networks can learn which functions. One obvious restriction, which is independent of the learning aspect, is that the network must be big enough to accommodate the circuit comp",Optimization & Theoretical ML,03afdbd66e7929b125f8597834fa83a4-Paper.pdf,1987
BIT - SERIAL NEURAL NETWORKS,"A bit - serial VLSI neural network is described from an initial architecture for a synapse array through to silicon layout and board design. The issues surrounding bit - serial computation, and analog/digital arithmetic are discussed and the parallel development of a hybrid analog/digital neural network is outlined. Learning and recall capabilities are reported for the bit - serial network along with a projected specification for a 64 - neuron, bit - serial board operating at 20 MHz. This tech nique is extended to a 256 (2562 synapses) network with an update time of 3ms, using a ""paging"" technique to time - multiplex calculations through the synapse array.",Computer Vision,02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf,1987
THE HOPFIELD MODEL WITH MULT I-LEVEL NEURONS,"The Hopfield neural network. model for associative memory is generalized. The generalization replaces two state neurons by neurons taking a richer set of values. Two classes of neuron input output relations are developed guaranteeing convergence to stable states. The first is a class of ""continuous"" rela- tions and the second is a class of allowed quantization rules for the neurons. The information capacity for networks from the second class is fOWld to be of order N 3 bits for a network with N neurons. A generalization of the sum of outer products learning rule is developed and investigated as well. © American Institute of Physics 1988 279 I. INTRODUCTION The ability to perfonn collective computation in a distributed system of flexible structure without global synchronization is an important engineering objective. Hopfield's neural network [1] is such a model of associative content addressable memory. An important property of the Hopfield neural network is its guaranteed convergence t",Optimization & Theoretical ML,072b030ba126b2f4b2374f342be9ed44-Paper.pdf,1987
How Neural Nets Work,"There is presently great interest in the abilities of neural networks to mimic ""qualitative reasoning"" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccu rate and therefore best suited for ""fuzzy"" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net ""number crunching"" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional tech niques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this , and in the process show that a large class of functions from Rn. ~ Rffl may be accurately approximated by a backpropa",Deep Learning,093f65e080a295f8076b1c5722a46aa2-Paper.pdf,1987
SPATIAL ORGANIZATION OF NEURAL,"The aim of this paper is to explore the spatial organization of neural networks under Markovian assumptions, in what concerns the be haviour of individual cells and the interconnection mechanism. Space organizational properties of neural nets are very relevant in image modeling and pattern analysis, where spatial computations on stocha stic two-dimensional image fields are involved. As a first approach we develop a random neural network model, based upon simple probabi listic assumptions, whose organization is studied by means of dis crete-event simulation. We then investigate the possibility of ap proXimating the random network's behaviour by using an analytical ap proach originating from the theory of general product-form queueing networks. The neural network is described by an open network of no des, in which customers moving from node to node represent stimula tions and connections between nodes are expressed in terms of sui tably selected routing probabilities. We obtain the solut",Reinforcement Learning,14bfa6bb14875e45bba028a21ed38046-Paper.pdf,1987
A NEURAL-NETWORK SOLUTION TO THE CONCENTRATOR,"Networks of simple analog processors having neuron-like properties have been employed to compute good solutions to a variety of optimization prob lems. This paper presents a neural-net solution to a resource allocation prob lem that arises in providing local access to the backbone of a wide-area com munication network. The problem is described in terms of an energy function that can be mapped onto an analog computational network. Simulation results characterizing the performance of the neural computation are also presented. INTRODUCTION This paper presents a neural-network solution to a resource allocation problem that arises in providing access to the backbone of a communication network. 1 In the field of operations research, this problem was first known as the warehouse location problem and heuristics for finding feasible, suboptimal solutions have been developed previously.2. More recently it has been known 3 as the multifacility location problem and as the concentrator assignment p",Optimization & Theoretical ML,1679091c5a880faf6fb5e6087eb1b2dc-Paper.pdf,1987
LEARNING BY ST ATE RECURRENCE DETECfION,"This research investigates a new technique for unsupervised learning of nonlinear control problems. The approach is applied both to Michie and Chambers BOXES algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and has significantly improved the convergence rate of stochastically based learning automata. Recurrence learning is a new nonlinear reward-penalty algorithm. It exploits information found during learning trials to reinforce decisions resulting in the recurrence of nonfailing states. Recurrence learning applies positive reinforcement during the exploration of the search space, whereas in the BOXES or ASE algorithms, only negative weight reinforcement is applied, and then only on failure. Simulation results show that the added information from recurrence learning increases the learning rate. Our empirical results show that recurrence learning is faster than both basic failure driven learning and failure prediction methods. Although recurrence learning has",Reinforcement Learning,182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf,1987
STABILITY RESULTS FOR NEURAL NETWORKS,"In the present paper we survey and utilize results from the qualitative theory of large scale interconnected dynamical systems in order to develop a qualitative theory for the Hopfield model of neural networks. In our approach we view such networks as an inter connection of many single neurons. Our results are phrased in terms of the qualitative properties of the individual neurons and in terms of the properties of the interconnecting structure of the neural networks. Aspects of neural networks which we address include asymptotic stability, exponential stability, and instability of an equilibrium; estimates of trajectory bounds; estimates of the domain of attraction of an asymptotically stable equilibrium; and stability of neural networks under structural perturbations. INTRODUCTION In recent years, neural networks have attracted considerable attention as candidates for novel computational systemsl-3. These types of large-scale dynamical systems, in analogy to biological structures, ta",Optimization & Theoretical ML,19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf,1987
INTRODUCTION TO A SYSTEM FOR IMPLEMENTING NEURAL NET,"Neural networks have attracted much interest recently, and using parallel architectures to simulate neural networks is a natural and necessary applica tion. The SIMD model of parallel computation is chosen, because systems of this type can be built with large numbers of processing elements. However, such systems are not naturally suited to generalized communication. A method is proposed that allows an implementation of neural network connections on massively parallel SIMD architectures. The key to this system is an algorithm that allows the formation of arbitrary connections between the ""neurons"". A feature is the ability to add new connections quickly. It also has error recov ery ability and is robust over a variety of network topologies. Simulations of the general connection system, and its implementation on the Connection Ma chine, indicate that the time and space requirements are proportional to the product of the average number of connections per neuron and the diameter of the int",Deep Learning,1c383cd30b7c298ab50293adfecb7b18-Paper.pdf,1987
OPTIMIZAn ON WITH ARTIFICIAL NEURAL NETWORK SYSTEMS:,General formulae for mapping optimization problems into systems of ordinary differential equations associated with artificial neural networks are presented. A comparison is made to optim ization using gradient-search methods. The perfonnance measure is the settling time from an initial state to a target state. A simple analytical example illustrates a situation where dynamical systems representing artificial neural network methods would settle faster than those representing gradient search. Settling time was investigated for a more complicated optimization problem using com puter simulations. The problem was a simplified version of a problem in medical imaging: deter mining loci of cerebral activity from electromagnetic measurements at the scalp. The simulations showed that gradient based systems typically settled 50 to 100 times faster than systems based on current neural network optimization methods. INTRODUCTION Solving optimization problems with systems of equations based on neurob,Optimization & Theoretical ML,1f0e3dad99908345f7439f8ffabdffc4-Paper.pdf,1987
OPTIMAL NEURAL SPIKE CLASSIFICATION,"Being able to record the electrical activities of a number of neurons simultaneously is likely to be important in the study of the functional organization of networks of real neurons. Using one extracellular microelectrode to record from several neurons is one approach to studying the response properties of sets of adjacent and therefore likely related neurons. However, to do this, it is necessary to correctly classify the signals generated by these different neurons. This paper considers this problem of classifying the signals in such an extracellular recording, based upon their shapes, and specifically considers the classification of signals in the case when spikes overlap temporally. Introduction How single neurons in a network of neurons interact when processing information is likely to be a fundamental question central to understanding how real neural networks compute. In the mammalian nervous system we know that spatially adjacent neurons are, in general, more likely to interact,",Optimization & Theoretical ML,1ff1de774005f8da13f42943881c655f-Paper.pdf,1987
REFLEXIVE ASSOCIATIVE MEMORIES,"In the synchronous discrete model, the average memory capacity of bidirectional associative memories (BAMs) is compared with that of Hopfield memories, by means of a calculat10n of the percentage of good recall for 100 random BAMs of dimension 64x64, for different numbers of stored vectors. The memory capac1ty Is found to be much smal1er than the Kosko upper bound, which Is the lesser of the two dimensions of the BAM. On the average, a 64x64 BAM has about 68 % of the capacity of the corresponding Hopfield memory with the same number of neurons. Ortho normal coding of the BAM Increases the effective storage capaCity by only 25 %. The memory capacity limitations are due to spurious stable states, which arise In BAMs In much the same way as in Hopfleld memories. Occurrence of spurious stable states can be avoided by replacing the thresholding in the backlayer of the BAM by another nonl1near process, here called ""Dominant Label Selection"" (DLS). The simplest DLS is the wlnner-take-all net,",Optimization & Theoretical ML,2838023a778dfaecdc212708f721b788-Paper.pdf,1987
The Performance of Convex Set projection Based Neural Networks,"We donsider a class of neural networks whose performance can be analyzed and geometrically visualized in a signal space environment. Alternating projection neural networks (APNN' s) perform by alternately projecting between two or more constraint sets. Criteria for desired and unique convergence are easily established. The network can be configured in either a homogeneous or layered form. The number of patterns that can be stored in the network is on the order of the number of input and hidden neurons. If the output neurons can take on only one of two states, then the trained layered APNN can be easily configured to converge in one iteration. More generally, convergence is at an exponential rate. Convergence can be improved by the use of sigmoid type nonlinearities, network relaxation and/or increasing the number of neurons in the hidden layer. The manner in which the network responds to data for which it was not specifically trained (i.e. how it generalizes) can be directly evaluated ",Optimization & Theoretical ML,28dd2c7955ce926456240b2ff0100bde-Paper.pdf,1987
SPEECH RECOGNITION EXPERIMENTS,"Artificial neural networks (ANNs) are capable of accurate recognition of simple speech vocabularies such as isolated digits [1]. This paper looks at two more difficult vocabularies, the alphabetic E-set and a set of polysyllabic words. The E-set is difficult because it contains weak discriminants and polysyllables are difficult because of timing variation. Polysyllabic word recognition is aided by a time pre-alignment technique based on dynamic pro gramming and E-set recognition is improved by focusing attention. Recogni tion accuracies are better than 98% for both vocabularies when implemented with a single layer perceptron. INTRODUCTION Artificial neural networks perform well on simple pattern recognition tasks. On speaker trained spoken digits a layered network performs as accu rately as a conventional nearest neighbor classifier trained on the same tokens [1]. Spoken digits are easy to recognize since they are for the most part monosyllabic and are distinguished by strong vowels. I",NLP,2a38a4a9316c49e5a833517c45d31070-Paper.pdf,1987
ON PROPERTIES OF NETWORKS,"The complexity and computational capacity of multi-layered, feedforward neural networks is examined. Neural networks for special purpose (structured) functions are examined from the perspective of circuit complexity. Known re sults in complexity theory are applied to the special instance of neural network circuits, and in particular, classes of functions that can be implemented in shallow circuits characterised. Some conclusions are also drawn about learning complexity, and some open problems raised. The dual problem of determining the computational capacity of a class of multi-layered networks with dynamics regulated by an algebraic Hamiltonian is considered. Formal results are pre sented on the storage capacities of programmed higher-order structures, and a tradeoff between ease of programming and capacity is shown. A precise de termination is made of the static fixed point structure of random higher-order constructs, and phase-transitions (0-1 laws) are shown. 1 INTRODUCTION In this",Optimization & Theoretical ML,3295c76acbf4caaed33c36b1b5fc2cb1-Paper.pdf,1987
'Ensemble' Boltzmann Units,No abstract found,Deep Learning,32bb90e8976aab5298d5da10fe66f21d-Paper.pdf,1987
ON TROPISTIC PROCESSING AND ITS APPLICATIONS,"The interaction of a set of tropisms is sufficient in many cases to explain the seemingly complex behavioral responses exhibited by varied classes of biological systems to combinations of stimuli. It can be shown that a straightforward generalization of the tropism phenomenon allows the efficient implementation of effective algorithms which appear to respond ""intelligently"" to changing environmental conditions. Examples of the utilization of tropistic processing techniques will be presented in this paper in applications entailing simulated behavior synthesis, path-planning, pattern analysis (clustering), and engineering design optimization. INTRODUCTION The goal of this paper is to present an intuitive overview of a general unsupervised procedure for addressing a variety of system control and cost minimization problems. This procedure is hased on the idea of utilizing ""stimuli"" produced by the environment in which the systems are designed to operate as basis for dynamically providing t",Optimization & Theoretical ML,33e75ff09dd601bbe69f351039152189-Paper.pdf,1987
NEUROMORPHIC NETWORKS BASED,No abstract found,Computer Vision,3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf,1987
A 'Neural' Network that Learns to Play Backgammon,"We describe a class of connectionist networks that have learned to play back gammon at an intermediate-to-advanced level. TIle networks were trained by a supervised learning procedure on a large set of sample positions evaluated by a human expert. In actual match play against humans and conventional computer programs, the networks demonstrate substantial ability to generalize on the basis of expert knowledge. Our study touches on some of the most important issues in net work learning theory, including the development of efficient coding schemes and training procedures, scaling, generalization, the use of real-valued inputs and out puts, and techniques for escaping from local minima. Practical applications in games and other domains are also discussed. INTRODUCTION A potentially quite useful testing ground for studying issues of knowledge representation and learning in networks can be found in the domain of game playing. Board games such as chess, go, backgammon, and Othello entail cons",Reinforcement Learning,34173cb38f07f89ddbebc2ac9128303f-Paper.pdf,1987
LEARNING REPRESENTATIONS BY RECIRCULATION,"We describe a new learning procedure for networks that contain groups of non linear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a ""visible"" group to be represented by activity vectors in a ""hidden"" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the reconstruction error, and the learning procedure aims to minimize this error. The learning procedure has two passes. On the fust pass, the original visible vector is passed around the loop, and on the second pass an average of the original vector and the reconstructed vector is passed around the loop. The learning procedure changes each weight by an amount proportional to the product of the ""presynaptic"" activity and the difference in the post-synaptic activity on the two passes. This procedure is much s",Deep Learning,35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf,1987
A COMPUTER SIMULATION OF CEREBRAL NEOCORTEX:,No abstract found,Deep Learning,37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf,1987
P A' ITERN CLASS DEGENERACY IN AN UNRESTRICfED STORAGE,"The study of distributed memory systems has produced a number of models which work well in limited domains. However, until recently, the application of such systems to real world problems has been difficult because of storage limitations, and their inherent architectural (and for serial simulation, computational) complexity. Recent development of memories with unrestricted storage capacity and economical feedforward architectures has opened the way to the application of such systems to complex pattern recognition problems. However, such problems are sometimes underspecified by the features which describe the environment, and thus a significant portion of the pattern environment is often non-separable. We will review current work on high density memory systems and their network implementations. We will discuss a general learning algorithm for such high density memories and review its application to separable point sets. Finally, we will introduce an extension of this method for learning",Computer Vision,3c59dc048e8850243be8079a5c74d079-Paper.pdf,1987
Strategies for Teaching Layered Networks,"There is a widespread misconception that the delta-rule is in some sense guaranteed to work on networks without hidden units. As previous authors have mentioned, there is no such guarantee for classification tasks. We will begin by presenting explicit counter examples illustrating two different interesting ways in which the delta rule can fail. We go on to provide conditions which do guarantee that gradient descent will successfully train networks without hidden units to perform two-category classification tasks. We discuss the generalization of our ideas to networks with hidden units and to multi category classification tasks. The Classification Task Consider networks of the form indicated in figure 1. We discuss various methods for training such a network, that is for adjusting its weight vector, w. If we call the input v, the output is g(w· v), where 9 is some function. The classification task we wish to train the network to perform is the following. Given two finite sets of vectors",Optimization & Theoretical ML,3ef815416f775098fe977004015c6193-Paper.pdf,1987
Invariant Object Recognition Using a Distributed Associative Memory,"This paper describes an approach to 2-dimensional object recognition. Complex-log con formal mapping is combined with a distributed associative memory to create a system which recognizes objects regardless of changes in rotation or scale. Recalled information from the memorized database is used to classify an object, reconstruct the memorized ver sion of the object, and estimate the magnitude of changes in scale or rotation. The system response is resistant to moderate amounts of noise and occlusion. Several experiments, us ing real, gray scale images, are presented to show the feasibility of our approach. Introduction The challenge of the visual recognition problem stems from the fact that the projec tion of an object onto an image can be confounded by several dimensions of variability such as uncertain perspective, changing orientation and scale, sensor noise, occlusion, and non-uniform illumination. A vision system must not only be able to sense the identity of an object despite thi",Computer Vision,43ec517d68b6edd3015b3edc9a11367b-Paper.pdf,1987
CYCLES: A Simulation Tool for Studying,"A computer program has been designed and implemented to allow a researcher to analyze the oscillatory behavior of simulated neural networks with cyclic con nectivity. The computer program, implemented on the Texas Instruments Ex plorer / Odyssey system, and the results of numerous experiments are discussed. The program, CYCLES, allows a user to construct, operate, and inspect neural networks containing cyclic connection paths with the aid of a powerful graphics based interface. Numerous cycles have been studied, including cycles with one or more activation points, non-interruptible cycles, cycles with variable path lengths, and interacting cycles. The final class, interacting cycles, is important due to its ability to implement time-dependent goal processing in neural networks. INTRODUCTION Neural networks are capable of many types of computation. However, the majority of researchers are currently limiting their studies to various forms of mapping systems; such as content addressable m",Optimization & Theoretical ML,44f683a84163b3523afe57c2e008bc8c-Paper.pdf,1987
LEARNING ON A GENERAL NETWORK,"This paper generalizes the backpropagation method to a general network containing feed back t;onnections. The network model considered consists of interconnected groups of neurons, where each group could be fully interconnected (it could have feedback connections, with pos sibly asymmetric weights), but no loops between the groups are allowed. A stochastic descent algorithm is applied, under a certain inequality constraint on each intra-group weight matrix which ensures for the network to possess a unique equilibrium state for every input. Introduction It has been shown in the last few years that large networks of interconnected ""neuron"" -like elemp.nts are quite suitable for performing a variety of computational and pattern recognition tasks. One of the well-known neural network models is the backpropagation model [1]-[4]. It is an elegant way for teaching a layered feedforward network by a set of given input/output examples. Neural network models having feedback connections, on the o",Optimization & Theoretical ML,45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf,1987
William Y. Huang and Richard P. Lippmann,"procedures to construct convex decision regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions with three-layer perceptrons (two hidden layers). Here we demonstrate that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions. Such classifiers are robust, train rapidly, and provide good performance with simple decision regions. When complex decision regions are required, however, convergence time can be excessively long and performance is often no better than that of k-nearest neighbor classifiers. Three neural net classifiers are presented that provide more rapid training under such situations. Two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms. A third ""feature map classifier"" uses both unsupervised and supervised training. It provides good performance with little supervised training in situations such as",Optimization & Theoretical ML,4e732ced3463d06de0ca9a15b6153677-Paper.pdf,1987
Scaling Properties of Coarse-Coded Symbol Memories,"symbol processing models. In order to determine how these models would scale, one must first have some understanding of the mathematics of coarse-coded representa tions. We define the general structure of coarse-coded symbol memories and derive mathematical relationships among their essential parameters: memory 8ize, 8ymbol-8et size and capacity. The computed capacity of one of the schemes agrees well with actual measurements oC tbe coarse-coded working memory of DCPS, Touretzky and Hinton's distributed connectionist production system. 1 Introduction A di8tributed repre8entation is a memory scheme in which each entity (concept, symbol) is represented by a pattern of activity over many units [3]. If each unit participates in the representation of many entities, it is said to be coar8ely tuned, and the memory itself is called a coar8e-coded memory. Coarse-coded memories have been used for storing symbols in several neural network symbol processing models, such as Touretzky and Hinton's d",Optimization & Theoretical ML,54229abfcfa5649e7003b83dd4755294-Paper.pdf,1987
SYNCHRONIZATION IN NEURAL NETS,"The paper presents an artificial neural network concept (the Synchronizable Oscillator Networks) where the instants of individual firings in the form of point processes constitute the only form of information transmitted between joining neurons. This type of communication contrasts with that which is assumed in most other models which typically are continuous or discrete value-passing networks. Limiting the messages received by each processing unit to time markers that signal the firing of other units presents significant implemen tation advantages. In our model, neurons fire spontaneously and regularly in the absence of perturbation. When interaction is present, the scheduled firings are advanced or delayed by the firing of neighboring neurons. Networks of such neurons become global oscillators which exhibit multiple synchronizing attractors. From arbitrary initial states, energy minimization learning procedures can make the network converge to oscillatory modes that satisfy multi-dim",Optimization & Theoretical ML,6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf,1987
A Neural Network Classifier Based on Coding Theory,"The new neural network classifier we propose transforms the classification problem into the coding theory problem of decoding a noisy codeword. An input vector in the feature space is transformed into an internal representation which is a codeword in the code space, and then error correction decoded in this space to classify the input feature vector to its class. Two classes of codes which give high performance are the Hadamard matrix code and the maximal length sequence code. We show that the number of classes stored in an N-neuron system is linear in N and significantly more than that obtainable by using the Hopfield type memory as a classifier. I. INTRODUCTION Associative recall using neural networks has recently received a great deal of attention. Hopfield in his papers [1,2) deSCribes a mechanism which iterates through a feedback loop and stabilizes at the memory element that is nearest the input, provided that not many memory vectors are stored in the machine. He has also shown t",Deep Learning,642e92efb79421734881b53e1e1b18b6-Paper.pdf,1987
MICROELECTRONIC IMPLEMENTATIONS OF CONNECTIONIST,"In this paper we discuss why special purpose chips are needed for useful implementations of connectionist neural networks in such applications as pattern recognition and classification. Three chip designs are described: a hybrid digital/analog programmable connection matrix, an analog connection matrix with adjustable connection strengths, and a digital pipe lined best-match chip. The common feature of the designs is the distribution of arithmetic processing power amongst the data storage to minimize data movement. RAMs •••••/ ..... Distributed / '. '. co mputati on chips ... 0 / ...... Q)Q) ,c""C Conventional E~ CPUs ''iiit:::::::;:::::, •••. . .. ::::S,.... ZO • • • • . . 1 1 10 3 10 6 10 9 Node Complexity (No. of Transistors) Figure 1. A schematic graph of addressable node complexity and size for conventional computer chips. Memories can contain millions of very simple nodes each with a very few transistors but with no processing power. CPU chips are essentially one very complex node",Optimization & Theoretical ML,6512bd43d9caa6e02c990b0a82652dca-Paper.pdf,1987
Analysis of distributed representation of,"A general method, the tensor product representation, is described for the distributed representation of value/variable bindings. The method allows the fully distributed representation of symbolic structures: the roles in the structures, as well as the fillers for those roles, can be arbitrarily non-local. Fully and partially localized special cases reduce to existing cases of connectionist representations of structured data; the tensor product representation generalizes these and the few existing examples of fuUy distributed representations of structures. The representation saturates gracefully as larger structures are represented; it penn its recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it pennits values to also serve as variables; it enables analysis of the interference of",Deep Learning,66f041e16a60928b05a7e228a89c3799-Paper.pdf,1987
HIERARCHICAL LEARNING CONTROL -,"Advances in brain theory need two complementary approaches: Analytical investigations by in situ measurements and as well syn thetic modelling supported by computer simulations to generate suggestive hypothesis on purposeful structures in the neural tissue. In this paper research of the second line is described: Starting from a neurophysiologically inspired model of stimulus response (S-R) and/or associative memorization and a psychological ly motivated ministructure for basic control tasks, pre-conditions and conditions are studied for cooperation of such units in a hierarchical organisation, as can be assumed to be the general layout of macrostructures in the brain. I. INTRODUCTION Theoretic modelling in brain theory is a highly speculative subject. However, it is necessary since it seems very unlikely to get a clear picture of this very complicated device by just analy zing the available measurements on sound and/or damaged brain parts only. As in general physics, one has to realize",Optimization & Theoretical ML,67c6a1e7ce56d3d6fa748ab6d9af3fd7-Paper.pdf,1987
PRESYNApnC NEURAL INFORMAnON PROCESSING,"The potential for presynaptic information processing within the arbor of a single axon will be discussed in this paper. Current knowledge about the activity dependence of the firing threshold, the conditions required for conduction failure, and the similarity of nodes along a single axon will be reviewed. An electronic circuit model for a site of low conduction safety in an axon will be presented. In response to single frequency stimulation the electronic circuit acts as a lowpass filter. I. INTRODUCTION The axon is often modeled as a wire which imposes a fixed delay on a propagating signal. Using this model, neural information processing is performed by synaptically sum m ing weighted contributions of the outputs from other neurons. However, substantial information processing may be performed in by the axon itself. Numerous researchers have observed periodic conruction failures at norma! physiological impulse activity rates (e.g., in cat, in frog2, and in man ). The oscillatory nature",Neuroscience,68d30a9594728bc39aa24be94b319d21-Paper.pdf,1987
AN OPTIMIZATION NETWORK FOR MATRIX INVERSION,"Inverse matrix calculation can be considered as an optimization. We have demonstrated that this problem can be rapidly solved by highly interconnected simple neuron-like analog processors. A network for matrix inversion based on the concept of Hopfield's neural network was designed, and implemented with electronic hardware. With slight modifications, the network is readily applicable to solving a linear simultaneous equation efficiently. Notable features of this circuit are potential speed due to parallel processing, and robustness against variations of device parameters. INTRODUCTION Highly interconnected simple analog processors which mmnc a biological neural network are known to excel at certain collective computational tasks. For example, Hopfield and Tank designed a network to solve the traveling salesman problem which is of the np -complete class,l and also designed an AID converter of novel architecture2 based on the Hopfield's neural network model?' 4 The net work could provide",Optimization & Theoretical ML,6c8349cc7260ae62e3b1396831a8398f-Paper.pdf,1987
BASINS OF ATTRACTION FOR,"We have studied the basins of attraction for fixed point and oscillatory attractors in an electronic analog neural network. Basin measurement circuitry periodically opens the network feedback loop, loads raster-scanned initial conditions and examines the resulting attractor. Plotting the basins for fixed points (memories), we show that overloading an associative memory network leads to irregular basin shapes. The network also includes analog time delay circuitry, and we have shown that delay in symmetric networks can introduce basins for oscillatory attractors. Conditions leading to oscillation are related to the presence of frustration; reducing frustration by diluting the connections can stabilize a delay network. (1) - INTRODUCTION The dynamical system formed from an interconnected network of nonlinear neuron-like elements can perform useful parallel computationl - 5 . Recent progress in controlling the dynamics has focussed on algorithms for encoding the location of fixed pOints1,4",Optimization & Theoretical ML,6ea9ab1baa0efb9e19094440c317e21b-Paper.pdf,1987
PROGRAMMABLE SYNAPTIC CHIP FOR,"A binary synaptic matrix chip has been developed for electronic neural networks. The matrix chip contains a programmable 32X32 array of ""long channel"" NMOSFET binary connection elements imple mented in a 3-um bulk CMOS process. Since the neurons are kept off chip, the synaptic chip serves as a ""cascadable"" building block for a multi-chip synaptic network as large as 512X512 in size. As an alternative to the programmable NMOSFET (long channel) connection elements, tailored thin film resistors are deposited, in series with FET switches, on some CMOS test chips, to obtain the weak synaptic connections. Although deposition and patterning of the resistors require additional processing steps, they promise substantial savings in silcon area. The performance of a synaptic chip in a 32- neuron breadboard system in an associative memory test application is discussed. INTRODUCTION The highly parallel and distributive architecture of neural networks offers potential advantages in fault-tolerant an",Optimization & Theoretical ML,6f4922f45568161a8cdf4ad2299f6d23-Paper.pdf,1987
LEARNING A COLOR ALGORITHM FROM EXAMPLES,"A lightness algorithm that separates surface reflectance from illumination in a Mondrian world is synthesized automatically from a set of examples, pairs of input (image irradiance) and desired output (surface reflectance). The algorithm, which re sembles a new lightness algorithm recently proposed by Land, is approximately equiva lent to filtering the image through a center-surround receptive field in individual chro matic channels. The synthesizing technique, optimal linear estimation, requires only one assumption, that the operator that transforms input into output is linear. This assumption is true for a certain class of early vision algorithms that may therefore be synthesized in a similar way from examples. Other methods of synthesizing algorithms from examples, or ""learning"", such as backpropagation, do not yield a significantly dif ferent or better lightness algorithm in the Mondrian world. The linear estimation and backpropagation techniques both produce simultaneous brightnes",Computer Vision,70efdf2ec9b086079795c442636b55fb-Paper.pdf,1987
GENERALIZATION OF BACKPROPAGATION,"A general method for deriving backpropagation algorithms for networks with recurrent and higher order networks is introduced. The propagation of activation in these networks is determined by dissipative differential equations. The error signal is backpropagated by integrating an associated differential equation. The method is introduced by applying it to the recurrent generalization of the feedforward backpropagation network. The method is extended to the case of higher order networks and to a constrained dynamical system for training a content addressable memory. The essential feature of the adaptive algorithms is that adaptive equation has a simple outer product form. Preliminary experiments suggest that learning can occur very rapidly in networks with recurrent connections. The continuous formalism makes the new approach more suitable for implementation in VLSI. Introduction One interesting class of neural networks, typified by the Hopfield neural networks (1,2) or the networks stud",Optimization & Theoretical ML,735b90b4568125ed6c3f678819b6e058-Paper.pdf,1987
Neural Network Implementation Approaches,No abstract found,Optimization & Theoretical ML,7647966b7343c29048673252e490f736-Paper.pdf,1987
On the Power of Neural Networks for,"This paper deals with a neural network model in which each neuron performs a threshold logic function. An important property of the model is that it always converges to a stable state when operating in a serial mode [2,5]. This property is the basis of the potential applications of the model such as associative memory devices and combinatorial optimization [3,6]. One of the motivations for use of the model for solving hard combinatorial problems is the fact that it can be implemented by optical devices and thus operate at a higher speed than conventional electronics. The main theme in this work is to investigate the power of the model for solving NP-hard problems [4,8], and to understand the relation between speed of operation and the size of a neural network. In particular, it will be shown that for any NP-hard problem the existence of a polynomial size network that solves it implies that NP=co-NP. Also, for Traveling Salesman Problem (TSP), even a polynomial size network that gets an",Optimization & Theoretical ML,7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf,1987
"HOW THE CATFISH TRACKS ITS PREY: AN INTERACTIVE ""PIPELINED""","Ictalurid catfish use a highly developed gustatory system to localize, track and acquire food from their aquatic environment. The neural organization of the gustatory system illustrates well the importance of the four fundamental ingredients (representation, architecture, search and knowledge) of an ""intelligent"" system. In addition, the ""pipelined"" design of architecture illustrates how a goal-directed system effectively utilizes interactive feedback from its environment. Anatomical analysis of neural networks involved in target-tracking indicated that reticular neurons within the medullary region of the brainstem, mediate connections between the gustatory (sensory) inputs and the motor outputs of the spinal cord. Electrophysiological analysis suggested that these neurons integrate selective spatio-temporal patterns of sensory input transduced through a rapidly adapting-type peripheral filter (responding tonically only to a continuously increasing stimulus concentration). The connecti",Computer Vision,7f39f8317fbdb1988ef4c628eba02591-Paper.pdf,1987
PHASOR NEURAL NETVORKS,"A novel network type is introduced which uses unit-length 2-vectors for local variables. As an example of its applications, associative memory nets are defined and their performance analyzed. Real systems corresponding to such 'phasor' models can be e.g. (neuro)biological networks of limit-cycle oscillators or optical resonators that have a hologram in their feedback path. INTRODUCTION Most neural network models use either binary local variables or scalars combined with sigmoidal nonlinearities. Rather awkward coding schemes have to be invoked if one wants to maintain linear relations between the local signals being processed in e.g. associative memory networks, since the nonlinearities necessary for any nontrivial computation act directly on the range of values assumed by the local variables. In addition, there is the problem of representing signals that take values from a space with a different topology, e.g. that of the circle, sphere, torus, etc. Practical examples of such a signal",Computer Vision,8613985ec49eb8f757ae6439e879bb2a-Paper.pdf,1987
COMPUTING MOTION USING RESISTIVE NETWORKS,No abstract found,Computer Vision,8e296a067a37563370ded05f5a3bf3ec-Paper.pdf,1987
EXPERIMENTAL DEMONSTRATIONS OF,"We describe two expriments in optical neural computing. In the first a closed optical feedback loop is used to implement auto-associative image recall. In the second a perceptron-Iike learning algorithm is implemented with photorefractive holography. INTRODUCTION The hardware needs of many neural computing systems are well matched with the capabilities of optical systemsl ,2,3. The high interconnectivity required by neural computers can be simply implemented in optics because channels for optical signals may be superimposed in three dimensions with little or no cross coupling. Since these channels may be formed holographically, optical neural systems can be designed to create and maintain interconnections very simply. Thus the optical system designer can to a large extent avoid the analytical and topological problems of determining individual interconnections for a given neural system and constructing physical paths for these interconnections. An archetypical design for a single layer ",Computer Vision,8f14e45fceea167a5a36dedd4bea2543-Paper.pdf,1987
MURPHY: A Robot that Learns by Doing,"MURPHY consists of a camera looking at a robot arm, with a connectionist network architecture situated in between. By moving its arm through a small, representative sample of the 1 billion possible joint configurations, MURPHY learns the relationships, backwards and forwards, between the positions of its joints and the state of its visual field. MURPHY can use its internal model in the forward direction to ""envision"" sequences of actions for planning purposes, such as in grabbing a visually presented object, or in the reverse direction to ""imitate"", with its arm, autonomous activity in its visual field. Furthermore, by taking explicit advantage of continuity in the mappings between visual space and joint space, MURPHY is able to learn non-linear mappings with only a single layer of modifiable weights. Background Current Focus Of Learning Research Most connectionist learning algorithms may be grouped into three general catagories, commonly referred to as supenJised, unsupenJised, and re",Reinforcement Learning,92cc227532d17e56e07902b254dfad10-Paper.pdf,1987
SPONTANEOUS AND INFORMATION-TRIGGERED SEGMENTS OF SERIES,"The brain works in a state-dependent manner: processin9 strate9ies and access to stored information depends on the momentary functional state which is continuously re-adjusted. The state is manifest as spatial confi9uration of the brain electric field. Spontaneous and information-tri9gered brain electric activity is a series of momentary field maps. Adaptive segmentation of spontaneous series into spatially stable epochs (states) exhibited 210 msec mean segments, discontinuous changes. Different maps imply different active neural populations, hence expectedly different effects on information processing: Reaction time differred between map classes at stimulus arrival. Segments might be units of brain information processin9 (content/mode/step), possibly operationalizin9 consciousness time. Related units (e.9. tri9gered by stimuli durin9 fi9ure perception and voluntary attention) mi9ht specify brain sub mechanisms of information treatment. BRAIN FUNCTIONAL STATES AND THEIR CHANGES The mom",NLP,93db85ed909c13838ff95ccfa94cebd9-Paper.pdf,1987
SIMULATIONS SUGGEST,"A computer model of the hippocampal pyramidal cell (HPC) is described which integrates data from a variety of sources in order to develop a con sistent description for this cell type. The model presently includes descrip tions of eleven non-linear somatic currents of the HPC, and the electrotonic structure of the neuron is modelled with a soma/short-cable approximation. Model simulations qualitatively or quantitatively reproduce a wide range of somatic electrical behavior i~ HPCs, and demonstrate possible roles for the various currents in information processing. 1 The Computational Properties of Neurons There are several substrates for neuronal computation, including connec tivity, synapses, morphometries of dendritic trees, linear parameters of cell membrane, as well as non-linear, time-varying membrane conductances, also referred to as currents or channels. In the classical description of neuronal function, the contribution of membrane channels is constrained to that of generating th",Optimization & Theoretical ML,9778d5d219c5080b9a6a17bef029331c-Paper.pdf,1987
AN ARTIFICIAL NEURAL NETWORK FOR SPATIO,"An artificial neural network is developed to recognize spatio-temporal bipolar patterns associatively. The function of a formal neuron is generalized by replacing multiplication with convolution, weights with transfer functions, and thresholding with nonlinear transform following adaptation. The Hebbian learn ing rule and the delta learning rule are generalized accordingly, resulting in the learning of weights and delays. The neural network which was first developed for spatial patterns was thus generalized for spatio-temporal patterns. It was tested using a set of bipolar input patterns derived from speech signals, showing robust classification of 30 model phonemes.",Computer Vision,98f13708210194c475687be6106a3b84-Paper.pdf,1987
Teaching Artificial Neural Systems to Drive:,No abstract found,Reinforcement Learning,9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf,1987
Correlational Strength and Computational Algebra,"Intracellular recordings in spinal cord motoneurons and cerebral cortex neurons have provided new evidence on the correlational strength of monosynaptic connections, and the relation between the shapes of postsynaptic potentials and the associated increased firing probability. In these cells, excitatory postsynaptic potentials (EPSPs) produce cross correlogram peaks which resemble in large part the derivative of the EPSP. Additional synaptic noise broadens the peak, but the peak area -- i.e., the number of above-chance firings triggered per EPSP -- remains proportional to the EPSP amplitude. A typical EPSP of 100 triggers about .01 firings per ~v EPSP. The consequences of these data for information processing by polysynaptic connections is discussed. The effects of sequential polysynaptic links can be calculated by convolving the effects of the underlying monosynaptic connections. The net effect of parallel pathways is the sum of the individual contributions. INTRODUCTION Interactions ",Optimization & Theoretical ML,9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf,1987
DISCOVERING STRUCfURE FROM MOTION IN,"The ability to obtain three-dimensional structure from visual motion is important for survival of human and non-human primates. Using a parallel process ing model, the current work explores how the biological visual system might solve this problem and how the neurophysiologist might go about understanding the solution. INTRODUcnON 1 Psychophysical experiments have shown that monke and man are equally adept at obtaining three dimensional structure from motion . In the present work, much effort has been expended mimicking the visual system. This was done for one main reason: the model was designed to help direct physiological experiments in the primate. It was hoped that if an approach for understanding the model could be developed, the approach could then be directed at the primate's visual system. Early in this century, von Helmholtz2 described the problem of extracting three-dimensional structure from motion: Suppose, for instance, that a person is standing still in a thick woods, whe",Computer Vision,9f61408e3afb633e50cdf1b20de6f466-Paper.pdf,1987
STATIC AND DYNAMIC ERROR PROPAGATION,"Error propagation nets have been shown to be able to learn a variety of tasks in which a static input pattern is mapped outo a static output pattern. This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns, and three possible architectures are explored. As an example, dynamic nets are applied to tbe problem of speech coding, in which a time sequence of speech data are coded by one net and decoded by another. The use of dynamic nets gives a better signal to noise ratio than that achieved using static nets.",Optimization & Theoretical ML,a1d0c6e83f027327d8461063f4ac58a6-Paper.pdf,1987
SCHEMA I'OR MOTOR CONTROL,"This paper outlines a schema for movement control based on two stages of signal processing. The higher stage is a neural network model that treats the cerebellum as an array of adjustable motor pattern generators. This network uses sensory input to preset and to trigger elemental pattern generators and to evaluate their performance. The actual patterned outputs, however, are produced by intrin sic circuitry that includes recurrent loops and is thus capable of self-sustained activity. These patterned outputs are sent as motor commands to local feedback systems called motor servos. The latter control the forces and lengths of individual muscles. Overall control is thus achieved in two stages: (1) an adaptive cerebellar network generates an array of feedforward motor commands and (2) a set of local feedback systems translates these commands into actual movements. INTRODUCTION There is considerable evidence that the cerebellum is involved in the adaptive control of movement1, although the ",Computer Vision,a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf,1987
DISTRIBUTED NEURAL INFORMATION PROCESSING,"A new distributed neural information-processing model is proposed to explain the response characteristics of the vestibulo-ocular system and to reflect more accurately the latest anatomical and neurophysiological data on the vestibular afferent fibers and vestibular nuclei. In this model, head motion is sensed topographically by hair cells in the semicircular canals. Hair cell signals are then processed by multiple synapses in the primary afferent neurons which exhibit a continuum of varying dynamics. The model is an application of the concept of ""multilayered"" neural networks to the description of findings in the bullfrog vestibular nerve, and allows us to formulate mathematically the behavior of an assembly of neurons whose physiological characteristics vary according to their anatomical properties. INTRODUCTION Traditionally the physiological properties of individual vestibular afferent neurons have been modeled as a linear time-invariant system based on Steinhausents description of",Optimization & Theoretical ML,a5771bce93e200c36f7cd9dfd0e5deaa-Paper.pdf,1987
TIME-SEQUENTIAL SELF-ORGANIZATION OF HIERARCHICAL,"Self-organization of multi-layered networks can be realized by time-sequential organization of successive neural layers. Lateral inhibition operating in the surround of firing cells in each layer provides for unsupervised capture of excitation patterns presented by the previous layer. By presenting patterns of increasing complexity, in co-ordination with network self organization, higher levels of the hierarchy capture concepts implicit in the pattern set. INTRODUCTION A fundamental difficulty in self-organization of hierarchical, multi-layered, networks of simple neuron-like cells is the determination of the direction of adjustment of synaptic link weights between neural layers not directly connected to input or output patterns. Several different approaches have been used to address this problem. One is to provide teaching inputs to the cells in internal layers of the hierarchy. Another is use of back-propagated error signals1,2 from the uppermost neural layer, which is fixed to a des",Computer Vision,a5bfc9e07964f8dddeb95fc584cd965d-Paper.pdf,1987
A METHOD FOR THE DESIGN OF STABLE LATERAL INHIBITION,"In the analog VLSI implementation of neural systems, it is sometimes convenient to build lateral inhibition networks by using a locally connected on-chip resistive grid. A serious problem of unwanted spontaneous oscillation often arises with these circuits and renders them unusable in practice. This paper reports a design approach that guarantees such a system will be stable, even though the values of designed elements and parasitic elements in the resistive grid may be unknown. The method is based on a rigorous, somewhat novel mathematical analysis using Tellegen's theorem and the idea of Popov multipliers from control theory. It is thoroughly practical because the criteria are local in the sense that no overall analysis of the interconnected system is required, empirical in the sense that they involve only measurable frequency response data on the individual cells, and robust in the sense that unmodelled parasitic resistances and capacitances in the inter connection network cannot af",Optimization & Theoretical ML,a684eceee76fc522773286a895bc8436-Paper.pdf,1987
Constrained Differential Optimization,"Many optimization models of neural networks need constraints to restrict the space of outputs to a subspace which satisfies external criteria. Optimizations using energy methods yield ""forces"" which act upon the state of the neural network. The penalty method, in which quadratic energy constraints are added to an existing optimization energy, has become popular recently, but is not guaranteed to satisfy the constraint conditions when there are other forces on the neural model or when there are multiple constraints. In this paper, we present the basic differential multiplier method (BDMM), which satisfies constraints exactly; we create forces which gradually apply the constraints over time, using ""neurons"" that estimate Lagrange multipliers. The basic differential multiplier method is a differential version of the method of multipliers from Numerical Analysis. We prove that the differential equations locally converge to a constrained minimum. Examples of applications of the differential",Optimization & Theoretical ML,a87ff679a2f3e71d9181a67b7542122c-Paper.pdf,1987
ENCODING GEOMETRIC INVARIANCES IN,"We describe a method of constructing higher-order neural networks that respond invariantly under geometric transformations on the input space. By requiring each unit to satisfy a set of constraints on the interconnection weights, a particular structure is imposed on the network. A network built using such an architecture maintains its invariant performance independent of the values the weights assume, of the learning rules used, and of the form of the nonlinearities in the network. The invariance exhibited by a first order network is usually of a trivial sort, e.g., responding only to the average input in the case of translation invariance, whereas higher-order networks can perform useful functions and still exhibit the invariance. We derive the weight constraints for translation, rotation, scale, and several combinations of these transformations, and report results of simulation studies. INTRODUCTION A persistent difficulty for pattern recognition systems is the requirement that patte",Computer Vision,aab3238922bcc25a6f606eb525ffdc56-Paper.pdf,1987
A NOVEL NET THAT LEARNS,We propose a new scheme to construct neural networks to classify pat terns. The new scheme has several novel features :,Deep Learning,ad61ab143223efbc24c7d2583be69251-Paper.pdf,1987
MATHEMATICAL ANALYSIS OF LEARNING BEHAVIOR,"In this paper, we wish to analyze the convergence behavior of a number of neuronal plasticity models. Recent neurophysiological research suggests that the neuronal behavior is adaptive. In particular, memory stored within a neuron is associated with the synaptic weights which are varied or adjusted to achieve learning. A number of adaptive neuronal models have been proposed in the literature. Three specific models will be analyzed in this paper, specifically the Hebb model, the Sutton-Barto model, and the most recent trace model. In this paper we will examine the conditions for convergence, the position of conver gence and the rate at convergence, of these models as they applied to classical conditioning. Simulation results are also presented to verify the analysis. INTRODUCTION A number of static models to describe the behavior of a neuron have been in use in the past decades. More recently, research in neurophysiology suggests that a static view may be insufficient. Rather, the param",Optimization & Theoretical ML,b53b3a3d6ab90ce0268229151c9bde11-Paper.pdf,1987
NEW HARDWARE FOR MASSIVE NEURAL NETWORKS,"Transient phenomena associated with forward biased silicon p + - n - n + struc tures at 4.2K show remarkable similarities with biological neurons. The devices play a role similar to the two-terminal switching elements in Hodgkin-Huxley equivalent circuit diagrams. The devices provide simpler and more realistic neuron emulation than transistors or op-amps. They have such low power and current requirements that they could be used in massive neural networks. Some observed properties of simple circuits containing the devices include action potentials, refractory periods, threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic weights, temporal integration, memory, network connectivity modification based on experience, pacemaker activity, firing thresholds, coupling to sensors with graded sig nal outputs and the dependence of firing rate on input current. Transfer functions for simple artificial neurons with spiketrain inputs and spiketrain outputs have been mea",Computer Vision,b6d767d2f8ed5d21a44b0e5886680cb9-Paper.pdf,1987
AN ADAPTIVE AND HETERODYNE FILTERING PROCEDURE,"Recent experimental work on the stimulus velocity dependent time resolving power of the neural units, situated in the highest order optic ganglion of the blowfly, revealed the at first sight amazing phenomenon that at this high level of the fly visual system, the time constants of these units which are involved in the processing of neural activity evoked by moving objects, are -roughly spoken inverse proportional to the velocity of those objects over an extremely wide range. In this paper we will discuss the implementation of a two dimensional heterodyne adaptive filter construction into a computer simulation model. The features of this simulation model include the ability to account for the experimentally observed stimulus-tuned adaptive temporal behaviour of time constants in the fly visual system. The simulation results obtained, clearly show that the application of such an adaptive processing procedure delivers an improved imaging technique of moving patterns in the high velocity r",Computer Vision,c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf,1987
PHASE TRANSITIONS IN NEURAL NETWORKS,"Various simulat.ions of cort.ical subnetworks have evidenced something like phase transitions with respect to key parameters. We demonstrate that. such transi t.ions must. indeed exist. in analogous infinite array models. For related finite array models classical phase transi t.ions (which describe steady-state behavior) may not. exist., but. there can be distinct. quali tative changes in (""metastable"") transient behavior as key system parameters pass through crit.ical values. INTRODUCTION Suppose that one st.imulates a neural network - actual or simulated - and in some manner records the subsequent firing activity of cells. Suppose further that. one repeats the experiment. for different. values of some parameter (p) of the system: and that one finds a ""cri t.ical value"" (p) of the parameter, such that. c > (say) for values p p the act.ivity tends to be much higher than c it. is for values p < p. Then, by analogy with statist.ical c mechanics (where, e.g., p may be temperature, with cr",Optimization & Theoretical ML,c16a5320fa475530d9583c34fd356ef5-Paper.pdf,1987
USING NEURAL NETWORKS TO IMPROVE,#NAME?,NLP,c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf,1987
SELF-ORGANIZATION OF ASSOCIATIVE DATABASE,"An efficient method of self-organizing associative databases is proposed together with applications to robot eyesight systems. The proposed databases can associate any input with some output. In the first half part of discussion, an algorithm of self-organization is proposed. From an aspect of hardware, it produces a new style of neural network. In the latter half part, an applicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated. INTRODUCTION Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some estimate j : X -+ Y of f to make small, the estimation error in some measure. Usually we say that: the faster the decrease of estimation error with increase of the num ber of samples, the better the learning machine. However, such expr",Computer Vision,c4ca4238a0b923820dcc509a6f75849b-Paper.pdf,1987
TEMPORAL PATTERNS OF ACTIVITY IN,"Patterns of activity over real neural structures are known to exhibit time dependent behavior. It would seem that the brain may be capable of utilizing temporal behavior of activity in neural networks as a way of performing functions which cannot otherwise be easily implemented. These might include the origination of sequential behavior and the recognition of time-dependent stimuli. A model is presented here which uses neuronal populations with recurrent feedback connec tions in an attempt to observe and describe the resulting time-dependent behavior. Shortcomings and problems inherent to this model are discussed. Current models by other researchers are reviewed and their similarities and differences discussed. METHODS / PRELIMINARY RESULTS In previous papers,[2,3] computer models were presented that simulate a net con sisting of two spatially organized populations of realistic neurons. The populations are richly interconnected and are shown to exhibit internally sustained activity. It",Neuroscience,c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf,1987
"Network Generality, Training Required,","We show how to estimate (1) the number of functions that can be implemented by a particular network architecture, (2) how much analog precision is needed in the con nections in the network, and (3) the number of training examples the network must see before it can be expected to form reliable generalizations. Generality versus Training Data Required Consider the following objectives: First, the network should be very powerful and ver satile, i.e., it should implement any function (truth table) you like, and secondly, it should learn easily, forming meaningful generalizations from a small number of training examples. Well, it is information-theoretically impossible to create such a network. We will present here a simplified argument; a more complete and sophisticated version can be found in Denker et al. (1987). It is customary to regard learning as a dynamical process: adjusting the weights (etc.) in a single network. In order to derive the results of this paper, however, we take a dif",Optimization & Theoretical ML,c74d97b01eae257e44aa9d5bade97baf-Paper.pdf,1987
HIGH ORDER NEURAL NETWORKS FOR EFFICIENT,"We propose learning rules for recurrent neural networks with high-order interactions between some or all neurons. The designed networks exhibit the desired associative memory function: perfect storage and retrieval of pieces of information and/or sequences of information of any complexity. INTRODUCTION In the field of information processing, an important class of potential applications of neural networks arises from their ability to perform as associative memories. Since the publication of J. Hopfield's seminal paper 1, investigations of the storage and retrieval properties of recurrent networks have led to a deep understanding of their properties. The basic limitations of these networks are the following: - their storage capacity is of the order of the number of neurons; - they are unable to handle structured problems; - they are unable to classify non-linearly separable data. © American Institute of Physics 1988 234 In order to circumvent these limitations, one has to introduce addit",Deep Learning,c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf,1987
THE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY IS EXPONENTIAL,"The capacity of an associative memory is defined as the maximum number of vords that can be stored and retrieved reliably by an address vithin a given sphere of attraction. It is shown by sphere packing arguments that as the address length increases. the capacity of any associati ve memory is limited to an exponential grovth rate of 1 - h 2( 0). vhere h2(0) is the binary entropy function in bits. and 0 is the radius of the sphere of attraction. This exponential grovth in capacity can actually be achieved by the Kanerva associative memory. if its parameters are optimally set. Formulas for these op.timal values are provided. The exponential grovth in capacity for the Kanerva associative memory contrasts sharply vith the sub-linear grovth in capacity for the Hopfield associative memory. ASSOCIATIVE MEMORY AND ITS CAPACITY Our model of an associative memory is the folloving. Let ()(,Y) be an (address. datum) pair. vhere )( is a vector of n ±ls and Y is a vector of m ±ls. and let ()(l),y(I)",Optimization & Theoretical ML,c81e728d9d4c2f636f067f89cc14862c-Paper.pdf,1987
THE SIGMOID NONLINEARITY IN PREPYRIFORM CORTEX,No abstract found,Optimization & Theoretical ML,c9f0f895fb98ab9159f51fd0297e236d-Paper.pdf,1987
PROBABILISTIC CHARACTERIZATION OF,"Information retrieval in a neural network is viewed as a procedure in which the network computes a ""most probable"" or MAP estimate of the unk nown information. This viewpoint allows the class of probability distributions, P, the neural network can acquire to be explicitly specified. Learning algorithms for the neural network which search for the ""most probable"" member of P can then be designed. Statistical tests which decide if the ""true"" or environmental probability distribution is in P can also be developed. Example applications of the theory to the highly nonlinear back-propagation learning algorithm, and the networks of Hopfield and Anderson are discussed. INTRODUCTION A connectionist system is a network of simple neuron-like computing elements which can store and retrieve information, and most importantly make generalizations. Using terminology suggested by Rumelhart & McClelland 1, the computing elements of a connectionist system are called units, and each unit is associated with",Optimization & Theoretical ML,d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,1987
LEARNING IN NETWORKS OF,"This paper presents a model of nondeterministic adaptive automata that are constructed from simpler nondeterministic adaptive information processing elements. The first half of the paper describes the model. The second half discusses some of its significant adaptive properties using computer simulation examples. Chief among these properties is that network aggregates of the model elements can adapt appropriately when a single reinforcement channel provides the same positive or negative reinforcement signal to all adaptive elements of the network at the same time. This holds for multiple-input, multiple-output, multiple-layered, combinational and sequential networks. It also holds when some network elements are ""hidden"" in that their outputs are not directly seen by the external environment. INTRODUCTION There are two primary motivations for studying models of adaptive automata constructed from simple parts. First, they let us learn things about real biological systems whose properties ",Reinforcement Learning,d1fe173d08e959397adf34b1d77e88d7-Paper.pdf,1987
HIGH DENSITY ASSOCIATIVE MEMORIES!,"A class of high dens ity assoc iat ive memories is constructed, starting from a description of desired properties those should exhib it. These propert ies include high capac ity, controllable bas ins of attraction and fast speed of convergence. Fortunately enough, the resulting memory is implementable by an artificial Neural Net. I NfRODUCTION Most of the work on assoc iat ive memories has been structure oriented, i.e.. given a Neural architecture, efforts were directed towards the analysis of the resulting network. Issues like capacity, basins of attractions, etc. were the main objects to be analyzed cf., e.g. [1], [2], [3], [4] and references there, among others. In this paper, we take a different approach, we start by explicitly stating the desired properties of the network, in terms of capacity, etc. Those requirements are given in terms of axioms (c.f. below). Then, we bring a synthesis method which enables one to design an architecture which will yield the desired performance. Su",Deep Learning,d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf,1987
A MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX,"A single cell theory for the development of selectivity and ocular dominance in visual cortex has been presented previously by Bienenstock, Cooper and Munrol. This has been extended to a network applicable to layer IV of visual cortex2. In this paper we present a mean field approximation that captures in a fairly transparent manner the qualitative, and many of the quantitative, results of the network theory. Finally, we consider the application of this theory to artificial neural networks and show that a significant reduction in architectural complexity is possible. A SINGLE LAYER NETWORK AND THE MEAN FIELD APPROXIMATION We consider a single layer network of ideal neurons which receive signals from outside of the layer and from cells within the layer (Figure 1). The activity of the ith cell in the network is c' - m' d + ~ T .. c' (1) 1 - 1 """"' ~J J' J d is a vector of afferent signals to the network. Each cell receives input from n fibers outside of the cortical network through the mat",Computer Vision,d3d9446802a44259755d38e6d163e820-Paper.pdf,1987
NEURAL NETWORKS FOR TEMPLATE MATCHING:,"Much experimental study of real neural networks relies on the proper classification of extracellulary sampled neural signals (i.e. action potentials) recorded from the brains of ex perimental animals. In most neurophysiology laboratories this classification task is simplified by limiting investigations to single, electrically well-isolated neurons recorded one at a time. However, for those interested in sampling the activities of many single neurons simultaneously, waveform classification becomes a serious concern. In this paper we describe and constrast three approaches to this problem each designed not only to recognize isolated neural events, but also to separately classify temporally overlapping events in real time. First we present two formulations of waveform classification using a neural network template matching approach. These two formulations are then compared to a simple template matching implementation. Analysis with real neural signals reveals that simple template matching",Deep Learning,d645920e395fedad7bbbed0eca3fe2e0-Paper.pdf,1987
CAPACITY FOR PATTERNS AND SEQUENCES IN KANERVA'S SDM,"The information capacity of Kanerva's Sparse, Distributed Memory (SDM) and Hopfield-type neural networks is investigated. Under the approximations used here, it is shown that the to tal information stored in these systems is proportional to the number connections in the net work. The proportionality constant is the same for the SDM and HopJreld-type models in dependent of the particular model, or the order of the model. The approximations are checked numerically. This same analysis can be used to show that the SDM can store se quences of spatiotemporal patterns, and the addition of time-delayed connections allows the retrieval of context dependent temporal patterns. A minor modification of the SDM can be used to store correlated patterns. INTRODUCTION Many different models of memory and thought have been proposed by scientists over the years. In (1943) McCulloch and Pitts prorosed a simple model neuron with two states of activity (on and off) and a large number of inputs. Hebb (1949) c",Optimization & Theoretical ML,d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf,1987
The Connectivity Analysis of Simple Association,"The efficient realization, using current silicon technology, of Very Large Connection Networks (VLCN) with more than a billion connections requires that these networks exhibit a high degree of communication locality. Real neural networks exhibit significant locality, yet most connectionist/neural network models have little. In this paper, the connectivity requirements of a simple associative network are analyzed using communication theory. Several techniques based on communication theory are presented that improve the robust ness of the network in the face of sparse, local interconnect structures. Also discussed are some potential problems when information is distributed too widely. INTRODUCTION Connectionist/neural network researchers are learning to program networks that exhi bit a broad range of cognitive behavior. Unfortunately, existing computer systems are lim ited in their ability to emulate such networks efficiently. The cost of emulating a network, whether with special purpose",Optimization & Theoretical ML,d82c8d1619ad8176d665453cfb2e55f0-Paper.pdf,1987
Performance Measures for Associative Memories,"Recently, many modifications to the McCulloch/Pitts model have been proposed where both learning and forgetting occur. Given that the network never saturates (ceases to function effectively due to an overload of information), the learning updates can con tinue indefinitely. For these networks, we need to introduce performance measmes in addi tion to the information capacity to evaluate the different networks. We mathematically define quantities such as the plasticity of a network, the efficacy of an information vector, and the probability of network saturation. From these quantities we analytically compare different networks.",Optimization & Theoretical ML,d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf,1987
Centric Models of the Orientation Map in Primary Visual Cortex,"In the visual cortex of the monkey the horizontal organization of the preferred orientations of orientation-selective cells follows two opposing rules: 1) neighbors tend to have similar orientation preferences, and 2) many different orientations are observed in a local region. Several orientation models which satisfy these constraints are found to differ in the spacing and the topological index of their singularities. Using the rate of orientation change as a measure, the models are compared to published experimental results. Introduction It has been known for some years that there exist orientation-sensitive neurons in the visual cortex of cats and mOnkeysl,2. These cells react to highly specific patterns of light occurring in narrowly circumscribed regiOns of the visual field, i.e., the cell's receptive field. The best patterns for such cells are typically not diffuse levels of illumination, but elongated bars or edges oriented at specific angles. An individual cell responds maximall",Computer Vision,e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf,1987
A Computer Simulation of Olfactory Cortex With Functional Implications for,"Based on anatomical and physiological data, we have developed a computer simulation of piri form (olfactory) cortex which is capable of reproducing spatial and temporal patterns of actual cortical activity under a variety of conditions. Using a simple Hebb-type learning rule in conjunc tion with the cortical dynamics which emerge from the anatomical and physiological organiza tion of the model, the simulations are capable of establishing cortical representations for differ ent input patterns. The basis of these representations lies in the interaction of sparsely distribut ed, highly divergent/convergent interconnections between modeled neurons. We have shown that different representations can be stored with minimal interference. and that following learning these representations are resistant to input degradation, allowing reconstruction of a representa tion following only a partial presentation of an original training stimulus. Further, we have demonstrated that the degree of overlap o",Computer Vision,e369853df766fa44e1ed0ff613f563bd-Paper.pdf,1987
TOWARDS AN ORGANIZING PRINCIPLE FOR,"An information-theoretic optimization principle is proposed for the development of each processing stage of a multilayered perceptual network. This principle of ""maximum information preservation"" states that the signal transformation that is to be realized at each stage is one that maximizes the information that the output signal values (from that stage) convey about the input signals values (to that stage), subject to certain constraints and in the presence of processing noise. The quantity being maximized is a Shannon information rate. I provide motivation for this principle and -- for some simple model cases -- derive some of its consequences, discuss an algorithmic implementation, and show how the principle may lead to biologically relevant neural architectural features such as topographic maps, map distortions, orientation selectivity, and extraction of spatial and temporal signal correlations. A possible connection between this information-theoretic principle and a principle of m",Optimization & Theoretical ML,e4da3b7fbbce2345d7772b0674a318d5-Paper.pdf,1987
A Trellis-Structured Neural Network*,"We have developed a neural network which consists of cooperatively inter connected Grossberg on-center off-surround subnets and which can be used to optimize a function related to the log likelihood function for decoding convolu tional codes or more general FIR signal deconvolution problems. Connections in the network are confined to neighboring subnets, and it is representative of the types of networks which lend themselves to VLSI implementation. Analytical and experimental results for convergence and stability of the network have been found. The structure of the network can be used for distributed representation of data items while allowing for fault tolerance and replacement of faulty units. 1 Introd uction In order to study the behavior of locally interconnected networks, we have focused on a class of ""trellis-structured"" networks which are similar in structure to multilayer networks [5] but use symmetric connections and allow every neuron to be an output. We are studying such· lo",Optimization & Theoretical ML,ea5d2f1c4608232e07d3aa3d998e5135-Paper.pdf,1987
Stochastic Learning Networks and their Electronic Implementation,No abstract found,Optimization & Theoretical ML,f033ab37c30201f73f142449d037028d-Paper.pdf,1987
CONNECTING TO THE PAST,"Recently there has been renewed interest in neural-like processing systems, evidenced for ex ample in the two volumes Parallel Distributed Processing edited by Rumelhart and McClelland, and discussed as parallel distributed systems, connectionist models, neural nets, value passing systems and multiple context systems. Dissatisfaction with symbolic manipulation paradigms for artificial intelligence seems partly responsible for this attention, encouraged by the promise of massively parallel systems implemented in hardware. This paper relates simple neural-like systems based on multiple context to some other well-known formalisms-namely production systems, k-Iength sequence prediction, finite-state machines and Turing machines-and presents earlier sequence prediction results in a new light. 1 INTRODUCTION The revival of neural net research has been very strong, exemplified recently by Rumelhart and McClelland!, new journals and a number of meetingsG• The nets are also described as paralle",Reinforcement Learning,f457c545a9ded88f18ecee47145a72c0-Paper.pdf,1987
"Richard Granger, Jose Ambros-Ingerson, Howard Henry, Gary Lynch",No abstract found,The category of the research paper cannot be determined from the provided context.,f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf,1987
Supervised Learning of Probability Distributions,"We propose that the back propagation algorithm for super vised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the val ues of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'. In the past thirty years many researchers have studied the question of supervised learning in 'neural'-like networks. Recently a learning algorithm called 'back propagationH - 4 or the 'general ized delta-rule' has been applied to numerous problems including the mapping of text to phonemes5, the diagnosis of illnesses6 and the classification of sonar targets 7• In these applications, it would often be natural to consider imperfect, or probabilistic informa tion. We believe that by considering supervised learning from this slightly larger perspective, one can not only place back propaga- t Permanent address: Institute for Theoretical Physics, ",Optimization & Theoretical ML,eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf,1987
A DYNAMICAL APPROACH TO TEMPORAL PATTERN,"Recognizing patterns with temporal context is important for such tasks as speech recognition, motion detection and signature verification. We propose an architecture in which time serves as its own representation, and temporal context is encoded in the state of the nodes. We contrast this with the approach of replicating portions of the architecture to represent time. As one example of these ideas, we demonstrate an architecture with capacitive inputs serving as temporal feature detectors in an otherwise standard back propagation model. Experiments involving motion detection and word discrimination serve to illustrate novel features of the system. Finally, we discuss possible extensions of the architecture. INTRODUCTION Recent interest in connectionist, or ""neural"" networks has emphasized their ability to store, retrieve and process patterns1,2. For most applications, the patterns to be processed are static in the sense that they lack temporal context. Another important class consists ",Computer Vision,fbd7939d674997cdb4692d34de8633c4-Paper.pdf,1987
Minkowski-r Back-Propaaation: Learnine in Connectionist,"Many connectionist learning models are implemented using a gradient descent in a least squares error function of the output and teacher signal. The present model Fneralizes. in particular. back-propagation [1] by using Minkowski-r power metrics. For small r's a ""city-block"" error metric is approximated and for large r's the ""maximum"" or ""supremum"" metric is approached. while for r=2 the standard back propagation model results. An implementation of Minkowski-r back-propagation is described. and several experiments are done which show that different values of r may be desirable for various purposes. Different r values may be appropriate for the reduction of the effects of outliers (noise). modeling the input space with more compact clusters. or modeling the statistics of a particular domain more naturally or in a way that may be more perceptually or psychologically meaningful (e.g. speech or vision).",Deep Learning,fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf,1987
ANALYSIS AND COMPARISON OF DIFFERENT LEARNING,"We investigate the behavior of different learning algorithms for networks of neuron-like units. As test cases we use simple pat tern association problems, such as the XOR-problem and symmetry de tection problems. The algorithms considered are either versions of the Boltzmann machine learning rule or based on the backpropagation of errors. We also propose and analyze a generalized delta rule for linear threshold units. We find that the performance of a given learning algorithm depends strongly on the type of units used. In particular, we observe that networks with ±1 units quite generally exhibit a significantly better learning behavior than the correspon ding 0,1 versions. We also demonstrate that an adaption of the weight-structure to the symmetries of the problem can lead to a drastic increase in learning speed. INTRODUCTION In the past few years, a number of learning procedures for neural network models with hidden units have been proposed1,2. They can all be considered as strategie",Optimization & Theoretical ML,fe9fc289c3ff0af142b6d3bead98a923-Paper.pdf,1987
Unknown Title,No abstract found,Not enough information available,~$3f65e080a295f8076b1c5722a46aa2-Paper.pdf,1987
Unknown Title,No abstract found,The category cannot be determined from the provided information.,~$732ced3463d06de0ca9a15b6153677-Paper.pdf,1987
Unknown Title,No abstract found,Category name cannot be determined from the provided context.,~$afdbd66e7929b125f8597834fa83a4-Paper.pdf,1987
NEURAL CONTROL OF SENSORY ACQUISITION:,"We present a new hypothesis that the cerebellum plays a key role in ac tively controlling the acquisition of sensory infonnation by the nervous system. In this paper we explore this idea by examining the function of a simple cerebellar-related behavior, the vestibula-ocular reflex or VOR, in which eye movements are generated to minimize image slip on the retina during rapid head movements. Considering this system from the point of view of statistical estimation theory, our results sug gest that the transfer function of the VOR, often regarded as a static or slowly modifiable feature of the system, should actually be continu ously and rapidly changed during head movements. We further suggest that these changes are under the direct control of the cerebellar cortex and propose experiments to test this hypothesis.",NLP,006f52e9102a8d3be2fe5614f42ba989-Paper.pdf,1988
MODELING THE OLFACTORY BULB,"The olfactory bulb of mammals aids in the discrimination of odors. A mathematical model based on the bulbar anatomy and electrophysiology is described. Simulations produce a 35-60 Hz modulated activity coherent across the bulb, mimicing the observed field potentials. The decision states (for the odor information) here can be thought of as stable cycles, rather than point stable states typical of simpler neuro-computing models. Analysis and simulations show that a group of coupled non-linear oscillators are responsible for the oscillatory activities determined by the odor in put, and that the bulb, with appropriate inputs from higher centers, can enhance or suppress the sensitivity to partiCUlar odors. The model provides a framework in which to understand the transform between odor input and the bulbar output to olfactory cortex.",Computer Vision,013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf,1988
EFFICIENT PARALLEL LEARNING,"Parallelizable optimization techniques are applied to the problem of learning in feedforward neural networks. In addition to having supe rior convergence properties, optimization techniques such as the Polak Ribiere method are also significantly more efficient than the Back propagation algorithm. These results are based on experiments per formed on small boolean learning problems and the noisy real-valued learning problem of hand-written character recognition. 1 INTRODUCTION The problem of learning in feedforward neural networks has received a great deal of attention recently because of the ability of these networks to represent seemingly complex mappings in an efficient parallel architecture. This learning problem can be characterized as an optimization problem, but it is unique in several respects. Function evaluation is very expensive. However, because the underlying network is parallel in nature, this evaluation is easily parallelizable. In this paper, we describe the network learn",Optimization & Theoretical ML,02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf,1988
THE BOLTZMANN PERCEPTRON NETWORK:,"The concept of the stochastic Boltzmann machine (BM) is auractive for decision making and pattern classification purposes since the probability of attaining the network states is a function of the network energy. Hence, the probability of attaining particular energy minima may be associated with the probabilities of making certain decisions (or classifications). However, because of its stochastic nature, the complexity of the BM is fairly high and therefore such networks are not very likely to be used in practice. In this paper we suggest a way to alleviate this drawback by converting the sto chastic BM into a deterministic network which we call the Boltzmann Per ceptron Network (BPN). The BPN is functionally equivalent to the BM but has a feed-forward structure and low complexity. No annealing is required. The conditions under which such a convmion is feasible are given. A learning algorithm for the BPN based on the conjugate gradient method is also provided which is somewhat akin to ",Optimization & Theoretical ML,045117b0e0a11a242b9765e79cbf113f-Paper.pdf,1988
NEURAL NETWORKS THAT LEARN TO,A neural network is applied to the problem of recognizing Kanji characters. Using a b a c k propagation network learning algorithm. a three layered. feed-forward network is trained to recognize similar handwritten Kanji characters. In addition. two new methods are utilized to make training effective. The recognition accuracy was higher than that of conventional methods. An analysis of connection weights showed that trained networks can discern the hierarchical structure of Kanji characters. This strategy of trained networks makes high recognition accuracy possible. Our results suggest that neural networks are very effective for Kanji character recognition. 1 INTRODUCTION Neural networks are applied to recognition tasks in many fields. with good results. In the field of letter recognition. net work s have been made which recognize hand-written digits [Burr 1986] and complex printed Chinese characters [Ho 1988]. The performance of these networks has been better than that of conventional ,Deep Learning,06409663226af2f3114485aa4e0a23b4-Paper.pdf,1988
COMPUTER MODELING OF ASSOCIATIVE LEARNING,No abstract found,Reinforcement Learning,069059b7ef840f0c74a814ec9237b6ec-Paper.pdf,1988
LINKS BETWEEN MARKOV MODELS AND,"Hidden Markov models are widely used for automatic speech recog nition. They inherently incorporate the sequential character of the speech signal and are statistically trained. However, the a-priori choice of the model topology limits their flexibility. Another draw back of these models is their weak discriminating power. Multilayer perceptrons are now promising tools in the connectionist approach for classification problems and have already been successfully tested on speech recognition problems. However, the sequential nature of the speech signal remains difficult to handle in that kind of ma chine. In this paper, a discriminant hidden Markov model is de fined and it is shown how a particular multilayer perceptron with contextual and extra feedback input units can be considered as a general form of such Markov models. INTRODUCTION Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al., 1985] are widely used for automatic isolated and connected speech recognition. Their main advan",NLP,0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf,1988
SKELETONIZATION:,"This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a cer tain performance criterion, compute a measure of relevance that identi fies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization tech nique can be used to simplify networks by eliminating units that con vey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal ""rules."" INTRODUCTION One thing that connectionist networks have in common with brains is that if you open them up and peer inside, all you can see is a big pile of goo. Internal organization",Optimization & Theoretical ML,07e1cd7dca89a1678042477183b7ac3f-Paper.pdf,1988
A PASSIVE SHARED ELEMENT ANALOG,"We present a simplified model of the micromechanics of the human cochlea, realized with electrical elements. Simulation of the model shows that it retains four signal processing features whose importance we argue on the basis of engineering logic and evolutionary evidence. Furthermore, just as the cochlea does, the model achieves massively parallel signal processing in a structurally economic way, by means of shared elements. By extracting what we believe are the five essential features of the cochlea, we hope to design a useful front-end filter to process acoustic images and to obtain a better understanding of the auditory system. INTRODUCTION Results of psychoacoustical and physiological experiments in humans indicate that the auditory system creates acoustic images via massively parallel neural computations. These computations enable the brain to perform voice detection, sound localization, and many other complex taskS. For example, by recording a random signal with a wide range of ",Computer Vision,0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf,1988
SPEECH PRODUCTION USING A NEURAL,"We propose a new neural network model and its learning algorithm. The proposed neural network consists of four layers -input, hidden, output and final output layers. The hidden and output layers are multiple. Using the proposed SICL(Spread Pattern Information and Cooperative Learning) algorithm, it is possible to learn analog data accurately and to obtain smooth outputs. Using this neural network, we have developed a speech production system consisting of a phonemic symbol production subsystem and a speech parameter production subsystem. We have succeeded in producing natural speech waves with high accuracy. INTRODUCTION Our purpose is to produce natural speech waves. In general, speech synthesis by rule is used for producing speech waves. However, there are some difficulties in speech synthesis by rule. First, the rules are very complicated. Second, extracting a generalized rule is difficult. Therefore, it is hard to synthesize a natural speech wave by using rules. We use a neural net",NLP,0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf,1988
ASSOCIATIVE LEARNING,"ALVIS is a reinforcement-based connectionist architecture that learns associative maps in continuous multidimensional environ ments. The discovered locations of positive and negative rein forcements are recorded in ""do be"" and ""don't be"" subnetworks, respectively. The outputs of the subnetworks relevant to the cur rent goal are combined and compared with the current location to produce an error vector. This vector is backpropagated through a motor-perceptual mapping network. to produce an action vec tor that leads the system towards do-be locations and away from don 't-be locations. AL VIS is demonstrated with a simulated robot posed a target-seeking task. INTRODUCTION The ""backpropagation algorithm"" or generalized delta rule (Rumelhart, Hinton, & Williams, 1986) is sometimes criticized on the grounds that it is a ""supervised"" learning algorithm, which requires a ""teacher"" to provide correct outputs, and apparently leaves open the question of how the teacher learned the right answers. ",Reinforcement Learning,1385974ed5904a438616ff7bdb3f7439-Paper.pdf,1988
Performance of a Stochastic Learning Microchip,No abstract found,Optimization & Theoretical ML,140f6969d5213fd0ece03148e62e461e-Paper.pdf,1988
SONG LEARNING IN BIRDS,No abstract found,Reinforcement Learning,149e9677a5989fd342ae44213df68868-Paper.pdf,1988
MODELING SMALL OSCILLATING,"We have used analog VLSI technology to model a class of small os cillating biological neural circuits known as central pattern gener ators (CPG). These circuits generate rhythmic patterns of activity which drive locomotor behaviour in the animal. We have designed, fabricated, and tested a model neuron circuit which relies on many of the same mechanisms as a biological central pattern generator neuron, such as delays and internal feedback. We show that this neuron can be used to build several small circuits based on known biological CPG circuits, and that these circuits produce patterns of output which are very similar to the observed biological patterns. To date, researchers in applied neural networks have tended to focus on mam malian systems as the primary source of potentially useful biological information. However, invertebrate systems may represent a source of ideas in many ways more appropriate, given current levels of engineering sophistication in building neural-like systems, a",Optimization & Theoretical ML,1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf,1988
COMPARING BIASES FOR MINIMAL NETWORK,"Rumelhart (1987). has proposed a method for choosing minimal or ""simple"" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units. (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart·s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general. the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima. INTRODUCTION Many supervised connectionist models use gradient descent in error to solve various kinds of tasks (Rumelhart. Hinton & Williams. 1986). However. such gradient descent methods tend to be """,Optimization & Theoretical ML,1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf,1988
WHAT SIZE NET GIVES VALID,"We address the question of when a network can be expected to generalize from m random training examples chosen from some ar bitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume o < £ $ 1/8. We show that if m > O( ~log~) random exam ples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction t 1 - of the examples are correctly classified, then one has confi dence approaching certainty that the network will correctly classify a fraction 1 - £ of future test examples drawn from the same dis tribution. Conversely, for fully-connected feedforward nets with '!') one hidden layer, any learning algorithm using fewer than O( random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to f",Optimization & Theoretical ML,1d7f7abc18fcb43975065399b0d1e48e-Paper.pdf,1988
A LOW-POWER CMOS CIRCUIT WHICH EMULATES,"excitation level as a binary code or a continuous voltage at the output of a summing amplifier. While such models have been shown to perform well for many applica tions, and form an integral part of much current work, they only partially emulate the manner in which natural neural networks operate. They ignore, for example, differences in relative arrival times of neighboring action potentials -- an important characteristic known to exist in natural auditory and visual networks {Sejnowski, 1986}. They are also less adaptable to fme-grained, neuron-centered learning, like the post-tetanic facilitation observed in natural neurons. We are investigating the implementation and application of neuron circuits which better approximate natural neuron function. BACKGROUND The major temporal artifacts associated with natural neuron function include the spacio-temporal integration of synaptic activity, the generation of an action potential (AP), and the post-AP hyperpolarization (refractory) period",Computer Vision,1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf,1988
LINEAR LEARNING: LANDSCAPES AND ALGORITHMS,No abstract found,Optimization & Theoretical ML,202cb962ac59075b964b07152d234b70-Paper.pdf,1988
APPLICATIONS OF,"This paper is concerced with the use of error back-propagation in phonetic classification. Our objective is to investigate the ba sic characteristics of back-propagation, and study how the frame work of multi-layer perceptrons can be exploited in phonetic recog nition. We explore issues such as integration of heterogeneous sources of information, conditioll~ that can affect performance of phonetic classification, internal representations, comparisons with traditional pattern classification techniques, comparisons of differ ent error metrics, and initialization of the network. Our investiga tion is performed within a set of experiments that attempts to rec ognize the 16 vowels in American English independent of speaker. Our results are comparable to human performance. Early approaches in phonetic recognition fall into two major extremes: heuristic and algorithmic. Both approaches have their own merits and shortcomings. The heuristic approach has the intuitive appeal that it focuses on t",NLP,2723d092b63885e0d7c260cc007e8b9d-Paper.pdf,1988
AN ANALOG SELF-ORGANIZING,"A design for a fully analog version of a self-organizing feature map neural network has been completed. Several parts of this design are in fabrication. The feature map algorithm was modified to accommodate circuit solutions to the various computations required. Performance effects were measured by simulating the design as part of a frontend for a speech recognition system. Circuits are included to implement both activation computations and weight adaption 'or learning. External access to the analog weight values is provided to facilitate weight initialization, testing and static storage. This fully analog implementation requires an order of magnitude less area than a comparable digital/analog hybrid version developed earlier. INTRODUCTION This paper describes an analog version of a self-organizing feature map circuit. The design implements Kohonen's self-organizing feature map algorithm [Kohonen, 1988] with some modifications imposed by practical circuit limitations. The feature map a",Deep Learning,2a79ea27c279e471f4d180b08d62b00a-Paper.pdf,1988
A BIFURCATION THEORY APPROACH TO THE,"A new learning algorithm for the storage of static and periodic attractors in biologically inspired recurrent analog neural networks is introduced. For a network of n nodes, n static or n/2 periodic attractors may be stored. The algorithm allows programming of the network vector field indepen dent of the patterns to be stored. Stability of patterns, basin geometry, and rates of convergence may be controlled. For orthonormal patterns, the operation reduces to a kind of periodic l~grning outer product rule that allows local, additive, commutative, incremental learning. Standing or traveling wave cycles may be stored to mimic the kind of oscillating spatial patterns that appear in the neural activity of the olfactory bulb and prepyriform cortex during inspiration and suffice, in the bulb, to predict the pattern recognition behavior of rabbits in classical conditioning ex periments. These attractors arise, during simulat ed inspiration, through a multiple Hopf bifurca tion, which can act a",Optimization & Theoretical ML,2b24d495052a8ce66358eb576b8912c8-Paper.pdf,1988
HETEROGENEOUS NEURAL NETWORKS FOR,"Research in artificial neural networks has genera1ly emphasized homogeneous architectures. In contrast, the nervous systems of natural animals exhibit great heterogeneity in both their elements and patterns of interconnection. This heterogeneity is crucial to the flexible generation of behavior which is essential for survival in a complex, dynamic environment. It may also provide powerful insights into the design of artificial neural networks. In this paper, we describe a heterogeneous neural network for controlling the wa1king of a simulated insect. This controller is inspired by the neuroethological and neurobiological literature on insect locomotion. It exhibits a variety of statically stable gaits at different speeds simply by varying the tonic activity of a single cell. It can also adapt to perturbations as a natural consequence of its design. INTRODUCTION Even very simple animals exhibit a dazzling variety of complex behaviors which they continuously adapt to the changing circums",Deep Learning,2b44928ae11fb9384c4cf38708677c48-Paper.pdf,1988
PROGRAMMABLE ANALOG PULSE-FIRING,"We describe pulse - stream firing integrated circuits that imple ment asynchronous analog neural networks. Synaptic weights are stored dynamically, and weighting uses time-division of the neural pulses from a signalling neuron to a receiving neuron. MOS transistors in their ""ON"" state act as variable resistors to control a capacitive discharge, and time-division is thus achieved by a small synapse circuit cell. The VLSI chip set design uses 2.5J.1.m CMOS technology. INTRODUCTION Neural network implementations fall into two broad classes - digital [1,2] and analog (e.g. [3,4]). The strengths of a digital approach include the ability to use well-proven design techniques, high noise immunity, and the ability to implement programmable networks. However digital circuits are synchronous, while biological neural networks are asynchronous. Further more, digital multipliers occupy large areas of silicon. Analog networks offer asynchronous behaviour, smooth neural activation and (potentially) sm",Optimization & Theoretical ML,31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf,1988
AN ANALOG VLSI CHIP FOR,"Reconstructing a surface from sparse sensory data is a well-known problem iIi computer vision. This paper describes an experimental analog VLSI chip for smooth surface interpolation from sparse depth data. An eight-node ID network was designed in 3J.lm CMOS and successfully tested. The network minimizes a second-order or ""thin plate"" energy of the surface. The circuit directly implements the cou pled depth/slope model of surface reconstruction (Harris, 1987). In addition, this chip can provide Gaussian-like smoothing of images. INTRODUCTION Reconstructing a surface from sparse sensory data is a well-known problem in computer vision. Early vision modules typically supply sparse depth, orientation, and discontinuity information. The surface reconstruction module incorporates these sparse and possibly conflicting measurements of a surface into a consistent, dense depth map. The coupled depth/slope model provides a novel solution to the surface reconstruc tion problem (Harris, 1987). A ID ",Computer Vision,3636638817772e42b59d74cff571fbb3-Paper.pdf,1988
SIMULATION AND MEASUREMENT OF,"The weakly electric fish, Gnathonemus peters;;, explores its environment by gener ating pulsed elecbic fields and detecting small pertwbations in the fields resulting from nearby objects. Accordingly, the fISh detects and discriminates objects on the basis of a sequence of elecbic ""images"" whose temporal and spatial properties depend on the tim ing of the fish's electric organ discharge and its body position relative to objects in its en vironmenl We are interested in investigating how these fish utilize timing and body-po sition during exploration to aid in object discrimination. We have developed a fmite-ele ment simulation of the fish's self-generated electric fields so as to reconstruct the elec trosensory consequences of body position and electric organ discharge timing in the fish. This paper describes this finite-element simulation system and presents preliminary elec tric field measurements which are being used to tune the simulation. INTRODUCTION The active positioning of sens",Computer Vision,37a749d808e46495a8da1e5352d03cae-Paper.pdf,1988
TRAINING MULTILAYER PERCEPTRONS WITH THE,"A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. a1. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two dimensional examples. INTRODUCTION Multilayer perceptrons are one of the most popular artificial neural net structures being used today. In most applications, the ""back propagation"" algorithm [Rllmelhart et ai, 1986] is used to train these networks. Although this algorithm works well for small nets or simple problems",Optimization & Theoretical ML,38b3eff8baf56627478ec76a704e9b52-Paper.pdf,1988
AN ELECTRONIC PHOTORECEPTOR,"We describe an electronic photoreceptor circuit that is sensitive to small changes in incident light intensity. The sensitivity to change8 in the intensity is achieved by feeding back to the input a filtered version of the output. The feedback loop includes a hysteretic el ement. The circuit behaves in a manner reminiscent of the gain control properties and temporal responses of a variety of retinal cells, particularly retinal bipolar cells. We compare the thresholds for detection of intensity increments by a human and by the cir cuit. Both obey Weber's law and for both the temporal contrast sensitivities are nearly identical. We previously described an electronic photoreceptor that outputs a voltage that is logarithmic in the light intensity (Mead, 1985). This report describes an extension of this circuit which was based on a suggestion by Frank Werblin that biological retinas may achieve greater sensitivity to change8 in the illumination by feeding back a filtered version of the outp",Optimization & Theoretical ML,3988c7f88ebcb58c6ce932b957b6f332-Paper.pdf,1988
IS NP-COMPLETE,"We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for the three nodes of this network so that it will produce output con sistent with a given set of training examples. We extend the result to other simple networks. This result suggests that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. It also suggests the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with non-linear functions such as sigmoids. INTRODUCTION One reason for the recent surge in interest in neural networks is the develop ment of the ""back-propagation"" algorithm for training neural networks. The ability to train large multi-layer neural networks is",Optimization & Theoretical ML,3def184ad8f4755ff269862ea77393dd-Paper.pdf,1988
AUTOMATIC LOCAL ANNEALING,This research involves a method for finding global maxima in constraint satisfaction networks. It is an annealing process butt unlike most otherst requires no annealing schedule. Temperature is instead determined locally by units at each updatet and thus all processing is done at the unit level. There are two major practical benefits to processing this way: 1) processing can continue in 'bad t areas of the networkt while 'good t areas remain stablet and 2) processing continues in the 'bad t areast as long as the constraints remain poorly satisfied (i.e. it does not stop after some predetermined number of cycles). As a resultt this method not only avoids the kludge of requiring an externally determined annealing schedulet but it also finds global maxima more quickly and consistently than externally scheduled systems (a comparison to the Boltzmann machine (Ackley et alt 1985) is made). FinallYt implementation of this method is computationally trivial. INTRODUCTION A constraint satisfacti,Optimization & Theoretical ML,42a0e188f5033bc65bf8d78622277c4e-Paper.pdf,1988
ADAPTIVE NEURAL NET PREPROCESSING,"A nonlinearity is required before matched filtering in mInimum error receivers when additive noise is present which is impulsive and highly non-Gaussian. Experiments were performed to determine whether the correct clipping nonlinearity could be provided by a single-input single output multi-layer perceptron trained with back propagation. It was found that a multi-layer perceptron with one input and output node, 20 nodes in the first hidden layer, and 5 nodes in the second hidden layer could be trained to provide a clipping nonlinearity with fewer than 5,000 presentations of noiseless and corrupted waveform samples. A network trained at a relatively high signal-to-noise (SIN) ratio and then used as a front end for a linear matched filter detector greatly reduced the probability of error. The clipping nonlinearity formed by this network was similar to that used in current receivers designed for impulsive noise and provided similar substantial improvements in performance. INTRODUCTION The",NLP,47d1e990583c9c67424d369f3414728e-Paper.pdf,1988
LEARNING THE SOLUTION TO THE,"The primate visual system learns to recognize the true direction of pattern motion using local detectors only capable of detecting the component of motion perpendicular to the orientation of the moving edge. A multilayer feedforward network model similar to Linsker's model was presented with input patterns each consisting of randomly oriented contours moving in a particular direction. Input layer units are granted component direction and speed tuning curves similar to those recorded from neurons in primate visual area VI that project to area MT. The network is trained on many such patterns until most weights saturate. A proportion of the units in the second layer solve the aperture problem (e.g., show the same direction-tuning curve peak to plaids as to gratings), resembling pattern-direction selective neurons, which ftrst appear inareaMT. INTRODUCTION Supervised learning schemes have been successfully used to learn a variety of input output mappings. Explicit neuron-by-neuron error si",Computer Vision,4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf,1988
GENESIS: A SYSTEM FOR SIMULATING NEURAL,"We have developed a graphically oriented, general purpose simulation system to facilitate the modeling of neural networks. The simulator is implemented under UNIX and X-windows and is designed to support simulations at many levels of detail. Specifically, it is intended for use in both applied network modeling and in the simulation of detailed, realistic, biologically based models. Examples of current models developed under this system include mammalian olfactory bulb and cortex, invertebrate central pattern generators, as well as more abstract connectionist simulations. INTRODUCTION Recently, there has been a dramatic increase in interest in exploring the computational properties of networks of parallel distributed processing elements (Rumelhart and McClelland, 1986) often referred to as Itneural networks"" (Anderson, 1988). Much of the current research involves numerical simulations of these types of networks (Anderson, 1988; Touretzky, 1989). Over the last several years, there has al",Optimization & Theoretical ML,4c5bde74a8f110656874902f07378009-Paper.pdf,1988
NEURAL ARCHITECTURE,No abstract found,Optimization & Theoretical ML,5878a7ab84fb43402106c575658472fa-Paper.pdf,1988
LEARNING BY CHOICE,"We introduce a learning algorithm for multilayer neural net works composed of binary linear threshold elements. Whereas ex isting algorithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal repre sentations as the fundamental entities to be determined. Once a correct set of internal representations is arrived at, the weights are found by the local aild biologically plausible Perceptron Learning Rule (PLR). We tested our learning algorithm on four problems: adjacency, symmetry, parity and combined symmetry-parity. I. INTRODUCTION = Consider a network of binary linear threshold elements i, whose state Si ±1 is determined according to the rule Si = sign(L WijSj + Oi) . (1) j Here Wij is the (unidirectional) weight assigned to the connection from unit j to i; 0i is a local bias. We focus our attention on feed-forward networks in which N units of the input layer determine the states of H units of a hidden layer; these, in turn, feed ",Deep Learning,5ef059938ba799aaa845e1c2e8a762bd-Paper.pdf,1988
Adaptive Neural Networks Using MOS Charge Storage,"MOS charge storage has been demonstrated as an effective method to store the weights in VLSI implementations of neural network models by several workers 2. However, to achieve the full power of a VLSI implementation of an adaptive algorithm, the learning operation must built into the circuit. We have fabricated and tested a circuit ideal for this purpose by connecting a pair of capacitors with a CCD like structure, allowing for variable size weight changes as well as a weight decay operation. A 2.51-' CMOS version achieves better than 10 bits of dynamic range in a 140/' X 3501-' area. A 1.25/' chip based upon the same cell has 1104 weights on a 3.5mm x 6.0mm die and is capable of peak learning rates of at least 2 x 109 weight changes per second. 1 Adaptive Networks Much of the recent excitement about neural network models of computation has been driven by the prospect of new architectures for fine grained parallel compu tation using analog VLSI. Adaptive systems are espescially good ta",Optimization & Theoretical ML,5f93f983524def3dca464469d2cf9f3e-Paper.pdf,1988
IMPLICATIONS OF,"I will describe my recent results on the automatic development of fixed width recursive distributed representations of variable-sized hierarchal data structures. One implication of this wolk is that certain types of AI-style data-structures can now be represented in fixed-width analog vectors. Simple inferences can be perfonned using the type of pattern associations that neural networks excel at Another implication arises from noting that these representations become self-similar in the limit Once this door to chaos is opened. many interesting new questions about the representational basis of intelligence emerge, and can (and will) be discussed. INTRODUCTION A major problem for any cognitive system is the capacity for, and the induction of the potentially infinite structures implicated in faculties such as human language and memory. Classical cognitive architectures handle this problem through finite but recursive sets of rules, such as fonnal grammars (Chomsky, 1957). Connectionist ar",NLP,5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf,1988
BACKPROPAGATION AND ITS,"A pool of handwritten signatures is used to train a neural net work for the task of deciding whether or not a given signature is a forgery. The network is a feedforward net, with a binary image as input. There is a hidden layer, with a single unit output layer. The weights are adjusted according to the backpropagation algorithm. The signatures are entered into a C software program through the use of a Datacopy Electronic Digitizing Camera. The binary signa tures are normalized and centered. The performance is examined as a function of the training set and network structure. The best scores are on the order of 2% true signature rejection with 2-4% false signature acceptance. INTRODUCTION Signatures are used everyday to authorize the transfer of funds for millions of people. We use our signature as a form of identity, consent, and authorization. Bank checks, credit cards, legal documents and waivers all require the everchanging personalized signature. Forgeries on such transactions amoun",Optimization & Theoretical ML,65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf,1988
THEORY OF SELF-ORGANIZATION OF,"We have mathematically shown that cortical maps in the primary sensory cortices can be reproduced by using three hypotheses which have physiological basis and meaning. Here, our main focus is on ocular.dominance column formation in the primary visual cortex. Monte Carlo simulations on the segregation of ipsilateral and contralateral afferent terminals are carried out. Based on these, we show that almost all the physiological experimental results concerning the ocular dominance patterns of cats and monkeys reared under normal or various abnormal visual conditions can be explained from a viewpoint of the phase transition phenomena. ROUGH SKETCH OF OUR THEORY In order to describe the use-dependent self-organization of neural connections {Singer,1987 and Frank,1987}, we have proposed a set of coupled equations involving the electrical activities and neural connection density {Tanaka, 1988}, by using the following physiologically based hypotheses: (1) Modifiable synapses grow or collapse du",Optimization & Theoretical ML,65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf,1988
AN ADAPTIVE NETWORK THAT LEARNS,"We describe an adaptive network, TIN2, that learns the transition function of a sequential system from observations of its behavior. It integrates two subnets, TIN-I (Winter, Ryan and Turner, 1987) and TIN-2. TIN-2 constructs state representations from examples of system behavior, and its dynamics are the main topics of the paper. TIN-I abstracts transition functions from noisy state representations and environmental data during training, while in operation it produces sequences of transitions in response to variations in input. Dynamics of both nets are based on the Adaptive Resonance Theory of Carpenter and Grossberg (1987). We give results from an experiment in which TIN2 learned the behavior of a system that recognizes strings with an even number of l's . INTRODUCTION Sequential systems respond to variations in their input environment with sequences of activities. They can be described in two ways. A black box description characterizes a system as an input-output function, m = B(u)",NLP,6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf,1988
DYNAMICS OF ANALOG NEURAL,"A time delay in the response of the neurons in a network can induce sustained oscillation and chaos. We present a stability criterion based on local stability analysis to prevent sustained oscillation in symmetric delay networks, and show an example of chaotic dynamics in a non-symmetric delay network. L INTRODUCTION Understanding how time delay affects the dynamics of neural networks is important for two reasons: First, some degree of time delay is intrinsic to any physically realized network, both in biological neural systems and in electronic artificial neural networks. As we will show, it is not obvious what constitutes a ""small"" (i.e. ignorable) delay which will not qualitatively change the network dynamics. For some network configurations, delay much smaller than the intrinsic relaxation time of the network can induce collective oscillatory behavior not predicted by mathematical models which ignore delay. These oscillations mayor may not be desirable; in either case, one should u",Optimization & Theoretical ML,698d51a19d8a121ce581499d7b701668-Paper.pdf,1988
ON THE K-WINNERS-TAKE-ALL NETWORK,"We present and rigorously analyze a generalization of the Winner Take-All Network: the K-Winners-Take-All Network. This net work identifies the K largest of a set of N real numbers. The network model used is the continuous Hopfield model. I - INTRODUCTION The Winner-Take-All Network is a network which identifies the largest of N real numbers. Winner-Take-All Networks have been developed using various neural networks models (Grossberg-73, Lippman-87, Feldman-82, Lazzaro-89). We present here a generalization of the Winner-Take-All Network: the K-Winners-Take-All (KWTA) Network. The KWTA Network identifies the K largest of N real numbers. The neural network model we use throughout the paper is the continuous Hopfield network model (Hopfield-84). If the states of the N nodes are initialized to the N real numbers, then, if the gain of the sigmoid is large enough, the network converges to the state with K positive real numbers in the positions of the nodes with the K largest initial states, ",Optimization & Theoretical ML,6c4b761a28b734fe93831e3fb400ce87-Paper.pdf,1988
"DOES THE NEURON ""LEARN"" LIKE THE SYNAPSE?","putation time during the supervised learning phase is described. It is based on extending the role that the neuron plays in artificial neural systems. Prior work has regarded the neuron as a strictly passive, non-linear processing element, and the synapse on the other hand as the primary source of information processing and knowledge retention. In this work, the role of the neuron is extended insofar as allow ing its parameters to adaptively participate in the learning phase. The temperature of the sigmoid function is an example of such a parameter. During learning, both the Tr synaptic interconnection weights w[j and the neuronal temperatures are opti mized so as to capture the knowledge contained within the training set. The method allows each neuron to possess and update its own characteristic local temperature. This algorithm has been applied to logic type of problems such as the XOR or parity problem, resulting in a significant decrease in the required number of training cycles. I",Optimization & Theoretical ML,6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf,1988
USE OF MULTI-LAYERED NETWORKS FOR,"Preliminary results on speaker-independant speech recognition are reported. A method that combines expertise on neural networks with expertise on speech recognition is used to build the recognition systems. For transient sounds, event driven property extractors with variable resolution in the time and frequency domains are used. For sonorant speech, a model of the human auditory system is preferred to FFT as a front-end module. INTRODUCTION Combining a structural or knowledge-based approach for describing speech units with neural networks capable of automatically learning relations between acoustic properties and speech units is the research effort we are attempting. The objective is that of using good generalization models for learning speech units that could be reliably used for many recognition tasks without having to train the system when a new speaker comes in or a new t~sk is considered. Domain (speech re.pognition) specific knowledge is applied for - segmentation and labeling of",NLP,73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf,1988
ELECTRONIC RECEPTORS FOR TACTILE/HAPTIC· SENSING,"We discuss synthetic receptors for haptic sensing. These are based on magnetic field sensors (Hall effect structures) fabricated using standard CMOS technologies. These receptors, biased with a small permanent magnet can detect the presence of ferro or ferri-magnetic objects in the vicinity of the sensor. They can also detect the magnitude and direction of the magnetic field. INTRODUCTION The organizational structure and functioning of the sensory periphery in living beings has always been the subject of extensive research. Studies of the retina and the cochlea have revealed a great deal of information as to the ways information is acquired and preprocessed; see for example review chapters in [Barlow and MoHon, 1982]. Understanding of the principles underlying the operation of sensory channels can be utilized to develop machines that can sense their environment and function in it, much like living beings. Although vision is the principal sensory channel to the outside world, the ""skin ",Optimization & Theoretical ML,76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf,1988
A PROGRAMMABLE ANALOG NEURAL COMPUTER,"This report describes the design of a programmable general purpose analog neural computer and simulator. It is intended primarily for real-world real-time computations such as analysis of visual or acoustical patterns, robotics and the development of special purpose neural nets. The machine is scalable and composed of interconnected modules containing arrays ofn eurons, modifiable synapses and switches. It runs entirely in analog mode but connection architecture, synaptic gains and time constants as well as neuron parameters are set digitally. Each neuron has a limited number of inputs and can be connected to any but not all other neurons. For the determination of synaptic gains and the implementation of learning algorithms the neuron outputs are multiplexed, AID converted and stored in digital memory. Even at moderate size of 1()3 to IDS neurons computational speed is expected to exceed that of any current digital computer. OVERVIEW The machine described in this paper is intended to s",Computer Vision,7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf,1988
AN INFORMATION THEORETIC APPROACH TO,"We discuss in this paper architectures for executing probabilistic rule-bases in a par allel manner, using as a theoretical basis recently introduced information-theoretic models. We will begin by describing our (non-neural) learning algorithm and theory of quantitative rule modelling, followed by a discussion on the exact nature of two particular models. Finally we work through an example of our approach, going from database to rules to inference network, and compare the network's performance with the theoretical limits for specific problems. Introduction With the advent of relatively cheap mass storage devices it is common in many domains to maintain large databases or logs of data, e.g., in telecommunications, medicine, finance, etc. The question naturally arises as to whether we can extract models from the data in an automated manner and use these models as the basis for an autonomous rational agent in the given domain, i.e., automatically generate ""expert systems"" from data. There",Optimization & Theoretical ML,7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf,1988
A CONNECTIONIST EXPERT SYSTEM,No abstract found,Optimization & Theoretical ML,7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf,1988
CRICKET WIND DETECTION,No abstract found,Computer Vision,7f6ffaa6bb0b408017b62254211691b5-Paper.pdf,1988
ALVINN:,"ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Cur rently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perfOIm the task differs dra matically when the networlc is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand. INTRODUCTION Autonomous navigation has been a difficult problem for traditional vision and robotic techniques, primarily because of the noise and variability associated with real world scenes. Autonomous navi",Computer Vision,812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,1988
"""FAST LEARNING IN","A class of fast, supervised learning algorithms is presented. They use lo cal representations, hashing, atld multiple scales of resolution to approximate functions which are piece-wise continuous. Inspired by Albus's CMAC model, the algorithms learn orders of magnitude more rapidly than typical imple mentations of back propagation, while often achieving comparable qualities of generalization. Furthermore, unlike most traditional function approximation methods, the algorithms are well suited for use in real time adaptive signal processing. Unlike simpler adaptive systems, such as linear predictive cod ing, the adaptive linear combiner, and the Kalman filter, the new algorithms are capable of efficiently capturing the structure of complicated non-linear systems. As an illustration, the algorithm is applied to the prediction of a chaotic timeseries. 1 Introduction A variety of approaches to adaptive information processing have been developed by workers in disparate disciplines. These incl",Optimization & Theoretical ML,82161242827b703e6acf9c726942a1e4-Paper.pdf,1988
Mapping Classifier Systems,"Classifier systems are machine learning systems incotporating a genetic al gorithm as the learning mechanism. Although they respond to inputs that neural networks can respond to, their internal structure, representation fonnalisms, and learning mechanisms differ marlcedly from those employed by neural network re searchers in the same sorts of domains. As a result, one might conclude that these two types of machine learning fonnalisms are intrinsically different. This is one of two papers that, taken together, prove instead that classifier systems and neural networks are equivalent. In this paper, half of the equivalence is demonstrated through the description of a transfonnation procedure that will map classifier systems into neural networks that are isomotphic in behavior. Several alterations on the commonly-used paradigms employed by neural networlc researchers are required in order to make the transfonnation worlc. These alterations are noted and their appropriateness is discussed. ",Optimization & Theoretical ML,82aa4b0af34c2313a562076992e50aa3-Paper.pdf,1988
A NETWORK FOR IMAGE SEGMENTATION,"We propose a parallel network of simple processors to find color boundaries irrespective of spatial changes in illumi n.a tion, and to spread uniform colors within marked re- glOns. INTRODUCTION To rely on color as a cue in recognizing objects, a visual system must have at least approximate color constancy. Otherwise it might ascribe different characteristics to the same object under different lights. But the first step in using color for recog nition, segmenting the scene into regions of different colors, does not require color constancy. In this crucial step color serves simply as a means of distinguishing one object from another in a given scene. Color differences, which mark material boundaries, are essential, while absolute color values are not. The goal of segmen tation algorithms is to achieve this first step toward object recognition by finding discontinuities in the image irradiance that mark material boundaries. The problems that segmentation algorithms must solve is how to c",Computer Vision,8d5e957f297893487bd98fa830fa6413-Paper.pdf,1988
TRAINING A,"Hardware implementation of neuromorphic algorithms is hampered by high degrees of connectivity. Functionally equivalent feedforward networks may be formed by using limited fan-in nodes and additional layers. but this complicates procedures for determining weight magnitudes. No direct mapping of weights exists between fully and limited-interconnect nets. Low-level nonlinearities prevent the formation of internal representations of widely separated spatial features and the use of gradient descent methods to minimize output error is hampered by error magnitude dissipation. The judicious use of linear summations or collection units is proposed as a solution. HARDWARE IMPLEMENTATIONS OF FEEDFORWARD, SYNTHETIC NEURAL SYSTEMS The pursuit of hardware implementations of artificial neural network models is motivated by the need to develop systems which are capable of executing neuromorphic algorithms in real time. The most significant barrier is the high degree of connectivity required between t",Optimization & Theoretical ML,8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf,1988
A MODEL FOR RESOLUTION ENHANCEMENT,"Heiligenberg (1987) recently proposed a model to explain how sen sory maps could enhance resolution through orderly arrangement of broadly tuned receptors. We have extended this model to the general case of polynomial weighting schemes and proved that the response function is also a polynomial of the same order. We further demon strated that the Hermitian polynomials are eigenfunctions of the sys tem. Finally we suggested a biologically plausible mechanism for sen sory representation of external stimuli with resolution far exceeding the inter-receptor separation. 1 INTRODUCTION In sensory systems, the stimulus continuum is sampled at discrete points by receptors of finite tuning width d and inter-receptor spacing a. In order to code both stimulus locus and stimulus intensity with a single output, the sampling of individual receptors must be overlapping (i. e. a < d). This discrete and overlapped sampling of the stimulus continuum poses a question of how then the system could reconstruc",Computer Vision,8f85517967795eeef66c225f7883bdcb-Paper.pdf,1988
TEMPORAL REPRESENTATIONS IN A,"SYREN is a connectionist model that uses temporal information in a speech signal for syllable recognition. It classifies the rates and directions of formant center transitions, and uses an adaptive method to associate transition events with each syllable. The system uses explicit spatial temporal representations through de lay lines. SYREN uses implicit parametric temporal representa tions in formant transition classification through node activation onset, decay, and transition delays in sub-networks analogous to visual motion detector cells. SYREN recognizes 79% of six repe titions of 24 consonant-vowel syllables when tested on unseen data, and recognizes 100% of its training syllables. INTRODUCTION Living organisms exist in a dynamic environment. Problem solving systems, both natural and synthetic, must relate and interpret events that occur over time. Although connectionist models are based on metaphors from the brain, few have been designed to capture temporal and sequential inform",NLP,903ce9225fca3e988c2af215d4e544d3-Paper.pdf,1988
A MASSIVELY PARALLEL SELF-TUNING,"The Parsing and Learning System(PALS) is a massively parallel self-tuning context-free parser. It is capable of parsing sentences of unbounded length mainly due to its parse-tree representation scheme. The system is capable of improving its parsing performance through the presentation of training examples. INTRODUCTION Recent PDP research[Rumelhart et al. • 1986; Feldman and Ballard, 1982; Lippmann, 1987] involving natural language processtng[Fanty, 1988; Selman, 1985; Waltz and Pollack, 1985] have unrealistically restricted sentences to a fixed length. A solution to this problem was presented in the system CONPARSE[Charniak and Santos. 1987]. A parse-tree representation scheme was utilized which allowed for processing sentences of any length. Although successful as a parser. it's achitecture was strictly hand-constructed with no learning of any form. Also. standard learning schemes were not appUcable since it differed from all the popular architectures, in particular. connectionist on",NLP,9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf,1988
NEURAL NET RECEIVERS IN,"The application of neural networks to the demodulation of spread-spectrum signals in a multiple-access environment is considered. This study is motivated in large part by the fact that, in a multiuser system, the conventional (matched fil ter) receiver suffers severe performance degradation as the relative powers of the interfering signals become large (the ""near-far"" problem). Furthermore, the optimum receiver, which alleviates the near-far problem, is too complex to be of practical use. Receivers based on multi-layer perceptrons are considered as a simple and robust alternative to the opti mum solution. The optimum receiver is used to benchmark the performance of the neural net receiver; in particular, it is proven to be instrumental in identifying the decision regions of the neural networks. The back-propagation algorithm and a modified version of it are used to train the neural net. An importance sampling technique is introduced to reduce the number of simulations necessary to eval",Computer Vision,9872ed9fc22fc182d371c3e9ed316094-Paper.pdf,1988
A COMPUTATIONA.LLY ROBUST,"We analyze a mathematical model for retinal directionally selective cells based on recent electrophysiological data, and show that its computation of motion direction is robust against noise and speed. INTROduCTION Directionally selective retinal ganglion cells discriminate direction of visual motion relatively independently of speed (Amthor and Grzywacz, 1988a) and with high contrast sensitivity (Grzywacz, Amthor, and Mistler, 1989). These cells respond well to motion in the ""preferred"" direction, but respond poorly to motion in the opposite, null, direction. There is an increasing amount of experimental work devoted to these cells. Three findings are particularly relevant for this paper: 1- An inhibitory process asymmetric to every point of the receptive field underlies the directional selectivity of ON-OFF ganglion cells of the rabbit retina (Barlow and Levick, 1965). This distributed inhibition allows small motions anywhere in the receptive field center to elicit directionally sele",Computer Vision,98dce83da57b0395e163467c9dae521b-Paper.pdf,1988
NEURAL NETWORK STAR PATTERN,"Currently, the most complex spacecraft attitude determination and control tasks are ultimately governed by ground-based systems and personnel. Conventional on-board systems face severe computational bottlenecks introduced by serial microprocessors operating on inherently parallel problems. New computer architectures based on the anatomy of the human brain seem to promise high speed and fault-tolerant solutions to the limitations of serial processing. This paper discusses the latest applications of artificial neural networks to the problem of star pattern recognition for spacecraft attitude determination. INTRODUCTION By design, a conventional on-board microprocessor can perform only one comparison or calculation at a time. Image or pattern recognition problems involving large template sets and high resolution can require an astronomical number of comparisons to a given database. Typical mission planning and optimization tasks require calculations involving a multitude of parameters, wh",Computer Vision,96da2f590cd7246bbde0051047b0d6f7-Paper.pdf,1988
STATISTICAL PREDICTION WITH KANERVA'S,"A new viewpoint of the processing performed by Kanerva's sparse distributed memory (SDM) is presented. In conditions of near- or over- capacity, where the associative-memory behavior of the mod el breaks down, the processing performed by the model can be inter preted as that of a statistical predictor. Mathematical results are presented which serve as the framework for a new statistical view point of sparse distributed memory and for which the standard for mulation of SDM is a special case. This viewpoint suggests possi ble enhancements to the SDM model, including a procedure for improving the predictiveness of the system based on Holland's work with 'Genetic Algorithms', and a method for improving the capacity of SDM even when used as an associative memory. OVERVIEW This work is the result of studies involving two seemingly separate topics that proved to share a common framework. The fIrst topic, statistical prediction, is the task of associating extremely large perceptual state vecto",Optimization & Theoretical ML,9b8619251a19057cff70779273e95aa6-Paper.pdf,1988
LEARNING SEQUENTIAL STRUCTURE,"We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-l, together with element t, to predict element t+ 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. Cluster analyses of the hidden-layer patterns of activation showed that they encode prediction-relevant information about the entire path traversed through the network. We illustrate the phases of learning with cluster analyses performed at different points during training. Several connectionist architectures that are explicitly constrained to capture sequential infonnation have been developed. Examples are Time Delay Networks (e.g. Sejnowski & Rosenberg. 1986) -- also called 'moving window' paradigms -- or algorithms such as back-propagation in time (Rumelhart. Hinton & Williams. 1986), Suc",NLP,9dcb88e0137649590b755372b040afad-Paper.pdf,1988
A BACK-PROPAGATION ALGORITHM,"This paper presents a variation of the back-propagation algo rithm that makes optimal use of a network hidden units by de an ""energy"" term written as a function of the squared cr~asing activations of these hidden units. The algorithm can automati cally find optimal or nearly optimal architectures necessary to solve known Boolean functions, facilitate the interpretation of the activation of the remaining hidden units and automatically estimate the complexity of architectures appropriate for phonetic labeling problems. The general principle of the algorithm can also be adapted to different tasks: for example, it can be used to eliminate the [0, 0] local minimum of the [-1. +1] logistic acti vation function while preserving a much faster convergence and forcing binary activations over the set of hidden units. PRINCIPLE This paper describes an algorithm which makes optimal use of the hidden units in a network using the standard back-propagation algorithm (Rumelhart. Hinton & Williams, 1986",Optimization & Theoretical ML,9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf,1988
GEMINI: GRADIENT ESTIMATION,"Learning procedures that measure how random perturbations of unit ac tivities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities af fect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforce ment procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing un known non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI. OVERVIEW Reinforcement learning procedures",Reinforcement Learning,a0a080f42e6f13b3a2df133f073095dd-Paper.pdf,1988
A SELF-LEARNING NEURAL NETWORK,No abstract found,Deep Learning,a2557a7b2e94197ff767970b67041697-Paper.pdf,1988
NEURAL NETWORKS FOR MODEL,"We introduce an optimization approach for solving problems in com puter vision that involve multiple levels of abstraction. Our objective functions include compositional and specialization hierarchies. We cast vision problems as inexact graph matching problems, formulate graph matching in terms of constrained optimization, and use analog neural networks to perform the optimization. The method is applicable to per ceptual grouping and model matching. Preliminary experimental results are shown. 1 Introduction The minimization of objective functions is an attractive way to formulate and solve visual recognition problems. Such formulations are parsimonious, being expressible in several lines of algebra, and may be converted into artificial neural networks which perform the optimization. Advantages of such networks including speed, parallelism, cheap analog computing, and biological plausibility have been noted [Hop field and Tank, 1985]. According to a common view of computational vision, ",Computer Vision,a3c65c2974270fd093ee8a9bf8ae7d0b-Paper.pdf,1988
NEURONAL MAPS FOR SENSORY- MOTOR,"The bam owl has fused visual/auditory/motor representations of space in its midbrain which are used to orient the head so that visu al or auditory stimuli are centered in the visual field of view. We present models and computer simulations of these structures which address various problems, inclu<lln~ the construction of a map of space from auditory sensory information, and the problem of driv ing the motor system from these maps. We compare the results with biological data. INTRODUCTION Many neural network models have little resemblance to real neural structures, part ly because the brain's higher functions, which they attempt to imitate, are not yet experimentally accessible. Nevertheless, some neural-net researchers are finding that the accessible structures are interesting, and that their functions are potentially use ful. Our group is modeling a part of the barn owl's nervous system which orients the head to visual and auditory stimuli. The bam owl's brain stem and midbrain contai",Computer Vision,a4a042cf4fd6bfb47701cbc8a1653ada-Paper.pdf,1988
PERFORMANCE OF SYNTHETIC NEURAL,"This study evaluates the performance of the multilayer-perceptron and the frequency-sensitive competitive learning network in iden tifying five commercial aircraft from radar backscatter measure ments. The performance of the neural network classifiers is com pared with that of the nearest-neighbor and maximum-likelihood classifiers. Our results indicate that for this problem, the neural network classifiers are relatively insensitive to changes in the net work topology, and to the noise level in the training data. While, for this problem, the traditional algorithms outperform these sim ple neural classifiers, we feel that neural networks show the poten tial for improved performance. INTRODUCTION The design of systems that identify objects based on measurements of their radar backscatter signals has traditionally been predicated upon decision-theoretic meth ods of pattern recognition [1]. While it is true that these methods are characterized by a well-defined sense of optimality, they de",Computer Vision,a5e00132373a7031000fd987a3c9f87b-Paper.pdf,1988
Connectionist Learning of Expert Preferences by,"A new training paradigm, caned the ""eomparison pa.radigm,"" is introduced for tasks in which a. network must learn to choose a prdcrred pattern from a set of n alternatives, based on examplcs of Imma.n expert prderences. In this pa.radigm, the inpu t to the network consists of t.wo uf the n alterna tives, and the trained output is the expert's judgement of which pa.ttern is better. This para.digm is applied to the lea,rning of hackgammon, a difficult board ga.me in wllieh the expert selects a move from a. set, of legal mm·es. \Vith compa.rison training, much higher levels of performance can hc a.chiew~d, with networks that are much smaller, and with coding sehemes t.hat are much simpler and easier to understand. Furthermorf', it is possible to set up the network so tha.t it always produces consisten t rank-orderings.",Reinforcement Learning,a8baa56554f96369ab93e4f3bb068c22-Paper.pdf,1988
WINNER-TAKE-ALL,"We have designed, fabricated, and tested a series of compact CMOS integrated circuits that realize the winner-take-all function. These analog, continuous-time circuits use only O(n) of interconnect to perform this function. We have also modified the winner-take-all circuit, realizing a circuit that computes local nonlinear inhibition. Two general types of inhibition mediate activity in neural systems: subtractive in hibition, which sets a zero level for the computation, and multiplicative (nonlinear) inhibition, which regulates the gain of the computation. We report a physical real ization of general nonlinear inhibition in its extreme form, known as winner-take-all. We have designed and fabricated a series of compact, completely functional CMOS integrated circuits that realize the winner-take-all function, using the full analog nature of the medium. This circuit has been used successfully as a component in several VLSI sensory systems that perform auditory localization (Lazzaro and Me",Optimization & Theoretical ML,a8f15eda80c50adb0e71943adc8015cf-Paper.pdf,1988
ANALOG IMPLEMENTATION OF SHUNTING,"An extremely compact, all analog and fully parallel implementa tion of a class of shunting recurrent neural networks that is ap plicable to a wide variety of FET-based integration technologies is proposed. While the contrast enhancement, data compression, and adaptation to mean input intensity capabilities of the network are well suited for processing of sensory information or feature extrac tion for a content addressable memory (CAM) system, the network also admits a global Liapunov function and can thus achieve stable CAM storage itself. In addition the model can readily function as a front-end processor to an analog adaptive resonance circuit. INTRODUCTION Shunting neural networks are networks in which multiplicative, or shunting, terms of the form Lj f;(Xj) or Lj Ij appear in the short term memory equations, Xi Xi where is activity of a cell or a cell population or an iso-potential portion of a Xi cell and Ii are external inputs arriving at each site. The first case shows recurrent",NLP,ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf,1988
Range Image Restoration,"A new optimization strategy, Mean Field Annealing, is presented. Its application to MAP restoration of noisy range images is derived and experimentally verified. 1 Introduction The application which motivates this paper is image analysis; specifically the anal ysis of range images. We [BS86] [GS87] and others [YA85][BJ88] have found that surface curvature has the potential for providing an excellent, view-invariant fea ture with which to segment range images. Unfortunately, computation of curvature requires, in turn, computation of second derivatives of noisy data. We cast this task as a restoration problem: Given a measurement g(z, y), we assume that g(z, y) resulted from the addition of noise to some ""ideal"" image fez, y) which we must estimate from three things:",Computer Vision,b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf,1988
ANALYZING THE ENERGY LANDSCAPES,"DCPS (the Distributed Connectionist Production System) is a neural network with complex dynamical properties. Visualizing the energy landscapes of some of its component modules leads to a better intuitive understanding of the model, and suggests ways in which its dynamics can be controlled in order to improve performance on difficult cases. INTRODUCTION Competition through mutual inhibition appears in a wide variety of network designs. This paper discusses a system with unusually complex competitive dynamics. The system is DCPS, the Distributed Connectionist Production System of Touretzky and Hinton (1988). DCPS is a Boltzmann machine composed of five modules, two of which, labeled ""Rule Space"" and ""Bind Space,"" are winner-take-all (WTA) networks. These modules interact via their effects on two attentional mod ules called clause spaces. Clause spaces are another type of competitive architecture based on mutual inhibition, but they do not produce WTA behavior. Both clause spaces provide",Optimization & Theoretical ML,b73ce398c39f506af761d2277d853a92-Paper.pdf,1988
NEURAL NETWORK RECOGNIZER FOR,"This paper describes the construction of a system that recognizes hand-printed digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks. MOTIVATION The problem of recognizing hand-written digits is of enormous practical and the~ retical interest [Kahan, Pavlidis, and Baird 1987; Watanabe 1985; Pavlidis 1982]. This project has forced us to formulate and deal with a number of questions rang ing from the basic psychophysics of human perception to analog integrated circuit design. This is a topic where ""neural net"" t",Computer Vision,a97da629b098b75c294dffdc3e463904-Paper.pdf,1988
NEURAL APPROACH FOR TV IMAGE COMPRESSION,"A self-organizing Hopfield network has been developed in the context of Vector Ouantiza -tion, aiming at compression of television images. The metastable states of the spin glass-like network are used as an extra storage resource using the Minimal Overlap learning rule (Krauth and Mezard 1987) to optimize the organization of the attractors. The sel f-organi zi ng scheme that we have devised results in the generation of an adaptive codebook for any qiven TV image. I NTRODOCTI ON The ability of an Hopfield network (Little,1974; Hopfield, 1982,1986; Amit. and al., 1987; Personnaz and al. 1985; Hertz, 1988) to behave as an associative memory usua 11 y aSSlJ11es a pri ori knowl edge of the patterns to be stored. As in many applications they are unknown, the aim of this work is to develop a network capable to learn how to select its attractors. TV image compression using Vector Quantization (V.Q.)(Gray, 1984), a key issue for HOTV transmission, is a typical case, since the non neural algorit",Computer Vision,bd4c9ab730f5513206b999ec0d90d1fb-Paper.pdf,1988
SPEECH RECOGNITION: STATISTICAL AND,No abstract found,NLP,bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf,1988
CONVERGENCE AND PATTERN STABILIZATION,"The Boltzmann Machine has been introduced as a means to perform global optimization for multimodal objective functions using the principles of simulated annealing. In this paper we consider its utility as a spurious-free content-addressable memory, and provide bounds on its performance in this context. We show how to exploit the machine's ability to escape local minima, in order to use it, at a constant temperature, for unambiguous associative pattern-retrieval in noisy environments. An association rule, which creates a sphere of influence around each stored pattern, is used along with the Machine's dynamics to match the machine's noisy input with one of the pre-stored patterns. Spurious fIxed points, whose regions of attraction are not recognized by the rule, are skipped, due to the Machine's fInite probability to escape from any state. The results apply to the Boltzmann machine and to the asynchronous net of binary threshold elements (Hopfield model'). They provide the network design",Optimization & Theoretical ML,c45147dee729311ef5b5c3003946c48f-Paper.pdf,1988
LINEAR LEARNING: LANDSCAPES AND ALGORITHMS,No abstract found,Optimization & Theoretical ML,c8ffe9a587b126f152ed3d89a146b445-Paper.pdf,1988
DIGITAL REALISATION OF SELF-ORGANISING MAPS,"A digital realisation of two-dimensional self-organising feature maps is presented. The method is based on subspace classification using an n-tuple technique. Weight vector approximation and orthogonal projections to produce a winner takes-all network are also discussed. Over one million effective binary weights can be applied in 25ms using a conventional microcomputer. Details of a number of image recognition tasks, including character recognition and object centring, are described. INTRODUCTION Background The overall aim of our work is to develop fast and flexible systems for image recognition, usually for commercial inspection tasks. There is an urgent need for automatic learning systems in such applications, since at present most systems employ heuristic classification techniques. This approach requires an extensive development effort for each new application, which exaggerates implementation costs; and for many tasks, there are no clearly defined features which can be employed for",Computer Vision,c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf,1988
Further Explorations in Visually-Guided,"MURPHY is a vision-based kinematic controller and path planner based on a connectionist architecture, and implemented with a video camera and Rhino XR-series robot arm. Imitative of the layout of sen sory and motor maps in cerebral cortex, MURPHY'S internal representa tions consist of four coarse-coded populations of simple units represent ing both static and dynamic aspects of the sensory-motor environment. In previously reported work [4], MURPHY first learned a direct kinematic model of his camera-arm system during a period of extended practice, and then used this ""mental model"" to heuristically guide his hand to unobstructed visual targets. MURPHY has since been extended in two ways: First, he now learns the inverse differential-kinematics of his arm in addition to ordinary direct kinematics, which allows him to push his hand directly towards a visual target without the need for search. Sec ondly, he now deals with the much more difficult problem of reaching in the presence of obsta",Computer Vision,cedebb6e872f539bef8c3f919874e9d7-Paper.pdf,1988
SCALING AND GENERALIZATION IN,"The issues of scaling and generalization have emerged as key issues in current studies of supervised learning from examples in neural networks. Questions such as how many training patterns and training cycles are needed for a problem of a given size and difficulty, how to represent the inllUh and how to choose useful training exemplars, are of considerable theoretical and practical importance. Several intuitive rules of thumb have been obtained from empirical studies, but as yet there are few rig orous results. In this paper we summarize a study Qf generalization in the simplest possible case-perceptron networks learning linearly separa ble functions. The task chosen was the majority function (i.e. return a 1 if a majority of the input units are on), a predicate with a num ber of useful properties. We find that many aspects of.generalization in multilayer networks learning large, difficult tasks are reproduced in this simple domain, in which concrete numerical results and even some ana",Optimization & Theoretical ML,d1f491a404d6854880943e5c3cd9ca25-Paper.pdf,1988
"A.B.Kirillov, G.N.Borisyuk, R.M.Borisyuk,",No abstract found,Cannot be classified from the provided context.,da4fb5c6e93e74d3df8527599fa62642-Paper.pdf,1988
AN OPTIMALITY PRINCIPLE FOR,"We propose an optimality principle for training an unsu pervised feedforward neural network based upon maximal ability to reconstruct the input data from the network out puts. We describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity. Examples of applications to the problems of image coding, feature detection, and analysis of random dot stereograms are presented.",Optimization & Theoretical ML,e00da03b685a0dd18fb6a08af0923de0-Paper.pdf,1988
NEURAL ANALOG DIFFUSION-ENHANCEMENT,"A new class of neural network aimed at early visual processing is described; we call it a Neural Analog Diffusion-Enhancement Layer or ""NADEL."" The network consists of two levels which are coupled through feedfoward and shunted feedback connections. The lower level is a two-dimensional diffusion map which accepts visual features as input, and spreads activity over larger scales as a function of time. The upper layer is periodically fed the activity from the diffusion layer and locates local maxima in it (an extreme form of contrast enhancement) using a network of local comparators. These local maxima are fed back to the diffusion layer using an on-center/off-surround shunting anatomy. The maxima are also available as output of the network. The network dynamics serves to cluster features on multiple scales as a function of time, and can be used in a variety of early visual processing tasks such as: extraction of comers and high curvature points along edge contours, line end detection, g",Computer Vision,e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf,1988
OPTIMIZATION BY MEAN FIELD ANNEALING,"Nearly optimal solutions to many combinatorial problems can be found using stochastic simulated annealing. This paper extends the concept of simulated annealing from its original formulation as a Markov process to a new formulation based on mean field theory. Mean field annealing essentially replaces the discrete de grees of freedom in simulated annealing with their average values as computed by the mean field approximation. The net result is that equilibrium at a given temperature is achieved 1-2 orders of magnitude faster than with simulated annealing. A general frame work for the mean field annealing algorithm is derived, and its re lationship to Hopfield networks is shown. The behavior of MFA is examined both analytically and experimentally for a generic combi natorial optimization problem: graph bipartitioning. This analysis indicates the presence of critical temperatures which could be im portant in improving the performance of neural networks. STOCHASTIC VERSUS MEAN FIELD In com",Optimization & Theoretical ML,ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf,1988
AN APPLICATION OF THE PRINCIPLE OF,"This paper addresses the problem of determining the weights for a set of linear filters (model ""cells"") so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal. INTRODUCTION I have previously proposed [Linsker, 1987, 1988] a principle of ""maximum information preservation,"" also called the ""infomax"" principle, that may account for certain aspects of the organization of a ",Optimization & Theoretical ML,ec8956637a99787bd197eacd77acce5e-Paper.pdf,1988
SPREADING ACTIVATION OVER,"One att·empt at explaining human inferencing is that of spread ing activat,ion, particularly in the st.ructured connectionist para digm. This has resulted in t.he building of systems with semanti cally nameable nodes which perform inferencing by examining t.he pat,t.erns of activation spread. In this paper we demonst.rate t.hat simple structured network infert'ncing can be p(>rformed by passing art.iva.t.ion over the weights learned by a distributed alga rit,hm. Thus, an account, is provided which explains a well behaved rela t ionship bet.ween structured and distri butt'd conn('c t.ionist. a.pproachrs. INTRODUCTION A primar~· difference brtween t,he nPllral net.works of 20 years ago and t.he (""urrent genera Lion of connect,ionist models is t.he addit.ion of mechanisms whic h permit t.he s),st,em to create all internal represent,ation. These subsymbolic, semantica.lly unnameable, feat.urrs which a.re induced by connectionist. learning algorithms havr been discussed as bt,ing of import.",NLP,ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf,1988
Consonant Recognition by Modular Construction of,In this paperl we show that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants.,NLP,eecca5b6365d9607ee5a9d336962c534-Paper.pdf,1988
CONSTRAINTS ON ADAPTIVE NETWORKS,"The potential of adaptive networks to learn categorization rules and to model human performance is studied by comparing how natural and artificial systems respond to new inputs, i.e., how they generalize. Like humans, networks can learn a detenninistic categorization task by a variety of alternative individual solutions. An analysis of the con straints imposed by using networks with the minimal number of hidden units shows that this ""minimal configuration"" constraint is not sufficient to explain and predict human performance; only a few solu tions were found to be shared by both humans and minimal adaptive networks. A further analysis of human and network generalizations indicates that initial conditions may provide important constraints on generalization. A new technique, which we call ""reversed learning"", is described for finding appropriate initial conditions. INTRODUCTION We are investigating the potential of adaptive networks to learn categorization tasks and to model human perfor",Optimization & Theoretical ML,f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf,1988
Self Organizing Neural Networks for the,"This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and demonstrates its use in a system identification task. The algorithm constructs the network, chooses the neuron functions, and adjusts the weights. It is compared to the Back-Propagation algorithm in the identification of the chaotic time series. The results shows that SONN constructs a simpler, more accurate model. requiring less training data and epochs. The algorithm can be applied and generalized to appilications as a classifier. I. INTRODUCTION 1.1 THE SYSTEM IDENTIFICATION PROBLEM In various engineering applications, it is important to be able to estimate, interpolate, and extrapolate the behavior of an unknown system when only its input-output pairs are available. Algorithms which produce an estimation of the system behavior based on these pairs fall under the category of system identification techniques. 1.2 SYSTEM IDENTIFICATION USING NEURAL NETWORKS A general form to represent systems,",Deep Learning,f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf,1988
USING BACKPROPAGATION,"Computing the inverse dynamics of a robot ann is an active area of research in the control literature. We hope to learn the inverse dynamics by training a neural network on the measured response of a physical ann. The input to the network is a temporal window of measured positions; output is a vector of torques. We train the network on data measured from the first two joints of the CMU Direct-Drive Arm II as it moves through a randomly-generated sample of ""pick-and-place"" trajectories. We then test generalization with a new trajectory and compare its output with the torque measured at the physical arm. The network is shown to generalize with a root mean square error/standard deviation (RMSS) of 0.10. We interpreted the weights of the network in tenns of the velocity and acceleration filters used in conventional control theory. INTRODUCTION Dynamics is the study of forces. The dynamic response of a robot arm relates joint torques to the position, velocity, and acceleration of its links.",Optimization & Theoretical ML,f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf,1988
LEARNING WITH TEMPORAL DERIVATIVES IN,"A number of learning models have recently been proposed which involve calculations of temporal differences (or derivatives in continuous-time models). These models. like most adaptive network models. are formulated in tenns of frequency (or activation), a useful implications of a neuronal model. it may be preferable to develop a model which transmits discrete pulse-coded information. We point out that many functions and properties of neuronal processing and learning may depend. in subtle ways. on the pulse-coded nature of the informa tion coding and transmission properties of neuron systems. When com pared to formulations in terms of activation. computing with temporal derivatives (or differences) as proposed by Kosko (1986). Klopf (1988). and Sutton (1988). is both more stable and easier when refor mulated for a more neuronally realistic pulse-coded system. In refor mulating these models in terms of pulse-coding. our motivation has been to enable us to draw further parallels and conne",Reinforcement Learning,f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf,1988
"DYNAMIC, NON·LOCAL ROLE BINDINGS AND","This paper introduces a means to handle the critical problem of non local role-bindings in localist spreading-activation networks. Every conceptual node in the network broadcasts a stable, uniquely-identifying activation pattern, called its signature. A dynamic role-binding is cre ated when a role's binding node has an activation that matches the bound concept's signature. Most importantly, signatures are propagated across long paths of nodes to handle the non-local role-bindings neces sary for inferencing. Our localist network model, ROBIN (ROle Binding and Inferencing Network), uses signature activations to ro bustly represent schemata role-bindings and thus perfonn the inferenc ing, plan/goal analysis, schema instantiation, word-sense disambigua tion, and dynamic re-interpretation portions of the natural language un derstanding process. MOTIVATION Understanding natural language is a difficult task, often requiring a reader to make multi ple inferences to understand the motives of ac",NLP,fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf,1988
STORING COVARIANCE BY THE ASSOCIATIVE,"In modeling studies or memory based on neural networks, both the selective or enhancement and depression synaptic strengths are required ror effident storage or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982; Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus, a cortical structure or the brain that is involved in long-term memory. A brier, high-frequency activation or excitatory synapses in the hippocampus produces an increase in synaptic strength known as long-term potentiation, or L TP (BUss and Lomo, 1973), that can last ror many days. LTP is known to be Hebbian since it requires the simultaneous release or neurotransmitter from presynaptic terminals coupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller, 1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or synaptic strength that could balance LTP has not yet been demonstrated. We stu died the associative interactions betwee",NLP,f899139df5e1059396431415e770c6dd-Paper.pdf,1988
FIXED POINT ANALYSIS FOR RECURRENT,"This paper provides a systematic analysis of the recurrent backpropaga tion (RBP) algorithm, introducing a number of new results. The main limitation of the RBP algorithm is that it assumes the convergence of the network to a stable fixed point in order to backpropagate the error signals. We show by experiment and eigenvalue analysis that this condi tion can be violated and that chaotic behavior can be avoided. Next we examine the advantages of RBP over the standard backpropagation al gorithm. RBP is shown to build stable fixed points corresponding to the input patterns. This makes it an appropriate tool for content address able memories, one-to-many function learning, and inverse problems. INTRODUCTION In the last few years there has been a great resurgence of interest in neural network learning algorithms. One of the most successful of these is the Backpropagation learning algorithm of [Rumelhart 86], which has shown its usefulness in a number of applications. This algorithm is repre",Optimization & Theoretical ML,fc221309746013ac554571fbd180e1c8-Paper.pdf,1988
Comparing the Performance of Connectionist,"In the development of an image segmentation system for real time image processing applications, we apply the classical decision anal ysis paradigm by viewing image segmentation as a pixel classifica. tion task. We use supervised training to derive a classifier for our system from a set of examples of a particular pixel classification problem. In this study, we test the suitability of a connection ist method against two statistical methods, Gaussian maximum likelihood classifier and first, second, and third degree polynomial classifiers, for the solution of a ""real world"" image segmentation problem taken from combustion research. Classifiers are derived using all three methods, and the performance of all of the classi fiers on the training data set as well as on 3 separate entire test images is measured. 1 Introduction We are applying the trainable machine paradigm in our development of an image segmentation system to be used in real time image processing applications. We view image seg",Computer Vision,01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf,1989
A nalog Circuits for Constrained Optimization,"This paper explores whether analog circuitry can adequately per form constrained optimization. Constrained optimization circuits are designed using the differential multiplier method. These cir cuits fulfill time-varying constraints correctly. Example circuits in clude a quadratic programming circuit and a constrained flip-flop. 1 INTRODUCTION Converting perceptual and cognitive tasks into constrained optimization problems is a useful way of generating neural networks to solve those tasks. Researchers have used constrained optimization networks to solve the traveling salesman problem [Durbin, 1987] [Hopfield, 1985], to perform object recognition [Gindi, 1988], and to decode error-correcting codes [Platt, 1986]. Implementing constrained optimization in analog VLSI is advantageous, because an analog VLSI chip can solve a large number of differential equations in parallel [Mead, 1989]. However, analog circuits only approximate the desired differential equations. Therefore, we have built t",Optimization & Theoretical ML,0266e33d3f546cb5436a10798e657d97-Paper.pdf,1989
Training Stochastic Model Recognition,"One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multi layer perceptron to provide mathematically correct probability dis tributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mu tual Information training, which has been used successfully to im prove the performance of hidden Markov models for speech recog nition. If the network is specially constructed to perform the recog nition computations of a given kind of stochastic model based clas sifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'. 1 INTRODUCTION It has often been suggested that one of the attractions of an adaptive neural network (NN) approach to pattern recognition is the availability of discrimi",Deep Learning,0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf,1989
Can Simple Cells Learn Curves? A,"In the mammalian visual cortex, orientation-selective 'simple cells' which detect straight lines may be adapted to detect curved lines instead. We test a biologically plausible, Hebbian, single-neuron model, which learns oriented receptive fields upon exposure to un structured (noise) input and maintains orientation selectivity upon exposure to edges or bars of all orientations and positions. This model can also learn arc-shaped receptive fields upon exposure to an environment of only circular rings. Thus, new experiments which try to induce an abnormal (curved) receptive field may pro vide insight into the plasticity of simple cells. The model suggests that exposing cells to only a single spatial frequency may induce more striking spatial frequency and orientation dependent effects than heretofore observed. 1 Introduction Although most mathematical theories of cortical function assume plasticity of indi vidual cells, there is a strong debate in the biological community between ""instru",Computer Vision,03c6b06952c750899bb03d998e631860-Paper.pdf,1989
Development and Regeneration of Eye-Brain,We outline a computational model of the development and regenera tion of specific eye-brain circuits. The model comprises a self-organiz ing map-forming network which uses local Hebb rules. constrained by molecular markers. Various simulations of the development of eye brain maps in fish and frogs are described. 1 INTRODUCTION The brain is a biological computer of immense complexity comprising highly specialized neurons and neural circuits. Such neurons are interconnected with high specificity in many regions of the brain. if not in all. There are also many observations which indicate that there is also considerable circuit plasticity. Both specificity and plasticity are found in the development and regeneration of eye-brain connections in vertebrates. Sperry (1944) frrst demonstrated specificity in the regeneration of eye-brain connections in frogs following optic nerve section and eye rotation; and Gaze and Sharma (1970) and Y oon (1972) found evidence for plasticity in the expanded ,Reinforcement Learning,060ad92489947d410d897474079c1477-Paper.pdf,1989
Predicting Weather Using a Genetic Memory: a Combi,"Kanerva's sparse distributed memory (SDM) is an associative-memo ry model based on the mathematical properties of high-dimensional binary address spaces. Holland's genetic algorithms are a search tech nique for high-dimensional spaces inspired by evolutionary processes of DNA. ""Genetic Memory"" is a hybrid of the above two systems, in which the memory uses a genetic algorithm to dynamically recon figure its physical storage locations to reflect correlations between the stored addresses and data. For example, when presented with raw weather station data, the Genetic Memory discovers specific fea tures in the weather data which correlate well with upcoming rain, and reconfigures the memory to utilize this information effectively. This architecture is designed to maximize the ability of the system to scale-up to handle real-world problems. INTRODUCTION The future success of neural networks depends on an ability to ""scale-up"" from small networks and low-dimensional toy problems to networks ",Optimization & Theoretical ML,06138bc5af6023646ede0e1f7c1eac75-Paper.pdf,1989
Neu.·al Network Analysis of,"Interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons. These ""hidden"" units coordinate several different behaviors. We used physiological and anatomical constraints to construct a model of the local bending reflex. Dynamical networks were trained on experimentally derived input-output patterns using recurrent back-propagation. Units in the model were modified to include electrical synapses and multiple synaptic time constants. The properties of the hidden units that emerged in the simulations matched those in the leech. The model and data support distributed rather than localist representations in the local bending reflex. These results also explain counterintuitive aspects of the local bending circuitry. INTRODUCTION Neural network modeling techniques have recently been used to predict and analyze the connectivity of biological neural circuits (Zipser and Andersen, 1988; Lehley and Sejnowski, 1988; Anastasio and Robinson, 1989",Deep Learning,077e29b11be80ab57e1a2ecabb7da330-Paper.pdf,1989
A computer modeling approach to understanding the,"This paper presents the results of a simulation of the spatial relationship between the inferior olivary nucleus and folium crus IIA of the lateral hemisphere of the rat cerebellum. The principal objective of this modeling effort was to resolve an apparent conflict between a proposed zonal organization of olivary projections to cerebellar cortex suggested by anatomical tract-tracing experiments (Brodal & Kawamura 1980; Campbell & Armstrong 1983) and a more patchy organization apparent with physiological mapping (Robertson 1987). The results suggest that several unique features of the olivocerebellar circuit may contribute to the appearance of zonal organization using anatomical techniques, but that the detailed patterns of patchy tactile projections seen with physiological techniques are a more accurate representation of the afferent organization of this region of cortex. 1 INTRODUCTION Determining the detailed anatomical structure of the nervous system has been a major focus of neurob",Computer Vision,084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper.pdf,1989
Generalization and scaling in reinforcement,"In associative reinforcement learning, an environment generates input vectors, a learning system generates possible output vectors, and a re inforcement function computes feedback signals from the input-output pairs. The task is to discover and remember input-output pairs that generate rewards. Especially difficult cases occur when rewards are rare, since the expected time for any algorithm can grow exponentially with the size of the problem. Nonetheless, if a reinforcement function possesses regularities, and a learning algorithm exploits them, learning time can be reduced below that of non-generalizing algorithms. This paper describes a neural network algorithm called complementary re inforcement back-propagation (CRBP), and reports simulation results on problems designed to offer differing opportunities for generalization. 1 REINFORCEMENT LEARNING REQUIRES SEARCH Reinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen, 1989) requires more from a learner than",Reinforcement Learning,091d584fced301b442654dd8c23b3fc9-Paper.pdf,1989
Neural Network Weight Matrix Synthesis Using,"Given a set of input-output training samples, we describe a proce dure for determining the time sequence of weights for a dynamic neural network to model an arbitrary input-output process. We formulate the input-output mapping problem as an optimal con trol problem, defining a performance index to be minimized as a function of time-varying weights. We solve the resulting nonlin ear two-point-boundary-value problem, and this yields the training rule. For the performance index chosen, this rule turns out to be a continuous time generalization of the outer product rule earlier sug gested heuristically by Hopfield for designing associative memories. Learning curves for the new technique are presented. 1 INTRODUCTION u - Suppose that we desire to model as best as possible some unknown map 4> : nn. V, where U, V ~ One way we might go about doing this is to collect as many input-output samples {(9in, 9 0ud : 4>(9 in) = 9 0ud as possible and ""find"" some func- tion f : U - V such that a suitabl",Optimization & Theoretical ML,0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf,1989
Collective Oscillations in the,"The firing patterns of populations of cells in the cat visual cor tex can exhibit oscillatory responses in the range of 35 - 85 Hz. Furthermore, groups of neurons many mm's apart can be highly synchronized as long as the cells have similar orientation tuning. We investigate two basic network architectures that incorporate ei ther nearest-neighbor or global feedback interactions and conclude that non-local feedback plays a fundamental role in the initial syn chronization and dynamic stability of the oscillations. 1 INTRODUCTION 40 - 60 Hz oscillations have long been reported in the rat and rabbit olfactory bulb and cortex on the basis of single- and multi-unit recordings as well as EEG activity (Freeman, 1972; Wilson & Bower 1990). Recently, two groups (Eckhorn et ai., 1988 and Gray et ai., 1989) have reported highly synchronized, stimulus specific oscillations in the 35 - 85 Hz range in areas 17, 18 and PMLS of anesthetized as well as awake cats. Neurons with similar orientation tuning",Computer Vision,0e01938fc48a2cfb5f2217fbfb00722d-Paper.pdf,1989
Neural networks: the early days,"A short account is given of various investigations of neural network properties, beginning with the classic work of McCulloch & Pitts. Early work on neurodynamics and statistical mechanics, analogies with magnetic materials, fault tolerance via parallel distributed processing, memory, learning, and pattern recognition, is described. 1 INTRODUCTION In this brief account of the early days in neural network research, it is not possible to be comprehensive. This article then is a somewhat subjective survey of some, but not all, of the developments in the theory of neural networks in the twent-five year period, from 1943 to 1968, when many of the ideas and concepts were formulated, which define the field of neural network research. This comprises work on connections with automata theory and computability; neurodynamics, both deterministic and statistical; analogies with magnetic materials and spin systems; reliability via parallel and parallel distributed processing; modifiable synapses and",Optimization & Theoretical ML,0e65972dce68dad4d52d063967f0a705-Paper.pdf,1989
Operational Fault Tolerance,"The performance sensitivity of Albus' CMAC network was studied for the scenario in which faults are introduced into the adjustable weights after training has been accomplished. It was found that fault sensitivity was reduced with increased generalization when ""loss of weight"" faults were considered, but sensitivity was increased for ""saturated weight"" faults. 1 INTRODUCTION Fault-tolerance is often cited as an inherent property of neural networks, and is thought by many to be a natural consequence of ""massively parallel"" computational architectures. Numerous anecdotal reports of fault-tolerance experiments, primarily in pattern classification tasks, abound in the literature. However, there has been surprisingly little rigorous investigation of the fault-tolerance properties of various network architectures in other application areas. In this paper we investigate the fault-tolerance of the CMAC (Cerebellar Model Arithmetic Computer) network [Albus 1975] in a systematic manner. CMAC netw",Optimization & Theoretical ML,0f49c89d1e7298bb9930789c8ed59d48-Paper.pdf,1989
Effects of Firing Synchrony on Signal,"Spiking neurons which integrate to threshold and fire were used to study the transmission of frequency modulated (FM) signals through layered networks. Firing correlations between cells in the input layer were found to modulate the transmission of FM sig nals under certain dynamical conditions. A tonic level of activity was maintained by providing each cell with a source of Poisson distributed synaptic input. When the average membrane depo larization produced by the synaptic input was sufficiently below threshold, the firing correlations between cells in the input layer could greatly amplify the signal present in subsequent layers. When the depolarization was sufficiently close to threshold, however, the firing synchrony between cells in the initial layers could no longer effect the propagation of FM signals. In this latter case, integrate and-fire neurons could be effectively modeled by simpler analog elements governed by a linear input-output relation. 1 Introduction Physiologists ha",Reinforcement Learning,115f89503138416a242f40fb7d7f338e-Paper.pdf,1989
Acoustic-Imaging Computations by Echolocating Bats:,"The echolocating bat, Eptesicus fuscus, perceives the distance to sonar targets from the delay of echoes and the shape of targets from the spectrum of echoes. However, shape is perceived in terms of the target's range proftle. The time separation of echo components from parts of the target located at different distances is reconstructed from the echo spectrum and added to the estimate of absolute delay already derived from the arrival-time of echoes. The bat thus perceives the distance to targets and depth within targets along the same psychological range dimension, which is computed. The image corresponds to the crosscorrelation function of echoes. Fusion of physiologically distinct time- and frequency-domain representations into a fmal, common time-domain image illustrates the binding of within modality features into a unified, whole image. To support the structure of images along the dimension of range, bats can perceive echo delay with a hyperacuity of 10 nanoseconds. Acoustic-Imag",Computer Vision,13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf,1989
Time DependentAdaptive Neural Networks,"A comparison of algorithms that minimize error functions to train the trajectories of recurrent networks, reveals how complexity is traded off for causality. These algorithms are also related to time-independent fonnalisms. It is suggested that causal and scalable algorithms are possible when the activation dynamics of adaptive neurons is fast compared to the behavior to be learned. Standard continuous-time recurrent backpropagation is used in an example. 1 INTRODUCTION Training the time dependent behavior of a neural network model involves the minimization of a function that measures the difference between an actual trajectory and a desired trajectory. The standard method of accomplishing this minimization is to calculate the gradient of an error function with respect to the weights of the system and then to use the gradient in a minimization algorithm (e.g. gradient descent or conjugate gradient). Techniques for evaluating gradients and performing minimizations are well developed in ",Optimization & Theoretical ML,1534b76d325a8f591b52d302e7181331-Paper.pdf,1989
NEURAL NETWORK VISUALIZATION,"We have developed graphics to visualize static and dynamic infor mation in layered neural network learning systems. Emphasis was placed on creating new visuals that make use of spatial arrange ments, size information, animation and color. We applied these tools to the study of back-propagation learning of simple Boolean predicates, and have obtained new insights into the dynamics of the learning process. 1 INTRODUCTION Although neural network learning systems are being widely investigated by many researchers via computer simulations, the graphical display of information in these simulations has received relatively little attention. In other fields such as fluid dynamics and chaos theory, the development of ""scientific visualization"" techniques (1,3) have proven to be a tremendously useful aid to research, development, and education. Similar benefits should result from the application of these techniques to neural networks research. In this article, several visualization methods are int",Computer Vision,16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf,1989
Discovering the Structure of a Reactive Environment,"Consider a robot wandering around an unfamiliar environment. performing ac tions and sensing the resulting environmental states. The robot's task is to con struct an internal model of its environment. a model that will allow it to predict the consequences of its actions and to determine what sequences of actions to take to reach particular goal states. Rivest and Schapire (1987&, 1987b; Schapire. 1988) have studied this problem and have designed a symbolic algo rithm to strategically explore and infer the structure of ""finite state"" environ ments. The heart of this algorithm is a clever representation of the environment called an update graph. We have developed a connectionist implementation of the update graph using a highly-specialized network architecture. With back propagation learning and a trivial exploration strategy - choosing random ac tions - the connectionist network can outperfonn the Rivest and Schapire al gorithm on simple problems. The network has the additional strength",Reinforcement Learning,1700002963a49da13542e0726b7bb758-Paper.pdf,1989
A Sell-organizing Associative,"The CHAC storage scheme has been used as a basis for a software implementation of an associative .emory system AHS, which itself is a major part of the learning control loop LERNAS. A major disadvantage of this CHAC-concept is that the degree of local generalization (area of interpo lation) is fixed. This paper deals with an algo rithm for self-organizing variable generaliza tion for the AKS, based on ideas of T. Kohonen. 1 INTRODUCTION For several years research at the Department of Control The ory and Robotics at the Technical University of Darmstadt has been concerned with the design of a learning real-time control loop with neuron-like associative memories (LERNAS) A Self-organizing Associative Memory System for Control Applications 333 for the control of unknown, nonlinear processes (Ersue, Tolle, 1988). This control concept uses an associative memo ry system AHS, based on the cerebellar cortex model CHAC by Albus (Albus, 1972), for the storage of a predictive nonlin ear process m",Reinforcement Learning,19f3cd308f1455b3fa09a282e0d496f4-Paper.pdf,1989
Complexity of Finite Precision,"A rigorous analysis on the finite precision computational <)Spects of neural network as a pattern classifier via a probabilistic approach is presented. Even though there exist negative results on the capa bility of perceptron, we show the following positive results: Given n pattern vectors each represented by en bits where e > 1, that are uniformly distributed, with high probability the perceptron can perform all possible binary classifications of the patterns. More over, the resulting neural network requires a vanishingly small pro portion O(log n/n) of the memory that would be required for com plete storage of the patterns. Further, the perceptron algorithm takes O(n2) arithmetic operations with high probability, whereas other methods such as linear programming takes O(n3.5) in the worst case. We also indicate some mathematical connections with VLSI circuit testing and the theory of random matrices. 1 Introduction It is well known that the percept ron algorithm can be used to find th",Optimization & Theoretical ML,20f07591c6fcb220ffe637cda29bb3f6-Paper.pdf,1989
Neuronal Group Selection Theory:,"In this paper, we discuss a current attempt at applying the organi zational principle Edelman calls Neuronal Group Selection to the control of a real, two-link robotic manipulator. We begin by moti vating the need for an alternative to the position-control paradigm of classical robotics, and suggest that a possible avenue is to look at the primitive animal limb 'neurologically ballistic' control mode. We have been considering a selectionist approach to coordinating a simple perception-action task. 1 MOTIVATION The majority of industrial robots in the world are mechanical manipUlators - often arm-like devices consisting of some number of rigid links with actuators mounted where the links join that move adjacent links relative to each other, rotationally or translation ally. At the joints there are typically also sensors measuring the relative position of adjacent links, and it is in terms of position that manipulators are generally controlled (a desired motion is specified as a desired ",Optimization & Theoretical ML,274ad4786c3abca69fa097b85867d9a4-Paper.pdf,1989
Dimensionality Reduction and Prior Knowledge in,"It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task. 1 Introduction The domain for our research was a speech recognition task that requires distinctions to be learned between recordings of four highl y confusable words: the names of the letters ""B"", ""D"", ""E"", and ""V"". The task was created at IBM's T. J. Watson Research Center, and is difficult because many speakers were included and also because the recordings were made under noisy office conditions using a remote microphone. One",NLP,289dff07669d7a23de0ef88d2f7129e7-Paper.pdf,1989
A self-organizing multiple-view representation,"We demonstrate the ability of a two-layer network of thresholded summation units to support representation of 3D objects in which several distinct 2D views are stored for ea.ch object. Using unsu pervised Hebbian relaxation, the network learned to recognize ten objects from different viewpoints. The training process led to the emergence of compact representations of the specific input views. When tested on novel views of the same objects, the network ex hibited a substantial generalization capability. In simulated psy chophysical experiments, the network's behavior was qualitatively similar to that of human subjects. 1 Background Model-based object recognition involves, by definition, a compa.rison between the input image and models of different objects that are internal to the recognition system. The form in which these models are best stored depends on the kind of information available in the input, and on the trade-off between the amount of memory allocated for the storage and the d",Computer Vision,335f5352088d7d9bf74191e006d8e24c-Paper.pdf,1989
Associative Memory in a Simple Model of,"A generic model of oscillating cortex, which assumes ""minimal"" coupling justified by known anatomy, is shown to function as an as sociative memory, using previously developed theory. The network has explicit excitatory neurons with local inhibitory interneuron feedback that forms a set of nonlinear oscillators coupled only by long range excitatofy connections. Using a local Hebb-like learning rule for primary and higher order synapses at the ends of the long range connections, the system learns to store the kinds of oscil lation amplitude patterns observed in olfactory and visual cortex. This rule is derived from a more general ""projection algorithm"" for recurrent analog networks, that analytically guarantees content addressable memory storage of continuous periodic sequences - capacity: N/ 2 Fourier components for an N node network - no ""spurious"" attractors. 1 Introduction This is a sketch of recent results stemming from work which is discussed completely in [1, 2, 3]. Patterns of 40",Optimization & Theoretical ML,3644a684f98ea8fe223c713b77189a77-Paper.pdf,1989
Adjoint Operator Algorithms for Faster,"A methodology for faster supervised learning in dynamical nonlin ear neural networks is presented. It exploits the concept of adjoint operntors to enable computation of changes in the network's re sponse due to perturbations in all system parameters, using the so lution of a single set of appropriately constructed linear equations. The lower bound on speedup per learning iteration over conven tional methods for calculating the neuromorphic energy gradient is O(N2), where N is the number of neurons in the network. 1 INTRODUCTION The biggest promise of artifcial neural networks as computational tools lies in the hope that they will enable fast processing and synthesis of complex information patterns. In particular, considerable efforts have recently been devoted to the for mulation of efficent methodologies for learning (e.g., Rumelhart et al., 1986; Pineda, 1988; Pearlmutter, 1989; Williams and Zipser, 1989; Barhen, Gulati and Zak, 1989). The development of learning algorithms is genera",Optimization & Theoretical ML,36660e59856b4de58a219bcf4e27eba3-Paper.pdf,1989
Computer Simulation of Oscillatory Behavior,"It has been known for many years that specific regions of the work ing cerebral cortex display periodic variations in correlated cellular activity. While the olfactory system has been the focus of much of this work, similar behavior has recently been observed in primary visual cortex. We have developed models of both the olfactory and visual cortex which replicate the observed oscillatory proper ties of these networks. Using these models we have examined the dependence of oscillatory behavior on single cell properties and net work architectures. We discuss the idea that the oscillatory events recorded from cerebral cortex may be intrinsic to the architecture of cerebral cortex as a whole, and that these rhythmic patterns may be important in coordinating neuronal activity during sensory processmg. 1 INTRODUCTION An obvious characteristic of the general behavior of cerebral cortex, as evident in EEG recordings, is its tendency to oscillate. Cortical oscillations have been observed both i",Optimization & Theoretical ML,38db3aed920cf82ab059bfccbd02be6a-Paper.pdf,1989
Coupled Markov Random Fields and,"In recent years many researchers have investigated the use of Markov Random Fields (MRFs) for computer vision. They can be applied for example to reconstruct surfaces from sparse and noisy depth data coming from the output of a visual process, or to integrate early vision processes to label physical discontinuities. In this pa per we show that by applying mean field theory to those MRFs models a class of neural networks is obtained. Those networks can speed up the solution for the MRFs models. The method is not restricted to computer vision. 1 Introduction In recent years many researchers (Geman and Geman, 1984) (Marroquin et. al. 1987) (Gamble et. al. 1989) have investigated the use of Markov Random Fields (MRFs) for early vision. Coupled MRFs models can be used for the reconstruction of a function starting from a set of noisy sparse data, such as intensity, stereo, or motion data. They have also been used to integrate early vision processes to label physical discontinuities. Two fiel",Computer Vision,39059724f73a9969845dfe4146c5660e-Paper.pdf,1989
PULSE-FIRING NEURAL CIDPS,"We announce new CMOS synapse circuits using only three and four MOSFETsisynapse. Neural states are asynchronous pulse streams, upon which arithmetic is performed directly. Chips implementing over 100 fully programmable synapses are described and projections to networks of hundreds of neurons are made. 1 OVERVIEW OF PULSE FIRING NEURAL VLSI The inspiration for the use of pulse firing in silicon neural networks is clearly the electrical/chemical pulse mechanism in ""real"" biological neurons. t Asynchronous, digital voltage pulses are used to signal states Si ) through synapse weights { to emulate neural dynamics. Neurons fire voltage Tij } pulses of a frequency determined by their level of activity but of a constant magnitude (usually 5 Volts) [Murray,1989a]. As indicated in Fig. 1, synapses perform arithmetic directly on these asynchronous pulses, to increment or decrement the receiving neuron's activity. The activity of a receiving neuron i, is altered at a frequency controlled by the s",Deep Learning,3b8a614226a953a8cd9526fca6fe9ba5-Paper.pdf,1989
Rule Representations in a Connectionist Chunker,"We present two connectionist architectures for chunking of symbolic rewrite rules. One uses backpropagation learning, the other competitive learning. Although they were developed for chunking the same sorts of rules, the two differ in their representational abilities and learning behaviors. 1 INTRODUCTION Chunking is a process for generating, from a sequence of if-then rules, a more complex rule that accomplishes the same task in a single step. It has been used to explain incre mental human perfonnance improvement in a wide variety of cognitive, perceptual, and motor tasks (Newell, 1987). The SOAR production system (Laird, Newell, & Rosen bloom, 1987) is a classical AI computer program that implements a ""unified theory of cognition"" based on chunking. SOAR's version of chunking is a symbolic process that examines the working memory trace of rules contributing to the chunk. In this paper we present two connectionist rule-following architectures that generate chunks a different way: they",NLP,45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf,1989
A Neural Network for Real-Time Signal Processing,"This paper describes a neural network algorithm that (1) performs temporal pattern matching in real-time, (2) is trained on-line, with a single pass, (3) requires only a single template for training of each representative class, (4) is continuously adaptable to changes in background noise, (5) deals with transient signals having low signal to-noise ratios, (6) works in the presence of non-Gaussian noise, (7) makes use of context dependencies and (8) outputs Bayesian proba bility estimates. The algorithm has been adapted to the problem of passive sonar signal detection and classification. It runs on a Con nection Machine and correctly classifies, within 500 ms of onset, signals embedded in noise and subject to considerable uncertainty. 1 INTRODUCTION This paper describes a neural network algorithm, STOCHASM, that was developed for the purpose of real-time signal detection and classification. Of prime concern was capability for dealing with transient signals having low signal-to-noise ra",Deep Learning,46ba9f2a6976570b0353203ec4474217-Paper.pdf,1989
Speaker Independent Speech Recognition with,"We attempt to combine neural networks with knowledge from speech science to build a speaker independent speech recogni tion system. This knowledge is utilized in designing the preprocessing, input coding, output coding, output supervision and architectural constraints. To handle the temporal aspect of speech we combine delays, copies of activations of hidden and output units at the input level, and Back-Propagation for Sequences (BPS), a learning algorithm for networks with local self-loops. This strategy is demonstrated in several experi ments, in particular a nasal discrimination task for which the application of a speech theory hypothesis dramatically im proved generalization. 1 INTRODUCTION The strategy put forward in this research effort is to combine the flexibility and learning abilities of neural networks with as much knowledge from speech science as possible in order to build a speaker independent automatic speech recognition system. This knowledge is utilized in each of the s",NLP,4734ba6f3de83d861c3176a6273cac6d-Paper.pdf,1989
Analytic Solutions to the Formation of,"Analytic solutions to the information-theoretic evolution equa tion of the connection strength of a three-layer feedforward neural net for visual information processing are presented. The results are (1) the receptive fields of the feature-analysing cells corre spond to the eigenvector of the maximum eigenvalue of the Fred holm integral equation of the first kind derived from the evolution equation of the connection strength; (2) a symmetry-breaking mechanism (parity-violation) has been identified to be respon sible for the changes of the morphology of the receptive field; (3) the conditions for the formation of different morphologies are explicitly identified. 1 INTRODUCTION The use of Shannon's information theory ( Shannon and Weaver,1949) to the study of neural nets has been shown to be very instructive in explaining the formation of different receptive fi~lds in the early visual information processing, as evident by the works of Linsker (1986,1988). It has been demonstrated that th",Deep Learning,48aedb8880cab8c45637abc7493ecddd-Paper.pdf,1989
An Analog VLSI Model of Adaptation,"The vestibulo-ocular reflex (VOR) is the primary mechanism that controls the compensatory eye movements that stabilize retinal im ages during rapid head motion. The primary pathways of this sys tem are feed-forward, with inputs from the semicircular canals and outputs to the oculomotor system. Since visual feedback is not used directly in the VOR computation, the system must exploit motor learning to perform correctly. Lisberger(1988) has proposed a model for adapting the VOR gain using image-slip information from the retina. We have designed and tested analog very large scale integrated (VLSI) circuitry that implements a simplified ver sion of Lisberger's adaptive VOR model. 1 INTRODUCTION A characteristic commonly found in biological systems is their ability to adapt their function based on their inputs. The combination of the need for precision and the variability inherent in the environment necessitates such learning in organisms. Sensorimotor systems present obvious examples of be",Reinforcement Learning,502e4a16930e414107ee22b6198c578f-Paper.pdf,1989
A Reconfigurable Analog VLSI Neural Network,"1024 distributed-neuron synapses have been integrated in an active area of 6.1mm x 3.3mm using a 0.9p.m, double-metal, single-poly, n-well CMOS technology. The distributed-neuron synapses are ar ranged in blocks of 16, which we call '4 x 4 tiles'. Switch matrices are interleaved between each of these tiles to provide programma bility of interconnections. With a small area overhead (15 %), the 1024 units of the network can be rearranged in various configura tions. Some of the possible configurations are, a 12-32-12 network, a 16-12-12-16 network, two 12-32 networks etc. (the numbers sep arated by dashes indicate the number of units per layer, including the input layer). Weights are stored in analog form on MaS ca pacitors. The synaptic weights are usable to a resolution of 1% of their full scale value. The limitation arises due to charge injection from the access switch and charge leakage. Other parameters like gain and shape of nonlinearity are also programmable. Introduction A wide va",Computer Vision,539fd53b59e3bb12d203f45a912eeaf2-Paper.pdf,1989
Handwritten Digit Recognition with a,"We present an application of back-propagation networks to hand written digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service. 1 INTRODUCTION The main point of this paper is to show that large back-propagation (BP) net works can be applied to real image-recognition problems without a large, complex preprocessing stage requiring detailed engineering. Unlike most previous work on the subject (Denker et al., 1989), the learning network is directly fed with images, rather than feature vectors, thus demonstrating the ability of BP networks to deal with large amounts of low level information. Previous work performed on simple digit images (Le Cun, 1989) showed that the architecture of the network strongly",Computer Vision,53c3bce66e43be4f209556518c2fcb54-Paper.pdf,1989
Digital-Analog Hybrid Synapse Chips for,No abstract found,Computer Vision,555d6702c950ecb729a966504af0a635-Paper.pdf,1989
Computational Efficiency:,"It is well-known that neural responses in particular brain regions are spatially organized, but no general principles have been de veloped that relate the structure of a brain map to the nature of the associated computation. On parallel computers, maps of a sort quite similar to brain maps arise when a computation is distributed across multiple processors. In this paper we will discuss the rela tionship between maps and computations on these computers and suggest how similar considerations might also apply to maps in the brain. 1 INTRODUCTION A great deal of effort in experimental and theoretical neuroscience is devoted to recording and interpreting spatial patterns of neural activity. A variety of map patterns have been observed in different brain regions and, presumably, these pat terns reflect something about the nature of the neural computations being carried out in these regions. To date, however, there have been no general principles for interpreting the structure of a brain map ",Reinforcement Learning,577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf,1989
A Cost Function for Internal Representations,"We introduce a cost function for learning in feed-forward neural networks which is an explicit function of the internal representa tion in addition to the weights. The learning problem can then be formulated as two simple perceptrons and a search for internal representations. Back-propagation is recovered as a limit. The frequency of successful solutions is better for this algorithm than for back-propagation when weights and hidden units are updated on the same timescale i.e. once every learning step. 1 INTRODUCTION In their review of back-propagation in layered networks, Rumelhart et al. (1986) describe the learning process in terms of finding good ""internal representations"" of the input patterns on the hidden units. However, the search for these representa tions is an indirect one, since the variables which are adjusted in its course are the connection weights, not the activations of the hidden units themselves when specific input patterns are fed into the input layer. Rather, the in",Deep Learning,57aeee35c98205091e18d1140e9f38cf-Paper.pdf,1989
HMM Speech Recognition,"Two approaches were explored which integrate neural net classifiers with Hidden Markov Model (HMM) speech recognizers. Both at tempt to improve speech pattern discrimination while retaining the temporal processing advantages of HMMs. One approach used neu ral nets to provide second-stage discrimination following an HMM recognizer. On a small vocabulary task, Radial Basis Function (RBF) and back-propagation neural nets reduced the error rate substantially (from 7.9% to 4.2% for the RBF classifier). In a larger vocabulary task, neural net classifiers did not reduce the error rate. They, however, outperformed Gaussian, Gaussian mixture, and k nearest neighbor (KNN) classifiers. In another approach, neural nets functioned as low-level acoustic-phonetic feature extractors. When classifying phonemes based on single 10 msec. frames, dis criminant RBF neural net classifiers outperformed Gaussian mix ture classifiers. Performance, however, differed little when classi fying phones by accumulatin",NLP,58a2fc6ed39fd083f55d4182bf88826d-Paper.pdf,1989
Generalization and Parameter Estimation,"We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization perfor mance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class pattern classification problem. We report the results of these studies, and show the application of cross-validation techniques to prevent overfitting. 1 INTRODUCTION It is well known that system models which have too many parameters (with respect to the number of measurements) do not generalize well to new measurements. For instance, an autoregressive (AR) model can be derived which will represent the training data with no error by using as many parameters as there are data points. This would Genera",Optimization & Theoretical ML,63923f49e5241343aa7acb6a06a751e7-Paper.pdf,1989
The Effects of Circuit Integration on a Feature,"The effects of parameter modifications imposed by hardware con straints on a self-organizing feature map algorithm were examined. Performance was measured by the error rate of a speech recogni tion system which included this algorithm as part of the front-end processing. System parameters which were varied included weight (connection strength) quantization, adap tation quantization, dis tance measures and circuit approximations which include device characteristics and process variability. Experiments using the TI isolated word database for 16 speakers demonstrated degradation in performance when weight quantization fell below 8 bits. The com petitive nature of the algorithm rela..xes constraints on uniformity and linearity which makes it an excellent candidate for a fully ana log circuit implementation. Prototype circuits have been fabricated and characterized following the constraints established through the simulation efforts. 1 Introduction The self-organizing feature map algorithm ",Computer Vision,621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf,1989
VLSI Implementation of a High-Capacity,"In this paper we describe the VLSI design and testing of a high capacity associative memory which we call the exponential cor relation associative memory (ECAM). The prototype 3J.'-CMOS programmable chip is capable of storing 32 memory patterns of 24 bits each. The high capacity of the ECAM is partly due to the use of special exponentiation neurons, which are implemented via sub-threshold MOS transistors in this design. The prototype chip is capable of performing one associative recall in 3 J.'S. 1 ARCHITECTURE Previously (Chiueh, 1989), we have proposed a general model for correlation-based associative memories, which includes a variant of the Hopfield memory and high order correlation memories as special cases. This new exponential correlation as sociative memory (ECAM) possesses a very large storage capacity, which scales exponentially with the length of memory patterns (Chiueh, 1988). Furthermore, it has been shown that the ECAM is asymptotically stable in both synchronous and 1 Tz",Computer Vision,63dc7ed1010d3c3b8269faf0ba7491d4-Paper.pdf,1989
The Cascade-Correlation Learning Architecture,"Cascade-Correlation is a new architecture and supervised learning algo rithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a min imal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detec tors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network .d etermines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network. 1 DESCRIPTION OF CASCADE·CORRELATION The most important problem preventing the widespread applicat",Deep Learning,69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf,1989
Sigma-Pi Learning:,"The goal in this work has been to identify the neuronal elements of the cortical column that are most likely to support the learning of nonlinear associative maps. We show that a particular style of network learning algorithm based on locally-tuned receptive fields maps naturally onto cortical hardware, and gives coherence to a variety of features of cortical anatomy, physiology, and biophysics whose relations to learning remain poorly understood. 1 INTRODUCTION Synaptic modification is widely believed to be the brain's primary mechanism for long-term information storage. The enormous practical and theoretical importance of biological synaptic plasticity has stimulated interest among both experimental neuroscientists and neural network modelers, and has provided strong incentive for the development of computational models that can both explain and predict. We present here a model for the synaptic basis of associative learning in cerebral cortex. The main hypothesis of this work is that",Computational Neuroscience,6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf,1989
Optimal Brain Damage,"We have used information-theoretic ideas to derive a class of prac tical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, sev eral improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative informa tion to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application. 1 INTRODUCTION Most successful applications of neural network learning to real-world problems have been achieved using highly structured networks of rather large size [for example (Waibel, 1989; Le Cun et al., 1990a)]. As applications become more complex, the networks will presumably become even larger and more structured. Design tools and techniques for comparing different architectures and minimizing the network size will be needed. More importantl",Optimization & Theoretical ML,6c9882bbac1c7093bd25041881277658-Paper.pdf,1989
Learning Aspect Graph Representations,"In our effort to develop a modular neural system for invariant learn ing and recognition of 3D objects, we introduce here a new module architecture called an aspect network constructed around adaptive axo-axo-dendritic synapses. This builds upon our existing system (Seibert & Waxman, 1989) which processes 20 shapes and classifies t.hem into view categories (i.e., aspects) invariant to illumination, position, orientat.ion, scale, and projective deformations. From a sequence 'of views, the aspect network learns the transitions be tween these aspects, crystallizing a graph-like structure from an initially amorphous network. Object recognition emerges by ac cumulating evidence over multiple views which activate competing object hypotheses. 1 INTRODUCTION One can ""learn"" a three-dimensional object by exploring it and noticing how its appearance changes. When moving from one view to another, intermediate views are presented. The imagery is continuous, unless some feature of the object appear",Computer Vision,6da9003b743b65f4c0ccd295cc484e57-Paper.pdf,1989
The Effect of Catecholamines on Performance:,"At the level of individual neurons. catecholamine release increases the responsivity of cells to excitatory and inhibitory inputs. We present a model of catecholamine effects in a network of neural-like elements. We argue that changes in the responsivity of individual elements do not affect their ability to detect a signal and ignore noise. However. the same changes in cell responsivity in a network of such elements do improve the signal detection performance of the network as a whole. We show how this result can be used in a computer simulation of behavior to account for the effect of eNS stimulants on the signal detection performance of human subjects. 1 Introduction The catecholamines-norepinephrine and dopamine-are neuroactive substances that are presumed to modulate information processing in the brain, rather than to convey discrete sensory or motor signals. Release of norepinephrine and dopamine occurs over wide areas of the central nervous system. and their post-synaptic effects",Optimization & Theoretical ML,6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf,1989
Meiosis Networks,"A central problem in connectionist modelling is the control of network and architectural resources during learning. In the present approach, weights reflect a coarse prediction history as coded by a distribution of values and parameterized in the mean and standard deviation of these weight distributions. Weight updates are a function of both the mean and standard deviation of each connection in the network and vary as a function of the error signal (""stochastic delta rule""; Hanson, 1990). Consequently, the weights maintain information on their central tendency and their ""uncertainty"" in prediction. Such information is useful in establishing a policy concerning the size of the nodal complexity of the network and growth of new nodes. For example, during problem solving the present network can undergo ""meiosis"", producing two nodes where there was one ""overtaxed"" node as measured by its coefficient of variation. It is shown in a number of benchmark problems that meiosis networks can find ",Deep Learning,705f2172834666788607efbfca35afb3-Paper.pdf,1989
Synergy Of Clustering Multiple Back Propagation Networks,"The properties of a cluster of multiple back-propagation (BP) networks are examined and compared to the performance of a single BP net work. The underlying idea is that a synergistic effect within the cluster improves the perfonnance and fault tolerance. Five networks were ini tially trained to perfonn the same input-output mapping. Following training, a cluster was created by computing an average of the outputs generated by the individual networks. The output of the cluster can be used as the desired output during training by feeding it back to the indi vidual networks. In comparison to a single BP network, a cluster of multiple BP's generalization and significant fault tolerance. It appear that cluster advantage follows from simple maxim ""you can fool some of the single BP's in a cluster all of the time but you cannot fool all of them all of the time"" {Lincoln} 1 INTRODUCTION Shortcomings of back-propagation (BP) in supervised learning has been well docu mented in the past {Soulie, 1",Optimization & Theoretical ML,74db120f0a8e5646ef5a30154e9f6deb-Paper.pdf,1989
Reading a Neural Code,"Traditional methods of studying neural coding characterize the en coding of known stimuli in average neural responses. Organisms face nearly the opposite task - decoding short segments of a spike train to extract information about an unknown, time-varying stim ulus. Here we present strategies for characterizing the neural code from the point of view of the organism, culminating in algorithms for real-time stimulus reconstruction based on a single sample of the spike train. These methods are applied to the design and anal ysis of experiments on an identified movement-sensitive neuron in the fly visual system. As far as we know this is the first instance in which a direct ""reading"" of the neural code has been accomplished. 1 Introduction Sensory systems receive information at extremely high rates, and much of this infor mation must be processed in real time. To understand real-time signal processing in biological systems we must understand the representation of this information in neural",Optimization & Theoretical ML,7a614fd06c325499f1680b9896beedeb-Paper.pdf,1989
A Method for the Associative Storage,"A method for storing analog vectors in Hopfield's continuous feed back model is proposed. By analog vectors we mean vectors whose components are real-valued. The vectors to be stored are set as equilibria of the network. The network model consists of one layer of visible neurons and one layer of hidden neurons. We propose a learning algorithm, which results in adjusting the positions of the equilibria, as well as guaranteeing their stability. Simulation results confirm the effectiveness of the method. 1 INTRODUCTION The associative storage of binary vectors using discrete feedback neural nets has been demonstrated by Hopfield (1982). This has attracted a lot of attention, and a number of alternative techniques using also the discrete feedback model have appeared. However, the problem of the distributed associative storage of analog vectors has received little attention in literature. By analog vectors we mean vec tors whose components are real-valued. This problem is important because ",Optimization & Theoretical ML,7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf,1989
Using Local Models to Control Movement,"This paper explores the use of a model neural network for motor learning. Steinbuch and Taylor presented neural network designs to do nearest neighbor lookup in the early 1960s. In this paper their nearest neighbor network is augmented with a local model network, which fits a local model to a set of nearest neighbors. The network design is equivalent to local regression. This network architecture can represent smooth nonlinear functions, yet has simple training rules with a single global optimum. The network has been used for motor learning of a simulated arm and a simulated running machine. 1 INTRODUCTION A common problem in motor learning is approximating a continuous function from samples of the function's inputs and outputs. This paper explores a neural net work architecture that simply remembers experiences (samples) and builds a local model to answer any particular query (an input for which the function's output is desired). This network design can represent smooth nonlinear func",Deep Learning,839ab46820b524afda05122893c2fe8e-Paper.pdf,1989
Learning to Control an Unstable System with,"The forward modeling approach is a methodology for learning con trol when data is available in distal coordinate systems. We extend previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time. In many learning control problems, the output variables of the controller are not the natural coordinates in which to specify tasks and evaluate performance. Tasks are generally more naturally specified in ""distal"" coordinate systems (e.g., endpoint coordinates for manipulator motion) than in the ""proximal"" coordinate system of the controller (e.g., joint angles or torques). Furthermore, the relationship between proximal coordinates and distal coordinates is often not known a priori and, if known, not easily inverted. The forward modeling approach is a methodology for learning control when train ing data is available in distal coordinate systems. A forward model is a network that learns the transformation fr",Optimization & Theoretical ML,84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf,1989
Incremental Parsing by Modular Recurrent,"We present a novel, modular, recurrent connectionist network architec ture which learns to robustly perform incremental parsing of complex sentences. From sequential input, one word at a time, our networks learn to do semantic role assignment, noun phrase attachment, and clause structure recognition for sentences with passive constructions and center embedded clauses. The networks make syntactic and semantic predictions at every point in time, and previous predictions are revised as expectations are affirmed or violated with the arrival of new informa tion. Our networks induce their own ""grammar rules"" for dynamically transforming an input sequence of words into a syntactic/semantic in terpretation. These networks generalize and display tolerance to input which has been corrupted in ways common in spoken language. 1 INTRODUCTION Previously, we have reported on experiments using connectionist models for a small pars ing task using a new network formalism which extends back-propagation t",NLP,757b505cfd34c64c85ca5b5690ee5293-Paper.pdf,1989
Learning in higher-order' artificial dendritic trees',"If neurons sum up their inputs in a non-linear way, as some simula tions suggest, how is this distributed fine-grained non-linearity ex ploited during learning? How are all the small sigmoids in synapse, spine and dendritic tree lined up in the right areas of their respective input spaces? In this report, I show how an abstract atemporal highly nested tree structure with a quadratic transfer function associated with each branchpoint, can self organise using only a single global reinforcement scalar, to perform binary classification tasks. The pro cedure works well, solving the 6-multiplexer and a difficult phoneme classification task as well as back-propagation does, and faster. Furthermore, it does not calculate an error gradient, but uses a statist ical scheme to build moving models of the reinforcement signal.",Deep Learning,854d6fae5ee42911677c739ee1734486-Paper.pdf,1989
Dynamic Behavior of Constrained,"The learning dynamics of the back-propagation algorithm are in vestigated when complexity constraints are added to the standard Least Mean Square (LMS) cost function. It is shown that loss of generalization performance due to overtraining can be avoided when using such complexity constraints. Furthermore, ""energy,"" hidden representations and weight distributions are observed and compared during learning. An attempt is made at explaining the results in terms of linear and non-linear effects in relation to the gradient descent learning algorithm. 1 INTRODUCTION It is generally admitted that generalization performance of back-propagation net works (Rumelhart, Hinton & Williams, 1986) will depend on the relative size ofthe training data and of the trained network. By analogy to curve-fitting and for theo retical considerations, the generalization performance of the network should de crease as the size of the network and the associated number of degrees of freedom increase (Rumelhart, 1987;",Optimization & Theoretical ML,85d8ce590ad8981ca2c8286f79f59954-Paper.pdf,1989
Designing Application-Specific,"We present a general and systematic method for neural network design based on the genetic algorithm. The technique works in conjunction with network learning rules, addressing aspects of the network's gross architecture, connectivity, and learning rule parameters. Networks can be optimiled for various application specific criteria, such as learning speed, generalilation, robustness and connectivity. The approach is model-independent. We describe a prototype system, NeuroGENESYS, that employs the backpropagation learning rule. Experiments on several small problems have been conducted. In each case, NeuroGENESYS has produced networks that perform significantly better than the randomly generated networks of its initial population. The com putational feasibility of our approach is discussed. 1 INTRODUCTION With the growing interest in the practical use of neural networks, addressing the problem of customiling networks for specific applications is becoming increas ingly critical. It has rep",Optimization & Theoretical ML,8c19f571e251e61cb8dd3612f26d5ecf-Paper.pdf,1989
A Computational Basis for Phonology,"The phonological structure of human languages is intricate, yet highly constrained. Through a combination of connectionist modeling and linguistic analysis, we are attempting to develop a computational basis for the nature of phonology. We present a connectionist architecture that performs multiple simultaneous insertion, deletion, and mutation operations on sequences of phonemes, and introduce a novel additional primitive, clustering. Clustering provides an interesting alternative to both iterative and relaxation accounts of assimilation processes such as vowel harmony. Our resulting model is efficient because it processes utterances entirely in parallel using only feed-forward circuitry. 1 INTRODUCTION Phonological phenomena can be quite complex, but human phonological behavior is also highly constrained. Many operations that are easily learned by a perceptron-like sequence mapping network are excluded from real languages. For example, as Pinker and Prince (1988) point out in their c",NLP,8f121ce07d74717e0b1f21d122e04521-Paper.pdf,1989
Neural Network Simulation,"The brain represents the skin surface as a topographic map in the somatosensory cortex. This map has been shown experimentally to be modifiable in a use-dependent fashion throughout life. We present a neural network simulation of the competitive dynamics underlying this cortical plasticity by detailed analysis of receptive field properties of model neurons during simulations of skin co activation, cortical lesion, digit amputation and nerve section. 1 INTRODUCTION Plasticity of adult somatosensory cortical maps has been demonstrated experimentally in a variety of maps and species (Kass, et al., 1983; Wall, 1988). This report focuses on modelling primary somatosensory cortical plasticity in the adult monkey. We model the long-term consequences of four specific experiments, taken in pairs. With the first pair, behaviorally controlled stimulation of restricted skin surfaces (Jen kins, et al., 1990) and induced cortical lesions (Jenkins and Merzenich, 1987), we demonstrate that Hebbian-typ",Computer Vision,918317b57931b6b7a7d29490fe5ec9f9-Paper.pdf,1989
A Neural Network for Feature Extraction,"The paper suggests a statistical framework for the parameter esti mation problem associated with unsupervised learning in a neural network, leading to an exploratory projection pursuit network that performs feature extraction, or dimensionality reduction. 1 INTRODUCTION The search for a possible presence of some unspecified structure in a high dimen sional space can be difficult due to the curse of dimensionality problem, namely the inherent sparsity of high dimensional spaces. Due to this problem, uniformly accurate estimations for all smooth functions are not possible in high dimensions with practical sample sizes (Cox, 1984, Barron, 1988). Recently, exploratory projection pursuit (PP) has been considered (Jones, 1983) as a potential method for overcoming the curse of dimensionality problem (Huber, 1985), and new algorithms were suggested by Friedman (1987), and by Hall (1988, 1989). The idea is to find low dimensional projections that provide the most revealing views of the full-dim",Deep Learning,9188905e74c28e489b44e954ec0b9bca-Paper.pdf,1989
Generalized Hopfield Networks,"A nonlinear neural framework, called the Generalized Hopfield network, is proposed, which is able to solve in a parallel distributed manner systems of nonlinear equations. The method is applied to the general nonlinear optimization problem. We demonstrate GHNs implementing the three most important optimization algorithms, namely the Augmented Lagrangian, Generalized Reduced Gradient and Successive Quadratic Programming methods. The study results in a dynamic view of the optimization problem and offers a straightforward model for the parallelization of the optimization computations, thus significantly extending the practical limits of problems that can be formulated as an optimization problem and which can gain from the introduction of nonlinearities in their structure (eg. pattern recognition, supervised learning, design of content-addressable memories). 1 To whom correspondence should be addressed. 356 Reklaitis, Tsirukis and Tenorio 1 RELATED WORK The ability of networks of highly in",Optimization & Theoretical ML,92c8c96e4c37100777c7190b76d28233-Paper.pdf,1989
Connectionist Architectures/or Multi-Speaker,"We present a number of Time-Delay Neural Network (TDNN) based architectures for multi-speaker phoneme recognition (/b,d,g/ task). We use speech of two females and four males to compare the performance of the various architectures against a baseline recognition rate of 95.9% for a single IDNN on the six-speaker /b,d,g/ task. This series of modu lar designs leads to a highly modular multi-network architecture capable of performing the six-speaker recognition task at the speaker dependent rate of 98.4%. In addition to its high recognition rate, the so-called ""Meta-Pi"" architecture learns - without direct supervision - to rec ognize the speech of one particular male speaker using internal models of other male speakers exclusively. 1 INTRODUCTION References [1,2] have show the Tune-Delay Neural Network to be an effective classifier of acoustic phonetic speech from individual speakers. The objective of this research has been to extend the TDNN paradigm to the multi-speaker phoneme recognitio",NLP,979d472a84804b9f647bc185a877a8b5-Paper.pdf,1989
Bayesian Inference of Regular Grammar,"In this paper we develop a Bayes criterion which includes the Rissanen complexity, for inferring regular grammar models. We develop two methods for regular grammar Bayesian inference. The fIrst method is based on treating the regular grammar as a I-dimensional Markov source, and the second is based on the combinatoric characteristics of the regular grammar itself. We apply the resulting Bayes criteria to a particular example in order to show the efficiency of each method. 1 MOTIVATION We are interested in segmenting electron-microscope autoradiography (EMA) images by learning representational models for the textures found in the EMA image. In studying this problem, we have recognized that both structural and statistical features may be useful for characterizing textures. This has motivated us to study the source modeling problem for both structural sources and statistical sources. The statistical sources that we have examined are the class of one and two-dimensional Markov sources (see",NLP,9b04d152845ec0a378394003c96da594-Paper.pdf,1989
Dataflow Architectures:,"Dataflow architectures are general computation engines optimized for the execution of fme-grain parallel algorithms. Neural networks can be simulated on these systems with certain advantages. In this paper, we review dataflow architectures, examine neural network simulation performance on a new generation dataflow machine, compare that performance to other simulation alternatives, and discuss the benefits and drawbacks of the dataflow approach. 1 DATAFLOW ARCHITECTURES Dataflow research has been conducted at MIT (Arvind & Culler, 1986) and elsewhere (Hiraki, et. aI., 1987) for a number of years. Dataflow architectures are general computation engines that treat each instruction of a program as a separate task which is scheduled in an asynchronous, data-driven fashion. Dataflow programs are compiled into graphs which explicitly describe the data dependencies of the computation. These graphs are directly executed by the machine. Computations which are not linked by a path in the graphs ca",Computer Vision,9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf,1989
The Perceptron Algorithm Is Fast tor,"algorithm is shown to learn an arbitrary half-space in time O(r;;) if D, the proba sn. bility distribution of examples, is taken uniform over the unit sphere Here is f the accuracy parameter. This is surprisingly fast, as ""standard"" approaches involve 7') solution of a linear programming problem involving O( constraints in n dimen sions. A modification of Valiant's distribution independent protocol for learning is proposed in which the distribution and the function to be learned may be cho sen by adversaries, however these adversaries may not communicate. It is argued that this definition is more reasonable and applicable to real world learning than Valiant's. Under this definition, the Perceptron algorithm is shown to be a distri bution independent learning algorithm. In an appendix we show that, for uniform distributions, some classes of infinite V-C dimension including convex sets and a class of nested differences of convex sets are learnable. §1: Introduction The Percept ron algori",Optimization & Theoretical ML,9cfdf10e8fc047a44b08ed031e1f0ed1-Paper.pdf,1989
Discovering high order features with mean field,"A new form of the deterministic Boltzmann machine (DBM) learn ing procedure is presented which can efficiently train network mod ules to discriminate between input vectors according to some cri terion. The new technique directly utilizes the free energy of these ""mean field modules"" to represent the probability that the criterion is met, the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learn ing fails to extract the higher order feature of shift at a network bottleneck, combining the new mean field modules with the mu tual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision. 1 INTRODUCTION The Boltzmann machine learning procedure (Hinton and Sejnowski, 1986) can be made much more efficient by using a mean field approximation in which stochastic binary units are replaced by deterministic real-valued units (Peterson and Ander",Deep Learning,a4f23670e1833f3fdb077ca70bbd5d66-Paper.pdf,1989
Sequential Decision Problems,"Decision making tasks that involve delayed consequences are very common yet difficult to address with supervised learning methods. If there is an accurate model of the underlying dynamical system, then these tasks can be formulated as sequential decision problems and solved by Dynamic Programming. This paper discusses rein forcement learning in terms of the sequential decision framework and shows how a learning algorithm similar to the one implemented by the Adaptive Critic Element used in the pole-balancer of Barto, Sutton, and Anderson (1983), and further developed by Sutton (1984), fits into this framework. Adaptive neural networks can play significant roles as modules for approximating the functions required for solving sequential decision problems. 1 INTRODUCTION Most neural network research on learning assumes the existence of a supervisor or teacher knowledgeable enough to supply desired, or target, network outputs during training. These network learning algorithms are function ",Reinforcement Learning,a597e50502f5ff68e3e25b9114205d4a-Paper.pdf,1989
Asymptotic Convergence of Backpropagation:,"We have calculated, both analytically and in simulations, the rate of convergence at long times in the backpropagation learning al gorithm for networks with and without hidden units. Our basic finding for units using the standard sigmoid transfer function is lit convergence of the error for large t, with at most logarithmic cor rections for networks with hidden units. Other transfer functions may lead to a 8lower polynomial rate of convergence. Our analytic calculations were presented in (Tesauro, He & Ahamd, 1989). Here we focus in more detail on our empirical measurements of the con vergence rate in numerical simulations, which confirm our analytic results. 1 INTRODUCTION Backpropagation is a popular learning algorithm for multilayer neural networks which minimizes a global error function by gradient descent (Werbos, 1974: Parker, 1985; LeCun, 1985; Rumelhart, Hinton & Williams, 1986). In this paper, we ex amine the rate of convergence of backpropagation late in learning when all of ",Optimization & Theoretical ML,ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf,1989
Training Connectionist Networks with,"""Selective sampling"" is a form of directed search that can greatly increase the ability of a connectionist network to generalize accu rately. Based on information from previous batches of samples, a network may be trained on data selectively sampled from regions in the domain that are unknown. This is realizable in cases when the distribution is known, or when the cost of drawing points from the target distribution is negligible compared to the cost of label ing them with the proper classification. The approach is justified by its applicability to the problem of training a network for power system security analysis. The benefits of selective sampling are studied analytically, and the results are confirmed experimentally. 1 Introduction: Random Sampling vs. Directed Search A great deal of attention has been applied to the problem of generalization based on random samples drawn from a distribution, frequently referred to as ""learning from examples."" Many natural learning learning systems",Optimization & Theoretical ML,b1a59b315fc9a3002ce38bbe070ec3f5-Paper.pdf,1989
Unsupervised Learning in Neurodynamics Using,"A new concept for unsupervised learning based upon examples in troduced to the neural network is proposed. Each example is con sidered as an interpolation node of the velocity field in the phase space. The velocities at these nodes are selected such that all the streamlines converge to an attracting set imbedded in the subspace occupied by the cluster of examples. The synaptic interconnections are found from learning procedure providing selected field. The theory is illustrated by examples. This paper is devoted to development of a new concept for unsupervised learning based upon examples introduced to an artificial neural network. The neural network is considered as an adaptive nonlinear dissipative dynamical system described by the following coupled differential equations: N L + = 11j g( + i=I,2, ... ,N (I) Ui K,Ui Uj ) Ii j=1 in which is an N-dimensional vector, function of time, representing the neuron U activity, T is a constant matrix whose elements represent synaptic interconnec",Optimization & Theoretical ML,b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf,1989
A Continuous Speech Recognition System,We are developing a phoneme based. speaker-dependent continuous speech recognition system embedding a Multilayer Perceptron (MLP) (Le .• a feedforward Artificial Neural Network). into a Hidden Markov Model (HMM) approach. In [Bourlard & Wellekens]. it was shown that MLPs were approximating Maximum a Posteriori (MAP) probabilities and could thus be embedded as an emission probability estimator in HMMs. By using contextual information from a sliding window on the input frames. we have been able to improve frame or phoneme clas sification performance over the corresponding performance for Simple Maximum Likelihood (ML) or even MAP probabilities that are esti mated without the benefit of context. However. recognition of words in continuous speech was not so simply improved by the use of an MLP. and several modifications of the original scheme were necessary for getting acceptable performance. It is shown here that word recognition performance for a simple discrete density HMM system appear,NLP,bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf,1989
Analysis of Linsker's Simulations,"Linsker has reported the development of centre---surround receptive fields and oriented receptive fields in simulations of a Hebb-type equation in a linear network. The dynamics of the learning rule are analysed in terms of the eigenvectors of the covariance matrix of cell activities. Analytic and computational results for Linsker's covariance matrices, and some general theorems, lead to an expla nation of the emergence of centre---surround and certain oriented structures. Linsker [Linsker, 1986, Linsker, 1988] has studied by simulation the evolution of weight vectors under a Hebb-type teacherless learning rule in a feed-forward linear network. The equation for the evolution of the weight vector w of a single neuron, derived by ensemble averaging the Hebbian rule over the statistics of the input patterns, is:! a at Wi = k! + L(Qij + k2)wj subject to -Wmax ~ Wi < Wmax (1) j lOur definition of equation I differs from Linsker's by the omission of a factor of liN before the sum term, where",Optimization & Theoretical ML,bd686fd640be98efaae0091fa301e613-Paper.pdf,1989
Analog Neural Networks of Limited Precision I:,"Experimental evidence has shown analog neural networks to be ex ~mely fault-tolerant; in particular. their performance does not ap pear to be significantly impaired when precision is limited. Analog neurons with limited precision essentially compute k-ary weighted multilinear threshold functions. which divide R"" into k regions with k-l hyperplanes. The behaviour of k-ary neural networks is investi gated. There is no canonical set of threshold values for k>3. although they exist for binary and ternary neural networks. The » weights can be made integers of only 0 «z +k ) log (z +k bits. where z is the number of processors. without increasing hardware or run ning time. The weights can be made ±1 while increasing running time by a constant multiple and hardware by a small polynomial in z and k. Binary neurons can be used if the running time is allowed to increase by a larger constant multiple and the hardware is allowed to increase by a slightly larger polynomial in z and k. Any symmetric ",Computer Vision,be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf,1989
A Systematic Study of the Input/Output Properties,"The input/output properties of a 2 compartment model neuron are systematically explored. Taken from the work of MacGregor (MacGregor, 1987), the model neuron compartments contain several active conductances, including a potassium conductance in the dendritic compartment driven by the accumulation of intradendritic calcium. Dynamics of the conductances and potentials are governed by a set of coupled first order differential equations which are integrated numerically. There are a set of 17 internal parameters to this model, specificying conductance rate constants, time constants, thresholds, etc. To study parameter sensitivity, a set of trials were run in which the input driving the neuron is kept fixed while each internal parameter is varied with all others left fixed. To study the input/output relation, the input to the dendrite (a square wave) was varied (in frequency and magnitude) while all internal parameters of the system were left flXed, and the resulting output firing rate and b",Optimization & Theoretical ML,c0e190d8267e36708f955d7ab048990d-Paper.pdf,1989
Subgrouping Reduces Complexity and Speeds Up,No abstract found,Optimization & Theoretical ML,c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf,1989
Neural Implementation of Motivated Behavior:,"Most complex behaviors appear to be governed by internal moti vational states or drives that modify an animal's responses to its environment. It is therefore of considerable interest to understand the neural basis of these motivational states. Drawing upon work on the neural basis of feeding in the marine mollusc Aplysia, we have developed a heterogeneous artificial neural network for con trolling the feeding behavior of a simulated insect. We demonstrate that feeding in this artificial insect shares many characteristics with the motivated behavior of natural animals. 1 INTRODUCTION While an animal's external environment certainly plays an extremely important role in shaping its actions, the behavior of even simpler animals is by no means solely reactive. The response of an animal to food, for example, cannot be explained only in terms of the physical stimuli involved. On two different occasions, the very same animal may behave in completely different ways when presented with seemingly",Reinforcement Learning,c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf,1989
A Neural Network to Detect,No abstract found,Computer Vision,ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf,1989
HIGHER ORDER RECURRENT NETWORKS,"A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars. When an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory, the combination can be interpreted as a neural net pushdown automata. The neural net finite state machine is given the primitives, push and POP. and is able to read the top of the stack. Through a gradient descent learning rule derived from the common error function, the hybrid network learns to effectively use the stack actions to manipUlate the stack memory and to learn simple context free grammars. INTRODUCTION Biological networks readily and easily process temporal information; artificial neural networks should do the same. Recurrent neural network models permit the encoding and learning of temporal sequences. There are many recurrent neural net models. for ex ample see [Jordan 1986. Pineda 1987, Williams & Zip",NLP,cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf,1989
Practical Characteristics of Neural Network,"Eight neural net and conventional pattern classifiers (Bayesian unimodal Gaussian, k-nearest neighbor, standard back-propagation, adaptive-stepsize back-propagation, hypersphere, feature-map, learn ing vector quantizer, and binary decision tree) were implemented on a serial computer and compared using two speech recognition and two artificial tasks. Error rates were statistically equivalent on almost all tasks, but classifiers differed by orders of magnitude in memory requirements, training time, classification time, and ease of adaptivity. Nearest-neighbor classifiers trained rapidly but re quired the most memory. Tree classifiers provided rapid classifica tion but were complex to adapt. Back-propagation classifiers typ ically required long training times and had intermediate memory requirements. These results suggest that classifier selection should often depend more heavily on practical considerations concerning memory and computation resources, and restrictions on training and clas",Deep Learning,cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf,1989
Contour-Map Encoding of Shape for Early Vision,"Contour maps provide a general method for recognizing two-dimensional shapes. All but blank images give rise to such maps, and people are good at recognizing objects and shapes from them. The maps are encoded easily in long feature vectors that are suitable for recognition by an associative memory. These properties of contour maps suggest a role for them in early visual perception. The prevalence of direction-sensitive neurons in the visual cortex of mammals supports this view. INTRODUCTION Early vision refers here to the first stages of visual perception of an experienced (adult human) observer. Overall, visual perception results in the identification of what is being viewed: We recognize an image as the letter A because it looks to us like other As we have seen. Early vision is the beginning of this process of identification- the making of the first guess. Early vision cannot be based on special or salient features. For example, we normally think of the letter A as being composed of ",Computer Vision,cfecdb276f634854f3ef915e2e980c31-Paper.pdf,1989
Maximum Likelihood Competitive Learning,"One popular class of unsupervised algorithms are competitive algo rithms. In the traditional view of competition, only one competitor, the winner, adapts for any given case. I propose to view compet itive adaptation as attempting to fit a blend of simple probability generators (such as gaussians) to a set of data-points. The maxi mum likelihood fit of a model of this type suggests a ""softer"" form of competition, in which all competitors adapt in proportion to the relative probability that the input came from each competitor. I investigate one application of the soft competitive model, place ment of radial basis function centers for function interpolation, and show that the soft model can give better performance with little additional computational cost. 1 INTRODUCTION Interest in unsupervised learning has increased recently due to the application of more sophisticated mathematical tools (Linsker, 1988; Plumbley and Fallside, 1988; Sanger, 1989) and the success of several elegant simula",Computer Vision,d1c38a09acc34845c6be3a127a5aacaf-Paper.pdf,1989
Note on Development of Modularity,"The existence of modularity in the organization of nervous systems (e.g. cortical columns and olfactory glomeruli) is well known. We show that localized activity patterns in a layer of cells, collective excitations, can induce the formation of modular structures in the anatomical connections via a Hebbian learning mechanism. The networks are spatially homogeneous before learning, but the spon taneous emergence of localized collective excitations and subse quently modularity in the connection patterns breaks translational symmetry. This spontaneous symmetry breaking phenomenon is similar to those which drive pattern formation in reaction-diffusion systems. We have identified requirements on the patterns of lateral connections and on the gains of internal units which are essential for the development of modularity. These essential requirements will most likely remain operative when more complicated (and bi ologically realistic) models are considered. 1 Present Address: Molecular and Cell",Optimization & Theoretical ML,d395771085aab05244a4fb8fd91bf4ee-Paper.pdf,1989
Model Based Image Compression and,"To achieve high-rate image data compression while maintainig a high quality reconstructed image, a good image model and an efficient way to represent the specific data of each image must be introduced. Based on the physiological knowledge of multi -channel characteristics and inhibitory interactions between them in the human visual system, a mathematically coherent parallel architecture for image data compression which utilizes the Markov random field Image model and interactions between a vast number of filter banks, is proposed.",Computer Vision,d6baf65e0b240ce177cf70da146c8dc8-Paper.pdf,1989
On the Distribution of the Number of Local,No abstract found,Optimization & Theoretical ML,d947bf06a885db0d477d707121934ff8-Paper.pdf,1989
Using A Translation-Invariant Neural Network,Distinctive electrocardiogram (EeG) patterns are created when the heart is beating normally and when a dangerous arrhythmia is present. Some devices which monitor the EeG and react to arrhythmias parameterize the ECG signal and make a diagnosis based on the parameters. The author discusses the use of a neural network to classify the EeG signals directly. without parameterization. The input to such a network must be translation-invariant. since the distinctive features of the EeG may appear anywhere in an arbritrarily-chosen EeG segment. The input must also be insensitive to the episode-to-episode and patient-to-patient variability in the rhythm pattern. 1 INTRODUCTION Figure 1 shows internally-recorded transcardiac ECG signals for one patient. The top trace is an example of normal sinus rhythm (NSR). The others are examples of two arrhythmias: ventricular tachycardia (V1) and ventricular fibrillation (VF). Visually. the patterns are quite distinctive. Two problems make recognition of t,NLP,d96409bf894217686ba124d7356686c9-Paper.pdf,1989
Non-Boltzmann Dynamics in Networks of,"We study networks of spiking neurons in which spikes are fired as a Poisson process. The state of a cell is determined by the instan taneous firing rate, and in the limit of high firing rates our model reduces to that studied by Hopfield. We find that the inclusion of spiking results in several new features, such as a noise-induced asymmetry between ""on"" and ""off"" states of the cells and probabil ity currents which destroy the usual description of network dynam ics in terms of energy surfaces. Taking account of spikes also al lows us to calibrate network parameters such as ""synaptic weights"" against experiments on real synapses. Realistic forms of the post synaptic response alters the network dynamics, which suggests a novel dynamical learning mechanism. 1 INTRODUCTION In 1943 McCulloch and Pitts introduced the concept of two-state (binary) neurons as elementary building blocks for neural computation. They showed that essentially any finite calculation can be done using these simple de",Optimization & Theoretical ML,db8e1af0cb3aca1ae2d0018624204529-Paper.pdf,1989
The 'Moving Targets' Training Algorithm,"A simple method for training the dynamical behavior of a neu ral network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past. 1 INTRODUCTION This paper presents a minimization-based algorithm for training the dynamical be havior of a discrete-time neural network model. The central idea is to treat hidden nodes as target nodes with variable training data. These ""moving targets"" are varied during the minimization process. Werbos (Werbos, 1983) used the term ""moving targets"" to describe the qualitative idea that a network should set itself intermediate objective",Reinforcement Learning,e165421110ba03099a1c0393373c5b43-Paper.pdf,1989
Performance Comparisons Between,"Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbi trary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees. Performance Comparisons 623 1 INTRODUCTION In this paper we compare regression and classification systems. A regression system can generate an output f for an input X, where both X and f are continuous a",Optimization & Theoretical ML,e2c0be24560d78c5e599c2a9c9d0bbd2-Paper.pdf,1989
A n Efficient Implementation of,"In this paper, we present a novel implementation of the widely used Back-propagation neural net learning algorithm on the Connection Machine CM-2 - a general purpose, massively parallel computer with a hypercube topology. This implementation runs at about 180 million interconnections per second (IPS) on a 64K processor CM-",Optimization & Theoretical ML,e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf,1989
Algorithms/or Better Representation and,"In this paper we present upper bounds for the learning rates for hybrid models that employ a combination of both self-organized and supervised learning, using radial basis functions to build receptive field representations in the hidden units. The learning performance in such networks with nearest neighbor heuristic can be improved upon by multiplying the individual receptive field widths by a suitable overlap factor. We present results indicat!ng optimal values for such overlap factors. We also present a new algorithm for determining receptive field centers. This method negotiates more hidden units in the regions of the input space as a function of the output and is conducive to better learning when the number of patterns (hidden units) is small. 1 INTRODUCTION Functional approximation of experimental data ongmating from a continuous dynamical process is an important problem. Data is usually available in the form of a set S consisting of {x,y} pairs, where x is a input vector and y is",Optimization & Theoretical ML,e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf,1989
The Cocktail Party Problem:,"This work introduces a new method called Self Organizing Neural Network (SONN) algorithm and compares its performance with Back Propagation in a signal separation application. The problem is to separate two signals; a modem data signal and a male speech signal, added and transmitted through a 4 khz channel. The signals are sam pled at 8 khz, and using supervised learning, an attempt is made to reconstruct them. The SONN is an algorithm that constructs its own network topology during training, which is shown to be much smaller than the BP network, faster to trained, and free from the trial-and error network design that characterize BP.",Optimization & Theoretical ML,e56954b4f6347e897f954495eab16a88-Paper.pdf,1989
Real- Time Computer Vision and Robotics,"The long-term goal of our laboratory is the development of analog resistive network-based VLSI implementations of early and inter mediate vision algorithms. We demonstrate an experimental cir cuit for smoothing and segmenting noisy and sparse depth data using the resistive fuse and a 1-D edge-detection circuit for com puting zero-crossings using two resistive grids with different space constants. To demonstrate the robustness of our algorithms and of the fabricated analog CMOS VLSI chips, we are mounting these circuits onto small mobile vehicles operating in a real-time, labo ratory environment. 1 INTRODUCTION A large number of computer vision algorithms for finding intensity edges, comput ing motion, depth, and color, and recovering the 3-D shapes of objects have been developed within the framework of minimizing an associated ""energy"" functional. Such a variational formalism is attractive because it allows a priori constraints to be explicitly stated. The single most important constra",Computer Vision,e96ed478dab8595a7dbda4cbcbee168f-Paper.pdf,1989
The CHIR Algorithm for Feed Forward,"A new learning algorithm, Learning by Choice of Internal Rep resetations (CHIR), was recently introduced. Whereas many algo rithms reduce the learning process to minimizing a cost function over the weights, our method treats the internal representations as the fundamental entities to be determined. The algorithm applies a search procedure in the space of internal representations, and a cooperative adaptation of the weights (e.g. by using the perceptron learning rule). Since the introduction of its basic, single output ver sion, the CHIR algorithm was generalized to train any feed forward network of binary neurons. Here we present the generalised version of the CHIR algorithm, and further demonstrate its versatility by describing how it can be modified in order to train networks with binary (±1) weights. Preliminary tests of this binary version on the random teacher problem are also reported. I. INTRODUCTION Learning by Choice oflnternal Representations (CHIR) was recently introduced [1",Optimization & Theoretical ML,eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf,1989
A LARGE-SCALE NEURAL NETWORK,"We propose a new way to construct a large-scale neural network for 3.000 handwritten Kanji characters recognition. This neural network consists of 3 parts: a collection of small-scale networks which are trained individually on a small number of Kanji characters; a network which integrates the output from the small-scale networks, and a process to facilitate the integration of these neworks. The recognition rate of the total system is comparable with those of the small-scale networks. Our results indicate that the proposed method is effective for constructing a large-scale network without loss of recognition performance. 1 INTRODUCTION Neural networks have been applied to recognition tasks in many fields. with good results [Denker, 1988][Mori,1988][Weideman, 1989]. They have performed better than conventional methods. However these networks currently operate with only a few categories, about 20 to 30. The Japanese writing system at present is composed of about 3,000 characters. For a ne",Computer Vision,eb163727917cbba1eea208541a643e74-Paper.pdf,1989
MECHANISMS FOR NEUROMODULATION,"The pyloric Central Pattern Generator of the crustacean stomatogastric ganglion is a well-defined biological neural network. This 14-neuron network is modulated by many inputs. These inputs reconfigure the network to produce multiple output patterns by three simple mechanisms: 1) detennining which cells are active; 2) modulating the synaptic efficacy; 3) changing the intrinsic response properties of individual neurons. The importance of modifiable intrinsic response properties of neurons for network function and modulation is discussed. 1 INTRODUCTION Many neural network models aim to understand how a particular process is accomplished by a unique network in the nervous system. Most studies have aimed at circuits for learning or sensory processing; unfortunately, almost no biological data are available on the actual anatomical structure of neural networks serving these tasks, so the accuracy of the theoretical models is unknown. Much more is known concerning the structure and function ",Optimization & Theoretical ML,ec8ce6abb3e952a85b8551ba726a1227-Paper.pdf,1989
Recognizing Hand-Printed Letters and Digits,"We are developing a hand-printed character recognition system using a multi layered neural net trained through backpropagation. We report on results of training nets with samples of hand-printed digits scanned off of bank checks and hand-printed letters interactively entered into a computer through a sty lus digitizer. Given a large training set, and a net with sufficient capacity to achieve high performance on the training set, nets typically achieved error rates of 4-5% at a 0% reject rate and 1-2% at a 10% reject rate. The topology and capacity of the system, as measured by the number of connections in the net, have surprisingly little effect on generalization. For those developing practical pattern recognition systems, these results suggest that a large and representative training sample may be the single, most important factor in achieving high recognition accuracy. From a scientific standpoint, these re sults raise doubts about the relevance to backpropagation of learning models ",Computer Vision,eda80a3d5b344bc40f3bc04f65b7a357-Paper.pdf,1989
TRAFFIC: Recognizing Objects Using,"We describe a model that can recognize two-dimensional shapes in an unsegmented image, independent of their orientation, position, and scale. The model, called TRAFFIC, efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations, with increasing complexity of features at each successive layer, the network can recognize multiple objects in parallel. An implemen tation of TRAFFIC is described, along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner. 1 INTRODUCTION A key goal of machine vision is to recognize familiar objects in an unsegmented image, independent of their orientation, position, and scale. Massively parallel models have long been used for lower-level vision tasks, s",Computer Vision,f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf,1989
Performance of Connectionist Learning Algorithms,"The mapping of the back-propagation and mean field theory learning algorithms onto a generic 2-D SIMD computer is described. This architecture proves to be very adequate for these applications since efficiencies close to the optimum can be attained. Expressions to find the learning rates are given and then particularized to the DAP array procesor. 1 INTRODUCTION The digital simulation of connectionist learning algorithms is flexible and accurate. However, with the exception of very small networks, conventional computer architectures spend a lot of time in the execution of simulation software. Parallel computers can be used to reduce the execution time. Vector pipelined, multiprocessors, and array processors are some of the most important classes of parallel computers3. Connectionist or neural net (NN) learning algorithms have been mapped onto all of them. The focus of this contribution is on the mapping of the back-propagation (BP) and mean field theory (MFT) learning algorithms onto t",Optimization & Theoretical ML,f718499c1c8cef6730f9fd03c8125cab-Paper.pdf,1989
Neurally Inspired Plasticity in Oculomotor,"We have constructed a two axis camera positioning system which is roughly analogous to a single human eye. This Artificial-Eye (A eye) combines the signals generated by two rate gyroscopes with motion information extracted from visual analysis to stabilize its camera. This stabilization process is similar to the vestibulo-ocular response (VOR); like the VOR, A-eye learns a system model that can be incrementally modified to adapt to changes in its structure, performance and environment. A-eye is an example of a robust sen sory system that performs computations that can be of significant use to the designers of mobile robots. 1 Introduction We have constructed an ""artificial eye"" (A-eye), an autonomous robot that incorpo rates a two axis camera positioning system (figure 1). Like a the human oculomotor system, A-eye can estimate the rotation rate of its body with a gyroscope and esti mate the rotation rate of its ""eye"" by measuring image slip acr~ its ""retina"". Using the gyroscope to sen",Computer Vision,f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf,1989
Combining Visual and Acoustic Speech Signals,"Acoustic speech recognition degrades in the presence of noise. Com pensatory information is available from the visual speech signals around the speaker's mouth. Previous attempts at using these visual speech signals to improve automatic speech recognition sys tems have combined the acoustic and visual speech information at a symbolic level using heuristic rules. In this paper, we demonstrate an alternative approach to fusing the visual and acoustic speech information by training feedforward neural networks to map the visual signal onto the corresponding short-term spectral amplitude envelope (STSAE) of the acoustic signal. This information can be directly combined with the degraded acoustic STSAE. Signif icant improvements are demonstrated in vowel recognition from noise-degraded acoustic signals. These results are compared to the performance of humans, as well as other pattern matching and es timation algorithms. 1 INTRODUCTION Current automatic speech recognition systems rely almost ",Computer Vision,f90f2aca5c640289d0a29417bcb63a37-Paper.pdf,1989
The Computation of Sound Source Elevation,"The midbrain of the barn owl contains a map-like representation of sound source direction which is used to precisely orient the head to ward targets of interest. Elevation is computed from the interaural difference in sound level. We present models and computer simula tions of two stages of level difference processing which qualitatively agree with known anatomy and physiology, and make several strik ing predictions. 1 INTRODUCTION The auditory system of the barn owl constructs a map of sound direction in the external nucleus of the inferior colliculus (lex) after several stages of processing the output of the cochlea. This representation of space enables the owl to orient its head to sounds with an accuracy greater than any other tested land animal [Knudsen, et aI, 1979]. Elevation and azimuth are processed in separate streams before being merged in the ICx [Konishi, 1986]. Much of this processing is done with neuronal maps, regions of tissue in which the position of active neurons va",Computer Vision,fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf,1989
Modeling Time Varying Systems,"Multi-layered neural networks have recently been proposed for non linear prediction and system modeling. Although proven successful for modeling time invariant nonlinear systems, the inability of neural networks to characterize temporal variability has so far been an obstacle in applying them to complicated non stationary signals, such as speech. In this paper we present a network architecture, called ""Hidden Control Neural Network"" (HCNN), for modeling signals generated by nonlinear dynamical systems with restricted time variability. The approach taken here is to allow the mapping that is implemented by a multi layered neural network to change with time as a function of an additional control input signal. This network is trained using an algorithm that is based on ""back-propagation"" and segmentation algorithms for estimating the unknown control together with the network's parameters. The HCNN approach was applied to several tasks including modeling of time-varying nonlinear systems an",Deep Learning,00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf,1990
Using Genetic Algorithms to Improve,"Genetic algorithms were used to select and create features and to select reference exemplar patterns for machine vision and speech pattern classi fication tasks. For a complex speech recognition task, genetic algorithms required no more computation time than traditional approaches to feature selection but reduced the number of input features required by a factor of five (from 153 to 33 features). On a difficult artificial machine-vision task, genetic algorithms were able to create new features (polynomial functions of the original features) which reduced classification error rates from 19% to almost 0%. Neural net and k nearest neighbor (KNN) classifiers were unable to provide such low error rates using only the original features. Ge netic algorithms were also used to reduce the number of reference exemplar patterns for a KNN classifier. On a 338 training pattern vowel-recognition problem with 10 classes, genetic algorithms reduced the number of stored exemplars from 338 to 43 without ",Optimization & Theoretical ML,00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf,1990
Generalization Dynamics in,"For a simple linear case, a mathematical analysis of the training and gener alization (validation) performance of networks trained by gradient descent on a Least Mean Square cost function is provided as a function of the learn ing parameters and of the statistics of the training data base. The analysis predicts that generalization error dynamics are very dependent on a pri ori initial weights. In particular, the generalization error might sometimes weave within a computable range during extended training. In some cases, the analysis provides bounds on the optimal number of training cycles for minimal validation error. For a speech labeling task, predicted weaving effects were qualitatively tested and observed by computer simulations in networks trained by the linear and non-linear back-propagation algorithm. 1 INTRODUCTION Recent progress in network design demonstrates that non-linear feedforward neu ral networks can perform impressive pattern classification for a variety of real-world",Optimization & Theoretical ML,01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf,1990
Optimal Filtering in the Salamander Retina,"The dark-adapted visual system can count photons wit h a reliability lim ited by thermal noise in the rod photoreceptors - the processing circuitry bet.ween t.he rod cells and the brain is essentially noiseless and in fact may be close to optimal. Here we design an optimal signal processor which estimates the time-varying light intensit.y at the retina based on the rod signals. \Ve show that. the first stage of optimal signal processing involves passing the rod cell out.put. t.hrough a linear filter with characteristics de termined entirely by the rod signal and noise spectra. This filter is very general; in fact it. is the first st.age in any visual signal processing task at. photon flux. \Ve iopntify the output of this first-st.age filter wit.h 10\\' the intracellular voltage response of the bipolar celL the first anatomical st.age in retinal signal processing. From recent. data on tiger salamander phot.oreceptors we extract t.he relevant. spect.ra and make parameter-free, quantit.at",NLP,019d385eb67632a7e958e23f24bd07d7-Paper.pdf,1990
Oscillation Onset,"This paper studies dynamical aspects of neural systems with delayed neg ative feedback modelled by nonlinear delay-differential equations. These systems undergo a Hopf bifurcation from a stable fixed point to a sta ble limit cycle oscillation as certain parameters are varied. It is shown that their frequency of oscillation is robust to parameter variations and noisy fluctuations, a property that makes these systems good candidates for pacemakers. The onset of oscillation is postponed by both additive and parametric noise in the sense that the state variable spends more time near the fixed point than it would in the absence of noise. This is also the case when noise affects the delayed variable, i.e. when the system has a faulty memory. Finally, it is shown that a distribution of delays (rather than a fixed delay) also stabilizes the fixed point solution. 1 INTRODUCTION In this paper, we study the dynamics of a class of neural delayed feedback models which have been used to understand e",Optimization & Theoretical ML,04025959b191f8f9de3f924f0940515f-Paper.pdf,1990
Relaxation Networks for Large Supervised Learning Problems,Feedback connections are required so that the teacher signal on the output neurons can modify weights during supervised learning. Relaxation methods are needed for learning static patterns with full-time feedback connections. Feedback network learning techniques have not achieved wide popularity because of the still greater computational efficiency of back-propagation. We show by simulation that relaxation networks of the kind we are implementing in VLSI are capable of learning large problems just like back-propagation networks. A microchip incorporates deterministic mean-field theory learning as well as stochastic Boltzmann learning. A multiple-chip electronic system implementing these networks will make high-speed parallel learning in them feasible in the future.,Optimization & Theoretical ML,05049e90fa4f5039a8cadc6acbb4b2cc-Paper.pdf,1990
Neural Networks Structured for Control,"We present a generic neural network architecture capable of con trolling non-linear plants. The network is composed of dynamic. parallel, linear maps gated by non-linear switches. Using a recur rent form of the back-propagation algorithm, control is achieved by optimizing the control gains and task-adapted switch parame ters. A mean quadratic cost function computed across a nominal plant trajectory is minimized along with performance constraint penalties. The approach is demonstrated for a control task con sisting of landing a commercial aircraft in difficult wind conditions. We show that the network yields excellent performance while re maining within acceptable damping response constraints. 1 INTRODUCTION This paper illustrates how a recurrent back-propagation neural network algorithm (Rumelhart, Hinton & Williams, 1986) may be exploited as a procedure for con trolling complex systems. In particular. a simplified mathematical model of an aircraft landing in the presence of severe win",Reinforcement Learning,0584ce565c824b7b7f50282d9a19945b-Paper.pdf,1990
Closed-Form Inversion of Backpropagation,"We describe a closed-form technique for mapping the output of a trained backpropagation network int.o input activity space. The mapping is an in verse mapping in the sense that, when the image of the mapping in input activity space is propagat.ed forward through the normal network dynam ics, it reproduces the output used to generate that image. When more than one such inverse mappings exist, our inverse ma.pping is special in that it has no projection onto the nullspace of the activation flow opera tor for the entire network. An important by-product of our calculation, when more than one invel'se mappings exist, is an orthogonal basis set of a significant portion of the activation flow operator nullspace. This basis set can be used to obtain an alternate inverse mapping that is optimized for a particular rea.l-world application. 1 Overview This paper describes a closed-form technique for mapping a particular output of a trained backpropagation net.work int.o input activity space. The m",Optimization & Theoretical ML,06eb61b839a0cefee4967c67ccb099dc-Paper.pdf,1990
Dynamics of Generalization in Linear Perceptrons,"We study the evolution of the generalization ability of a simple linear per ceptron with N inputs which learns to imitate a ""teacher perceptron"". The system is trained on p = aN binary example inputs and the generaliza tion ability measured by testing for agreement with the teacher on all 2N possible binary input patterns. The dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at a = 1. Except at this point the generalization ability approaches its asymptotic value exponentially, with critical slowing down near the tran sition; the relaxation time is ex (1 - y'a)-2. Right at the critical point, 1 the approach to perfect generalization follows a power law ex t -'2. In the presence of noise, the generalization ability is degraded by an amount (va - ex 1)-1 just above a = 1. 1 INTRODUCTION It is very important in practical situations to know how well a neural network will generalize from the examples it is trained on to the entire ",Optimization & Theoretical ML,0bb4aec1710521c12ee76289d9440817-Paper.pdf,1990
Translating Locative Prepositions,"A network was trained by back propagation to map locative expressions of the form ""noun-preposition-noun"" to a semantic representation, as in Cosic and Munro (1988). The network's performance was analyzed over several simulations with training sets in both English and German. Translation of prepositions was attempted by presenting a locative expression to a network trained in one language to generate a semantic representation; the semantic representation was then presented to the network trained in the other language to generate the appropriate preposition. 1 INTRODUCTION Connectionist approaches have enjoyed success, relative to competing frameworks, in accounting for context sensitivity and have become an attractive approach to NLP. An ar chitecture (Figure 1) was put forward by Cosic and Munro (1988) to map locative expres sions of the form ""noun-preposition-noun"" to a representation of the spatial relationship between the referents of the two nouns. The features used in the spatial",NLP,0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf,1990
ADAPTIVE SPLINE NETWORKS,"A network based on splines is described. It automatically adapts the num ber of units, unit parameters, and the architecture of the network for each application. 1 INTRODUCTION In supervised learning one has a system under study that responds to a set of simultaneous input signals {Xl'"" xn }. The response is characterized by a set of output signals {Y1, Y2,""', Ym}. The goal is to learn the relationship between the inputs and the outputs. This exercise generally has two purposes: prediction and understanding. With prediction one is given a set of input values and wishes to predict or forecast likely values of the corresponding outputs without having to actually run the system. Sometimes prediction is the only purpose. Often, however, one wishes to use the derived relationship to gain understanding of how the system works. Such knowledge is often useful in its own right, for example in science, or it may be used to help improve the characteristics of the system, as in industrial or engin",Optimization & Theoretical ML,0d0fd7c6e093f7b804fa0150b875b868-Paper.pdf,1990
An Analog VLSI Chip for Finding Edges,"We have designed and tested a one-dimensional 64 pixel, analog CMOS VLSI chip which localizes intensity edges in real-time. This device exploits on-chip photoreceptors and the natural filtering properties of resistive net works to implement a scheme similar to and motivated by the Difference of Gaussians (DOG) operator proposed by Marr and Hildreth (1980). Our chip computes the zero-crossings associated with the difference of two ex ponential weighting functions. If the derivative across this zero-crossing is above a threshold, an edge is reported. Simulations indicate that this technique will extend well to two dimensions. 1 INTRODUCTION The zero-crossings of the Laplacian of the Gaussian,V2G, are often used for detect ing edges. Marr and Hildreth (1980) argued that the Mexican-hat shape of the V2G operator can be approximated by the difference of two Gaussians (DOG). In this spirit, we have built a chip that takes the difference of two resistive-network smooth ings of photoreceptor i",Computer Vision,0deb1c54814305ca9ad266f53bc82511-Paper.pdf,1990
"Comparison of three classification techniques,","In this paper, after some introductory remarks into the classification prob lem as considered in various research communities, and some discussions concerning some of the reasons for ascertaining the performances of the three chosen algorithms, viz., CART (Classification and Regression Tree), C4.5 (one of the more recent versions of a popular induction tree tech nique known as ID3), and a multi-layer perceptron (MLP), it is proposed to compare the performances of these algorithms under two criteria: classi fication and generalisation. It is found that, in general, the MLP has better classification and generalisation accuracies compared with the other two algorithms. 1 Introduction Classification of data into categories has been pursued by a number of research communities, viz., applied statistics, knowledge acquisition, neural networks. In applied statistics, there are a number of techniques, e.g., clustering algorithms (see e.g., Hartigan), CART (Classification and Regression Trees, s",Optimization & Theoretical ML,1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf,1990
dynamic processes,"The goal has been to construct a supervised artificial neural network that learns incrementally an unknown mapping. As a result a network con sisting of a combination of ART2 and backpropagation is proposed and is called an ""ART2/BP"" network. The ART2 network is used to build and focus a supervised backpropagation network. The ART2/BP network has the advantage of being able to dynamically expand itself in response to input patterns containing new information. Simulation results show that the ART2/BP network outperforms a classical maximum likelihood method for the estimation of a discrete dynamic and nonlinear transfer function. 1 INTRODUCTION Most current neural network architectures such as backpropagation require a cyclic presentation of the entire training set to converge. They are thus not very well suited for adaptive estimation tasks where the training vectors arrive one by one, and where the network may never see the same training vector twice. The ART2/BP network system is an ",Deep Learning,11b9842e0a271ff252c1903e7132cd68-Paper.pdf,1990
Self-organization of Hebbian Synapses,"We are exploring the significance of biological complexity for neuronal computation. Here we demonstrate that Hebbian synapses in realistical ly-modeled hippocampal pyramidal cells may give rise to two novel forms of self- organization in response to structured synaptic input. First, on the basis of the electrotonic relationships between synaptic contacts, a cell may become tuned to a small subset of its input space. Second, the same mechanisms may produce clusters of potentiated synapses across the space of the dendrites. The latter type of self-organization may be functionally significant in the presence of nonlinear dendritic conduc tances. 1 INTRODUCTION Long-term potentiation (LT P) is an experimentally observed form of synaptic plasticity that has been interpreted as an instance of a Hebbian modification (Kelso et al, 1986; Brown et al, 1990). The induction ofLTP requires synchronous presynaptic activity and postsynaptic depolarization (Kelso et al, 1986). We have previously deve",Optimization & Theoretical ML,138bb0696595b338afbab333c555292a-Paper.pdf,1990
Connection Topology and Dynamics,"We show analytically how the stability of two-dimensional lateral inhibition neural networks depends on the local connection topology. For various network topologies, we calculate the critical time delay for the onset of oscillation in continuous-time networks and present analytic phase diagrams characterizing the dynamics of discrete-time networks. 1 INTRODUCTION Mutual inhibition in an array of neurons is a common feature of sensory systems including vision, olfaction, and audition in organisms ranging from invertebrates to man. A well-studied instance of this configuration is lateral inhibition between neighboring photosensitive neurons in the retina (Dowling, 1987). Inhibition serves in this case to enhance the perception of edges and to broaden the dynamic range by setting a local reference point for measuring intensity variations. Lateral inhibition thus constitutes the first stage of visual information processing. Many artificial vision systems also take advantage of the computa",Optimization & Theoretical ML,13f9896df61279c928f19721878fac41-Paper.pdf,1990
A Delay-Line Based,"Inspired by a visual motion detection model for the ra.bbit retina and by a computational architecture used for early audition in the barn owl, we have designed a chip that employs a correlation model to report the one-dimensional field motion of a scene in real time. Using subthreshold analog VLSI techniques, we have fabricated and successfully tested a 8000 transistor chip using a standard MOSIS process.",Computer Vision,142949df56ea8ae0be8b5306971900a4-Paper.pdf,1990
Back Propagation is Sensitive to Initial Conditions,"This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique. We first demonstrate. through the use of Monte Carlo techniques. that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability. In order to further understand this result. additional deterministic experiments were performed. The results of these experiments demon~trate the extreme sensitivity of back propagation to initial weight configuration. 1 INTRODUCTION Back Propagation (Rwnelhart et al .• 1986) is the network training method of choice for many neural network projects. and for good reason. Like other weak methods, it is simple to implement, faster than many other ""general"" approaches. well-tested by the field. and easy to mold (with domain knowledge encoded in the learning environment) into very specific and efficient algorithms. Rumelhart et al. made a confident st",Optimization & Theoretical ML,1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf,1990
Applications of Neural Networks in,"Although color TV is an established technology, there are a number of longstanding problems for which neural networks may be suited. Impulse noise is such a problem, and a modular neural network approach is pre sented in this paper. The training and analysis was done on conventional computers, while real-time simulations were performed on a massively par allel computer called the Princeton Engine. The network approach was compared to a conventional alternative, a median filter. Real-time simula tions and quantitative analysis demonstrated the technical superiority of the neural system. Ongoing work is investigating the complexity and cost of implementing this system in hardware. 1 THE POTENTIAL FOR NEURAL NETWORKS IN CONSUMER ELECTRONICS Neural networks are most often considered for application in emerging new tech nologies, such as speech recognition, machine vision, and robotics. The fundamental ideas behind these technologies are still being developed, and it will be some time befor",Computer Vision,158f3069a435b314a80bdcb024f8e422-Paper.pdf,1990
Grouping Contours by Iterated Pairing Network,"We describe in this paper a network that performs grouping of image con tours. The input to the net are fragments of image contours, and the output is the partitioning of the fragments into groups, together with a saliency measure for each group. The grouping is based on a measure of overall length and curvature. The network decomposes the overall opti mization problem into independent optimal pairing problems performed at each node. The resulting computation maps into a uniform locally connected network of simple computing elements. 1 The Problenl: Contour Grouping A problem that often arises in visual information processing is the linking of con tour fragments into optimal groups. For example, certain subsets of contours spon taneously form perceptual groups, as illustrated in Fig. 1, and are often detected immediately without scanning the image in a systematic manner. Grouping process of this type are likely to play an important role in object recognition by segmenting the image and",Computer Vision,1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf,1990
Simulation of the Neocognitron on a CCD,"The neocognitron is a neural network for pattern recognition and feature extraction. An analog CCD parallel processing architecture developed at Lincoln Laboratory is particularly well suited to the computational re quirements of shared-weight networks such as the neocognitron, and imple mentation of the neocognitron using the CCD architecture was simulated. A modification to the neocognitron training procedure, which improves network performance under the limited arithmetic precision that would be imposed by the CCD architecture, is presented. 1 INTRODUCTION Multilayer neural networks characterized by local interlayer connectivity and groups of nodes that are constrained to have the same weights on their input lines are often refered to as shared-weight networks. A group of nodes with identical weights where each node is connected to a different portion of the layer immediately beneath can be thought of as a collection of spatially replicated receptive fields. Among the desirable attr",Computer Vision,17d63b1625c816c22647a73e1482372b-Paper.pdf,1990
Order Reduction for Dynamical Systems,"We have devised a scheme to reduce the complexity of dynamical systems belonging to a class that includes most biophysically realistic neural models. The reduction is based on transformations of variables and perturbation expansions and it preserves a high level of fidelity to the original system. The techniques are illustrated by reductions of the Hodgkin-Huxley system and an augmented Hodgkin-Huxley system. INTRODUCTION For almost forty years, biophysically realistic modeling of neural systems has followed the path laid out by Hodgkin and Huxley (Hodgkin and Huxley, 1952). Their seminal work culminated in the accurately detailed description of the membrane currents expressed by the giant axon of the squid Loligo, as a system of four coupled non-linear differential equations. Soon afterward (and ongoing now) simplified, abstract models were introduced that facilitated the conceptualization of the model's behavior, e.g. (FitzHugh, 1961). Yet the mathematical relationships between these",Optimization & Theoretical ML,18997733ec258a9fcaf239cc55d53363-Paper.pdf,1990
Note on Learning Rate Schedules for Stochastic,"We present and compare learning rate schedules for stochastic gradient descent, a general algorithm which includes LMS, on-line backpropaga tion and k-means clustering as special cases. We introduce ""search-then converge"" type schedules which outperform the classical constant and ""running average"" (1ft) schedules both in speed of convergence and quality of solution. 1 Introduction: Stochastic Gradient Descent The optimization task is to find a parameter vector W which =min imizes a func tion G(W). In the context of learning systems typically G(W) £x E(W, X), i.e. G is the average of an objective function over the exemplars, labeled E and X respectively. The stochastic gradient descent algorithm is Ll Wet) = -1](t)V'w E(W(t), X(t)). where t is the ""time"", and X(t) is the most recent independently-chosen random exemplar. For comparison, the deterministic gradient descent algorithm is Ll Wet) = -1](t)V'w£x E(W(t), X). 832 Note on Learning Rate Schedules for Stochastic Optimization 833 Ia'",Optimization & Theoretical ML,18d8042386b79e2c279fd162df0205c8-Paper.pdf,1990
REMARKS ON INTERPOLATION AND,"We consider different types of single-hidden-Iayer feedforward nets: with or without direct input to output connections, and using either thresh old or sigmoidal activation functions. The main results show that direct connections in threshold nets double the recognition but not the interpo lation power, while using sigmoids rather than thresholds allows (at least) doubling both. Various results are also given on VC dimension and other measures of recognition capabilities. 1 INTRODUCTION In this work we continue to develop the theme of comparing threshold and sigmoidal feedforward nets. In (Sontag and Sussmann, 1989) we showed that the ""general ized delta rule"" (backpropagation) can give rise to pathological behavior -namely, the existence of spurious local minima even when no hidden neurons are used, in contrast to the situation that holds for threshold nets. On the other hand, in (Sontag and Sussmann, 1989) we remarked that provided that the right variant be 'Used, separable sets do g",Optimization & Theoretical ML,2421fcb1263b9530df88f7f002e78ea5-Paper.pdf,1990
Rapidly Adapting Artificial Neural Networks for,"The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN ,is a back-propagation network that uses inputs from a video camera and an imaging laser rangefinder to drive the CMU Navlab, a modified Chevy van. This paper describes training techniques which allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching a human driver's response to new situations. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, multilane lined and unlined roads, and obstacle-ridden on- and off-road environments, at speeds of up to 20 miles per hour. 1 INTRODUCTION Previous trainable connectionist perception systems have often ignored important aspects of the form and content of available sensor data. Because of the assumed impracticality of training networks to perform reali",Computer Vision,248e844336797ec98478f85e7626de4a-Paper.pdf,1990
A Recurrent Neural Network for Word Identification,"A neural network architecture was designed for locating word boundaries and identifying words from phoneme sequences. This architecture was tested in three sets of studies. First, a highly redundant corpus with a restricted vocabulary was generated and the network was trained with a limited number of phonemic variations for the words in the corpus. Tests of network performance on a transfer set yielded a very low error rate. In a second study, a network was trained to identify words from expert transcriptions of speech. On a transfer test, error rate for correct simultaneous identification of words and word boundaries was 18%. The third study used the output of a phoneme classifier as the input to the word and word boundary identification network. The error rate on a transfer test set was 49% for this task. Overall, these studies provide a first step at identifying words in connected discourse with a neural network. 1 INTRODUCTION During the past several years, researchers have explore",NLP,24b16fede9a67c9251d3e7c7161c83ac-Paper.pdf,1990
Adjoint-Functions and Temporal Learning,"The development of learning algorithms is generally based upon the min imization of an energy function. It is a fundamental requirement to com pute the gradient of this energy function with respect to the various pa rameters of the neural architecture, e.g., synaptic weights, neural gain,etc. In principle, this requires solving a system of nonlinear equations for each parameter of the model, which is computationally very expensive. A new methodology for neural learning of time-dependent nonlinear mappings is presented. It exploits the concept of adjoint operators to enable a fast global computation of the network's response to perturbations in all the systems parameters. The importance of the time boundary conditions of the adjoint functions is discussed. An algorithm is presented in which the adjoint sensitivity equations are solved simultaneously (Le., forward in time) along with the nonlinear dynamics of the neural networks. This methodology makes real-time applications and hardware",Optimization & Theoretical ML,25b2822c2f5a3230abfadd476e8b04c9-Paper.pdf,1990
Language Induction by Phase Transition,"A higher order recurrent neural network architecture learns to recognize and generate languages after being ""trained"" on categorized exemplars. Studying these networks from the perspective of dynamical systems yields two interesting discoveries: First, a longitudinal examination of the learning process illustrates a new form of mechanical inference: Induction by phase transition. A small weight adjustment causes a ""bifurcation"" in the limit behavior of the network. This phase transition corresponds to the onset of the network's capacity for generalizing to arbitrary-length strings. Second, a study of the automata resulting from the acquisition of previously published languages indicates that while the architecture is NOT guaranteed to find a minimal finite automata consistent with the given exemplars, which is an NP-Hard problem, the architecture does appear capable of generating non regular languages by exploiting fractal and chaotic dynamics. I end the paper with a hypothesis relatin",NLP,26e359e83860db1d11b6acca57d8ea88-Paper.pdf,1990
Chaitin-Kolmogorov Complexity,"We present a unified framework for a number of different ways of failing to generalize properly. During learning, sources of random information contaminate the network, effectively augmenting the training data with random information. The complexity of the function computed is therefore increased, and generalization is degraded. We analyze replicated networks, in which a number of identical networks are independently trained on the same data and their results averaged. We conclude that replication almost always results in a decrease in the expected complexity of the network, and that replication therefore increases expected generalization. Simulations confirming the effect are also presented. 1 BROKEN SYMMETRY CONSIDERED HARMFUL Consider a one-unit backpropagation network trained on exclusive or. Without hidden units, the problem is insoluble. One point where learning would stop is when all weights are zero and the output is always ~, resulting in an mean squared error of ~. But this i",Optimization & Theoretical ML,28f0b864598a1291557bed248a998d4e-Paper.pdf,1990
VLSI Implementations of Learning,"A large number of VLSI implementations of neural network models have been reported. The diversity of these implementations is noteworthy. This paper attempts to put a group of representative VLSI implementations in perspective by comparing and contrast ing them. Design trade-offs are discussed and some suggestions forthe direction of future implementation efforts are made. IMPLEMENTATION Changing the way information is represented can be beneficial. For example a change of representation can make information more compact for storage and transmission. Implementation of neural computational models is just the process of changing the representation of a neural model from mathmatical symbolism to a physical embodi ement for the purpose of shortening the time it takes to process information according to the neural model. FLEXIBIliTY VS. PERFORMANCE Today most neural models are already implemented in silicon VLSI, in the form of pro grams running on general purpose digital von Neumann comput",Computer Vision,2f2b265625d76a6704b08093c652fd79-Paper.pdf,1990
Neural Dynamics of,"A neural network model of motion segmentation by visual cortex is de scribed. The model clarifies how preprocessing of motion signals by a Motion Oriented Contrast Filter (MOC Filter) is joined to long-range co operative motion mechanisms in a motion Cooperative Competitive Loop (CC Loop) to control phenomena such as as induced motion, motion cap ture, and motion aftereffects. The total model system is a motion Bound ary Contour System (BCS) that is computed in parallel with a static BCS before both systems cooperate to generate a boundary representation for three dimensional visual form perception. The present investigations clari fy how the static BCS can be modified for use in motion segmentation prob lems, notably for analyzing how ambiguous local movements (the aperture problem) on a complex moving shape are suppressed and actively reorga nized into a coherent global motion signal. 1 INTRODUCTION: WHY ARE STATIC AND MOTION BOUNDARY CONTOUR SYSTEMS NEEDED? Some regions, notably MT,",Computer Vision,310dcbbf4cce62f762a2aaa148d556bd-Paper.pdf,1990
Exploratory Feature Extraction in Speech Signals,"A novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing multimodality is presented, and its connec tion to exploratory projection pursuit methods is discussed. This leads to a new statistical insight to the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982). The importance of a dimensionality reduction principle based solely on distinguishing features, is demonstrated using a linguistically motivated phoneme recognition experiment, and compared with feature extraction using back-propagation network. 1 Introduction Due to the curse of dimensionality (Bellman, 1961) it is desirable to extract fea tures from a high dimensional data space before attempting a classification. How to perform this feature extraction/dimensionality reduction is not that clear. A first simplification is to consider only features defined by linear (or semi-linear) projec tions of high dimensional data. This class of",NLP,320722549d1751cf3f247855f937b982-Paper.pdf,1990
Constructing Hidden Units,"While the network loading problem for 2-layer threshold nets is NP-hard when learning from examples alone (as with backpropaga tion), (Baum, 91) has now proved that a learner can employ queries to evade the hidden unit credit assignment problem and PAC-load nets with up to four hidden units in polynomial time. Empirical tests show that the method can also learn far more complicated functions such as randomly generated networks with 200 hidden units. The algorithm easily approximates Wieland's 2-spirals func tion using a single layer of 50 hidden units, and requires only 30 minutes of CPU time to learn 200-bit parity to 99.7% accuracy. 1 Introd uction Recent theoretical results (Baum & Haussler, 89) promise good generalization from multi-layer feedforward nets that are consistent with sufficiently large training sets. Unfortunately, the problem of finding such a net has been proved intractable due to the hidden unit credit assignment problem - even for nets containing only 2 hidden unit",Optimization & Theoretical ML,34ed066df378efacc9b924ec161e7639-Paper.pdf,1990
Analog Neural Networks as Decoders,"Analog neural networks with feedback can be used to implement l( Winner-Take-All (KWTA) networks. In turn, KWTA networks can be used as decoders of a class of nonlinear error-correcting codes. By in terconnecting such KWTA networks, we can construct decoders capable of decoding more powerful codes. We consider several families of inter connected KWTA networks, analyze their performance in terms of coding theory metrics, and consider the feasibility of embedding such networks in VLSI technologies. 1 INTRODUCTION: THE K-WINNER-TAKE-ALL NETWORK We have previously demonstrated the use of a continuous Hopfield neural network as a K-Winner-Take-All (KWTA) network [Majani et al., 1989, Erlanson and Abu Mostafa, 1988}. Given an input of N real numbers, such a network will converge to a vector of K positive one components and (N - K) negative one components, with the positive positions indicating the K largest input components. In addition, we have shown that the (~) such vectors are the only s",Optimization & Theoretical ML,352fe25daf686bdb4edca223c921acea-Paper.pdf,1990
A Reinforcement Learning Variant for Control,"We present an algorithm based on reinforcement and state recurrence learning techniques to solve control scheduling problems. In particular, we have devised a simple learning scheme called ""handicapped learning"", in which the weights of the associative search element are reinforced, either positively or negatively, such that the system is forced to move towards the desired setpoint in the shortest possible trajectory. To improve the learning rate, a variable reinforcement scheme is employed: negative reinforcement values are varied depending on whether the failure occurs in handicapped or normal mode of operation. Furthermore, to realize a simulated annealing scheme for accelerated learning, if the system visits the same failed state successively, the negative reinforcement value is increased. In examples studied, these learning schemes have demonstrated high learning rates, and therefore may prove useful for in-situ learning. 1 INTRODUCTION Reinforcement learning techniques have been ",Reinforcement Learning,357a6fdf7642bf815a88822c447d9dc4-Paper.pdf,1990
Kohonen Networks and Clustering: Comparative,"The problem of color clustering is defined and shown to be a problem of assigning a large number (hundreds of thousands) of 3-vectors to a small number (256) of clusters. Finding those clusters in such a way that they best represent a full color image using only 256 distinct colors is a burdensome computational problem. In this paper, the problem is solved using ""classical"" techniques --k-means clustering, vector quantization (which turns out to be the same thing in this application), competitive learning, and Kohonen self-organizing feature maps. Quality of the result is judged subjectively by how much the pseudo-color result resembles the true color image, by RMS quantization error, and by run time. The Kohonen map provides the best solution. 1 INTRODUCTION ""Clusteringn, ""vector quantization"", and ""unsupervised learning"" are all words which descn'be the same process: assigning a few exemplars to represent a large set of samples. Perfonning that process is the subject of a substantial",Computer Vision,371bce7dc83817b7893bcdeed13799b5-Paper.pdf,1990
Connections,"We describe a CMOS neural net chip with a reconfigurable network archi tecture. It contains 32,768 binary, programmable connections arranged in 256 'building block' neurons. Several 'building blocks' can be connected to form long neurons with up to 1024 binary connections or to form neurons with analog connections. Single- or multi-layer networks can be imple mented with this chip. We have integrated this chip into a board system together with a digital signal processor and fast memory. This system is currently in use for image processing applications in which the chip extracts features such as edges and corners from binary and gray-level images. 1 INTRODUCTION A key problem for a hardware implementation of neural nets is to find the proper network architecture. With a fixed network structure only few problems can be solved efficiently. Therefore, we opted for a programmable architecture that can be changed under software control. A large, fully interconnected network can, in prin cipl",Computer Vision,37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf,1990
c-Entropy and the Complexity of,"We develop a. new feedforward neuralnet.work represent.ation of Lipschitz functions from [0, p]n into [0,1] ba'3ed on the level sets of the function. We show that h) ~~ ~€r (:~) + + (1 + n is an upper bound on the number of nodes needed to represent f to within uniform error Cr, where L is the Lipschitz constant. \Ve also show that the number of bits needed to represent the weights in the network in order to achieve this approximation is given by n) . (~2;~r (:~) o \Ve compare this bound with the [-entropy of the functional class under consideration. 1 INTRODUCTION We are concerned with the problem of the number of nodes needed in a feedforward neural network in order to represent a fUllction to within a specified accuracy. All results to date (e.g. [7,10,15]) have been in the form of existence theorems, stating that there does exist a neural network which achieves a certain accuracy of representation, but no indication is given of the number of nodes necessary in order to achieve this",Optimization & Theoretical ML,39461a19e9eddfb385ea76b26521ea48-Paper.pdf,1990
Extensions of a Theory of Networks for,"Learning an input-output mapping from a set of examples can be regarded as synthesizing an approximation of a multi-dimensional function. From this point of view, this form of learning is closely related to regularization theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b) the equivalence between reglilari~at.ioll and a. class of three-layer networks that we call regularization networks. In this note, we ext.end the theory by introducing ways of <lealing with t.wo aspect.s of learning: learning in presence of unreliable examples or outliel·s, an<llearning from positive and negative examples. 1 Introduction In previous papers (Poggio and Girosi, 1990a, 1990b) we have shown the equivalence between certain regularization techniques and a. cla'3s of tlll·ee-layer networks - that we call regularization networks - which are relat.ed to the Ra<lial Basis Functions interpolation method (Powell, 1987). In this not.e we indicat.e how it is possible to extend our theory of lear",Optimization & Theoretical ML,3ad7c2ebb96fcba7cda0cf54a2e802f5-Paper.pdf,1990
Stochastic Neurodynamics,"The main point of this paper is that stochastic neural networks have a mathematical structure that corresponds quite closely with that of quantum field theory. Neural network Liouvillians and Lagrangians can be derived, just as can spin Hamiltonians and Lagrangians in QFf. It remains to show the efficacy of such a description. 1 INTRODUCTION A basic problem in the analysis of large-scale neural network activity, is that one can never know the initial state of such activity, nor can one safely assume that synaptic weights are symmetric, or skew-symmetric. How can one proceed, therefore, to analyse such activity? One answer is to use a ""Master Equation"" (Van Kampen, 1981). In principle this can provide statistical information, moments and correlation functions of network activity by making use of ensemble averaging over all possible initial states. In what follows I give a short account of such an approach. 1.1 THE BASIC NEURAL MODEL In this approach neurons are represented as simple gat",Optimization & Theoretical ML,3c7781a36bcd6cf08c11a970fbe0e2a6-Paper.pdf,1990
Phonetic Classification and Recognition,"In this paper, we will describe several extensions to our earlier work, utiliz ing a segment-based approach. We will formulate our segmental framework and report our study on the use of multi-layer perceptrons for detection and classification of phonemes. We will also examine the outputs of the network, and compare the network performance with other classifiers. Our investigation is performed within a set of experiments that attempts to recognize 38 vowels and consonants in American English independent of speaker. When evaluated on the TIMIT database, our system achieves an accuracy of 56%. 1 Introduction Thus far, the neural networks research community has placed heavy emphasis on the problem of pattern classification. In many applications, including speech recog nition, one must also address the issue of detection. Thus, for example, one must detect the presence of phonetic segments as well as classify them. Recently, the community has moved more towards recognition of continuous spe",NLP,3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf,1990
Learning Trajectory and Force Control,"We propose a new parallel-hierarchical neural network model to enable motor learning for simultaneous control of both trajectory and force. by integrating Hogan's control method and our previous neural network control model using a feedback-error-learning scheme. Furthermore. two hierarchical control laws which apply to the model, are derived by using the Moore-Penrose pseudo inverse matrix. One is related to the minimum muscle-tension-change trajectory and the other is related to the minimum motor-command-change trajectory. The human arm is redundant at the dynamics level since joint torque is generated by agonist and antagonist muscles. Therefore, acquisition of the inverse model is an ill-posed problem. However. the combination of these control laws and feedback-error-learning resolve the ill-posed problem. Finally. the efficiency of the parallel-hierarchical neural network model is shown by learning experiments using an artificial muscle arm and computer simulations. 1 INTRODUCTION",Reinforcement Learning,3fe94a002317b5f9259f82690aeea4cd-Paper.pdf,1990
Basis-Function Trees as a Generalization of Local,"Local variable selection has proven to be a powerful technique for ap proximating functions in high-dimensional spaces. It is used in several statistical methods, including CART, ID3, C4, MARS, and others (see the bibliography for references to these algorithms). In this paper I present a tree-structured network which is a generalization of these techniques. The network provides a framework for understanding the behavior of such algorithms and for modifying them to suit particular applications. 1 INTRODUCTION Function approximation on high-dimensional spaces is often thwarted by a lack of sufficient data to adequately ""fill"" the space, or lack of sufficient computational resources. The technique of local variable selection provides a partial solution to these problems by attempting to approximate functions locally using fewer than the complete set of input dimensions. Several algorithms currently exist which take advantage of local variable selection, including AID (Morgan and Sonquist",Optimization & Theoretical ML,40008b9a5380fcacce3976bf7c08af5b-Paper.pdf,1990
A Short-Term Memory Architecture for the,"Despite its successes, Rumelhart and McClelland's (1986) well-known ap proach to the learning of morphophonemic rules suffers from two deficien cies: (1) It performs the artificial task of associating forms with forms rather than perception or production. (2) It is not constrained in ways that humans learners are. This paper describes a model which addresses both objections. Using a simple recurrent architecture which takes both forms and ""meanings"" as inputs, the model learns to generate verbs in one or another ""tense"", given arbitrary meanings, and to recognize the tenses of verbs. Furthermore, it fails to learn reversal processes unknown in human language. 1 BACKGROUND In the debate over the power of connectionist models to handle linguistic phenom ena, considerable attention has been focused on the learning of simple morphological rules. It is a straightforward matter in a symbolic system to specify how the mean ings of a stem and a bound morpheme combine to yield the meaning of a ",NLP,41ae36ecb9b3eee609d05b90c14222fb-Paper.pdf,1990
Associative Memory in a Network of 'biological',"The Hopfield network (Hopfield, 1982,1984) provides a simple model of an associative memory in a neuronal structure. This model, however, is based on highly artificial assumptions, especially the use of formal-two state neu rons (Hopfield, 1982) or graded-response neurons (Hopfield, 1984). \Vhat happens if we replace the formal neurons by 'real' biological neurons? \Ve address this question in two steps. First, we show that a simple model of a neuron can capture all relevant features of neuron spiking, i. e., a wide range of spiking frequencies and a realistic distribution of interspike inter vals. Second, we construct an associative memory by linking these neurons together. The analytical solution for a large and fully connected network shows that the Hopfield solution is valid only for neurons with a short re fractory period. If the refractory period is longer than a crit.ical duration ie, the solutions are qualitatively different. The associative character of the solutions, however,",Optimization & Theoretical ML,41f1f19176d383480afa65d325c06ed0-Paper.pdf,1990
Oriented Non-Radial Basis Functions for Image,"We introduce oriented non-radial basis function networks (ONRBF) as a generalization of Radial Basis Function networks (RBF)- wherein the Euclidean distance metric in the exponent of the Gaussian is re placed by a more general polynomial. This permits the definition of more general regions and in particular- hyper-ellipses with orienta tions. In the case of hyper-surface estimation this scheme requires a smaller number of hidden units and alleviates the ""curse of dimen sionality"" associated kernel type approximators.In the case of an im age, the hidden units correspond to features in the image and the parameters associated with each unit correspond to the rotation, scal ing and translation properties of that particular ""feature"". In the con text of the ONBF scheme, this means that an image can be represented by a small number of features. Since, transformation of an image by rotation, scaling and translation correspond to identical transformations of the individual features, the ONBF s",Computer Vision,42e7aaa88b48137a16a1acd04ed91125-Paper.pdf,1990
Evaluation of Adaptive Mixtures,"We compare the performance of the modular architecture, composed of competing expert networks, suggested by Jacobs, Jordan, Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex, but low-dimensional, vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task. 1 Introduction If back-propagation is used to train a single, multilayer network to perform different subtasks on different occasions, there will generally be strong interference effects which lead to slow learning and poor generalization. If we know in advance that a set of training cases may be naturally divideJ into subsets that correspond to distinct subtasks, interferen",Optimization & Theoretical ML,432aca3a1e345e339f35a30c8f65edce-Paper.pdf,1990
Design and Implementation of a High Speed,"A high speed implementation of the CMAC neural network was designed using dedicated CMOS logic. This technology was then used to implement two general purpose CMAC associative memory boards for the VME bus. Each board implements up to 8 independent CMAC networks with a total of one million adjustable weights. Each CMAC network can be configured to have from 1 to 512 integer inputs and from 1 to 8 integer outputs. Response times for typical CMAC networks are well below 1 millisecond, making the networks sufficiently fast for most robot control problems, and many pattern recognition and signal processing problems. 1 INTRODUCTION We have been investigating learning techniques for the control of robotic manipu lators which utilize extensions of the CMAC neural network as developed by Albus 1022 Design and Implementation of a High Speed CMAC Neural Network 1023 (1972; 1975; 1979). The learning control techniques proposed have been studied in our laboratory in a series of real time experimen",Optimization & Theoretical ML,4f4adcbf8c6f66dcfc8a3282ac2bf10a-Paper.pdf,1990
Spoken Letter Recognition,"Through the use of neural network classifiers and careful feature selection, we have achieved high-accuracy speaker-independent spoken letter recog nition. For isolated letters, a broad-category segmentation is performed Location of segment boundaries allows us to measure features at specific locations in the signal such as vowel onset, where important information resides. Letter classification is performed with a feed-forward neural net work. Recognition accuracy on a test set of 30 speakers was 96%. Neu ral network classifiers are also used for pitch tracking and broad-category segmentation of letter strings. Our research has been extended to recog nition of names spelled with pauses between the letters. When searching a database of 50,000 names, we achieved 95% first choice name retrieval. Work has begun on a continuous letter classifier which does frame-by-frame phonetic classification of spoken letters. 1 INTRODUCTION Although spoken letter recognition may seem like a modest goal ",NLP,49182f81e6a13cf5eaa496d51fea6406-Paper.pdf,1990
"compression. The network is trained to simply reproduce its input, and so can","of California, San Diego who received partial course credit for the procedure outlined by Galton (1878), images were aligned The dimens~onali~y of a set Off 160 1:~ :a:~s ~~·.10 . female subjects IS reduced rom ........ . eyes and mouth. These images were captured by a fraOl~ grabber, pixels by averaging. To prevent the use of first order statistics for network The extracted features do not correspond to were normalized to have equal brightness and variance. The in previ~us face recognition systems (Ka· na~e, 19~;)y' ......••.•.. R scaled to the range [0 •.8 ]. Part of the training set and its d' tances between facial elements. at.er, .. ............ \ are shown in Figure 2. f~tures we call holons. The hol.ons are fV~~ t~! ~I'; ~lIItclenl~oder propagation networks th.at are teamed toc ~:~~~ . y faceness gender identity. feigned emouonal state. and g. f.. .. .. O~··· •. ·~ extracted holons provide a. suf~cient basIS or discriminations, 99% of the Idenuty .' .t emotion discriminations ~m",Computer Vision,52720e003547c70561bf5e03b95aa99f-Paper.pdf,1990
Shaping the State Space Landscape in Recurrent,"Fully recurrent (asymmetrical) networks can be thought of as dynamic systems. The dynamics can be shaped to perform content addressable memories, recognize sequences, or generate trajectories. Unfortunately several problems can arise: First, the convergence in the state space is not guaranteed. Second, the learned fixed points or trajectories are not necessarily stable. Finally, there might exist spurious fixed points and/or spurious ""attracting"" trajectories that do not correspond to any patterns. In this paper, we introduce a new energy function that presents solutions to all of these problems. We present an efficient gradient descent algorithm which directly acts on the stability of the fixed points and trajectories and on the size and shape of the corresponding basin and valley of attraction. The results are illustrated by the simulation of a small content addressable memory. 1 INTRODUCTION Recurrent neural networks have the capability of storing information in the state of their u",Optimization & Theoretical ML,4f6ffe13a5d75b2d6a3923922b3922e5-Paper.pdf,1990
"INTERACTION AMONG OCULARITY,","The development of projections from the retinas to the cortex is mathematically analyzed according to the previously proposed thermodynamic formulation of the self-organization of neural networks. Three types of submodality included in the visual afferent pathways are assumed in two models: model (A), in which the ocularity and retinotopy are considered separately, and model (B), in which on-center/off-center pathways are considered in addition to ocularity and retinotopy. Model (A) shows striped ocular dominance spatial patterns and, in ocular dominance histograms, reveals a dip in the binocular bin. Model (B) displays spatially modulated irregular patterns and shows single-peak behavior in the histograms. When we compare the simulated results with the observed results, it is evident that the ocular dominance spatial patterns and histograms for models (A) and (B) agree very closely with those seen in monkeys and cats. 1 INTRODUCTION A recent experimental study has revealed that spatia",Computer Vision,5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf,1990
Phase-coupling in Two-Dimensional,"Coherent oscillatory activity in large networks of biological or artifi cial neural units may be a useful mechanism for coding information pertaining to a single perceptual object or for detailing regularities within a data set. We consider the dynamics of a large array of simple coupled oscillators under a variety of connection schemes. Of particular interest is the rapid and robust phase-locking that results from a ""sparse"" scheme where each oscillator is strongly coupled to a tiny, randomly selected, subset of its neighbors. 1 INTRODUCTION Networks of interacting oscillators provide an excellent model for numerous physical processes ranging from the behavior of magnetic materials to models of atmospheric dynamics to the activity of populations of neurons in a variety of cortical locations. Particularly prominent in the neurophysiological data are the 40-60 Hz oscillations that have long been reported in the rat and rabbit olfactory bulb and cortex on the basis of single-and multi-un",Reinforcement Learning,577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf,1990
An Analog VLSI Splining Network,"We have produced a VLSI circuit capable of learning to approximate ar bitrary smooth of a single variable using a technique closely related to splines. The circuit effectively has 512 knots space on a uniform grid and has full support for learning. The circuit also can be used to approximate multi-variable functions as sum of splines. An interesting, and as of yet, nearly untapped set of applications for VLSI imple mentation of neural network learning systems can be found in adaptive control and non-linear signal processing. In most such applications, the learning task consists of approximating a real function of a small number of continuous variables from discrete data points. Special purpose hardware is especially interesting for applica tions of this type since they generally require real time on-line learning and there can be stiff constraints on the power budget and size of the hardware. Frequently, the already difficult learning problem is made more complex by the non-stationary ",Optimization & Theoretical ML,58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf,1990
Designing Linear Threshold Based Neural,"The three problems that concern us are identifying a natural domain of pattern classification applications of feed forward neural networks, select ing an appropriate feedforward network architecture, and assessing the tradeoff between network complexity, training set size, and statistical reli ability as measured by the probability of incorrect classification. We close with some suggestions, for improving the bounds that come from Vapnik Chervonenkis theory, that can narrow, but not close, the chasm between theory and practice. 1 Speculations on Neural Network Pattern Classifiers (1) The goal is to provide rapid, reliable classification of new inputs from a pattern source. Neural networks are appropriate as pattern classifiers when the pattern sources are ones of which we have little understanding, beyond perhaps a nonparametric statistical model, but we have been provided with classified samples of features drawn from each of the pattern categories. Neural networks should be able to p",Optimization & Theoretical ML,5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf,1990
Qualitative structure from motion,"Exact structure from motion is an ill-posed computation and therefore very sensitive to noise. In this work I describe how a qualitative shape representation, based on the sign of the Gaussian curvature, can be com puted directly from motion disparities, without the computation of an exact depth map or the directions of surface normals. I show that humans can judge the curvature sense of three points undergoing 3D motion from two, three and four views with success rate significantly above chance. A simple RBF net has been trained to perform the same task. 1 INTRODUCTION When a scene is recorded from two or more different positions in space, e.g. by a moving camera, objects are projected into disparate locations in each image. This disparity can be used to recover the three-dimensional structure of objects that is lost in the projection process. The computation of structure requires knowledge of the 3D motion parameters. Although these parameters can themselves be com puted from the dis",Computer Vision,5b8add2a5d98b1a652ea7fd72d942dac-Paper.pdf,1990
Learning to See Rotation and,"Previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) showed that a feedforward network with area VI-like input-layer units and a Hebb rule can develop area MT-like second layer units that solve the aperture problem for pattern motion. The present study extends this earlier work to more complex motions. Saito et al. (1986) showed that neurons with large receptive fields in macaque visual area MST are sensitive to different senses of rotation and dilation, irrespective of the receptive field location of the movement singularity. A network with an MT-like second layer was trained and tested on combinations of rotating, dilating, and translating patterns. Third-layer units learn to detect specific senses of rotation or dilation in a position-independent fashion, despite having position-dependent direction selectivity within their receptive fields. 1 INTRODUCTION The visual systems of mammals and especially primates are capable of prodigious feats of movement. object. and scene recognit",Computer Vision,5ef698cd9fe650923ea331c15af3b160-Paper.pdf,1990
A Comparative Study of the Practical,"Seven different pattern classifiers were implemented on a serial computer and compared using artificial and speech recognition tasks. Two neural network (radial basis function and high order polynomial GMDH network) and five conventional classifiers (Gaussian mixture, linear tree, K nearest neighbor, KD-tree, and condensed K nearest neighbor) were evaluated. Classifiers were chosen to be representative of different approaches to pat tern classification and to complement and extend those evaluated in a previous study (Lee and Lippmann, 1989). This and the previous study both demonstrate that classification error rates can be equivalent across different classifiers when they are powerful enough to form minimum er ror decision regions, when they are properly tuned, and when sufficient training data is available. Practical characteristics such as training time, classification time, and memory requirements, however, can differ by or ders of magnitude. These results suggest that the selectio",Computer Vision,66368270ffd51418ec58bd793f2d9b1b-Paper.pdf,1990
Real-time autonomous robot navigation using,"We describe a real time robot navigation system based on three VLSI neural network modules. These are a resistive grid for path planning, a nearest-neighbour classifier for localization using range data from a time of-flight infra-red sensor and a sensory-motor associative network for dy namic obstacle avoidance. 1 INTRODUCTION There have been very few demonstrations ofthe application ofVLSI neural networks to real world problems. Yet there are many signal processing, pattern recognition or optimization problems where a large number of competing hypotheses need to be explored in parallel, most often in real time. The massive parallelism of VLSI neural network devices, with one multiplier circuit per synapse, is ideally suited to such problems. In this paper, we present preliminary results from our design for a real time robot navigation system based on VLSI neural network modules. This is a • Also: RSRE, Great Malvern, Worcester, WR14 3PS 422 Real-time Autonomous Robot Navigation Using",Computer Vision,66808e327dc79d135ba18e051673d906-Paper.pdf,1990
Generalization Properties of Radial Basis,"We examine the ability of radial basis functions (RBFs) to generalize. We compare the performance of several types of RBFs. We use the inverse dy namics of an idealized two-joint arm as a test case. We find that without a proper choice of a norm for the inputs, RBFs have poor generalization properties. A simple global scaling of the input variables greatly improves performance. We suggest some efficient methods to approximate this dis tance metric. 1 INTRODUCTION The Radial Basis Functions (RBF) approach to approximating functions consists of modeling an input-output mapping as a linear combination of radially symmetric functions (Powell, 1987; Poggio and Girosi, 1990; Broomhead and Lowe, 1988; Moody and Darken, 1989). The RBF approach has some properties which make it attractive as a function interpolation and approximation tool. The coefficients that multiply the different basis functions can be found with a linear regression. Many RBFs are derived from regularization principles whic",Optimization & Theoretical ML,6883966fd8f918a4aa29be29d2c386fb-Paper.pdf,1990
CAM Storage of Analog Patterns and,"A simple architecture and algorithm for analytically guaranteed associa tive memory storage of analog patterns, continuous sequences, and chaotic attractors in the same network is described. A matrix inversion determines network weights, given prototype patterns to be stored. There are N units of capacity in an N node network with 3N2 weights. It costs one unit per static attractor, two per Fourier component of each sequence, and four per chaotic attractor. There are no spurious attractors, and there is a Lia punov function in a special coordinate system which governs the approach of transient states to stored trajectories. Unsupervised or supervised incre mental learning algorithms for pattern classification, such as competitive learning or bootstrap Widrow-Hoff can easily be implemented. The archi tecture can be ""folded"" into a recurrent network with higher order weights that can be used as a model of cortex that stores oscillatory and chaotic attractors by a Hebb rule. Hierarchical ",Deep Learning,6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf,1990
Learning Time-varying Concepts,"This work extends computational learning theory to situations in which concepts vary over time, e.g., system identification of a time-varying plant. We have extended formal definitions of concepts and learning to provide a framework in which an algorithm can track a concept as it evolves over time. Given this framework and focusing on memory-based algorithms, we have derived some PAC-style sample complexity results that determine, for example, when tracking is feasible. We have also used a similar framework and focused on incremental tracking algorithms for which we have derived some bounds on the mistake or error rates for some specific concept classes. 1 INTRODUCTION The goal of our ongoing research is to extend computational learning theory to include concepts that can change or evolve over time. For example, face recognition is complicat ed by the fact that a persons face changes slowly with age and more quickly with changes in make up, hairstyle, or facial hair. Speech recognition",Optimization & Theoretical ML,69cb3ea317a32c4e6143e665fdb20b14-Paper.pdf,1990
A Theory for Neural Networks with Time Delays,"We present a new neural network model for processing of temporal patterns. This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t). We show that the gamma model can be formulated as a (partially prewired) additive model. A temporal hebbian learning rule is derived and we establish links to related existing models for temporal processing. 1 INTRODUCTION In this paper, we are concerned with developing neural nets with short term memory for processing of temporal patterns. In the literature, basically two ways have been reported to incorporate short-term memory in the neural system equations. The first approach utilizes reverberating (self-recurrent) units of type : = - aa (x) + e, that hold a trace of the past neural net states x(t) or the input e(t). Elman (1988) and Jordan (1986) have successfully used this approach. The disadvantage of this method is the lack of weighting flexibility in the temporal domain, since the system equ",Optimization & Theoretical ML,6c524f9d5d7027454a783c841250ba71-Paper.pdf,1990
Compact EEPROM-based Weight Functions,"We are focusing on the development of a highly compact neural net weight function based on the use of EEPROM devices. These devices have already proven useful for analog weight storage, but existing designs rely on the use of conventional voltage multiplication as the weight function, requiring additional transistors per synapse. A parasitic capacitance between the floating gate and the drain of the EEPROM structure leads to an unusual J-V characteristic which can be used to advantage in designing a compact synapse. This novel behavior is well characterized by a model we have developed. A single-device circuit results in a 1-quadrant synapse function which is nonlinear, though monotonic. A simple extension employing 2 EEPROMs results in a 2 quadrant function which is much more linear. This approach offers the potential for more than a ten-fold increase in the density of neural net implementations. 1 INTRODUCTION - ANALOG WEIGHTING The recent surge of interest in neural networks and par",Optimization & Theoretical ML,6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf,1990
Natural Dolphin Echo Using an Integrator,"We have been studying the performance of a bottlenosed dolphin on a delayed matching-to-sample task to gain insight into the processes and mechanisms that the animal uses during echolocation. The dolphin recognizes targets by emitting natural sonar signals and listening to the echoes that return. This paper describes a novel neural network architecture, called an integrator gateway network, that we have de veloped to account for this performance. The integrator gateway network combines information from multiple echoes to classify targets with about 90% accuracy. In contrast, a standard backpropagation network performed with only about 63% accuracy.",Computer Vision,6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf,1990
Discovering Viewpoint-Invariant Relationships,"Using an unsupervised learning procedure, a network is trained on an en semble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network ""sees"" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size. 1 INTRODUCTION A difficult problem for neural networks is to recognize obje",Computer Vision,6faa8040da20ef399b63a72d0e4ab575-Paper.pdf,1990
Reinforcenlent Learning in Markovian and,"This work addresses three problems with reinforcement learning and adap tive neuro-control: 1. Non-Markovian interfaces between learner and en vironment. 2. On-line learning based on system realization. 3. Vector valued adaptive critics. An algorithm is described which is based on system realization and on two interacting fully recurrent continually running net works which may learn in parallel. Problems with parallel learning are attacked by 'adaptive randomness'. It is also described how interacting model/controller systems can be combined with vector-valued 'adaptive critics' (previous critics have been scalar). 1 INTRODUCTION At a given time, an agent with a non-Markovian interface to its environment cannot derive an optimal next action by considering its current input only. The algorithm described below differs from previous reinforcement algorithms in at least some of the following issues: It has a potential for on-line learning and non-Markovian environments, it is local in time",Reinforcement Learning,70c639df5e30bdee440e4cdf599fec2b-Paper.pdf,1990
Second Order Properties of Error Surfaces :,"The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables. 1 INTRODUCTION Consider the class of learning algorithms which explore a space {W} of possible couplings looking for optimal values W· for which a cost function E(W) is minimal. The dynamical properties of searches based on gradient descent are controlled by the second order properties of the E(W) surface. An analytic investigation of such properties provides a characterization of the time scales involved in the relaxation to the solution W·. The discussion focuses on layered networks with no feedback, a class of architectures remarkably succ",Optimization & Theoretical ML,758874998f5bd0c393da094e1967a72b-Paper.pdf,1990
Connectionist Music Composition Based on,"We describe a recurrent connectionist network, called CONCERT, that uses a set of melodies written in a given style to compose new melodies in that style. CONCERT is an extension of a traditional algorithmic composition tech nique in which transition tables specify the probability of the next note as a function of previous context. A central ingredient of CONCERT is the use of a psychologically-grounded representation of pitch. 1 INTRODUCTION In creating music, composers bring to bear a wealth of knowledge about musical conven tions. If we hope to build automatic music composition systems that can mimic the abili ties of human composers, it will be necessary to incorporate knowledge about musical conventions into the systems. However, this knowledge is difficult to express: even hu man composers are unaware of many of the constraints under which they operate. In this paper, we describe a connectionist network that composes melodies. The network is called CONCERT, an acronym for connect",NLP,75fc093c0ee742f6dddaa13fff98f104-Paper.pdf,1990
Transforming Neural-Net Output Levels,"(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known ""soft max"" scheme. 1 Distribution of Categories in Outp",Deep Learning,7eacb532570ff6858afd2723755ff790-Paper.pdf,1990
Discovering Discrete Distributed Representations,"Competitive learning is an unsupervised algorithm that classifies input pat terns into mutually exclusive clusters. In a neural net framework, each clus ter is represented by a processing unit that competes with others in a winner take-all pool for an input pattern. I present a simple extension to the algo rithm that allows it to construct discrete, distributed representations. Discrete representations are useful because they are relatively easy to analyze and their information content can readily be measured. Distributed representa tions are useful because they explicitly encode similarity. The basic idea is to apply competitive learning iteratively to an input pattern, and after each stage to subtract from the input pattern the component that was captured in the representation at that stage. This component is simply the weight vector of the winning unit of the competitive pool. The subtraction procedure forces competitive pools at different stages to encode different aspects of the i",Deep Learning,819f46e52c25763a55cc642422644317-Paper.pdf,1990
A Neural Expert System with Automated Extraction,This paper proposes ajuzzy neural expert system (FNES) with the following two functions: (1) Generalization of the information derived from the training data and embodiment of knowledge in the form of the fuzzy neural network; (2) Extraction of fuzzy If-Then rules with linguistic relative importance of each proposition in an antecedent (I f -part) from a trained neural network. This paper also gives a method to extract automatically fuzzy If-Then rules from the trained neural network. To prove the effectiveness and validity of the proposed fuzzy neural expert system. a fuzzy neural expert system for medical diagnosis has been developed. 1 INTRODUCTION Expert systems that have neural networks for their knowledge bases are sometimes called neural expert system (Gallant & Hayashi. 1990; Hayashi et at. 1990; Yoshida et al .• 1990) or connectionist expert system (Gallant. 1988; Yoshida et a1.. 1989). This paper extends work reported in (Hayashi & Nakai. 1990; Hayashi et a!.. 1990) and shows,NLP,82cec96096d4281b7c95cd7e74623496-Paper.pdf,1990
Learning by Combining Memorization,"We have created a radial basis function network that allocates a new computational unit whenever an unusual pattern is presented to the network. The network learns by allocating new units and adjusting the parameters of existing units. If the network performs poorly on a presented pattern, then a new unit is allocated which memorizes the response to the presented pattern. If the network performs well on a presented pattern, then the network parameters are updated using standard LMS gradient descent. For predicting the Mackey Glass chaotic time series, our network learns much faster than do those using back-propagation and uses a comparable number of synapses. 1 INTRODUCTION Currently, networks that perform function interpolation tend to fall into one of two categories: networks that use gradient descent for learning (e.g., back-propagation), and constructive networks that use memorization for learning (e.g., k-nearest neigh bors). Networks that use gradient descent for learning tend to",Optimization & Theoretical ML,89f0fd5c927d466d6ec9a21b9ac34ffa-Paper.pdf,1990
Can neural networks do better than the,"\Ve describe a series of careful llumerical experiments which measure the average generalization capability of neural networks trained on a variety of simple functions. These experiments are designed to test whether average generalization performance can surpass the worst-case bounds obtained from formal learning theory using the Vapnik-Chervonenkis dimension (Blumer et al., 1989). We indeed find that, in some cases, the average generalization is significantly better than the VC bound: the approach to perfect performance is exponential in the number of examples m, rather than the 11m result of the bound. In other cases, we do find the 11m behavior of the VC bound, and in these cases, the numerical prefactor is closely related to prefactor contained in the bound. 1 INTRODUCTION Probably the most important issue in the study of supervised learning procedures is the issue of generalization, i.e., how well the learning system can perform on inputs not seen during training. Significant prog",Optimization & Theoretical ML,816b112c6105b3ebd537828a39af4818-Paper.pdf,1990
A Neural Network Approach for,No abstract found,Deep Learning,8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf,1990
Distributed Recursive Structure Processing,"Harmonic grammar (Legendre, et al., 1990) is a connectionist theory of lin guistic well-formed ness based on the assumption that the well-formedness of a sentence can be measured by the harmony (negative energy) of the corresponding connectionist state. Assuming a lower-level connectionist network that obeys a few general connectionist principles but is otherwise unspecified, we construct a higher-level network with an equivalent har mony function that captures the most linguistically relevant global aspects of the lower level network. In this paper, we extend the tensor product representation (Smolensky 1990) to fully recursive representations of re cursively structured objects like sentences in the lower-level network. We show theoretically and with an example the power of the new technique for parallel distributed structure processing. 1 Introduction A new technique is presented for representing recursive structures in connectionist networks. It has been developed in the context of ",Optimization & Theoretical ML,8cb22bdd0b7ba1ab13d742e22eed8da2-Paper.pdf,1990
Dynamics of Learning in Recurrent,"The self-organization of recurrent feature-discovery networks is studied from the perspective of dynamical systems. Bifurcation theory reveals pa rameter regimes in which multiple equilibria or limit cycles coexist with the equilibrium at which the networks perform principal component analysis. 1 Introduction Oja (1982) made the remarkable observation that a simple model neuron with an Hebbian adaptation rule develops into a filter for the first principal component of the input distribution. Several researchers have extended Oja's work, developing networks that perform a complete principal component analysis (PCA). Sanger (1989) proposed an algorithm that uses a single layer of weights with a set of cascaded feedback projections to force nodes to filter for the principal components. This architecture singles out a particular node for each principal component. Oja (1989) and Oja and Karhunen (1985) give a related algorithm that projects inputs onto an orthogonal basis spanning the princ",Optimization & Theoretical ML,8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf,1990
Navigating through Temporal Difference,"Barto, Sutton and Watkins [2] introduced a grid task as a didactic ex ample of temporal difference planning and asynchronous dynamical pre> gramming. This paper considers the effects of changing the coding of the input stimulus, and demonstrates that the self-supervised learning of a particular form of hidden unit representation improves performance. 1 INTRODUCTION Temporal difference (TD) planning [6, 7] uses prediction for control. Consider an agent moving around a finite grid such as the one in figure 1 (the agent is incapable of crossing the barrier) trying to reach a goal whose position it does not know. If it can predict how far away from the goal it is at the current step, and how far away from the goal it is at the next step, after making a move, then it can decide whether or not that move was helpful or harmful. If, in addition, it can record this fact, then it can learn how to navigate to the goal. This generation of actions from predictions is closely related to the mechanis",Reinforcement Learning,8d7d8ee069cb0cbbf816bbb65d56947e-Paper.pdf,1990
On The Circuit Complexity of Neural Networks,"'~le introduce a geometric approach for investigating the power of threshold circuits. Viewing n-variable boolean functions as vectors in 'R'2"", we invoke tools from linear algebra and linear programming to derive new results on the realizability of boolean functions using threshold gat.es. Using this approach, one can obtain: (1) upper-bounds on the number of spurious memories in HopfielJ networks, and on the number of functions implementable by a depth-d threshold circuit; (2) a lower bound on the number of ort.hogonal input. functions required to implement. a threshold function; (3) a necessary condit.ion for an arbit.rary set of input. functions to implement a threshold function; (4) a lower bound on the error introduced in approximating boolean functions using sparse polynomials; (5) a limit on the effectiveness of the only known lower-bound technique (based on computing correlations among boolean functions) for the depth of thresh old circuit.s implement.ing boolean functions, an",Optimization & Theoretical ML,8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf,1990
An Attractor Neural Network Model of Recall,"This work presents an Attractor Neural Network (ANN) model of Re call and Recognition. It is shown that an ANN model can qualitatively account for a wide range of experimental psychological data pertaining to the these two main aspects of memory access. Certain psychological phenomena are accounted for, including the effects of list-length, word frequency, presentation time, context shift, and aging. Thereafter, the probabilities of successful Recall and Recognition are estimated, in order to possibly enable further quantitative examination of the model. 1 Motivation The goal of this paper is to demonstrate that a Hopfield-based [Hop82] ANN model can qualitatively account for a wide range of experimental psychological data per taining to the two main aspects of memory access, Recall and Recognition. Recall is defined as the ability to retrieve an item from a list of items (words) originally presented during a previous learning phase, given an appropriate cue (cued RecalQ, or spontaneou",NLP,8e98d81f8217304975ccb23337bb5761-Paper.pdf,1990
Training Knowledge-Based Neural Networks to,"We describe the application of a hybrid symbolic/connectionist machine learning algorithm to the task of recognizing important genetic sequences. The symbolic portion of the KBANN system utilizes inference rules that provide a roughly-correct method for recognizing a class of DNA sequences known as eukaryotic splice-junctions. We then map this ""domain theory"" into a neural network and provide training examples. Using the samples, the neural network's learning algorithm adjusts the domain theory so that it properly classifies these DNA sequences. Our procedure constitutes a general method for incorporating preexisting knowledge into artificial neural networks. We present an experiment in molecular genetics that demonstrates the value of doing so. 1 Introduction Often one has some preconceived notions about how to perform some classifica tion task. It would be useful to incorporate this knowledge into a, neural net work, and then use some training examples to refine these approximately-c",Deep Learning,8efb100a295c0c690931222ff4467bb8-Paper.pdf,1990
ALCOVE: A Connectionist Model of,"ALCOVE is a connectionist model of human category learning that fits a broad spectrum of human learning data. Its architecture is based on well established psychological theory, and is related to networks using radial basis functions. From the perspective of cognitive psychology, ALCOVE can be construed as a combination of exemplar-based representation and error driven learning. From the perspective of connectionism, it can be seen as incorporating constraints into back-propagation networks appropriate for modelling human learning. 1 INTRODUCTION ALCOVE is intended to accurately model human, perhaps non-optimal, performance in category learning. While it is a feed-forward network that learns by gradient descent on error, it is unlike standard back propagation (Rumelhart, Hinton & '''illiams, 1986) in its architecture, its behavior, and its goals. Unlike the standard back-propagation network, which was motivated by generalizing neuron-like per ceptrons, the architecture of ALCOVE was mo",NLP,8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf,1990
Multi-Layer Perceptrons,Multi-layer perceptrons are often slow to learn nonlinear functions with complex local structure due to the global nature of their function approximations. It is shown that standard multi-layer perceptrons are actually a special case of a more general network formulation that incorporates B-splines into the node computations. This allows novel spline network architectures to be developed that can combine the generalization capabilities and scaling properties of global multi-layer feedforward networks with the computational efficiency and learning speed of local computational paradigms. Simulation results are presented for the well known spiral problem of Weiland and of Lang and Witbrock to show the effectiveness of the Spline Net approach.,Deep Learning,94f6d7e04a4d452035300f18b984988c-Paper.pdf,1990
"Bumptrees for Efficient Function, Constraint, and","A new class of data structures called ""bumptrees"" is described. These structures are useful for efficiently implementing a number of neural network related operations. An empirical comparison with radial basis functions is presented on a robot ann mapping learning task. Applica tions to density estimation. classification. and constraint representation and learning are also outlined. 1 WHAT IS A BUMPTREE? A bumptree is a new geometric data structure which is useful for efficiently learning. rep resenting. and evaluating geometric relationships in a variety of contexts. They are a natural generalization of several hierarchical geometric data structures including oct-trees. k-d trees. balltrees and boxtrees. They are useful for many geometric learning tasks including approximating functions. constraint surfaces. classification regions. and probability densi ties from samples. In the function approximation case. the approach is related to radial basis function neural networks, but supports",Deep Learning,950a4152c2b4aa3ad78bdd6b366cc179-Paper.pdf,1990
Planning with an Adaptive World Model,"We present a new connectionist planning method [TML90]. By interaction with an unknown environment, a world model is progressively construc ted using gradient descent. For deriving optimal actions with respect to future reinforcement, planning is applied in two steps: an experience net work proposes a plan which is subsequently optimized by gradient descent with a chain of world models, so that an optimal reinforcement may be obtained when it is actually run. The appropriateness of this method is demonstrated by a robotics application and a pole balancing task. 1 INTRODUCTION Whenever decisions are to be made with respect to some events in the future, planning has been proved to be an important and powerful concept in problem solving. Planning is applicable if an autonomous agent interacts with a world, and if a reinforcement is available which measures only the over-all performance of the agent. Then the problem of optimizing actions yields the temporal credit assignment problem [Sut8",Reinforcement Learning,9be40cee5b0eee1462c82c6964087ff9-Paper.pdf,1990
Simple Spin Models,"Simple classical spin models well-known to physicists as the ANNNI and Heisenberg XY Models. in which long-range interactions occur in a pattern given by the Mexican Hat operator. can generate many of the structural properties characteristic of the ocular dominance columns and iso-orientation patches seen in cat and primate visual cortex. 1 INTRODUCTION In recent years numerous models for the formation of ocular dominance columns (Malsburg, 1979; Swindale. 1980; Miller. Keller, & Stryker. 1989) and of iso-orientation patches (Malsburg 1973; Swindale 1982 & Linsker 1986)have been published. Here we show that simple spin models can reproduce many of the observed features. Our work is similar to, but independent of a recent study employing spin models (Tanaka. 1990). 26 Simple Spin Models 27 1.1 OCULAR DOMINANCE COLUMNS We use a one-dimensional classical spin Hamiltonian on a two-dimensional lattice with long-range interactions. Let O'i be a spin vector restricted to the orientations i an",Computer Vision,9de6d14fff9806d4bcd1ef555be766cd-Paper.pdf,1990
A Multiscale Adaptive Network Model of,"We demonstrate a multiscale adaptive network model of motion computation in primate area MT. The model consists of two stages: (l) local velocities are measured across multiple spatio-temporal channels, and (2) the optical flow field is computed by a network of direction selective neurons at multiple spatial resolutions. This model embeds the computational efficiency of Multigrid algorithms within a parallel network as well as adaptively computes the most reliable estimate of the flow field across different spatial scales. Our model neurons show the same nonclassical receptive field properties as Allman's type I MT neurons. Since local velocities are measured across multiple channels, various channels often provide conflicting measurements to the network. We have incorporated a veto scheme for conflict resolution. This mechanism provides a novel explanation for the spatial frequency dependency of the psychophysical phenomenon called Motion Capture. 1 MOTIVATION We previously developed ",Computer Vision,9dfcd5e558dfa04aaf37f137a1d9d3e5-Paper.pdf,1990
Spherical Units as Dynamic Consequential Regions:,"Spherical Units can be used to construct dynamic reconfigurable consequential regions, the geometric bases for Shepard's (1987) theory of stimulus generalization in animals and humans. We derive from Shepard's (1987) generalization theory a particular multi-layer network with dynamic (centers and radii) spherical regions which possesses a specific mass function (Cauchy). This learning model generalizes the configural-cue network model (Gluck & Bower 1988): (1) configural cues can be learned and do not require pre-wiring the power-set of cues, (2) Consequential regions are continuous rather than discrete and (3) Competition amoungst receptive fields is shown to be increased by the global extent of a particular mass function (Cauchy). We compare other common mass functions (Gaussian; used in models of Moody & Darken; 1989, Krushke, 1990) or just standard backpropogation networks with hyperplane/logistic hidden units showing that neither fare as well as models of human generalization and ",Optimization & Theoretical ML,9fd81843ad7f202f26c1a174c7357585-Paper.pdf,1990
Speech Recognition using Connectionist Approaches,"This paper is a summary of SPRINT project aims and results. The project focus on the use of neuro-computing techniques to tackle various problems that remain unsolved in speech recognition. First results concern the use of feed forward nets for phonetic units classification, isolated word recognition, and speaker adaptation. 1 INTRODUCTION Speech is a complex phenomenon but it is useful to divide it into levels of repre sentation. Connectionism paradigms and particularities are exploited to tackle the major problems in relationship with intra and inter speaker variabilities in order to improve the recognizer performance. For that purpose the project has been split into individual tasks which are depicted below: I..-._S_ign_al_-,H H H Param. .., . Phon.tie Lexieon The work described herein concerns : • Parameters-to-Phonetic: Classification of speech parameters using a set of ""phonetic"" symbols and extraction of speech features from signal. • Parameters-to-Lexical: Classification of a s",NLP,a01a0380ca3c61428c26a231f0e49a09-Paper.pdf,1990
FEEDBACK SYNAPSE TO CONE AND LIGHT ADAPTATION,"Light adaptation (LA) allows cone vIslOn to remain functional between twilight and the brightest time of day even though, at anyone time, their intensity-response (I-R) characteristic is limited to 3 log units of the stimu lating light. One mechanism underlying LA, was localized in the outer seg ment of an isolated cone (1,2). We found that by adding annular illhmination, an I-R characteristic of a cone can be shifted along the intensity domain. Neural network involving feedback synapse from horizontal cells to cones is involved to be in register with ambient light level of the periphery. An equivalent electrical circuit with three different transmembrane channels leakage, photocurrent and feedback was used to model static behavior of a cone. SPICE simulation showed that interactions between feedback synapse and the light sensitive conductance in the outer segment can shift the I-R curves along the intensity domain, provided that phototransduction mechan ism is not saturated during max",Optimization & Theoretical ML,a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf,1990
Direct memory access using two cues: Finding,"For lack of alternative models, search and decision processes have provided the dominant paradigm for human memory access using two or more cues, despite evidence against search as an access process (Humphreys, Wiles & Bain, 1990). We present an alternative process to search, based on calculating the intersection of sets of targets activated by two or more cues. Two methods of computing the intersection are presented, one using information about the possible targets, the other constraining the cue-target strengths in the memory matrix. Analysis using orthogonal vectors to represent the cues and targets demonstrates the competence of both processes, and simulations using sparse distributed representations demonstrate the performance of the latter process for tasks involving 2 and 3 cues. 1 INTRODUCTION Consider a task in which a subject is asked to name a word that rhymes with oast. The subject answers ""most"", (or post, host, toast, boast, ... ). Now the subject is asked to find a word ",NLP,a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf,1990
Flight Control in the Dragonfly:,"Neural network simulations of the dragonfly flight neurocontrol system have been developed to understand how this insect uses complex, unsteady aerodynamics. The simulation networks account for the ganglionic spatial distribution of cells as well as the physiologic operating range and the stochastic cellular fIring history of each neuron. In addition the motor neuron firing patterns, ""flight command sequences"", were utilized. Simulation training was targeted against both the cellular and flight motor neuron firing patterns. The trained networks accurately resynthesized the intraganglionic cellular firing patterns. These in tum controlled the motor neuron fIring patterns that drive wing musculature during flight. Such networks provide both neurobiological analysis tools and fIrst generation controls for the use of ""unsteady"" aerodynamics. 1 INTRODUCTION Hebb (1949) proposed a theory of inter-neuronal learning. ""Hebbian Learning"", in which cells acting together as assemblies alter the ef",Computer Vision,a666587afda6e89aec274a3657558a27-Paper.pdf,1990
A Framework for the Cooperation,"We introduce a framework for training architectures composed of several modules. This framework, which uses a statistical formulation of learning systems, provides a unique formalism for describing many classical connectionist algorithms as well as complex systems where several algorithms interact. It allows to design hybrid systems which combine the advantages of connectionist algorithms as well as other learning algorithms. 1 INTRODUCTION Many recent achievements in the connectionist area have been carried out by designing systems where different algorithms interact. For example (Bourlard & Morgan, 1991) have mixed a Multi-Layer Perceptron (MLP) with a Dynamic Programming algorithm. Another impressive application (Le Cun, Boser & al., 1990) uses a very complex multi layer architecture, followed by some statistical decision process. Also, in speech or image recognition systems, input signals are sequentially processed through different modules. Modular systems are the most promising w",Optimization & Theoretical ML,a8c88a0055f636e4a163a5e3d16adab7-Paper.pdf,1990
Continuous Speech Recognition by,"We present a large vocabulary, continuous speech recognition system based on Linked Predictive Neural Networks (LPNN's). The system uses neu ral networks as predictors of speech frames, yielding distortion measures which are used by the One Stage DTW algorithm to perform continuous speech recognition. The system, already deployed in a Speech to Speech Translation system, currently achieves 95%, 58%, and 39% word accuracy on tasks with perplexity 5, 111, and 402 respectively, outperforming sev eral simple HMMs that we tested. We also found that the accuracy and speed of the LPNN can be slightly improved by the judicious use of hidden control inputs. We conclude by discussing the strengths and weaknesses of the predictive approach. 1 INTRODUCTION Neural networks are proving to be useful for difficult tasks such as speech recogni tion, because they can easily be trained to compute smooth, nonlinear, nonpara metric functions from any input space to output space. In speech recognition, the ",NLP,aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf,1990
A B-P ANN Commodity Trader,"An Artificial Neural Network (ANN) is trained to recognize a buy/sell (long/short) pattern for a particular commodity future contract. The Back Propagation of errors algorithm was used to encode the relationship between the Long/Short desired output and 18 fundamental variables plus 6 (or 18) technical variables into the ANN. Trained on one year of past data the ANN is able to predict long/short market positions for 9 months in the future that would have made $10,301 profit on an investment of less than $1000. 1 INTRODUCTION An Artificial Neural Network (ANN) is trained to recognize a long/short pattern for a particular commodity future con tract. The Back-Propagation of errors algorithm was used to encode the relationship between the Long/Short desired out put and 18 fundamental variables plus 6 (or 18) technical variables into the ANN. 2 NETWORK ARCHITECTURE The ANNs used were simple, feed forward, single hidden layer networks with no input units, N hidden units and one output unit. ",Optimization & Theoretical ML,ad13a2a07ca4b7642959dc0c4c740ab6-Paper.pdf,1990
Statistical Mechanics of Temporal Association,"We study the representation of static patterns and temporal associa tions in neural networks with a broad distribution of signal delays. For a certain class of such systems, a simple intuitive understanding of the spatia-temporal computation becomes possible with the help of a novel Lyapunov functional. It allows a quantitative study of the asymptotic network behavior through a statistical mechanical analysis. We present analytic calculations of both retrieval quality and storage capacity and compare them with simulation results. 1 INTRODUCTION Basic computational functions of associative neural structures may be analytically studied within the framework of attractor neural networks where static patterns are stored as stable fixed-points for the system's dynamics. If the interactions between single neurons are instantaneous and mediated by symmetric couplings, there is a Lyapunov function for the retrieval dynamics (Hopfield 1982). The global compu tation corresponds in that case to a ",Optimization & Theoretical ML,ad972f10e0800b49d76fed33a21f6698-Paper.pdf,1990
Cholinergic Modulation May Enhance Cortical,"Combining neuropharmacological experiments with computational model ing, we have shown that cholinergic modulation may enhance associative memory function in piriform (olfactory) cortex. We have shown that the acetylcholine analogue carbachol selectively suppresses synaptic transmis sion between cells within piriform cortex, while leaving input connections unaffected. When tested in a computational model of piriform cortex, this selective suppression, applied during learning, enhances associative memory performance. 1 INTRODUCTION A wide range of behavioral studies support a role for the neurotransmitter acetyl choline in memory function (Kopelman, 1986; Hagan and Morris, 1989). However, the role of acetylcholine in memory function has not been linked to the specific neuropharmacological effects of this transmitter within cerebral cortical networks. For several years, we have explored cerebral cortical associative memory function using the piriform cortex as a model system (Wilson and ",NLP,b2eb7349035754953b57a32e2841bda5-Paper.pdf,1990
Neural Network Application to Diagnostics and,"Diagnosis of faults in complex, real-time control systems is a complicated task that has resisted solution by traditional methods. We have shown that neural networks can be successfully employed to diagnose faults in digitally controlled powertrain systems. This paper discusses the means we use to develop the appropriate databases for training and testing in order to select the optimum network architectures and to provide reasonable estimates of the classification accuracy of these networks on new samples of data. Recent work applying neural nets to adaptive control of an active suspension system is presented. 1 INTRODUCTION This paper reports on work performed on the application of artificial neural systems (ANS) techniques to the diagnosis and control of vehicle systems. Specifically, we have examined the diagnosis of common faults in powertrain systems and investigated the problem of developing an adaptive controller for an active suspension system. In our diagnostic investigations ",Optimization & Theoretical ML,b3967a0e938dc2a6340e258630febd5a-Paper.pdf,1990
EVOLUTION AND LEARNING IN,"Learning can increase the rate of evolution of a population of biological organisms (the Baldwin effect). Our simulations show that in a population of artificial neural networks solving a pattern recognition problem, no learning or too much learning leads to slow evolution of the genes whereas an intermediate amount is optimal. Moreover, for a given total number of training presentations, fastest evoution occurs if different individuals within each generation receive different numbers of presentations, rather than equal numbers. Because genetic algorithms (GAs) help avoid local minima in energy functions, our hybrid learning-GA systems can be applied successfully to complex, high dimensional pattern recognition problems. INTRODUCTION The structure and function of a biological network derives from both its evolutionary precursors and real-time learning. Genes specify (through development) coarse attributes of a neural system, which are then refined based on experience in an environment ",Evolution & Theoretical ML,b6f0479ae87d244975439c6124592772-Paper.pdf,1990
A Lagrangian Approach to Fixed Points,"We present a new way to derive dissipative, optimizing dynamics from the Lagrangian formulation of mechanics. It can be used to obtain both standard and novel neural net dynamics for optimization problems. To demonstrate this we derive standard descent dynamics as well as nonstan dard variants that introduce a computational attention mechanism. 1 INTRODUCTION Neural nets are often designed to optimize some objective function E of the current state of the system via a dissipative dynamical system that has a circuit-like imple mentation. The fixed points of such a system are locally optimal in E. In physics the preferred formulation for many dynamical derivations and calculations is by means of an objective function which is an integral over time of a ""Lagrangian"" function, L. From Lagrangians one usually derives time-reversable, non-dissipative dynamics which cannot converge to a fixed point, but we present a new way to circumvent this limitation and derive optimizing neural net dynamic",Optimization & Theoretical ML,b7b16ecf8ca53723593894116071700c-Paper.pdf,1990
Neural Network Implementation of Admission Control,"A feedforward layered network implements a mapping required to control an unknown stochastic nonlinear dynamical system. Training is based on a novel approach that combines stochastic approximation ideas with back propagation. The method is applied to control admission into a queueing sys tem operating in a time-varying environment. 1 INTRODUCTION A controller for a discrete-time dynamical system must provide, at time tn, a value un for the control variable. Information about the state of the system when such decision is made is available through the observable Yn' The value un is determined on the basis of the current observation Yn and the preceding control action Un-I' Given the information In = (Yn' Un-I), the controllerimplements a mapping In -+ Un. Open-loop controllers suffice in static situations which require a single-valued control policy U : a constant mapping In -+ u·, regardless of In. Closed-loop controllers pro vide a dynamic control action un, determined by the availabl",Optimization & Theoretical ML,b83aac23b9528732c23cc7352950e880-Paper.pdf,1990
Computing with Arrays of Bell-Shaped and,"We consider feed-forward neural networks with one non-linear hidden layer and linear output units. The transfer function in the hidden layer are ei ther bell-shaped or sigmoid. In the bell-shaped case, we show how Bern stein polynomials on one hand and the theory of the heat equation on the other are relevant for understanding the properties of the corresponding networks. In particular, these techniques yield simple proofs of universal approximation properties, i.e. of the fact that any reasonable function can be approximated to any degree of precision by a linear combination of bell shaped functions. In addition, in this framework the problem of learning is equivalent to the problem of reversing the time course of a diffusion pro cess. The results obtained in the bell-shaped case can then be applied to the case of sigmoid transfer functions in the hidden layer, yielding similar universality results. A conjecture related to the problem of generalization is briefly examined. 1 INTRODUCT",Optimization & Theoretical ML,b9228e0962a78b84f3d5d92f4faa000b-Paper.pdf,1990
VLSI Implementation of TInMANN,"A massively parallel, all-digital, stochastic architecture - TlnMAN N - is described which performs competitive and Kohonen types of learning. A VLSI design is shown for a TlnMANN neuron which fits within a small, inexpensive MOSIS TinyChip frame, yet which can be used to build larger networks of several hundred neurons. The neuron operates at a speed of 15 MHz which allows the network to process 290,000 training examples per second. Use of level sensitive scan logic provides the chip with 100% fault coverage, permitting very reliable neural systems to be built. 1 INTRODUCTION Uniprocessor simulation of neural networks has been the norm, but benefiting from the parallelism in neural networks is impossible without specialized hardware. Most hardware-based neural network simulators use a single high-speed AL U or multiple DSP chips connected through communication buses. The first approach does not allow exploration of the effects of parallelism, while the complex processors used in the s",Computer Vision,bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf,1990
SEXNET: A NEURAL NETWORK,"Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30x30 were compressed using a 900x40x900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation ""SexNet"" trained to produce values of 1 for male and o for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans. 1 INTRODUCTION People can capably tell if a human face is male or female. Recognizing the sex of conspecifics is important. ''''hile some animals use pheromones to recognize sex, in humans this task is primarily visual. How is sex recognized from faces? By and large we are unable to say. Although certain features are nearly pathognomonic for on",Computer Vision,bbcbff5c1f1ded46c25d28119a85c6c2-Paper.pdf,1990
From Speech Recognition to Spoken Language,"Spoken language is one of the most natural, efficient, flexible, and econom ical means of communication among humans. As computers play an ever increasing role in our lives, it is important that we address the issue of providing a graceful human-machine interface through spoken language. In this paper, we will describe our recent efforts in moving beyond the scope of speech recognition into the realm of spoken-language understand ing. Specifically, we report on the development of an urban navigation and exploration system called VOYAGER, an application which we have used as a basis for performing research in spoken-language understanding. 1 Introduction Over the past decade, research in speech coding and synthesis has matured to the extent that speech can now be transmitted efficiently and generated with high in telligibility. Spoken input to computers, however, has yet to pass the threshold of practicality. Despite some recent successful demonstrations, current speech recog nition sys",NLP,bbf94b34eb32268ada57a3be5062fe7d-Paper.pdf,1990
Generalization by Weight-Elimination,"Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates. 1 INTRODUCTION Learning procedures for connectionist networks are essentially statistical devices for per forming inductive inference. There is a trade-off between two goals: on the one hand, we want such devices to be as general as possible so that they are able to learn a broad range of problems. This recommends large and flexible networks. On the other hand, the true measure of an inductive device is not how well it performs on the example",Optimization & Theoretical ML,bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf,1990
A Novel Approach to Prediction of the,"Three-dimensional (3D) structures of protein backbones have been pre dicted using neural networks. A feed forward neural network was trained on a class of functionally, but not structurally, homologous proteins, us ing backpropagation learning. The network generated tertiary structure information in the form of binary distance constraints for the Co atoms in the protein backbone. The binary distance between two Co atoms was o if the distance between them was less than a certain threshold distance, and 1 otherwise. The distance constraints predicted by the trained neu ral network were utilized to generate a folded conformation of the protein backbone, using a steepest descent minimization approach. 1 INTRODUCTION One current aim of molecular biology is determination of the (3D) tertiary struc tures of proteins in their folded native state from their sequences of amino acid 523 524 Fredholm, Bohr, Bohr, Brunak, Cotterill, Lautrup, and Thtersen residues. Since Kendrew & Perutz solved the ",Computer Vision,bca82e41ee7b0833588399b1fcd177c7-Paper.pdf,1990
Back Propagation Implementation on the,"The Adaptive Solutions CN APS architecture chip is a general purpose neurocomputer chip. It has 64 processors, each with 4 K bytes of local memory, running at 25 megahertz. It is capable of implementing most current neural network algorithms with on chip learning. This paper dis cusses the implementation of the Back Propagation algorithm on an array of these chips and shows performance figures from a clock accurate hard ware simulator. An eight chip configuration on one board can update 2.3 billion connections per second in learning mode and process 9.6 billion connections per second in feed forward mode. 1 Introduction The huge computational requirements of neural networks and their natural paral lelism have led to a number of interesting hardware innovations for executing such networks. Most investigators have created large parallel computers or special pur pose chips limited to a small subset of algorithms. The Adaptive Solutions CNAPS architecture describes a general-purpose 64-pro",Optimization & Theoretical ML,beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf,1990
Asymptotic slowing down of the,"If patterns are drawn from an n-dimensional feature space according to a probability distribution that obeys a weak smoothness criterion, we show that the probability that a random input pattern is misclassified by a nearest-neighbor classifier using M random reference patterns asymptoti cally satisfies a + PM(error) """" Poo(error) M2/n' for sufficiently large values of M. Here, Poo(error) denotes the probability of error in the infinite sample limit, and is at most twice the error of a Bayes classifier. Although the value of the coefficient a depends upon the underlying probability distributions, the exponent of M is largely distri bution free. We thus obtain a concise relation between a classifier's ability to generalize from a finite reference sample and the dimensionality of the feature space, as well as an analytic validation of Bellman's well known ""curse of dimensionality."" 1 INTRODUCTION One of the primary tasks assigned to neural networks is pattern classification. Com mon appl",Optimization & Theoretical ML,c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf,1990
Connectionist Approaches to the Use of,"Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Mod els (HMMs). The advantages of a speech recognition system incor porating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. This paper presents results on the speaker-dependent portion of DARPA's English language Resource Management database. Results support the previously reported utility of MLP probability estimation for continuous speech recog nition. An additional approach we are pursuing is to use MLPs as nonlinear predictors for autoregressive HMMs. While this is shown to be more compatible with the HMM formalism, it still suffers from several limitations. This approach is generalized to take ac count of time correlation between successive observations, without any restrictive assumptions about t",NLP,c058f544c737782deacefa532d9add4c-Paper.pdf,1990
"A Second-Order Translation, Rotation and","A second-order architecture is presented here for translation, rotation and scale invariant processing of 2-D images mapped to n input units. This new architecture has a complexity of O( n) weights as opposed to the O( n3) weights usually required for a third-order, rotation invariant architecture. The reduction in complexity is due to the use of discrete frequency infor mation. Simulations show favorable comparisons to other neural network architectures. 1 INTRODUCTION Multiplicative interactions in neural networks have been proposed (Pitts and Mc Culloch, 1947; Giles and Maxwell, 1987; McClelland et aI, 1988) both to explain bi ological neural functions and to provide invariances in pattern recognition. Higher order neural networks are useful for invariant pattern recognition problems, but their complexity prohibits their use in mal1Y large image processing applications. The complexity of the third-order rotation invariant neural network of Reid et aI, 1990 is 0(n3), which will clear",Computer Vision,c3e878e27f52e2a57ace4d9a76fd9acf-Paper.pdf,1990
Signal Processing by Multiplexing and,"Signal processing capabilities of biological neurons are investigated. Temporally coded signals in neurons can be multiplexed to increase the transmission capacity. Multiplexing of signal is suggested in bi-threshold neurons with high-threshold and low-thre shold for switching firing II II II II modes. To extract the signal embedded in the interspike intervals of firing, the encoded signal are de multiplexed and multiplexed by a network of neurons with delayed-line circuitry for signal processing. The temporally coded input signal is transformed spatially by mapping the firing intervals topographically to the output of the network, thus decoding the specific firing inters pike-intervals. The network also provides a band-pass filtering capability where the variability of the timing of the original signal can be decoded. 1 INTRODUCTION Signals of biological neurons are encoded in the firing patterns of spike trains or the time series of action potentials generated by neurons. The signal ",Optimization & Theoretical ML,c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf,1990
Lg DEPTH ESTIMATION AND RIPPLE FIRE,"This srudy has demonstrated how artificial neural networks (ANNs) can be used to characterize seismic sources using high-frequency regional seismic data. We have taken the novel approach of using ANNs as a research tool for obtaining seismic source information, specifically depth of focus for earthquakes and ripple-fire characteristics for economic blasts, rather than as just a feature classifier between earthquake and explosion populations. Overall, we have found that ANNs have potential applications to seismic event characterization and identification, beyond just as a feature classifier. In future studies, these techniques should be applied to actual data of regional seismic events recorded at the new regional seismic arrays. The results of this study indicates that an ANN should be evaluated as part of an operational seismic event identification system. 1 INTRODUCTION 1.1 NEURAL NET,\VORKS FOR SEISl\UC SOURCE ANALYSIS In this study, we have explored the application of artificial ne",Computer Vision,c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf,1990
Adaptive Range Coding,"This paper examines a class of neuron based learning systems for dynamic control that rely on adaptive range coding of sensor inputs. Sensors are assumed to provide binary coded range vectors that coarsely describe the system state. These vectors are input to neuron-like processing elements. Output decisions generated by these ""neurons"" in turn affect the system state, subsequently producing new inputs. Reinforcement signals from the environment are received at various intervals and evaluated. The neural weights as well as the ran g e b u n dar i e s determining the output decisions are 0 then altered with the goal of maximizing future reinforcement from the environment. Preliminary experiments show the promise of adapting ""neural receptive fields"" when learning dynamical control. The observed performance with this method exceeds that of earlier approaches. 486 Adaptive Range Coding 487 1 INTRODUCTION A major criticism of unsupervised learning and control techniques such as those used ",Optimization & Theoretical ML,caf1a3dfb505ffed0d024130f58c5cfa-Paper.pdf,1990
RecNorm: Simultaneous Normalisation and,"A particular form of neural network is described, which has terminals for acoustic patterns, class labels and speaker parameters. A method of training this network to ""tune in"" the speaker parameters to a particular speaker is outlined, based on a trick for converting a supervised network to an unsupervised mode. We describe experiments using this approach in isolated word recognition based on whole-word hidden Markov models. The results indicate an improvement over speaker-independent perfor mance and, for unlabelled data, a performance close to that achieved on labelled data. 1 INTRODUCTION We are concerned to emulate some aspects of perception. In particular, the way that a stimulus which is ambiguous, perhaps because of unknown lighting conditions, can become unambiguous in the context of other such stimuli: the fact that they are subject to tbe same unknown conditions gives our perceptual apparatus enough constraints to solve tbe problem. Individual words are often ambiguous even ",NLP,cd00692c3bfe59267d5ecfac5310286c-Paper.pdf,1990
A four neuron circuit accounts for change sensitive,"In salamander retina, the response of On-Off ganglion cells to a central flash is reduced by movement in the receptive field surround. Through computer simulation of a 2-D model which takes into account their anatomical and physiological properties, we show that interactions between four neuron types (two bipolar and two amacrine) may be responsible for the generation and lateral conductance of this change sensitive inhibition. The model shows that the four neuron circuit can account for previously observed movement sensitive reductions in ganglion cell sensitivity and allows visualization and prediction of the spatio-temporal pattern of activity in change sensitive retinal cells. 1 INTRODUCTION In the salamander retina. the response of transient (On-Off) ganglion cells to a central flash is reduced by movement in the receptive field surround (Werblin. 1972; Werblin & Copenhagen. 1974) as illustrated in Fig 1. This phenomenon requires the detection of change in the surround and the lat",Computer Vision,cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf,1990
How Receptive Field Parameters Affect Neural,"We identify the three principle factors affecting the performance of learn ing by networks with localized units: unit noise, sample density, and the structure of the target function. We then analyze the effect of unit recep tive field parameters on these factors and use this analysis to propose a new learning algorithm which dynamically alters receptive field properties during learning. 1 LEARNING WITH LOCALIZED RECEPTIVE FIELDS Locally-tuned representations are common in both biological and artificial neural networks. Several workers have analyzed the effect of receptive field size, shape, and overlap on representation accuracy: (Baldi, 1988), (Ballard, 1987), and (Hinton, 1986). This paper investigates the additional interactions introduced by the task of function learning. Previous studies which have considered learning have for the most part restricted attention to the use of the input probability distribution to determine receptive field layout (Kohonen, 1984) and (Moody and Darke",Deep Learning,d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf,1990
Proximity Effect Corrections in Electron Beam,We have used a neural network to compute corrections for images written by electron beams to eliminate the proximity effects caused by electron scattering. Iterative methods are effective. but require prohibitively computation time. We have instead trained a neural network to perform equivalent corrections. resulting in a significant speed-up. We have examined hardware implementations using both analog and digital electronic networks. Both had an acceptably small error of 0.5% compared to the iterative results. Additionally. we verified that the neural network correctly generalized the solution of the problem to include patterns not contained in its training set. We have experimentally verified this approach on a Cambridge Instruments EBMF 10.5 exposure system. 1 INTRODUCTION Scattering imposes limitations on the minImum feature sizes that can be reliably obtained with electron beam lithography. Linewidth corrections can be used to control the dimensions of isolated features (i.e. intr,Computer Vision,d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf,1990
Exploiting Syllable Structure,"In a previous paper (Touretzky & Wheeler, 1990a) we showed how adding a clustering operation to a connectionist phonology model produced a parallel pro cessing account of certain ""iterative"" phenomena. In this paper we show how the addition of a second structuring primitive, syllabification, greatly increases the power of the model. We present examples from a non-Indo-European language that appear to require rule ordering to at least a depth of four. By adding syllab ification circuitry to structure the model's perception of the input string, we are able to handle these examples with only two derivational steps. We conclude that in phonology, derivation can be largely replaced by structuring. 1 Introduction In linguistics a grammar is an abstract formal system describing a language. The term psycho-grammar has been suggested for systems that express the linguistic knowledge that actually exists in speakers' heads (George, 1989). Psycho-grammars may differ from grammars as a result of p",NLP,d709f38ef758b5066ef31b18039b8ce5-Paper.pdf,1990
Integrated Modeling and Control,"This is a summary of results with Dyna, a class of architectures for intel ligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned forward model of the world. We describe and show results for two Dyna architectures, Dyna-AHC and Dyna-Q. Using a navigation task, results are shown for a simple Dyna-AHC system which simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. We show that Dyna-Q architectures (based on Watkins's Q-Iearning) are easy to adapt for use in changing environments. 1 Introduction to Dyna Dyna architectures (Sutton, 1990) use learning algorithms to approximate the con ventional optimal control technique known as dynamic programming (DP) (Bell man, 1957; Bertsekas, 1987). DP itself is not a learning method, but rather a compu",Reinforcement Learning,d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf,1990
A Method for the Efficient Design,"We introduce a method for the efficient design of a Boltzmann machine (or a Hopfield net) that computes an arbitrary given Boolean function f . This method is based on an efficient simulation of acyclic circuits with threshold gates by Boltzmann machines. As a consequence we can show that various concrete Boolean functions f that are relevant for classification problems can be computed by scalable Boltzmann machines that are guaranteed to converge to their global maximum configuration with high probability after constantly many steps. 1 INTRODUCTION A Boltzmann machine ([AHS], [HS], [AK]) is a neural network model in which the units update their states according to a stochastic decision rule. It consists of a set U of units, a set C of unordered pairs of elements of U, and an assignment of connection strengths S : C -- R. A configuration of a Boltzmann machine is a map k : U -- {O, I}. The consensus C(k) of a configuration k is given by C(k) = L:{u,v}ECS({u,v}) .k(u) .k(v). If the Bolt",Optimization & Theoretical ML,d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf,1990
A Connectionist Learning Control,"A novel learning control architecture is used for navigation. A sophisti cated test-bed is used to simulate a cylindrical robot with a sonar belt in a planar environment. The task is short-range homing in the pres ence of obstacles. The robot receives no global information and assumes no comprehensive world model. Instead the robot receives only sensory information which is inherently limited. A connectionist architecture is presented which incorporates a large amount of a priori knowledge in the form of hard-wired networks, architectural constraints, and initial weights. Instead of hard-wiring static potential fields from object models, myarchi tecture learns sensor-based potential fields, automatically adjusting them to avoid local minima and to produce efficient homing trajectories. It does this without object models using only sensory information. This research demonstrates the use of a large modular architecture on a difficult task. 1 OVERVIEW I present a connectionist learning co",Reinforcement Learning,dc912a253d1e9ba40e2c597ed2376640-Paper.pdf,1990
On Stochastic Complexity and Admissible,"Given some training data how should we choose a particular network clas sifier from a family of networks of different complexities? In this paper we discuss how the application of stochastic complexity theory to classifier design problems can provide some insights into this problem. In particular we introduce the notion of admissible models whereby the complexity of models under consideration is affected by (among other factors) the class entropy, the amount of training data, and our prior belief. In particular we discuss the implications of these results with respect to neural architec tures and demonstrate the approach on real data from a medical diagnosis task. 1 Introduction and Motivation In this paper we examine in a general sense the application of Minimum Description Length (MDL) techniques to the problem of selecting a good classifier from a large set of candidate models or hypotheses. Pattern recognition algorithms differ from more conventional statistical modeling techniques",Optimization & Theoretical ML,ddb30680a691d157187ee1cf9e896d03-Paper.pdf,1990
Analog Computation at a Critical Point: A Novel,"\Ve show that a simple spin system bia.sed at its critical point can en code spatial characteristics of external signals, sHch as the dimensions of ""objects"" in the visual field. in the temporal correlation functions of indi vidual spins. Qualit.ative arguments suggest that regularly firing neurons should be described by a planar spin of unit lengt.h. and such XY models exhibit critical dynamics over a broad range of parameters. \Ve show how to extract these spins from spike trains and then mea'3ure t.he interaction Hamilt.onian using simulations of small dusters of cells. Static correla tions among spike trains obtained from simulations of large arrays of cells are in agreement with the predictions from these Hamiltonians, and dy namic correlat.ions display the predicted encoding of spatial information. \Ve suggest that this novel representation of object dinwnsions in temporal correlations may be relevant t.o recent experiment.s on oscillatory neural firing in the visual cortex. 1 IN",Deep Learning,e0c641195b27425bb056ac56f8953d24-Paper.pdf,1990
Integrated Segmentation and Recognition of,"Neural network algorithms have proven useful for recognition of individ ual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Con ventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recog nized yet one cannot properly recognize a character until it is segmented. We present here a neural network algorithm that simultaneously segments and recognizes in an integrated system. This algorithm has several novel features: it uses a supervised learning algorithm (backpropagation), but is able to take position-independent information as targets and self-organize the activities of the units in a competitive fashion to infer the positional information. We demonstrate this ability with overlapping hand-printed numerals. 1 INTRODUCTION A major problem with ",Computer Vision,e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf,1990
Speech Recognition,"The Neural Prediction Model is the speech recognition model based on pattern prediction by multilayer perceptrons. Its effectiveness was con firmed by the speaker-independent digit recognition experiments. This paper presents an improvement in the model and its application to large vocabulary speech recognition, based on subword units. The improvement involves an introduction of ""backward prediction,"" which further improves the prediction accuracy of the original model with only ""forward predic tion"". In application of the model to speaker-dependent large vocabulary speech recognition, the demi-syllable unit is used as a subword recognition unit. Experimental results indicated a 95.2% recognition accuracy for a 5000 word test set and the effectiveness was confirmed for the proposed model improvement and the demi-syllable subword units. 1 INTRODUCTION The Neural Prediction Model (NPM) is the speech recognition model based on pattern prediction by multilayer perceptrons (MLPs). Its effec",NLP,e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf,1990
A Recurrent Neural Network Model of,"A three-layered neural network model was used to explore the organization of the vestibulo-ocular reflex (VOR). The dynamic model was trained using recurrent back-propagation to produce compensatory, long duration eye muscle motoneuron outputs in response to short duration vestibular afferent head velocity inputs. The network learned to produce this response prolongation, known as velocity storage, by developing complex, lateral inhibitory interac tions among the interneurons. These had the low baseline, long time constant, rectified and skewed responses that are characteristic of real VOR inter neurons. The model suggests that all of these features are interrelated and result from lateral inhibition. 1 SIGNAL PROCESSING IN THE VOR The VOR stabilizes the visual image by producing eye rotations that are nearly equal and opposite to head rotations (Wilson and Melvill Jones 1979). The VOR utilizes head rotational velocity signals, which originate in the semicircular canal receptors of the",NLP,eddea82ad2755b24c4e168c5fc2ebd40-Paper.pdf,1990
The Devil and the Network:,"Robustness is a commonly bruited property of neural networks; in particu lar, a folk theorem in neural computation asserts that neural networks-in contexts with large interconnectivity-continue to function efficiently, al beit with some degradation, in the presence of component damage or loss. A second folk theorem in such contexts asserts that dense interconnectiv ity between neural elements is a sine qua non for the efficient usage of resources. These premises are formally examined in this communication in a setting that invokes the notion of the ""devil"" 1 in the network as an agent that produces sparsity by snipping connections. 1 ON REMOVING THE FOLK FROM THE THEOREM Robustness in the presence of component damage is a property that is commonly attributed to neural networks. The content of the following statement embodies this sentiment. Folk Theorem 1: Computation in neural networks is not substantially affected by damage to network components. While such a statement is manifestly ",Optimization & Theoretical ML,eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf,1990
Development and Spatial Structure of Cortical,"Feature selective cells in the primary visual cortex of several species are or ganized in hierarchical topographic maps of stimulus features like ""position in visual space"", ""orientation"" and"" ocular dominance"". In order to un derstand and describe their spatial structure and their development, we in vestigate a self-organizing neural network model based on the feature map algorithm. The model explains map formation as a dimension-reducing mapping from a high-dimensional feature space onto a two-dimensional lattice, such that ""similarity"" between features (or feature combinations) is translated into ""spatial proximity"" between the corresponding feature selective cells. The model is able to reproduce several aspects of the spatial structure of cortical maps in the visual cortex. 1 Introduction Cortical maps are functionally defined structures of the cortex, which are charac terized by an ordered spatial distribution of functionally specialized cells along the cortical surface. In the pr",Computer Vision,ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf,1990
Connectionist Implementation of a Theory of Generalization,"Empirically, generalization between a training and a test stimulus falls off in close approximation to an exponential decay function of distance between the two stimuli in the ""stimulus space"" obtained by multidimensional scaling. Math ematically, this result is derivable from the assumption that an individual takes the training stimulus to belong to a ""consequential"" region that includes that stimulus but is otherwise of unknown location, size, and shape in the stimulus space (Shepard, 1987). As the individual gains additional information about the consequential region-by finding other stimuli to be consequential or nOl-the theory predicts the shape of the generalization function to change toward the function relating actual probability of the consequence to location in the stimulus space. This paper describes a natural connectionist implementation of the theory, and illustrates how implications of the theory for generalization, discrimination, and classification learning can be explo",Optimization & Theoretical ML,efe937780e95574250dabe07151bdc23-Paper.pdf,1990
Discrete Affine Wavelet Transforms For Analysis,"In this paper we show that discrete affine wavelet transforms can provide a tool for the analysis and synthesis of standard feedforward neural net works. It is shown that wavelet frames for L2(IR) can be constructed based upon sigmoids. The spatia-spectral localization property of wavelets can be exploited in defining the topology and determining the weights of a feedforward network. Training a network constructed using the synthe sis procedure described here involves minimization of a convex cost func tional and therefore avoids pitfalls inherent in standard backpropagation algorithms. Extension of these methods to L2(IRN) is also discussed. 1 INTRODUCTION Feedforward type neural network models constructed from empirical data have been found to display significant predictive power [6]. Mathematical justification in sup port of such predictive power may be drawn from various density and approximation theorems [1, 2, 5]. Typically this latter work doesn't take into account the spec tral",Optimization & Theoretical ML,f2fc990265c712c49d51a18a32b39f0c-Paper.pdf,1990
Convergence of a Neural Network Classifier,"In this paper, we prove that the vectors in the LVQ learning algorithm converge. We do this by showing that the learning algorithm performs stochastic approximation. Convergence is then obtained by identifying the appropriate conditions on the learning rate and on the underlying statistics of the classification problem. We also present a modification to the learning algorithm which we argue results in convergence of the LVQ error to the Bayesian optimal error as the appropriate parameters become large. 1 Introduction Learning Vector Quantization (LVQ) originated in the neural network community and was introduced by Kohonen (Kohonen [1986]). There have been extensive simulation studies reported in the literature demonstrating the effectiveness of LVQ as a classifier and it has generated considerable interest as the training times asso ciated with LVQ are significantly less than those associated with backpropagation networks. In this paper we analyse the convergence properties of LVQ. Us",Optimization & Theoretical ML,f4f6dce2f3a0f9dada0c2b5b66452017-Paper.pdf,1990
Optimal Sampling of Natural Images: A Design,"We formulate the problem of optimizing the sampling of natural images using an array of linear filters. Optimization of information capacity is constrained by the noise levels of the individual channels and by a penalty for the construction of long-range interconnections in the array. At low signal-to-noise ratios the optimal filter characteristics correspond to bound states of a Schrodinger equation in which the signal spectrum plays the role of the potential. The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates. The observed scale invariance of natural images plays an essential role in this construction. 363 364 Bialek, Ruderman, and Zee 1 Introduction Under certain conditions the visual system is capable of performing extremely effi cient signal processing [I]. One ofthe major theoretical issues in neural computation is to understand how this efficiency is reached given the constrain",Computer Vision,f61d6947467ccd3aa5af24db320235dd-Paper.pdf,1990
Time Trials on Second-Order and,"The performance of seven minimization algorithms are compared on five neural network problems. These include a variable-step-size algorithm, conjugate gradient, and several methods with explicit analytic or numerical approximations to the Hessian. 1 Introduction There are several minimization algorithms in use which in the nth iteration vary the ith coordinate Xi in the direction S~+l = r~s~ + h~V~ (1) , "" "" ;:.1 where Vf = is the ith component of the gradient of the error measure E • z .. = at zn, sO VO, and rn and hn are chosen differently in different algorithms. Algorithms also use various methods for choosing the step size .,.,n to be taken along direction sn. In this study, 7 algorithms were compared on a suite of 5 neural network problems. These algorithms are defined in table 1. 1.1 The algorithms The algorithms investigated are Silva and Almeida's variable-step-size algorithm (Silva, 1990) which closely resembles Toolenaere's ""SuperSAB"" algorithm (Toole- 977 978 Rohwer r--r---",Optimization & Theoretical ML,f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf,1990
A competitive modular connectionist architecture,"We describe a multi-network, or modular, connectionist architecture that captures that fact that many tasks have structure at a level of granularity intermediate to that assumed by local and global function approximation schemes. The main innovation of the architecture is that it combines associative and competitive learning in order to learn task decompositions. A task decomposition is discovered by forcing the networks comprising the architecture to compete to learn the training patterns. As a result of the competition, different networks learn different training patterns and, thus, learn to partition the input space. The performance of the architecture on a ""what"" and ""where"" vision task and on a multi-payload robotics task are presented. 1 INTRODUCTION A dichotomy has arisen in recent years in the literature on nonlinear network learn ing rules between local approximation of functions and global approximation of functions. Local approximation, as exemplified by lookup tables, neare",Reinforcement Learning,f74909ace68e51891440e4da0b65a70c-Paper.pdf,1990
A Model of Distributed Sensorimotor Control in,"In response to a puff of wind, the American cockroach turns away and runs. The circuit underlying the initial turn of this escape response consists of three populations of individually identifiable nerve cells and appears to em ploy distributed representations in its operation. We have reconstructed several neuronal and behavioral properties of this system using simplified neural network models and the backpropagation learning algorithm con strained by known structural characteristics of the circuitry. In order to test and refine the model, we have also compared the model's responses to various lesions with the insect's responses to similar lesions. 1 INTRODUCTION It is becoming generally accepted that many behavioral and cognitive capabilities of the human brain must be understood as resulting from the cooperative activity of populations of nerve cells rather than the individual activity of any particu lar cell. For example, distributed representation of orientation by populations of ",Reinforcement Learning,f85454e8279be180185cac7d243c5eb3-Paper.pdf,1990
A VLSI Neural Network for Color Constancy,"A system for color correction has been designed, built, and tested suc cessfully; the essential components are three custom chips built using sub threshold analog CMOS VLSI. The system, based on Land's Retinex the ory of color constancy, produces colors similar in many respects to those produced by the visual system. Resistive grids implemented in analog VLSI perform the smoothing operation central to the algorithm at video rates. With the electronic system, the strengths and weaknesses of the algorithm are explored. 1 A MODEL FOR COLOR CONSTANCY Humans have the remarkable ability to perceive object colors as roughly constant even if the color of the illumination is varied widely. Edwin Land, founder of the Polaroid Corporation, models the computation that results in this ability as three identical center-surround operations performed independently in three color planes, such as red, green, and blue (Land, 1986). The basis for this model is as follows. Consider first an array of grey p",Computer Vision,f8c1f23d6a8d8d7904fc0ea8e066b3bb-Paper.pdf,1990
Stereopsis by a Neural Network,This paper presents a neural network (NN) approach to the problem of stereopsis. The correspondence problem (finding the correct matches between the pixels of the epipolar lines of the stereo pair from amongst all the possible matches) is posed as a non-iterative many-to-one mapping. A two-layer feed forward NN architecture is developed to learn and code this nonlinear and complex mapping using the back-propagation learning rule and a training set. The important aspect of this technique is that none of the typical constraints such as uniqueness and continuity are explicitly imposed. All the applicable constraints are learned and internally coded by the NN enabling it to be more flexible and more accurate than the existing methods. The approach is successfully tested on several random dot stereograms. It is shown that the net can generalize its learned map ping to cases outside its training set. Advantages over the Marr-Poggio Algorithm are discussed and it is shown that the NN performa,Computer Vision,f9b902fc3289af4dd08de5d1de54f68f-Paper.pdf,1990
Supervised Learning,"In this work we describe a new method that adjusts time-delays and the widths of time-windows in artificial neural networks automatically. The input of the units are weighted by a gaussian input-window over time which allows the learning rules for the delays and widths to be derived in the same way as it is used for the weights. Our results on a phoneme classification task compare well with results obtained with the TDNN by Waibel et al., which was manually optimized for the same task. 1 INTRODUCTION The processing of pattern-sequences has been investigated with several neural network architectures. One approach to processing of temporal context with neural networks is to implement time-delays. This approach is neurophysiologically plausible, because real axons have a limited conduction speed (which is dependent on the diameter of the axon and whether it is myelinated or not). Additionally, the length of most axons is much greater than the euclidean distance between the connected neuro",Computer Vision,faa9afea49ef2ff029a833cccc778fd0-Paper.pdf,1990
Learning Theory and Experiments with,"We apply the theory of Tishby, Levin, and Sol1a (TLS) to two problems. First we analyze an elementary problem for which we find the predictions consistent with conventional statistical results. Second we numerically examine the more realistic problem of training a competitive net to learn a probability density from samples. We find TLS useful for predicting average training behavior. . 1 TLS APPLIED TO LEARNING DENSITIES Recently a theory of learning has been constructed which describes the learning of a relation from examples (Tishby, Levin, and Sol1a, 1989), (Schwarb, Samalan, Sol1a, and Denker, 1990). The original derivation relies on a statistical mechanics treatment of the probability of independent events in a system with a specified average value of an additive error function. The resulting theory is not restricted to learning relations and it is not essentially statistical mechanical. The TLS theory can be derived from the principle of maz imum entropy, a general inference tool",Optimization & Theoretical ML,fb7b9ffa5462084c5f4e7e85a093e6d7-Paper.pdf,1990
Further Studies of a Model for the,"We describe a computational model of the development and regenera tion of specific eye-brain circuits. The model comprises a self-organiz ing map-forming network which uses local Hebb rules, constrained by (genetically determined) molecular markers. Various simulations of the development and regeneration of eye-brain maps in fish and frogs are described, in particular successful simulations of experiments by Schmidt-Cicerone-Easter; Meyer; and Yo on. 1 INTRODUCTION In a previous paper published in last years proceedings (Cowan & Friedman 1990) we outlined a new computational model for the development and regeneration of eye-brain maps. We indicated that such a model can simulate the results of a number of the more complicated surgical manipulations carried out on the visual pathways of goldfish and frogs. In this paper we describe in more detail some of these experiments, and our simulations of them. 1.1 EYE-BRAIN MAPS We refer to figure 1 from the previous paper which shows the retina",Optimization & Theoretical ML,fccb60fb512d13df5083790d64c4d5dd-Paper.pdf,1990
The Recurrent Cascade-Correlation Architecture,"Recurrent Cascade-Correlation CRCC) is a recurrent version of the Cascade Correlation learning architecture of Fah Im an and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training. 1 THE ARCHITECTURE Cascade-Correlation [Fahlman, 1990] is a supervised learning architecture that builds a near-minimal multi-layer network topology in the course of training. Initially the network contains only inputs, output units, and the connections between them. This single layer of connections is trained (using the Quickprop algorithm [Fahlman, 1988]) to minimize the error.",NLP,fe73f687e5bc5280214e0486b273a5f9-Paper.pdf,1990
Sequential Adaptation of Radial Basis Function,"We develop a sequential adaptation algorithm for radial basis function (RBF) neural networks of Gaussian nodes, based on the method of succes sive F-Projections. This method makes use of each observation efficiently in that the network mapping function so obtained is consistent with that information and is also optimal in the least L 2-norm sense. The RBF network with the F-Projections adaptation algorithm was used for pre dicting a chaotic time-series. We compare its performance to an adapta tion scheme based on the method of stochastic approximation, and show that the F-Projections algorithm converges to the underlying model much faster. 1 INTRODUCTION Sequential adaptation is important for signal processing applications such as time series prediction and adaptive control in nonstationary environments. With increas ing computational power, complex algorithms that can offer better performance can be used for these tasks. A sequential adaptation scheme, called the method of successive ",Optimization & Theoretical ML,ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf,1990
English Alphabet Recognition,"A recognition system is reported which recognizes names spelled over the telephone with brief pauses between letters. The system uses separate neural networks to locate segment boundaries and classify letters. The letter scores are then used to search a database of names to find the best scoring name. The speaker-independent classification rate for spoken let ters is 89%. The system retrieves the correct name, spelled with pauses between letters, 91 % of the time from a database of 50,000 names. 1 INTRODUCTION The English alphabet is difficult to recognize automatically because many letters sound alike; e.g., BID, PIT, VIZ and F IS. When spoken over the telephone, the information needed to discriminate among several of these pairs, such as F IS, PIT, BID and VIZ, is further reduced due to the limited bandwidth of the channel Speaker-independent recognition of spelled names over the telephone is difficult due to variability caused by channel distortions, different handsets, and a variet",Computer Vision,01f78be6f7cad02658508fe4616098a9-Paper.pdf,1991
Recognizing Overlapping Hand-Printed Characters by,"This paper describes an approach, called centered object integrated seg mentation and recognition (COISR). for integrating object segmenta tion and recognition within a single neural network. The application is hand-printed character recognition. 1\vo versions of the system are described. One uses a backpropagation network that scans exhaus tively over a field of characters and is trained to recognize whether it is centered over a single character or between characters. When it is centered over a character, the net classifies the cnaracter. The approach is tested on a dataset of hand-printed digits. Vel)' low errOr rates are reported. The second version, COISR-SACCADE, avoids the need for exhaustive scans. The net is trained as before. but also is trained to compute ballistic 'eye' movements that enable the input window to jump from one character to the next. The common model of visual processing includes multiple, independent stages. First, flltering operations act on the raw image to",Computer Vision,0353ab4cbed5beae847a7ff6e220b5cf-Paper.pdf,1991
Adaptive Soft Weight Tying,"One way of simplifying neural networks so they generalize better is to add an extra t.erm 10 the error fUllction that will penalize complexit.y. \Ve propose a new penalt.y t.erm in which the dist rihution of weight values is modelled as a mixture of multiple gaussians. C nder this model, a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values. We allow the parameters of the mixture model to adapt at t.he same time as t.he network learns. Simulations demonstrate that this complexity term is more effective than previous complexity terms. 1 Introduction A major problem in training artificial nellral network:> is to ellsure t.hat they wIll gel/eraiIze well to ra. .. ,f'~ thaI they h(lvl> 1I0t been tralHeu OIl. SUIlle recellt t.heuretical results (Baurn anu Iiallssier. 10S~I) Itave :,.ug,g,e~teU that ill order to guaralltee goou generalizatioll Ilw <IIIIOllnl of lllforillatiull requireJ te. dlr""L""t1~ ""p~'cif~ Ihe Ulltpu",Optimization & Theoretical ML,05f971b5ec196b8c65b75d2ef8267331-Paper.pdf,1991
Multi-State Time Delay Neural Networks,"We present the ""Multi-State Time Delay Neural Network"" (MS-TDNN) as an extension of the TDNN to robust word recognition. Unlike most other hybrid methods. the MS-TDNN embeds an alignment search procedure into the con nectionist architecture. and allows for word level supervision. The resulting system has the ability to manage the sequential order of subword units. while optimizing for the recognizer performance. In this paper we present extensive new evaluations of this approach over speaker-dependent and speaker-indepen dent connected alphabet. 1 INTRODUCTION Classification based Neural Networks (NN) have been successfully applied to phoneme recognition tasks. Extending those classification capabilities to word recognition is an important research direction in speech recognition. However. connectionist architectures do not model time alignment properly. and they have to be combined with a Dynamic Pro gramming (DP) alignment procedure to be applied to word recognition. Most of these ""h",NLP,069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf,1991
Neural Network - Gaussian Mixture Hybrid for,"The subject of this paper is the integration of multi-layered Artificial Neu ral Networks (ANN) with probability density functions such as Gaussian mixtures found in continuous density Hidden Markov Models (HMM). In the first part of this paper we present an ANN/HMM hybrid in which all the parameters of the the system are simultaneously optimized with respect to a single criterion. In the second part of this paper, we study the relationship between the density of the inputs of the network and the density of the outputs of the networks. A few experiments are presented to explore how to perform density estimation with ANNs. 1 INTRODUCTION This paper studies the integration of Artificial Neural Networks (ANN) with prob ability density functions (pdf) such as the Gaussian mixtures often used in contin uous density Hidden Markov Models. The ANNs considered here are multi-layered or recurrent networks with hyperbolic tangent hidden units. Raw or preprocessed data is fed to the ANN, and the o",Deep Learning,07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf,1991
Threshold Network Learning in the Presence of,"This paper applies the theory of Probably Approximately Correct (PAC) learning to multiple output feedforward threshold networks in which the weights conform to certain equivalences. It is shown that the sample size for reliable learning can be bounded above by a formula similar to that required for single output networks with no equivalences. The best previ ously obtained bounds are improved for all cases. 1 INTRODUCTION This paper develops the results of Baum and Haussler [3] bounding the sample sizes required for reliable generalisation of a single output feedforward threshold network. They prove their result using the theory of Probably Approximately Correct (PAC) learning introduced by Valiant [11]. They show that for 0 < «: :S 1/2, if a sample of sIze 64W 64N m 2:: rna = -- log -- «: «: is loaded into a feedforward network of linear threshold units with N nodes and W weights, so that a fraction 1-«:/2 of the examples are correctly classified, then with confidence approaching cert",Optimization & Theoretical ML,087408522c31eeb1f982bc0eaf81d35f-Paper.pdf,1991
Node Splitting: A Constructive Algorithm for,"A constructive algorithm is proposed for feed-forward neural networks, which uses node-splitting in the hidden layers to build large networks from smaller ones. The small network forms an approximate model of a set of training data, and the split creates a larger more powerful network which is initialised with the approximate solution already found. The insufficiency of the smaller network in modelling the system which generated the data leads to oscillation in those hidden nodes whose weight vectors cover re gions in the input space where more detail is required in the model. These nodes are identified and split in two using principal component analysis, allowing the new nodes t.o cover the two main modes of each oscillating vector. Nodes are selected for splitting using principal component analysis on the oscillating weight vectors, or by examining the Hessian matrix of second derivatives of the network error with respect to the weight.s. The second derivat.ive method can also be app",Deep Learning,0fcbc61acd0479dc77e3cccc0f5ffca7-Paper.pdf,1991
Structural Risk Minimization,"The method of Structural Risk Minimization refers to tuning the capacity of the classifier to the available amount of training data. This capac ity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) learning algorithm. Actions based on these three factors are combined here to control the ca pacity of linear classifiers and improve generalization on the problem of handwritten digit recognition. 1 RISK MINIMIZATION AND CAPACITY 1.1 EMPIRICAL RISK MINIMIZATION A common way of training a given classifier is to adjust the parameters w in the classification function F( x, w) to minimize the training error Etrain, i.e. the fre quency of errors on a set of p training examples. Etrain estimates the expected risk based on the empirical data provided by the p available examples. The method is thus called Empirical Risk Minimization. But the classification function F(x, w*) which minimizes the empirical risk does not ",Optimization & Theoretical ML,10a7cdd970fe135cf4f7bb55c0e3b59f-Paper.pdf,1991
Computer Recognition of Wave Location,"Five experiments were performed using several neural network architectures to identify the location of a wave in the time ordered graphical results from a medical test. Baseline results from the first experiment found correct identification of the target wave in 85% of cases (n=20). Other experiments investigated the effect of different architectures and preprocessing the raw data on the results. The methods used seem most appropriate for time oriented graphical data which has a clear starting point such as electrophoresis Or spectrometry rather than continuous teSts such as ECGs and EEGs. I INTRODUCTION Complex wave form recognition is generally considered to be a difficult task for machines. Analytical approaches to this problem have been described and they work with reasonable accuracy (Gabriel et al. 1980. Valdes-Sosa et al. 1987) The use of these techniques, however, requires substantial mathematical Iraining and the process is often time consuming and labor intensive (Boston 1987",Computer Vision,11b921ef080f7736089c757404650e40-Paper.pdf,1991
Improved Hidden Markov Model,"A high performance speaker-independent isolated-word hybrid speech rec ognizer was developed which combines Hidden Markov Models (HMMs) and Radial Basis Function (RBF) neural networks. In recognition ex periments using a speaker-independent E-set database, the hybrid rec ognizer had an error rate of 11.5% compared to 15.7% for the robust unimodal Gaussian HMM recognizer upon which the hybrid system was based. These results and additional experiments demonstrate that RBF networks can be successfully incorporated in hybrid recognizers and sug gest that they may be capable of good performance with fewer parameters than required by Gaussian mixture classifiers. A global parameter opti mization method designed to minimize the overall word error rather than the frame recognition error failed to reduce the error rate. 1 HMM/RBF HYBRID RECOGNIZER A hybrid isolated-word speech recognizer was developed which combines neural network and Hidden Markov Model (HMM) approaches. The hybrid approach is",NLP,13f320e7b5ead1024ac95c3b208610db-Paper.pdf,1991
Connectionist Optimisation of Tied Mixture,"Issues relating to the estimation of hidden Markov model (HMM) local probabilities are discussed. In particular we note the isomorphism of ra dial basis functions (RBF) networks to tied mixture density modellingj additionally we highlight the differences between these methods arising from the different training criteria employed. We present a method in which connectionist training can be modified to resolve these differences and discuss some preliminary experiments. Finally, we discuss some out standing problems with discriminative training. 1 INTRODUCTION In a statistical approach to continuous speech recognition the desired quantity is the posterior probability p(Wrlxf, 8) of a word sequence Wr = Wl ..... Ww given the acoustic evidence X[ = Xl ..... XT and the parameters of the speech model used",Optimization & Theoretical ML,13f3cf8c531952d72e5847c4183e6910-Paper.pdf,1991
Principled Architecture Selection,"The notion of generalization ability can be defined precisely as the pre diction risk, the expected performance of an estimator in predicting new observations. In this paper, we propose the prediction risk as a measure of the generalization ability of multi-layer perceptron networks and use it to select an optimal network architecture from a set of possible architec tures. We also propose a heuristic search strategy to explore the space of possible architectures. The prediction risk is estimated from the available data; here we estimate the prediction risk by v-fold cross-validation and by asymptotic approximations of generalized cross-validation or Akaike's final prediction error. We apply the technique to the problem of predicting corporate bond ratings. This problem is very attractive as a case study, since it is characterized by the limited availability of the data and by the lack of a complete a priori model which could be used to impose a structure to the network architecture. 1 ",Optimization & Theoretical ML,15d4e891d784977cacbfcbb00c48f133-Paper.pdf,1991
Extracting and Learning an Unknown Grammar with,"sman Simple secood-order recurrent netwoIts are shown to readily learn brown regular grammars when trained with positive and negative strings examples. We show that similar methods are appropriate for learning unknown grammars from examples of their strings. TIle training algorithm is an incremental real-time, re current learning (RTRL) method that computes the complete gradient and updates the weights at the end of each string. After or during training. a dynamic clustering algorithm extracts the production rules that the neural network has learned.. TIle methods are illustrated by extracting rules from unknown deterministic regular grammars. For many cases the extracted grammar outperforms the neural net from which it was extracted in correctly classifying unseen strings. 1 INTRODUCTION For many reasons, there has been a long interest in ""language"" models of neural netwoIts; see [Elman 1991] for an excellent discussion. TIle orientation of this work is somewhat dif ferent TIle focus ",NLP,15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf,1991
Network activity determines,"Single nerve cells with static properties have traditionally been viewed as the building blocks for networks that show emergent phenomena. In contrast to this approach, we study here how the overall network activity can control single cell parameters such as input resistance, as well as time and space constants, parameters that are crucial for excitability and spatio temporal integration. Using detailed computer simulations of neocortical pyramidal cells, we show that the spontaneous background firing of the network provides a means for setting these parameters. The mechanism for this control is through the large conductance change of the membrane that is induced by both non-NMDA and NMDA excitatory and inhibitory synapses activated by the spontaneous background activity. 1 INTRODUCTION Biological neurons display a complexity rarely heeded in abstract network models. Dendritic trees allow for local interactions, attenuation, and delays. Voltage- and *To whom all correspondence should b",Optimization & Theoretical ML,16c222aa19898e5058938167c8ab6c57-Paper.pdf,1991
Image Segmentation with Networks of Variable Scales,"We developed a neural net architecture for segmenting complex images, i.e., to localize two-dimensional geometrical shapes in a scene, without prior knowledge of the objects' positions and sizes. A scale variation is built into the network to deal with varying sizes. This algo rithm has been applied to video images of railroad cars, to find their identification numbers. Over 95% of the characlers were located correctly in a data base of 300 images, despile a large variation in light ing conditions and often a poor quality of the characters. A part of the network is executed on a processor board containing an analog neural net chip (Graf et aI. 1991). while the rest is implemented as a software model on a workstation or a digital signal processor. 1 INTRODUCTION Neural nets have been applied successfully to the classification of shapes, such as charac ters. However, typically, these networks do not tolerate large variations of an object's size. Rather, a normalization of the size has to",Computer Vision,1728efbda81692282ba642aafd57be3a-Paper.pdf,1991
Decoding of Neuronal Signals in Visual Pattern,"We have investigated the properties of neurons in inferior temporal (IT) cortex in monkeys performing a pattern matching task. Simple back propagation networks were trained to discriminate the various stimulus conditions on the basis of the measured neuronal signal. We also trained networks to predict the neuronal response waveforms from the spatial pat terns of the stimuli. The results indicate t.hat IT neurons convey tempo rally encoded information about both current and remembered patterns, as well as about their behavioral context. 356 Decoding of Neuronal Signals in Visual Pattern Recognition 357 1 INTRODUCTION Anatomical and neurophysiological studies suggest that there is a cortical pathway specialized for visual object recognition, beginning in the primary visual cortex and ending in the inferior temporal (IT) cortex (Ungerleider and Mishkin, 1982). Studies of IT neurons in awake behaving monkeys have found that visually elicited responses depend on the pattern of the stimulus ",Computer Vision,1a5b1e4daae265b790965a275b53ae50-Paper.pdf,1991
Unknown Title,No abstract found,Not enough information provided,1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf,1991
Learning in Feedforward Networks with Nonsmooth,"This paper is concerned with the problem of learning in networks where some or all of the functions involved are not smooth. Examples of such networks are those whose neural transfer functions are piecewise-linear and those whose error function is defined in terms of the 100 norm. Up to now, networks whose neural transfer functions are piecewise-linear have received very little consideration in the literature, but the possibility of using an error function defined in terms of the 100 norm has received some attention. In this latter work, however, the problems that can occur when gradient methods are used for non smooth error functions have not been addressed. In this paper we draw upon some recent results from the field of nonsmooth optimization (NSO) to present an algorithm for the non smooth case. Our moti vation for this work arose out of the fact that we have been able to show that, in backpropagation, an error function based upon the 100 norm overcomes the difficulties which can o",Optimization & Theoretical ML,1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf,1991
Constructing Proofs in Symmetric Networks,"This paper considers the problem of expressing predicate calculus in con nectionist networks that are based on energy minimization. Given a first order-logic knowledge base and a bound k, a symmetric network is con structed (like a Boltzman machine or a Hopfield network) that searches for a proof for a given query. If a resolution-based proof of length no longer than k exists, then the global minima of the energy function that is associated with the network represent such proofs. The network that is generated is of size cubic in the bound k and linear in the knowledge size. There are no restrictions on the type of logic formulas that can be represented. The network is inherently fault tolerant and can cope with inconsistency and nonmonotonicity. 1 Introduction The ability to reason from acquired knowledge is undoubtedly one of the basic and most important components of human intelligence. Among the major tools for reasoning in the area of AI are deductive proof techniques. However, tra",Optimization & Theoretical ML,2050e03ca119580f74cca14cc6e97462-Paper.pdf,1991
A Weighted Probabilistic Neural Network,"The Probabilistic Neural Network (PNN) algorithm represents the likeli hood function of a given class as the sum of identical, isotropic Gaussians. In practice, PNN is often an excellent pattern classifier, outperforming other classifiers including backpropagation. However, it. is not. robust with respect to affine transformations of feature space, and this can lead to poor performance on certain data. We have derived an extension of PNN called Weighted PNN (WPNN) which compensates for this flaw by allow ing anisotropic Gaussians, i.e. Gaussians whose covariance is not a mul tiple of the identity matrix. The covariance is optimized using a genetic algorithm, some interesting features of which are its redundant, logarith mic encoding and large population size. Experimental results validate our claims. 1 INTRODUCTION 1.1 PROBABILISTIC NEURAL NETWORKS (PNN) PNN (Specht 1990) is a pattern classification algorithm which falls into the broad class of ""nearest-neighbor-like"" algorithms. It is",Deep Learning,218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf,1991
Shooting Craps in Search of an Optimal Strategy for,We compare two strategies for training connectionist (as well as non connectionist) models for statistical pattern recognition. The probabilistic strat egy is based on the notion that Bayesian discrimination (i.e .• optimal classifica tion) is achieved when the classifier learns the a posteriori class distributions of the random feature vector. The differential strategy is based on the notion that the identity of the largest class a posteriori probability of the feature vector is all that is needed to achieve Bayesian discrimination. Each strategy is directly linked to a family of objective functions that can be used in the supervised training procedure. We prove that the probabilistic strategy - linked with error measure objective functions such as mean-squared-error and cross-entropy - typically used to train classifiers necessarily requires larger training sets and more complex classifier architectures than those needed to approximate the Bayesian discrim inant function. In contrast,Optimization & Theoretical ML,250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf,1991
Multimodular Architecture for Remote Sensing,"This paper deals with an application of Neural Networks to satellite remote sensing observations. Because of the complexity of the application and the large amount of data, the problem cannot be solved by using a single method. The solution we propose is to build multi modules NN architectures where several NN cooperate together. Such system suffer from generic problem for whom we propose solutions. They allow to reach accurate performances for multi-valued function approximations and probability estimations. The results are compared with six other methods which have been used for this problem. We show that the methodology we have developed is general and can be used for a large variety of applications. 675 676 Thiria, Mejia, Badran, and Crepon 1 INTRODUCTION Neural Networks have been used for many years to solve hard real world applications which involve large amounts of data. Most of the time, these problems cannot be solved with a unique technique and involve successive processing o",Computer Vision,258be18e31c8188555c2ff05b4d542c3-Paper.pdf,1991
Learning in the Vestibular System:,"Vestibular compensation is the process whereby normal functioning is regained following destruction of one member of the pair of peripheral vestibular receptors. Compensation was simulated by lesioning a dynamic neural network model of the vestibulo~ular reflex (VOR) and retraining it using recurrent back-propagation. The model reproduced the pattern of VOR neuron activity experimentally observed in compensated animals, but only if connections heretofore considered uninvolved were allowed to be plastic. Because the model incorporated nonlinear units, it was able to reconcile previously conflicting, linear analyses of experimental results on the dynamic properties of VOR neurons in normal and compensated animals. 1 VESTIBULAR COMPENSATION Vestibular compensation is one of the oldest and most well studied paradigms in motor learning. Although it is neurophysiologically well described, the adaptive mechanisms underlying vestibular compensation, and its effects on the dynamics of vestibula",Reinforcement Learning,25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf,1991
Repeat Until Bored: A Pattern Selection Strategy,"An alternative to the typical technique of selecting training examples independently from a fixed distribution is fonnulated and analyzed, in which the current example is presented repeatedly until the error for that item is reduced to some criterion value, ~; then, another item is ran domly selected. The convergence time can be dramatically increased or decreased by this heuristic, depending on the task, and is very sensitive to the value of ~. 1 INTRODUCTION In order to implement the back propagation learning procedure (Werbos, 1974; Parker, 1985; Rumelhart, Hinton and Williams, 1986), several issues must be addressed. In addi tion to designing an appropriate network architecture and detennining appropriate values for the learning parameters, the batch size and a scheme for selecting training examples must be chosen. The batch size is the number of patterns presented for which the corre sponding weight changes are computed before they are actually implemented; immediate update is equ",Reinforcement Learning,26337353b7962f533d78c762373b3318-Paper.pdf,1991
Refining PIn Controllers using Neural Networks,"The KBANN approach uses neural networks to refine knowledge that can be written in the form of simple propositional rules. We extend this idea further by presenting the MANNCON algorithm by which the mathematical equations governing a PID controller determine the topology and initial weights of a network, which is further trained using backpropagation. We apply this method to the task of controlling the outflow and temperature of a water tank, producing statistically-significant gains in accuracy over both a standard neural network approach and a non-learning PID con troller. Furthermore, using the PID knowledge to initialize the weights of the network produces statistically less variation in testset accuracy when compared to networks initialized with small random numbers. 1 INTRODUCTION Research into the design of neural networks for process control has largely ignored existing knowledge about the task at hand. One form this knowledge (often called the ""domain theory"") can take is emb",Computer Vision,285e19f20beded7d215102b49d5c09a0-Paper.pdf,1991
A Cortico-Cerebellar Model that Learns to,"A neurophysiologically-based model is presented that controls a simulated kinematic arm during goal-directed reaches. The network generates a quasi-feedforward motor command that is learned using training signals generated by corrective movements. For each target, the network selects and sets the output of a subset of pattern generators. During the move ment, feedback from proprioceptors turns off the pattern generators. The task facing individual pattern generators is to recognize when the arm reaches the target and to turn off. A distributed representation of the mo tor command that resembles population vectors seen in vivo was produced naturally by these simulations. 1 INTRODUCTION We have recently begun to explore the properties of sensorimotor networks with architectures inspired by the anatomy and physiology of the cerebellum and its in terconnections with the red nucleus and the motor cortex (Houk 1989; Houk et al.. 611 612 Berthier, Singh, Barto, and Houk 1990). It is widely ac",Reinforcement Learning,298f95e1bf9136124592c8d4825a06fc-Paper.pdf,1991
Linear Operator for Object Recognition,"Visual object recognition involves the identification of images of 3-D ob jects seen from arbitrary viewpoints. We suggest an approach to object recognition in which a view is represented as a collection of points given by their location in the image. An object is modeled by a set of 2-D views together with the correspondence between the views. We show that any novel view of the object can be expressed as a linear combination of the stored views. Consequently, we build a linear operator that distinguishes between views of a specific object and views of other objects. This opera tor can be implemented using neural network architectures with relatively simple structures. 1 Introduction Visual object recognition involves the identification of images of 3-D objects seen from arbitrary viewpoints. In particular, objects often appear in images from previ ously unseen viewpoints. In this paper we suggest an approach to object recognition in which rigid objects are recognized from arbitrary vi",Computer Vision,2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf,1991
LEARNING UNAMBIGUOUS REDUCED,"Do you want your neural net algorithm to learn sequences? Do not lim it yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your fi nal goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information about the time step at which they occurred) become inputs to a higher-level network of the same kind (working on a slower, self-adjusting time scale). Go on building a hierarchy of such networks. This principle reduces the descriptions of event sequences without 1088 of information, thus easing supervised or reinforcement learning tasks. Alternatively, you may use two recurrent networks to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that systems based",Deep Learning,2bb232c0b13c774965ef8558f0fbd615-Paper.pdf,1991
Dynamically-Adaptive Winner-Take-All Networks,"Winner-Take-All (WTA) networks. in which inhibitory interconnec tions are used to determine the most highly-activated of a pool of unilS. are an important part of many neural network models. Unfortunately, convergence of normal WT A networks is extremely sensitive to the magnitudes of their weights, which must be hand-tuned and which gen erally only provide the right amount of inhibition across a relatively small range of initial conditions. This paper presents Dynamjcally Adaptive Winner-Telke-All (DA WTA) netw<rls, which use a regulatory unit to provide the competitive inhibition to the units in the network. The DA WT A regulatory unit dynamically adjusts its level of activation during competition to provide the right amount of inhibition to differ entiate between competitors and drive a single winner. This dynamic adaptation allows DA WT A networks to perform the winner-lake-all function for nearly any network size or initial condition. using O(N) connections. In addition, the DA WT",Optimization & Theoretical ML,2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf,1991
Some Approximation Properties of Projection,"This paper will address an important question in machine learning: What kind of network architectures work better on what kind of problems? A projection pursuit learning network has a very similar structure to a one hidden layer sigmoidal neural network. A general method based on a continuous version of projection pursuit regression is developed to show that projection pursuit regression works better on angular smooth func tions than on Laplacian smooth functions. There exists a ridge function approximation scheme to avoid the curse of dimensionality for approxi mating functions in L2(¢d). 1 INTRODUCTION Projection pursuit is a nonparametric statistical technique to find ""interesting"" low dimensional projections of high dimensional data sets. It has been used for nonparametric fitting and other data-analytic purposes (Friedman and Stuetzle, 1981, Huber, 1985). Approximation properties have been studied by Diaconis & Shahshahani (1984) and Donoho & Johnstone (1989). It was first introdu",Optimization & Theoretical ML,2f55707d4193dc27118a0f19a1985716-Paper.pdf,1991
Unsupervised learning,We study a particular type of Boltzmann machine with a bipartite graph structure called a harmo nium. Our interest is in using such a machine to model a probability distribution on binary input vectors. We analyze the class of probability distributions that can be modeled by such machines. showing that for each n ~ 1 this class includes arbitrarily good appwximations to any distribution on the set of all n-vectors of binary inputs. We then present two learning algorithms for these machines .. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters (i.e. weights and thresholds) of the modeL Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of the standard method for p,Optimization & Theoretical ML,33e8075e9970de0cfea955afd4644bb2-Paper.pdf,1991
Best-First Model Merging for,"""Best-first model merging"" is a general technique for dynamically choosing the structure of a neural or related architecture while avoid ing overfitting. It is applicable to both leaming and recognition tasks and often generalizes significantly better than fixed structures. We dem onstrate the approach applied to the tasks of choosing radial basis func tions for function learning, choosing local affine models for curve and constraint surface modelling, and choosing the structure of a balltree or bumptree to maximize efficiency of access. 1 TOWARD MORE COGNITIVE LEARNING Standard backpropagation neural networks learn in a way which appears to be quite differ ent from human leaming. Viewed as a cognitive system, a standard network always main tains a complete model of its domain. This model is mostly wrong initially, but gets gradually better and better as data appears. The net deals with all data in much the same way and has no representation for the strength of evidence behind a certai",Deep Learning,35051070e572e47d2c26c241ab88307f-Paper.pdf,1991
Neural Computing with Small Weights,"An important issue in neural computation is the dynamic range of weights in the neural networks. Many experimental results on learning indicate that the weights in the networks can grow prohibitively large with the size of the inputs. Here we address this issue by studying the tradeoffs between the depth and the size of weights in polynomial-size networks of linear threshold elements (LTEs). We show that there is an efficient way of simulating a network of LTEs with large weights by a network of LTEs with small weights. In particular, we prove that every depth-d, polynomial-size network of LTEs with exponentially large integer weights + can be simulated by a depth-(2d 1), polynomial-size network of LTEs with polynomially bounded integer weights. To prove these results, we use tools from harmonic analysis of Boolean functions. Our technique is quite general, it provides insights to some other problems. For example, we are able to improve the best known results on the depth of a network ",Optimization & Theoretical ML,37f0e884fbad9667e38940169d0a3c95-Paper.pdf,1991
A Segment-based Automatic Language,"We have developed a four-language automatic language identification sys tem for high-quality speech. The system uses a neural network-based segmentation algorithm to segment speech into seven broad phonetic cat egories. Phonetic and prosodic features computed on these categories are then input to a second network that performs the language classification. The system was trained and tested on separate sets of speakers of Ameri can English, Japanese, Mandarin Chinese and Tamil. It currently performs with an accuracy of 89.5% on the utterances of the test set. 1 INTRODUCTION Automatic language identification is the rapid automatic determination of the lan guage being spoken, by any speaker, saying anything. Despite several important applications of automatic language identification, this area has suffered from a lack of basic research and the absence of a standardized, public-domain database of languages. It is well known that languages have characteristic sound patterns. Languages have b",NLP,38913e1d6a7b94cb0f55994f679f5956-Paper.pdf,1991
A Topographic Product for the Optimization,"Optimizing the performance of self-organizing feature maps like the Ko honen map involves the choice of the output space topology. We present a topographic product which measures the preservation of neighborhood relations as a criterion to optimize the output space topology of the map with regard to the global dimensionality DA as well as to the dimensi ons in the individual directions. We test the topographic product method not only on synthetic mapping examples, but also on speech data. In the latter application our method suggests an output space dimensionality of = DA 3, in coincidence with recent recognition results on the same data set. 1 INTRODUCTION Self-organizing feature maps like the Kohonen map (Kohonen, 1989, Ritter et al., 1990) not only provide a plausible explanation for the formation of maps in brains, e.g. in the visual system (Obermayer et al., 1990), but have also been applied to problems like vector quantization, or robot arm control (Martinetz et al., 1990). The u",Optimization & Theoretical ML,389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf,1991
Learning How To Teach,"Learning a map from an input set to an output set is similar to the prob lem of reconstructing hypersurfaces from sparse data (Poggio and Girosi, 1990). In this framework, we discuss the problem of automatically select ing ""minimal"" surface data. The objective is to be able to approximately reconstruct the surface from the selected sparse data. We show that this problem is equivalent to the one of compressing information by data re moval and the one oflearning how to teach. Our key step is to introduce a process that statistically selects the data according to the model. During the process of data selection (learning how to teach) our system (teacher) is capable of predicting the new surface, the approximated one provided by the selected data. We concentrate on piecewise smooth surfaces, e.g. images, and use mean field techniques to obtain a deterministic network that is shown to compress image data. 1 Learning and surface reconstruction Given a dense input data that represents a hyper",Deep Learning,3a0772443a0739141292a5429b952fe6-Paper.pdf,1991
Visual Grammars and their Neural Nets,"I exhibit a systematic way to derive neural nets for vision problems. It involves formulating a vision problem as Bayesian inference or decision on a comprehensive model of the visual domain given by a probabilistic grammar. 1 INTRODUCTION I show how systematically to derive optimizing neural networks that represent quan titative visual models and match them to data. This involves a design methodology which starts from first principles, namely a probabilistic model of a visual domain, and proceeds via Bayesian inference to a neural network which performs a visual task. The key problem is to find probability distributions sufficiently intricate to model general visual tasks and yet tractable enough for theory. This is achieved by probabilistic and expressive grammars which model the image-formation pro cess, including heterogeneous sources of noise each modelled with a grammar rule. In particular these grammars include a crucial ""relabelling"" rule that removes the undetectable internal ",Computer Vision,3cf166c6b73f030b4f67eeaeba301103-Paper.pdf,1991
Hierarchical Transformation of Space in the,"Neurons encoding simple visual features in area VI such as orientation, direction of motion and color are organized in retinotopic maps. How ever, recent physiological experiments have shown that the responses of many neurons in VI and other cortical areas are modulated by the di rection of gaze. We have developed a neural network model of the visual cortex to explore the hypothesis that visual features are encoded in head centered coordinates at early stages of visual processing. New experiments are suggested for testing this hypothesis using electrical stimulations and psychophysical observations. 1 Introduction Early visual processing in cortical areas VI, V2 and MT appear to encode visual features in eye-centered coordinates. This is based primarily on anatomical data and recordings from neurons in these areas, which are arranged in retinotopic maps. In addition, when neurons in the visual cortex are electrically stimulated [9], the direction of the evoked eye movement depends only",Computer Vision,3dc4876f3f08201c7c76cb71fa1da439-Paper.pdf,1991
Recognition,"It is shown that both changes in viewing position and illumination con ditions can be compensated for, prior to recognition, using combinations of images taken from different viewing positions and different illumina tion conditions. It is also shown that, in agreement with psychophysical findings, the computation requires at least a sign-bit image as input - contours alone are not sufficient. 1 Introduction The task of visual recognition is natural and effortless for biological systems, yet the problem of recognition has been proven to be very difficult to analyze from a computational point of view. The fundamental reason is that novel images of familiar objects are often not sufficiently similar to previously seen images of that object. Assuming a rigid and isolated object in the scene, there are two major sources for this variability: geometric and photometric. The geometric source of variability comes from changes of view position. A 3D object can be viewed from a variety of directi",Computer Vision,428fca9bc1921c25c5121f9da7815cde-Paper.pdf,1991
Networks,"The complexity of learning in shallow I-Dimensional neural networks has been shown elsewhere to be linear in the size of the network. However, when the network has a huge number of units (as cortex has) even linear time might be unacceptable. Furthermore, the algorithm that was given to achieve this time was based on a single serial processor and was biologically implausible. In this work we consider the more natural parallel model of processing and demonstrate an expected-time complexity that is constant (i.e. in dependent of the size of the network). This holds even when inter-node communication channels are short and local, thus adhering to more bio logical and VLSI constraints. 1 Introduction Shallow neural networks are defined in [J ud90]; the definition effectively limits the depth of networks while allowing the width to grow arbitrarily, and it is used as a model of neurological tissue like cortex where neurons are arranged in arrays tens of millions of neurons wide but only ten",Optimization & Theoretical ML,42998cf32d552343bc8e460416382dca-Paper.pdf,1991
Combined Neural Network and Rule-Based,"A combined neural network and rule-based approach is suggested as a general framework for pattern recognition. This approach enables unsu pervised and supervised learning, respectively, while providing probability estimates for the output classes. The probability maps are utilized for higher level analysis such as a feedback for smoothing over the output la bel maps and the identification of unknown patterns (pattern ""discovery""). The suggested approach is presented and demonstrated in the texture - analysis task. A correct classification rate in the 90 percentile is achieved for both unstructured and structured natural texture mosaics. The advan tages of the probabilistic approach to pattern analysis are demonstrated. 1 INTRODUCTION In this work we extend a recently suggested framework (Greenspan et al,1991) for a combined neural network and rule-based approach to pattern recognition. This approach enables unsupervised and supervised learning, respectively, as presented 444 A Framewor",Computer Vision,46922a0880a8f11f8f69cbb52b1396be-Paper.pdf,1991
Experimental Evaluation of Learning in a Neural Microsystem,"We report learning measurements from a system composed of a cascadable learning chip, data generators and analyzers for training pattern presentation, and an X-windows based software interface. The 32 neuron learning chip has 496 adaptive synapses and can perform Boltzmann and mean-field learning using separate noise and gain controls. We have used this system to do learning experiments on the parity and replication problem. The system settling time limits the learning speed to about 100,000 patterns per second roughly independent of system size.",Optimization & Theoretical ML,49ae49a23f67c759bf4fc791ba842aa2-Paper.pdf,1991
A Neural Network for Motion Detection of,"This paper briefly describes an artificial neural network for preattentive visual processing. The network is capable of determiuing image motioll in a type of stimulus which defeats most popular methods of motion detect.ion - a subset of second-order visual motion stimuli known as drift-balanced stimuli(DBS). The processing st.ages of the network described in this paper are integratable into a model capable of simultaneous motion extractioll. edge detection, and the determination of occlusion. 1 INTRODUCTION Previous methods of motion detection have generally been based on one of two underlying approaches: correlation; and gradient-filter. Probably the best known example of the correlation approach is th(! Reichardt movement detEctor [Reiehardt 1961]. The gradient-filter (GF) approach underlies the work of AdElson and Bergen [Adelson 1985], and Heeger [Heeger L9H8], amongst others. These motion-detecting methods eannot track DBS, because DBS Jack essential componellts of information ne",Computer Vision,4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf,1991
Against Edges: Function Approximation with,"Networks for reconstructing a sparse or noisy function often use an edge field to segment the function into homogeneous regions, This approach assumes that these regions do not overlap or have disjoint parts, which is often false. For example, images which contain regions split by an occlud ing object can't be properly reconstructed using this type of network. We have developed a network that overcomes these limitations, using support maps to represent the segmentation of a signal. In our approach, the sup port of each region in the signal is explicitly represented. Results from an initial implementation demonstrate that this method can reconstruct images and motion sequences which contain complicated occlusion. 1 Introduction The task of efficiently approximating a function is central to the solution of many important problems in perception and cognition. Many vision algorithms, for in stance, integrate depth or other scene attributes into a dense map useful for robotic tasks such as ",Computer Vision,51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf,1991
Induction of Multiscale Temporal Structure,"Learning structure in temporally-extended sequences is a difficult com putational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical struc ture that occurs locally in time-e.g., relations among notes within a mu sical phrase-but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced deacription of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different tim",Reinforcement Learning,53fde96fcc4b4ce72d7739202324cd49-Paper.pdf,1991
Locomotion in a Lower Vertebrate:,"To test whether the known connectivies of neurons in the lamprey spinal cord are sufficient to account for locomotor rhythmogenesis, a CCconnection ist"" neural network simulation was done using identical cells connected ac cording to experimentally established patterns. It was demonstrated that the network oscillates in a stable manner with the same phase relation ships among the neurons as observed in the lamprey. The model was then used to explore coupling between identical <?scillators. It was concluded that the neurons can have a dual role as rhythm generators and as coordi nators between oscillators to produce the phase relations observed among segmental oscillators during swimming. 1 INTRODUCTION One approach to analyzing neurobiological systems is to use simpler preparations that are amenable to techniques which can investigate the cellular, synaptic, and network levels of organization involved in the generation of behavior. This ap proach has yielded significant progress in the",Optimization & Theoretical ML,550a141f12de6341fba65b0ad0433500-Paper.pdf,1991
SINGLE NEURON MODEL: RESPONSE TO WEAK,"We consider a noisy bist.able single neuron model driven by a periodic external modulation. The modulation introduces a correlated switching between st.ates driven by the noise. The information flow through the sys tem from the modulation to the output switching events, leads to a succes sion of strong peaks in the power spectrum. The signal-to-noise ratio (SNR) obtained from this power spectrum is a measure of the information content in the neuron response. With increasing noise intensity, the SNR passes t.hrough a maximum, an effect which has been called stochastic resonance. We treat t.he problem wit.hin the framework of a recently developed approx imate theory, valid in the limits of weak noise intensity, weak periodic forc ing and low forcing frequency. A comparison of the results of this theory with those obtained from a linear syst.em FFT is also presented. INTRODUCTION Recently, there has been an upsurge of interest in s1ngie or few-neuron nonlinear dynamics (see e.g. Li and Ho",Optimization & Theoretical ML,559cb990c9dffd8675f6bc2186971dc2-Paper.pdf,1991
Fast Learning with Predictive Forward Models,"A method for transforming performance evaluation signals distal both in space and time into proximal signals usable by supervised learning algo rithms, presented in [Jordan & Jacobs 90], is examined. A simple obser vation concerning differentiation through models trained with redundant inputs (as one of their networks is) explains a weakness in the original architecture and suggests a modification: an internal world model that encodes action-space exploration and, crucially, cancels input redundancy to the forward model is added. Learning time on an example task, cart pole balancing, is thereby reduced about 50 to 100 times. 1 INTRODUCTION In many learning control problems, the evaluation used to modify (and thus im prove) control may not be available in terms of the controller's output: instead, it may be in terms of a spatial transformation of the controller's output variables (in which case we shall term it as being ""distal in space""), or it may be available only several time steps ",Reinforcement Learning,55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf,1991
Nonlinear Pattern Separation in Single Hippocampal,"The dendritic trees of cortical pyramidal neurons seem ideally suited to perfonn local processing on inputs. To explore some of the implications of this complexity for the computational power of neurons, we simulated a realistic biophysical model of a hippocampal pyramidal cell in which a ""cold spot""-a high density patch of inhibitory Ca-dependent K channels and a colocalized patch of Ca channels-was present at a dendritic branch point. The cold spot induced a non monotonic relationship be tween the strength of the synaptic input and the probability of neuronal fIring. This effect could also be interpreted as an analog stochastic XOR. 1 INTRODUCTION Cortical neurons consist of a highly branched dendritic tree that is electrically coupled to the soma. In a typical hippocampal pyramidal cell, over 10,000 excitatory synaptic inputs are distributed across the tree (Brown and Zador, 1990). Synaptic activity results in current flow through a transient conductance increase at the point of syn",Optimization & Theoretical ML,5737034557ef5b8c02c0e46513b98f90-Paper.pdf,1991
Generalization Performance in PARSEC-A,"This paper presents PARSEC-a system for generating connectionist parsing networks from example parses. PARSEC is not based on formal grammar systems and is geared toward spoken language tasks. PARSEC networks exhibit three strengths important for application to speech pro cessing: 1) they learn to parse, and generalize well compared to hand coded grammars; 2) they tolerate several types of noise; 3) they can learn to use multi-modal input. Presented are the PARSEC architecture and performance analyses along several dimensions that demonstrate PARSEC's features. PARSEC's performance is compared to that of tra ditional grammar-based parsing systems. 1 INTRODUCTION While a great deal of research has been done developing parsers for natural language, ade quate solutions for some of the particular problems involved in spoken language have not been found. Among the unsolved problems are the difficulty in constructing task-specific grammars, lack of tolerance to noisy input, and inability to ",NLP,598b3e71ec378bd83e0a727608b5db01-Paper.pdf,1991
Hierarchies of adaptive experts,"In this paper we present a neural network architecture that discovers a recursive decomposition of its input space. Based on a generalization of the modular architecture of Jacobs, Jordan, Nowlan, and Hinton (1991), the architecture uses competition among networks to recursively split the input space into nested regions and to learn separate associative mappings within each region. The learning algorithm is shown to perform gradient ascent in a log likelihood function that captures the architecture's hierarchical structure. 1 INTRODUCTION Neural network learning architectures such as the multilayer perceptron and adap tive radial basis function (RBF) networks are a natural nonlinear generalization of classical statistical techniques such as linear regression, logistic regression and additive modeling. Another class of nonlinear algorithms, exemplified by CART (Breiman, Friedman, Olshen, & Stone, 1984) and MARS (Friedman, 1990), gen eralizes classical techniques by partitioning the trai",Deep Learning,59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf,1991
Optical Implementation of a Self·Organizing,"We demonstrate a self-organizing system based on photorefrac tive ring oscillators. We employ the system in two ways that can both be thought of as feature extractors; one acts on a set of images exposed repeatedly to the system strictly as a linear feature extractor, and the other serves as a signal demultiplex er for fiber optic communications. Both systems implement unsupervised competitive learning embedded within the mode interaction dynamics between the modes of a set of ring oscilla tors. After a training period, the modes of the rings become as sociated with the different image features or carrier frequencies within the incoming data stream. 1 Introduction Self-organizing networks (Kohonen, Hertz, Domany) discover features or qual ities about their input environment on their own; they learn without a teacher making explicit what is to be learned. This property reminds us of severa] ubiquitous behaviors we see in the physical and natural sciences such as pat tern formation, morp",Computer Vision,5b69b9cb83065d403869739ae7f0995e-Paper.pdf,1991
A Neural Network for Motion Detection of,"This paper briefly describes an artificial neural network for preattentive visual processing. The network is capable of determiuing image motioll in a type of stimulus which defeats most popular methods of motion detect.ion - a subset of second-order visual motion stimuli known as drift-balanced stimuli(DBS). The processing st.ages of the network described in this paper are integratable into a model capable of simultaneous motion extractioll. edge detection, and the determination of occlusion. 1 INTRODUCTION Previous methods of motion detection have generally been based on one of two underlying approaches: correlation; and gradient-filter. Probably the best known example of the correlation approach is th(! Reichardt movement detEctor [Reiehardt 1961]. The gradient-filter (GF) approach underlies the work of AdElson and Bergen [Adelson 1985], and Heeger [Heeger L9H8], amongst others. These motion-detecting methods eannot track DBS, because DBS Jack essential componellts of information ne",Computer Vision,58ae749f25eded36f486bc85feb3f0ab-Paper.pdf,1991
A Neurocomputer Board Based on the ANNA,"A board is described that contains the ANN A neural-network chip, and a DSP32C digital signal processor. The ANNA (Analog Neural Network Arithmetic unit) chip performs mixed analog/digital processing. The combination of ANNA with the DSP allows high-speed, end-to-end ex ecution of numerous signal-processing applications, including the prepro cessing, the neural-net calculations, and the postprocessing steps. The ANNA board evaluates neural networks 10 to 100 times faster than the DSP alone. The board is suitable for implementing large (million con nections) networks with sparse weight matrices. Three applications have been implemented on the board: a convolver network for slant detection of text blocks, a handwritten digit recognizer, and a neural network for recognition-based segmentation. 1 INTRODUCTION Many researchers have built neural-network chips, but few chips have been installed in board-level systems, even though this next level of integration provides insights and advantages",Computer Vision,5e388103a391daabe3de1d76a6739ccd-Paper.pdf,1991
Adaptive Synchronization of,"Animal locomotion patterns are controlled by recurrent neural networks called central pattern generators (CPGs). Although a CPG can oscillate autonomously, its rhythm and phase must be well coordinated with the state of the physical system using sensory inputs. In this paper we propose a learning algorithm for synchronizing neural and physical oscillators with specific phase relationships. Sensory input connections are modified by the correlation between cellular activities and input signals. Simulations show that the learning rule can be used for setting sensory feedback connections to a CPG as well as coupling connections between CPGs. 1 CENTRAL AND SENSORY MECHANISMS IN LOCOMOTION CONTROL Patterns of animal locomotion, such as walking, swimming, and fiying, are generated by recurrent neural networks that are located in segmental ganglia of invertebrates and spinal cords of vertebrates (Barnes and Gladden, 1985). These networks can produce basic rhythms of locomotion without sensory ",Computer Vision,5ea1649a31336092c05438df996a3e59-Paper.pdf,1991
Recurrent Networks and N ARMA Modeling,"There exist large classes of time series, such as those with nonlinear moving average components, that are not well modeled by feedforward networks or linear models, but can be modeled by recurrent networks. We show that recurrent neural networks are a type of nonlinear autoregressive-moving average (N ARMA) model. Practical ability will be shown in the results of a competition sponsored by the Puget Sound Power and Light Company, where the recurrent networks gave the best performance on electric load forecasting. 1 Introduction This paper will concentrate on identifying types of time series for which a recurrent network provides a significantly better model, and corresponding prediction, than a feedforward network. Our main interest is in discrete time series that are par simoniously modeled by a simple recurrent network, but for which, a feedforward neural network is highly non-parsimonious by virtue of requiring an infinite amount of past observations as input to achieve the same ac",NLP,5ef0b4eba35ab2d6180b0bca7e46b6f9-Paper.pdf,1991
ANN Based Classification for Heart Defibrillators,Current Intra-Cardia defibrillators make use of simple classification algo rithms to determine patient conditions and subsequently to enable proper therapy. The simplicity is primarily due to the constraints on power dissipa tion and area available for implementation. Sub-threshold implementation of artificial neural networks offer potential classifiers with higher perfor mance than commercially available defibrillators. In this paper we explore several classifier architectures and discuss micro-electronic implementation issues. 1.0 INTRODUCTION Intra-Cardia Defibrillators (lCDs) represent an important therapy for people with heart dis ease. These devices are implanted and perform three types of actions: l.monitor the heart,Deep Learning,605ff764c617d3cd28dbbdd72be8f9a2-Paper.pdf,1991
Adaptive Development of Connectionist Decoders,"\Ve present. an approach for df'velopment of a decoder for any complex binary error-correct.ing code- (ECC) via training from examples of decoded received words. Our decoder is a connectionist architecture. We describe two sepa.rate solutions: A system-level solution (the Cascaded Networks Decoder); and the ECC-Enhanced Decoder, a solution which simplifies the mapping problem which must be solved for decoding. Although both solutions meet our basic approach constraint for simplicity and compact ness. only the ECC-Enhanced Decoder meet.s our second basic constraint of being a generic solution. 1 INTRODUCTION 1.1 THE DECODING PROBLEM An error-correcting code (ECC) is used to identify and correct errors in a received binary vector which is possibly corrupted clue to transmission across a noisy channel. In order to use a selected error-correcting code. the information bits, or the bits containing t.he message. are tllCOdid int.o a valid ECC codeword by the addition of a set of f'xtra hits,",Optimization & Theoretical ML,63538fe6ef330c13a05a3ed7e599d5f7-Paper.pdf,1991
Self-organisation in real neurons:,"Ion channels are the dynamical systems of the nervous system. Their distribution within the membrane governs not only communication of in formation between neurons, but also how that information is integrated within the cell. Here, an argument is presented for an 'anti-Hebbian' rule for changing the distribution of voltage-dependent ion channels in order to flatten voltage curvatures in dendrites. Simulations show that this rule can account for the self-organisation of dynamical receptive field properties such as resonance and direction selectivity. It also creates the conditions for the faithful conduction within the cell of signals to which the cell has been exposed. Various possible cellular implementations of such a learn ing rule are proposed, including activity-dependent migration of channel proteins in the plane of the membrane. 1 INTRODUCTION 1.1 NEURAL DYNAMICS Neural inputs and outputs are temporal, but there are no established ways to think about temporal learning and dynami",Reinforcement Learning,647bba344396e7c8170902bcf2e15551-Paper.pdf,1991
Tangent Prop - A formalism for specifying,"In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired be havior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spa tial distortions of the input images (translations, rotations, scale changes, etcetera). We have implemented a scheme that allows a network to learn the deriva tive of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform. 1 INTRODUCTION In machine learning, one very often knows more about the function to be learned than just the training data. An interesting case is when certain directional deriva tives of the desired function are known at certain points. For example, an image 895 896 Simard, Victorri, Le Cu",Optimization & Theoretical ML,65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf,1991
Human and Machine 'Quick Modeling',"We present here an interesting experiment in 'quick modeling' by humans, performed independently on small samples, in several languages and two continents, over the last three years. Comparisons to decision tree proce dures and neural net processing are given. From these, we conjecture that human reasoning is better represented by the latter, but substantially dif ferent from both. Implications for the 'strong convergence hypothesis' be tween neural networks and machine learning are discussed, now expanded to include human reasoning comparisons. 1 INTRODUCTION Until recently the fields of symbolic and connectionist learning evolved separately. Suddenly in the last two years a significant number of papers comparing the two methodologies have appeared. A beginning synthesis of these two fields was forged at the NIPS '90 Workshop #5 last year (Pratt and Norton, 1990), where one may find a good bibliography of the recent work of Atlas, Dietterich, Omohundro, Sanger, Shavlik, Tsoi, Utgoff a",NLP,67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf,1991
A Network of Localized Linear Discriminants,"The localized linear discriminant network (LLDN) has been designed to address classification problems containing relatively closely spaced data from different classes (encounter zones [1], the accuracy problem [2]). Locally trained hyper plane segments are an effective way to define the decision boundaries for these regions [3]. The LLD uses a modified perceptron training algorithm for effective discovery of separating hyperplane/sigmoid units within narrow boundaries. The basic unit of the network is the discriminant receptive field (DRF) which combines the LLD function with Gaussians representing the dispersion of the local training data with respect to the hyperplane. The DRF implements a local distance mea sure [4], and obtains the benefits of networks oflocalized units [5]. A constructive algorithm for the two-class case is described which incorporates DRF's into the hidden layer to solve local discrimination problems. The output unit produces a smoothed, piecewise linear decision",Computer Vision,69421f032498c97020180038fddb8e24-Paper.pdf,1991
Multi-Digit Recognition Using A Space,"We present a feed-forward network architecture for recognizing an uncon strained handwritten multi-digit string. This is an extension of previous work on recognizing isolated digits. In this architecture a single digit rec ognizer is replicated over the input. The output layer of the network is coupled to a Viterbi alignment module that chooses the best interpretation of the input. Training errors are propagated through the Viterbi module. The novelty in this procedure is that segmentation is done on the feature maps developed in the Space Displacement Neural Network (SDNN) rather than the input (pixel) space. 1 Introduction In previous work (Le Cun et al., 1990) we have demonstrated a feed-forward back propagation network that recognizes isolated handwritten digits at state-of-the-art performance levels. The natural extension of this work is towards recognition of unconstrained strings of handwritten digits. The most straightforward solution is to divide the process into two: segmenta",Computer Vision,6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf,1991
JANUS: Speech-to-Speech Translation Using,"We present JANUS, a speech-to-speech translation system that utilizes diverse processing strategies, including connectionist learning, tradi tional AI knowledge representation approaches, dynamic programming, and stochastic techniques. JANUS translates continuously spoken English and German into German, English, and Japanese. JANUS cur rently achieves 87% translation fidelity from English speech and 97% from German speech. We present the JANUS system along with com parative evaluations of its interchangeable processing components, with special emphasis on the connectionist modules. • Also with University of Karlsruhe, Karlsruhe. Germany. 1N""ow with Alliant Techsystems Research and Technology Center. Hopkins. Minnesota. 183 184 Waibel, et al. 1 INTRODUCTION In an age of increasing globalization of our economies and ever more efficient communi cation media. one important challenge is the need for effective ways of overcoming lan guage barriers. Human translation efforts are generally exp",NLP,6ea2ef7311b482724a9b7b0bc0dd85c6-Paper.pdf,1991
Practical Issues in Temporal Difference Learning,"This paper examines whether temporal difference methods for training connectionist networks, such as Suttons's TO('\) algorithm, can be suc cessfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical per spective. These practical issues are then examined in the context of a case study in which TO('\) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex nontrivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. The hidden units in these network have apparently discovered useful features, a longstanding goal of computer games research",Reinforcement Learning,68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,1991
A Parallel Analog CCD/CMOS Signal Processor,"A CCO based signal processing IC that computes a fully parallel single quadrant vector-matrix multiplication has been designed and fabricated with a 2j..un CCO/CMOS process. The device incorporates an array of Charge Coupled Devices (CCO) which hold an analog matrix of charge encoding the matrix elements. Input vectors are digital with 1 - 8 bit accuracy. 1 INTRODUCTION Vector-matrix multiplication (VMM) is often used in neural network theories to describe the aggregation of signals by neurons. An input vector encoding the activation levels of input neurons is multiplied by a matrix encoding the synaptic connection strengths to create an output vector. The analog VLSI architecture presented here has been devised to perfonn the vector-matrix multiplication using CCO technology. The architecture calculates a VMM in one clock cycle, an improvement over previous semiparallel devices (Agranat et al., 1988), (Chiang. 1990). This architecture is also useful for general signal processing appli",Optimization & Theoretical ML,74071a673307ca7459bcf75fbd024e09-Paper.pdf,1991
Iterative Construction of,"We present an iterative algorithm for nonlinear regression based on con struction of sparse polynomials. Polynomials are built sequentially from lower to higher order. Selection of new terms is accomplished using a novel look-ahead approach that predicts whether a variable contributes to the remaining error. The algorithm is based on the tree-growing heuristic in LMS Trees which we have extended to approximation of arbitrary poly nomials of the input features. In addition, we provide a new theoretical justification for this heuristic approach. The algorithm is shown to dis cover a known polynomial from samples, and to make accurate estimates of pixel values in an image-processing task. 1 INTRODUCTION Linear regression attempts to approximate a target function by a model that is a linear combination of the input features. Its approximation ability is thus limited by the available features. We describe a method for adding new features that are products or powers of existing features. Rep",Optimization & Theoretical ML,7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf,1991
A Computational Mechanism To Account For,"Using the double-step target displacement paradigm the mechanisms un derlying arm trajectory modification were investigated. Using short (10- 110 msec) inter-stimulus intervals the resulting hand motions were initially directed in between the first and second target locations. The kinematic features of the modified motions were accounted for by the superposition scheme, which involves the vectorial addition of two independent point-to point motion units: one for moving the hand toward an internally specified location and a second one for moving between that location and the final target location. The similarity between the inferred internally specified lo cations and previously reported measured end-points of the first saccades in double-step eye-movement studies may suggest similarities between per ceived target locations in eye and hand motor control. 1 INTRODUCTION The generation of reaching movements toward unexpectedly displaced targets in volves more complicated planning and cont",Optimization & Theoretical ML,7d04bbbe5494ae9d2f5a76aa1c00fa2f-Paper.pdf,1991
Learning Global Direct Inverse Kinematics,"We introduce and demonstrate a bootstrap method for construction of an in verse function for the robot kinematic mapping using only sample configuration space/workspace data. Unsupervised learning (clustering) techniques are used on pre-image neighborhoods in order to learn to partition the configuration space into subsets over which the kinematic mapping is invertible. Supervised leam ing is then used separately on each of the partitions to approximate the inverse function. The ill-posed inverse kinematics function is thereby regularized, and a globa1 inverse kinematics solution for the wristless Puma manipulator is devel oped. 1 INTRODUCTION The robot forward kinematics function is a continuous mapping f : C ~ en - w ~ Xm which maps a set of n joint parameters from the configuration space, C, to the m dimensiona1 task space, W. If m S n, the robot has redundant degrees-of-freedom (dof's). In general, control objectives such as the positioning and orienting of the end effector are spe",Computer Vision,7dcd340d84f762eba80aa538b0c527f7-Paper.pdf,1991
VISIT: A Neural Model of Covert Visual,"Visual attention is the ability to dynamically restrict processing to a subset of the visual field. Researchers have long argued that such a mechanism is necessary to efficiently perform many intermediate level visual tasks. This paper describes VISIT, a novel neural network model of visual attention. The current system models the search for target objects in scenes contain ing multiple distractors. This is a natural task for people, it is studied extensively by psychologists, and it requires attention. The network's be havior closely matches the known psychophysical data on visual search and visual attention. VISIT also matches much of the physiological data on attention and provides a novel view of the functionality of a number of visual areas. This paper concentrates on the biological plausibility of the model and its relationship to the primary visual cortex, pulvinar, superior colliculus and posterior parietal areas. 1 INTRODUCTION Visual attention is perhaps best understood in th",Computer Vision,7f24d240521d99071c93af3917215ef7-Paper.pdf,1991
Kernel Regression and,"One method proposed for improving the generalization capability of a feed forward network trained with the backpropagation algorithm is to use artificial training vectors which are obtained by adding noise to the orig inal training vectors. We discuss the connection of such backpropagation training with noise to kernel density and kernel regression estimation. We compare by simulated examples (1) backpropagation, (2) backpropagation with noise, and (3) kernel regression in mapping estimation and pattern classification contexts. 1 INTRODUCTION Let X and Y be random vectors taking values in R d and RP, respectively. Suppose that we want to estimate Y in terms of X using a feedforward network whose = input-output mapping we denote by y g(x, w). Here the vector w includes all the weights and biases of the network. Backpropagation training using the quadratic loss (or error) function can be interpreted as an attempt to minimize the expected loss = '\(w) ElIg(X, w) _ Y1I2. (1) Suppose that E",Optimization & Theoretical ML,7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf,1991
Recognition of Manipulated Objects,"We present two neural network controller learning schemes based on feedback error-learning and modular architecture for recognition and control of multiple manipulated objects. In the first scheme, a Gating Network is trained to acquire object-specific representations for recognition of a number of objects (or sets of objects). In the second scheme, an Estimation Network is trained to acquire function-specific, rather than object-specific, representations which directly estimate physical parameters. Both recognition networks are trained to identify manipulated objects using somatic and/or visual information. After learning, appropriate motor commands for manipulation of each object are issued by the control networks. 1 INTRODUCTION Conventional feedforward neural-network controllers (Barto et aI., 1983; Psaltis et al., 1987; Kawato et aI., 1987, 1990; Jordan, 1988; Katayama & Kawato, 1991) can not cope with multiple or changeable manipulated objects or disturbances because they cannot ",Reinforcement Learning,81448138f5f163ccdba4acc69819f280-Paper.pdf,1991
Temporal Adaptation,"Many auditory theorists consider the temporal adaptation of the auditory nerve a key aspect of speech coding in the auditory periph ery. Experiments with models of auditory localization and pitch perception also suggest temporal adaptation is an important ele ment of practical auditory processing. I have designed, fabricated, and successfully tested an analog integrated circuit that models many aspects of auditory nerve response, including temporal adap tation.",Computer Vision,821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf,1991
Oscillatory Model of Short Term Memory,"We investigate a model in which excitatory neurons have dynamical thresh olds which display both fatigue and potentiation. The fatigue property leads to oscillatory behavior. It is responsible for the ability of the model to perform segmentation, i.e., decompose a mixed input into staggered oscillations of the activities of the cell-assemblies (memories) affected by it. Potentiation is responsible for sustaining these staggered oscillations after the input is turned off, i.e. the system serves as a model for short term memory. It has a limited STM capacity, reminiscent of the magical number 7 ± 2. 1 Introduction The limited capacity (7 ± 2) of the short term memory (STM) has been a subject of major interest in the psychological and physiological literature. It seems quite natural to assume that the limited capacity is due to the special dynamical nature of STM. Recently, Crick and Koch (1990) suggested that the working memory is functionally related to the binding process, and is obtai",Optimization & Theoretical ML,85422afb467e9456013a2a51d4dff702-Paper.pdf,1991
Estimating Average-Case Learning Curves,"In this paper we investigate an average-case model of concept learning, and give results that place the popular statistical physics and VC dimension theories of learning curve behavior in a common framework. 1 INTRODUCTION In this paper we study a simple concept learning model in which the learner attempts I, to infer an unknown target concept chosen from a known concept class:F of {O, 1} valued functions over an input space X. At each trial i, the learner is given a point Xi E X and asked to predict the value of I(xi). If the learner predicts I(xi) incorrectly, we say the learner makes a mistake. After making its prediction, the learner is told the correct value. This simple theoretical paradigm applies to many areas of machine learning, includ ing much of the research in neural networks. The quantity of fundamental interest in this setting is the learning curve, which is the function of m defined as the prob- ·Contact author. Address: AT&T Bell Laboratories, 600 Mountain Avenue, Room",Optimization & Theoretical ML,854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf,1991
The Efficient Learning of Multiple Task,"I present a modular network architecture and a learning algorithm based on incremental dynamic programming that allows a single learning agent to learn to solve multiple Markovian decision tasks (MDTs) with signif icant transfer of learning across the tasks. I consider a class of MDTs, called composite tasks, formed by temporally concatenating a number of simpler, elemental MDTs. The architecture is trained on a set of compos ite and elemental MDTs. The temporal structure of a composite task is assumed to be unknown and the architecture learns to produce a tempo ral decomposition. It is shown that under certain conditions the solution of a composite MDT can be constructed by computationally inexpensive modifications of the solutions of its constituent elemental MDTs. 1 INTRODUCTION Most applications of domain independent learning algorithms have focussed on learning single tasks. Building more sophisticated learning agents that operate in complex environments will require handling mult",Reinforcement Learning,8b16ebc056e613024c057be590b542eb-Paper.pdf,1991
Benchmarking Feed-Forward Neural Networks:,"Existing metrics for the learning performance of feed-forward neural networks do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the results of the comparison. I propose new metrics which have the desirable property of being independent of the training epoch limit. The efficiency measures the yield of correct networks in proportion to the training effort expended. The optimal epoch limit provides the greatest efficiency. The learning performance is modelled statistically, and asymptotic performance is estimated. Implementation details may be found in (Harney, 1992). 1 Introduction The empirical comparison of neural network training algorithms is of great value in the development of improved techniques and in algorithm selection for problem solving. In view of the great sensitivity of learning times to the random starting weights (Kolen and Pollack, 1990), individual trial times such as reported in (Rumelhart, et al., 1986) ar",Optimization & Theoretical ML,8d34201a5b85900908db6cae92723617-Paper.pdf,1991
CCD Neural Network Processors for Pattern,"A CCD-based processor that we call the NNC2 is presented. The NNC2 implements a fully connected 192-input, 32-output two-layer network and can be cascaded to form multilayer networks or used in parallel for ad ditional input or output nodes. The device computes 1.92 x 109 connec tions/sec when clocked at 10 MHz. Network weights can be specified to six bits of accuracy and are stored on-chip in programmable digital memories. A neural network pattern recognition system using NNC2 and CCD im age feature extractor (IFE) devices is described. Additionally, we report a CCD output circuit that exploits inherent nonlinearities in the charge injection process to realize an adjustable-threshold sigmoid in a chip area of 40 x 80 J.tlU2. 1 INTRODUCTION A neural network chip based on charge-coupled device (CCD) technology, the NNC2, is presented. The NNC2 implements a fully connected two-layer net and can be cas caded to form multilayer networks. An image feature extractor (IFE) device (Chiang and ",Computer Vision,8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf,1991
Operators and curried functions:,"We present a framework for programming tbe bidden unit representations of simple recurrent networks based on the use of hint units (additional targets at the output layer). We present two ways of analysing a network trained within this framework: Input patterns act as operators on the information encoded by the context units; symmetrically, patterns of activation over tbe context units act as curried functions of the input sequences. Simulations demonstrate that a network can learn to represent three different functions simultaneously and canonical discriminant analysis is used to investigate bow operators and curried functions are represented in the space of bidden unit activations. 1 INTRODUCTION Many recent papers have contributed to the understanding of recurrent networks and their potential for modelling sequential pbenomena (see for example Giles, Sun, Chen, Lee, & Chen, 1990; Elman, 1989; 1990; Jordan, 1986; Cleeremans, Servan-Schreiber & McClelland, 1989; Williams & Zipser, 198",NLP,877a9ba7a98f75b90a9d49f53f15a858-Paper.pdf,1991
A Simple Weight Decay Can Improve,"It has been observed in numerical simulations that a weight decay can im prove generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk. 1 INTRODUCTION Many recent studies have shown that the generalization ability of a neural network (or any other 'learning machine') depends on a balance between the information in the training examples and the complexity of the network, see for instance [1,2,3]. Bad generalization occurs if th",Optimization & Theoretical ML,8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf,1991
Neural Network Diagnosis of Avascular Necrosis,"A vascular necrosis (AVN) of the femoral head is a common yet poten tially serious disorder which can be detected in its very early stages with magnetic resonance imaging. We have developed multi-layer perceptron networks, trained with conjugate gradient optimization, which diagnose AV N from single magnetic resonance images of the femoral head with 100% accuracy on training data and 97% accuracy on test data. 1 INTRODUCTION Diagnostic radiology may be a very natural field of application for neural networks, since a simple answer is desired from a complex image, and the learning process that human experts undergo is to a large extent a supervised learning experience based on looking at large numbers of images with known interpretations. Although many workers have applied neural nets to various types of I-dimensional medical data (e.g. ECG and EEG waveforms), little work has been done on applying neural nets to diagnosis directly from medical images. 645 646 Manduca, Christy, and Ehman ",Computer Vision,941e1aaaba585b952b62c14a3a175a61-Paper.pdf,1991
Obstacle Avoidance through Reinforcement,"A method is described for generating plan-like. reflexive. obstacle avoidance behaviour in a mobile robot. The experiments reported here use a simulated vehicle with a primitive range sensor. Avoidance behaviour is encoded as a set of continuous functions of the perceptual input space. These functions are stored using CMACs and trained by a variant of Barto and Sutton's adaptive critic algorithm. As the vehicle explores its surroundings it adapts its responses to sensory stimuli so as to minimise the negative reinforcement arising from collisions. Strategies for local navigation are therefore acquired in an explicitly goal-driven fashion. The resulting trajectories form elegant collision free paths through the environment 1 INTRODUCTION Following Simon's (1969) observation that complex behaviour may simply be the reflection of a complex environment a number of researchers (eg. Braitenberg 1986. Anderson and Donath 1988. Chapman and Agre 1987) have taken the view that interesting, plan-",Reinforcement Learning,9431c87f273e507e6040fcb07dcb4509-Paper.pdf,1991
The VC-Dimension versus the Statistical,"A general relationship is developed between the VC-dimension and the statistical lower epsilon-capacity which shows that the VC-dimension can be lower bounded (in order) by the statistical lower epsilon-capacity of a network trained with random samples. This relationship explains quan titatively how generalization takes place after memorization, and relates the concept of generalization (consistency) with the capacity of the optimal classifier over a class of classifiers with the same structure and the capacity of the Bayesian classifier. Furthermore, it provides a general methodology to evaluate a lower bound for the VC-dimension of feedforward multilayer neural networks. This general methodology is applied to two types of networks which are important for hardware implementations: two layer (N - 2L - 1) net works with binary weights, integer thresholds for the hidden units and zero threshold for the output unit, and a single neuron ((N - 1) net works) with binary weigths and a zero th",Optimization & Theoretical ML,9461cce28ebe3e76fb4b931c35a169b0-Paper.pdf,1991
Improving the Performance of Radial Basis,"Three methods for improving the performance of (gaussian) radial basis function (RBF) networks were tested on the NETtaik task. In RBF, a new example is classified by computing its Euclidean distance to a set of centers chosen by unsupervised methods. The application of supervised learning to learn a non-Euclidean distance metric was found to reduce the error rate of RBF networks, while supervised learning of each center's vari ance resulted in inferior performance. The best improvement in accuracy was achieved by networks called generalized radial basis function (GRBF) networks. In GRBF, the center locations are determined by supervised learning. After training on 1000 words, RBF classifies 56.5% of letters correct, while GRBF scores 73.4% letters correct (on a separate test set). From these and other experiments, we conclude that supervised learning of center locations can be very important for radial basis function learning. 1 Introduction Radial basis function (RBF) networks are 3-",Optimization & Theoretical ML,97e8527feaf77a97fc38f34216141515-Paper.pdf,1991
MODELS WANTED: MUST FIT DIMENSIONS,"During waking and sleep, the brain and mind undergo a tightly linked and precisely specified set of changes in state. At the level of neurons, this process has been modeled by variations of Volterra-Lotka equations for cyclic fluctuations of brainstem cell populations. However, neural network models based upon rapidly developing knowledge ofthe specific population connectivities and their differential responses to drugs have not yet been developed. Furthermore, only the most preliminary attempts have been made to model across states. Some of our own attempts to link rapid eye movement (REM) sleep neurophysiology and dream cognition using neural network approaches are summarized in this paper. 1 INTRODUCTION New models are needed to test the closely linked neurophysiological and cognitive theories that are emerging from recent scientific studies of sleep and dreaming. This section describes four separate but related levels of analysis at which modeling may ·Based, in part, upon an invit",Computer Vision,98b297950041a42470269d56260243a1-Paper.pdf,1991
Merging Constrained Optimisation with,"Several parallel analogue algorithms, based upon mean field theory (MFT) approximations to an underlying statistical mechanics formulation, and re quiring an externally prescribed annealing schedule, now exist for finding approximate solutions to difficult combinatorial optimisation problems. They have been applied to the Travelling Salesman Problem (TSP), as well as to various issues in computational vision and cluster analysis. I show here that any given MFT algorithm can be combined in a natural way with notions from the areas of constrained optimisation and adaptive simulated annealing to yield a single homogenous and efficient parallel re laxation technique, for which an externally prescribed annealing schedule is no longer required. The results of numerical simulations on 50-city and 100-city TSP problems are presented, which show that the ensuing algo rithms are typically an order of magnitude faster than the MFT algorithms alone, and which also show, on occasion, superior solut",Optimization & Theoretical ML,99c5e07b4d5de9d18c350cdf64c5aa3d-Paper.pdf,1991
Neural Control for Rolling Mills: Incorporating,"In a Bayesian framework, we give a principled account of how domain specific prior knowledge such as imperfect analytic domain theories can be optimally incorporated into networks of locally-tuned units: by choosing a specific architecture and by applying a specific training regimen. Our method proved successful in overcoming the data deficiency problem in a large-scale application to devise a neural control for a hot line rolling mill. It achieves in this application significantly higher accuracy than optimally-tuned standard algorithms such as sigmoidal backpropagation, and outperforms the state-of-the-art solution. 1 INTRODUCTION Learning in connectionist networks typically requires many training examples and relies more or less explicitly on some kind of syntactic preference bias such as ""mini mal architecture"" (Rumelhart, 1988; Le Cun et ai., 1990; Weigend, 1991; inter alia) or a smoothness constraint operator (Poggio et ai., 1990), but does not make use of explicit representation",Optimization & Theoretical ML,9a96876e2f8f3dc4f3cf45f02c61c0c1-Paper.pdf,1991
Polynomial Uniform Convergence of,"We define the concept of polynomial uniform convergence of relative frequencies to probabilities in the distribution-dependent context. Let Xn = {O, l}n, let Pn be a probability distribution on Xn and let Fn C 2X,. be a family of events. The family {(Xn, Pn, Fn)}n~l has the property of polynomial uniform convergence if the probability that the maximum difference (over Fn) between the relative frequency and the probabil ity of an event exceed a given positive e be at most 6 (0 < 6 < 1), when the sample on which the frequency is evaluated has size polynomial in n,l/e,l/b. Given at-sample (Xl, ... ,Xt), let C~t)(XI, ... ,Xt) be the Vapnik-Chervonenkis dimension of the family {{x}, ... ,xtl n f I f E Fn} and M(n, t) the expectation E(C~t) It). We show that {(Xn, Pn, Fn)}n~l has the property of polynomial uniform convergence iff there exists f3 > 0 = such that M(n, t) O(n/t!3). Applications to distribution-dependent PAC learning are discussed. 1 INTRODUCTION The probably approximately corre",Optimization & Theoretical ML,9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf,1991
Forward Dynamics Modeling,"We propose a paradigm for modeling speech production based on neural networks. We focus on characteristics of the musculoskeletal system. Using real physiological data - articulator movements and EMG from muscle activity a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior. After learning, simulated perturbations, were used to asses properties of the acquired model, such as natural frequency, damping, and interarticulator couplings. Finally, a cascade neural network is used to generate continuous motor commands from a sequence of discrete articulatory targets. 1 INTRODUCTION A key problem in the formal study of human language is to understand the process by which linguistic intentions become speech. Speech production entails extraordinary coordination among diverse neurophysiological and anatomical structures from which unfolds through time a complex acoustic signal that conveys to listeners something of the speaker's inte",Computer Vision,9b70e8fe62e40c570a322f1b0b659098-Paper.pdf,1991
Learning to Segment Images,"Despite the fact that complex visual scenes contain multiple, overlapping objects, people perform object recognition with ease and accuracy. One operation that facilitates recognition is an early segmentation process in which features of objects are grouped and labeled according to which ob ject they belong. Current computational systems that perform this oper ation are based on predefined grouping heuristics. We describe a system called MAGIC that learn. how to group features based on a set of pre segmented examples. In many cases, MAGIC discovers grouping heuristics similar to those previously proposed, but it also has the capability of find ing nonintuitive structural regularities in images. Grouping is performed by a relaxation network that aUempts to dynamically bind related fea tures. Features transmit a complex-valued signal (amplitude and phase) to one another; binding can thus be represented by phase locking related features. MAGIC'S training procedure is a generalization of r",Computer Vision,9b72e31dac81715466cd580a448cf823-Paper.pdf,1991
Simulation of Optimal Movements Using the,"This work discusses various optimization techniques which were proposed in models for controlling arm movements. In particular, the minimum-muscle-tension-change model is investigated. A dynamic simulator of the monkey's arm, including seventeen single and double joint muscles, is utilized to generate horizontal hand movements. The hand trajectories produced by this algorithm are discussed. 1 INTRODUCTION To perform a voluntary hand movement, the primate nervous system must solve the following problems: (A) Which trajectory (hand path and velocity) should be used while moving the hand from the initial to the desired position. (lB) What muscle forces should be generated. Those two problems are termed ""ill-posed"" because they can be solved in an infinite number of ways. The interesting question to us is: what strategy does the nervous system use while choosing a specific solution for these problems? The chosen solutions must comply with the known experimental data: Human and monkey's fre",Optimization & Theoretical ML,a516a87cfcaef229b342c437fe2b95f7-Paper.pdf,1991
Neural Network Analysis of Event Related,"Automated monitoring of vigilance in attention intensive tasks such as air traffic control or sonar operation is highly desirable. As the opera tor monitors the instrument, the instrument would monitor the operator, insuring against lapses. We have taken a first step toward this goal by us ing feedforward neural networks trained with backpropagation to interpret event related potentials (ERPs) and electroencephalogram (EEG) associ ated with periods of high and low vigilance. The accuracy of our system on an ERP data set averaged over 28 minutes was 96%, better than the 83% accuracy obtained using linear discriminant analysis. Practical vigilance monitoring will require prediction over shorter time periods. We were able to average the ERP over as little as 2 minutes and still get 90% correct prediction of a vigilance measure. Additionally, we achieved similarly good performance using segments of EEG power spectrum as short as 56 sec. 1 INTRODUCTION Many tasks in society demand sustained",NLP,a760880003e7ddedfef56acb3b09697f-Paper.pdf,1991
A Self-Organizing Integrated Segmentation And,"We present a neural network algorithm that simultaneously performs seg mentation and recognition of input patterns that self-organizes to detect input pattern locations and pattern boundaries. We demonstrate this neu ral network architecture on character recognition using the NIST database and report on results herein. The resulting system simultaneously seg ments and recognizes touching or overlapping characters, broken charac ters, and noisy images with high accuracy. 1 INTRODUCTION Standard pattern recognition systems usually involve a segmentation step prior to the recognition step. For example, it is very common in character recognition to segment characters in a pre-processing step then normalize the individual characters and pass them to a recognition engine such as a neural network, as in the work of LeCun et al. 1988, Martin and Pittman (1988). This separation between segmentation and recognition becomes unreliable if the characters are touching each other, touching bounding b",Computer Vision,a86c450b76fb8c371afead6410d55534-Paper.pdf,1991
A Comparison of Projection Pursuit and Neural,"Two projection based feedforward network learning methods for model free regression problems are studied and compared in this paper: one is the popular back-propagation learning (BPL); the other is the projection pursuit learning (PPL). Unlike the totally parametric BPL method, the PPL non-parametrically estimates unknown nonlinear functions sequen tially (neuron-by-neuron and layer-by-Iayer) at each iteration while jointly estimating the interconnection weights. In terms of learning efficiency, both methods have comparable training speed when based on a Gauss Newton optimization algorithm while the PPL is more parsimonious. In terms of learning robustness toward noise outliers, the BPL is more sensi tive to the outliers. 1 INTRODUCTION The back-propagation learning (BPL) networks have been used extensively for es sentially two distinct problem types, namely model-free regression and classification, 1159 1160 Hwang, Li, Maechler, Martin, and Schimert which have no a priori assumption a",Optimization & Theoretical ML,a8849b052492b5106526b2331e526138-Paper.pdf,1991
"Unsupervised Classifiers, Mutual Information","We derive criteria for training adaptive classifier networks to perform unsu pervised data analysis. The first criterion turns a simple Gaussian classifier into a simple Gaussian mixture analyser. The second criterion, which is much more generally applicable, is based on mutual information. It simpli fies to an intuitively reasonable difference between two entropy functions, one encouraging 'decisiveness,' the other 'fairness' to the alternat.ive in terpretations of the input. This 'firm but fair' criterion can be applied to any network that produces probability-type outputs, but it does not necessarily lead to useful behavior. 1 Unsupervised Classification One of the main distinctions made in discussing neural network architectures, and pattern analysis algorithms generally, is between supervised and unsupervised data analysis. We should therefore be interested in any method of building bridges between techniques in these two categories. For instance, it is possible to use an unsuperv",Deep Learning,a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf,1991
HARMONET: A Neural Net for Harmonizing,"HARMONET, a system employing connectionist networks for music pro cessing, is presented. After being trained on some dozen Bach chorales using error backpropagation, the system is capable of producing four-part chorales in the style of J .s.Bach, given a one-part melody. Our system solves a musical real-world problem on a performance level appropriate for musical practice. HARMONET's power is based on (a) a new coding scheme capturing musically relevant information and (b) the integration of backpropagation and symbolic algorithms in a hierarchical system, com bining the advantages of both. 1 INTRODUCTION Neural approaches to music processing have been previously proposed (Lischka, 1989) and implemented (Mozer, 1991)(Todd, 1989). The promise neural networks offer is that they may shed some light on an aspect of human creativity that doesn't seem to be describable in terms of symbols and rules. Ultimately what music is (or isn't) lies in the eye (or ear) of the beholder. The great compo",NLP,a7aeed74714116f3b292a982238f83d2-Paper.pdf,1991
"Fast, Robust Adaptive Control by Learning only","A large class of motor control tasks requires that on each cycle the con troller is told its current state and must choose an action to achieve a specified, state-dependent, goal behaviour. This paper argues that the optimization of learning rate, the number of experimental control deci sions before adequate performance is obtained, and robustness is of prime importance-if necessary at the expense of computation per control cy cle and memory requirement. This is motivated by the observation that a robot which requires two thousand learning steps to achieve adequate performance, or a robot which occasionally gets stuck while learning, will always be undesirable, whereas moderate computational expense can be accommodated by increasingly powerful computer hardware. It is not un reasonable to assume the existence of inexpensive 100 Mflop controllers within a few years and so even processes with control cycles in the low tens of milliseconds will have millions of machine instructions in whi",Reinforcement Learning,a9a1d5317a33ae8cef33961c34144f84-Paper.pdf,1991
Induction of Finite-State Automata Using,"Second-order recurrent networks that recognize simple finite state lan guages over {0,1}* are induced from positive and negative examples. Us ing the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solu tions are obtained that correctly recognize strings of arbitrary length. A method for extracting a finite state automaton corresponding to an opti mized network is demonstrated. 1 Introduction We address the problem of inducing languages from examples by considering a set of finite state languages over {O, 1}* that were selected for study by Tomita (Tomita, 1982): L1. 1* L2. (10)* L3. no odd-length O-string anywhere after an odd-length I-string L4. not more than 20's in a row + = L5. bit pairs, #01 's #10's 0 mod 2 309 310 Watrous and Kuhn = L6. abs(#l's - #O's) 0 mod 3 L 7. 0*1*0*1* Tomita also selected for each language a set of positive and negative examples (summarized in Table 1) to be used as a ",Optimization & Theoretical ML,a9a6653e48976138166de32772b1bf40-Paper.pdf,1991
Network generalization for production:,"We designed and trained a connectionist network to generate letterfonns in a new font given just a few exemplars from that font. During learning. our network constructed a distributed internal representation of fonts as well as letters. despite the fact that each training instance exemplified both a font and a letter. It was necessary to have separate but interconnected hidden units for ""letter"" and ""font"" representations - several alternative architectures were not successful. l. INTRODUCTION Generalization from examples is central to the notion of cognition and intelligent behavior (Margolis, 1987). Much research centers on generalization in recognition, as in optical character recognition, speech recognition. and so fonh. In all such cases, during the recognition event the information content of the representation is reduced; sometimes categorization is binary, representing just one bit of infonnation. Thus the infonnation reduction in answering ""Is this symphony by Mozan?"" is very ",Deep Learning,ab817c9349cf9c4f6877e1894a1faa00-Paper.pdf,1991
Green's Function Method for Fast On-line Learning,"The two well known learning algorithms of recurrent neural networks are the back-propagation (Rumelhart & el al., Werbos) and the forward propa gation (Williams and Zipser). The main drawback of back-propagation is its off-line backward path in time for error cumulation. This violates the on-line requirement in many practical applications. Although the forward propaga tion algorithm can be used in an on-line manner, the annoying drawback is the heavy computation load required to update the high dimensional sensitiv ity matrix (0( fir) operations for each time step). Therefore, to develop a fast forward algorithm is a challenging task. In this paper w~ proposed a forward learning algorithm which is one order faster (only 0(fV3) operations for each time step) than the sensitivity matrix algorithm. The basic idea is that instead of integrating the high dimensional sensitivity dynamic equation we solve forward in time for its Green's function to avoid the redundant computations, and then u",Optimization & Theoretical ML,b337e84de8752b27eda3a12363109e80-Paper.pdf,1991
Constrained Optimization Applied to the,"We use constrained optimization to select operating parameters for two circuits: a simple 3-transistor square root circuit, and an analog VLSI artificial cochlea. This automated method uses computer controlled mea surement and test equipment to choose chip parameters which minimize the difference between the actual circuit's behavior and a specified goal behavior. Choosing the proper circuit parameters is important to com pensate for manufacturing deviations or adjust circuit performance within a certain range. As biologically-motivated analog VLSI circuits become increasingly complex, implying more parameters, setting these parameters by hand will become more cumbersome. Thus an automated parameter setting method can be of great value [Fleischer 90]. Automated parameter setting is an integral part of a goal-based engineering design methodology in which circuits are constructed with parameters enabling a wide range of behaviors, and are then ""tuned"" to the desired behaviors automatical",Optimization & Theoretical ML,b534ba68236ba543ae44b22bd110a1d6-Paper.pdf,1991
Fault Diagnosis of Antenna Pointing Systems,"We describe in this paper a novel application of neural networks to system health monitoring of a large antenna for deep space communications. The paper outlines our approach to building a monitoring system using hybrid signal processing and neural network techniques, including autoregressive modelling, pattern recognition, and Hidden Markov models. We discuss several problems which are somewhat generic in applications of this kind - in particular we address the problem of detecting classes which were not present in the training data. Experimental results indicate that the proposed system is sufficiently reliable for practical implementation. 1 Background: The Deep Space Network The Deep Space Network (DSN) (designed and operated by the Jet Propulsion Lab oratory (JPL) for the National Aeronautics and Space Administration (NASA)) is unique in terms of providing end-to-end telecommunication capabilities between earth and various interplanetary spacecraft throughout the solar system. The",NLP,b5b41fac0361d157d9673ecb926af5ae-Paper.pdf,1991
NETWORK MODEL OF STATE-DEPENDENT,"A network model with temporal sequencing and state-dependent modula tory features is described. The model is motivated by neurocognitive data characterizing different states of waking and sleeping. Computer studies demonstrate how unique states of sequencing can exist within the same network under different aminergic and cholinergic modulatory influences. Relationships between state-dependent modulation, memory, sequencing and learning are discussed. 1 INTRODUCTION Models of biological information processing often assume only one mode or state of operation. In general, this state depends upon a high degree of fidelity or mod ulation among the neural elements. In contrast, real neural networks often have a. repertoire of processing states that is greatly affected by the relative balances of various neuromodulators (Selverston, 1988; Harris-Warrick and Marder, 1991). One area where changes in neuromodulation and network behavior are tightly and dra matically coupled is in the sleep-wake ",NLP,ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf,1991
"Splines, Rational Functions and Neural Networks","Connections between spline approximation, approximation with rational functions, and feedforward neural networks are studied. The potential improvement in the degree of approximation in going from single to two hidden layer networks is examined. Some results of Birman and Solomjak regarding the degree of approximation achievable when knot positions are chosen on the basis of the probability distribution of examples rather than the function values are extended. 1 INTRODUCTION Feedforward neural networks have been proposed as parametrized representations suitable for nonlinear regression. Their approximation theoretic properties are still not well understood. This paper shows some connections with the more widely known methods of spline and rational approximation. A result due to Vitushkin is applied to determine the relative improvement in degree of approximation possible by having more than one hidden layer. Furthermore, an approximation result rel evant to statistical regression origi",Optimization & Theoretical ML,c203d8a151612acf12457e4d67635a95-Paper.pdf,1991
Learning to Make Coherent Predictions in,"We have previously described an unsupervised learning procedure that discovers spatially coherent propertit>_<; of the world by maximizing the in formation that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract sur face depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hint.oll. 1992). 1n this paper, we pro pose two new models which handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to de tect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities. 1 Introd uction Standard backpropagation is implausible as a model of percept",Computer Vision,c399862d3b9d6b76c8436e924a68c45b-Paper.pdf,1991
Bayesian Model Comparison and Backprop Nets,"The Bayesian model comparison framework is reviewed, and the Bayesian Occam's razor is explained. This framework can be applied to feedforward networks, making possible (1) objective comparisons between solutions using alternative network architectures; (2) objective choice of magnitude and type of weight decay terms; (3) quantified estimates of the error bars on network parameters and on network output. The framework also gen erates a measure of the effective number of parameters determined by the data. The relationship of Bayesian model comparison to recent work on pre diction of generalisation ability (Guyon et al., 1992, Moody, 1992) is dis cussed. 1 BAYESIAN INFERENCE AND OCCAM'S RAZOR In science, a central task is to develop and compare models to account for the data that are gathered. Typically, two levels of inference are involved in the task of data modelling. At the first level of inference, we assume that one of the models that we invented is true, and we fit that model to t",Optimization & Theoretical ML,c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf,1991
Networks for the Separation of Sources,"We have created new networks to unmix signals which have been mixed either with time delays or via filtering. We first show that a subset of the Herault-Jutten learning rules fulfills a principle of minimum output power. We then apply this principle to extensions of the Herault-Jutten network which have delays in the feedback path. Our networks perform well on real speech and music signals that have been mixed using time delays or filtering. 1 INTRODUCTION Recently, there has been much interest in neural architectures to solve the ""blind separation of signals"" problem (Herault & Jutten, 1986) (Vittoz & Arreguit, 1989). The separation is called ""blind,"" because nothing is assumed known about the frequency or phase of the signals. A concrete example of blind separation of sources is when the pure signals are sounds generated in a room and the mixed signals are the output of some microphones. The mixture process would model the delay of the sound to each microphone, and the mixing of the ",Optimization & Theoretical ML,c410003ef13d451727aeff9082c29a5c-Paper.pdf,1991
Burst Synchronization Without,"The dynamic behavior of a network model consisting of all-to-all excitatory coupled binary neurons with global inhibition is studied analytically and numerically. We prove that for random input signals, the output of the network consists of synchronized bursts with apparently random intermis sions of noisy activity. Our results suggest that synchronous bursts can be generated by a simple neuronal architecture which amplifies incoming coin cident signals. This synchronization process is accompanied by dampened oscillations which, by themselves, however, do not play any constructive role in this and can therefore be considered to be an epiphenomenon. 1 INTRODUCTION Recently synchronization phenomena in neural networks have attracted considerable attention. Gray et al. (1989, 1990) as well as Eckhorn et al. (1988) provided electrophysiological evidence that neurons in the visual cortex of cats discharge in a semi-synchronous, oscillatory manner in the 40 Hz range and that the firing activ",Optimization & Theoretical ML,c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf,1991
Incrementally Learning Time-varying Half-planes,"We present a distribution-free model for incremental learning when concepts vary with time. Concepts are caused to change by an adversary while an incremental learning algorithm attempts to track the changing concepts by minimizing the error between the current target concept and the hypothesis. For a single half plane and the intersection of two half-planes, we show that the average mistake rate depends on the maximum rate at which an adversary can modify the concept. These theoretical predictions are verified with simulations of several learning algorithms including back propagation. 1 INTRODUCTION The goal of our research is to better understand the problem of learning when concepts are allowed to change over time. For a dichotomy, concept drift means that the classification function changes over time. We want to extend the theoretical analyses of learning to include time-varying concepts; to explore the behavior of current learning algorithms in the face of concept drift; and to de",Optimization & Theoretical ML,c75b6f114c23a4d7ea11331e7c00e73c-Paper.pdf,1991
Oscillatory Neural Fields for,"A neural network solution is proposed for solving path planning problems faced by mobile robots. The proposed network is a two-dimensional sheet of neurons forming a distributed representation of the robot's workspace. Lateral interconnections between neurons are ""cooperative"", so that the network exhibits oscillatory behaviour. These oscillations are used to gen erate solutions of Bellman's dynamic programming equation in the context of path planning. Simulation experiments imply that these networks locate global optimal paths even in the presence of substantial levels of circuit nOlse. 1 Dynamic Programming and Path Planning Consider a 2-DOF robot moving about in a 2-dimensional world. A robot's location is denoted by the real vector, p. The collection of all locations forms a set called the workspace. An admissible point in the workspace is any location which the robot may occupy. The set of all admissible points is called the free workspace. The free workspace's complement represen",Deep Learning,c9892a989183de32e976c6f04e700201-Paper.pdf,1991
Retinogeniculate Development:,"During visual development, projections from retinal ganglion cells (RGCs) to the lateral geniculate nucleus (LGN) in cat are refined to produce ocular dominance layering and precise topographic mapping. Normal development depends upon activity in RGCs, suggesting a key role for activity-dependent synaptic plasticity. Recent experiments on prenatal retina show that during early development, ""waves"" of activity pass across RGCs (Meister, et aI., 1991). We provide the first simulations to demonstrate that such retinal waves, in conjunction with Hebbian synaptic competition and early arrival of contralateral axons, can account for observed patterns of retinogeniculate projections in normal and experimentally-treated animals. 1 INTRODUCTION During the development of the mammalian visual system, initially diffuse axonal inputs are refined to produce the precise and orderly projections seen in the adult. In the lateral geniculate nucleus (LGN) of the cat, projections arriving from retinal gan",NLP,cbcb58ac2e496207586df2854b17995f-Paper.pdf,1991
A Neural Net Model for Adaptive Control of,"Accurate saccades require interaction between brainstem circuitry and the cerebeJJum. A model of this interaction is described, based on Kawato's principle of feedback-error-Iearning. In the model a part of the brainstem (the superior colliculus) acts as a simple feedback controJJer with no knowledge of initial eye position, and provides an error signal for the cerebeJJum to correct for eye-muscle nonIinearities. This teaches the cerebeJJum, modelled as a CMAC, to adjust appropriately the gain on the brainstem burst-generator's internal feedback loop and so alter the size of burst sent to the motoneurons. With direction-only errors the system rapidly learns to make accurate horizontal eye movements from any starting position, and adapts realistically to subsequent simulated eye-muscle weakening or displacement of the saccadic target. 1 INTRODUCTION The use of artificial neural nets (ANNs) to control robot movement offers advantages in situations where the relevant analytic solutions ar",Deep Learning,ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf,1991
Segmentation Circuits Using Constrained,"A novel segmentation algorithm has been developed utilizing an absolute value smoothness penalty instead of the more common quadratic regu larizer. This functional imposes a piece-wise constant constraint on the segmented data. Since the minimized energy is guaranteed to be convex, there are no problems with local minima and no complex continuation methods are necessary to find the unique global minimum. By interpret ing the minimized energy as the generalized power of a nonlinear resistive network, a continuous-time analog segmentation circuit was constructed. 1 INTRODUCTION Analog hardware has obvious advantages in terms of its size, speed, cost, and power consumption. Analog chip designers, however, should not feel constrained to map ping existing digital algorithms to silicon. Many times, new algorithms must be adapted or invented to ensure efficient implementation in analog hardware. Novel analog algorithms embedded in the hardware must be simple and obey the natural constraints o",Optimization & Theoretical ML,cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf,1991
Rule Induction through Integrated Symbolic and,"We describe a neural network, called RufeNet, that learns explicit, sym bolic condition-action rules in a formal string manipulation domain. RuleNet discovers functional categories over elements of the domain, and, at various points during learning, extracts rules that operate on these categories. The rules are then injected back into RuleNet and training continues, in a process called iterative projection. By incorpo rating rules in this way, RuleNet exhibits enhanced learning and gener alization performance over alternative neural net approaches. By integrating symbolic rule learning and subsymbolic category learning, RuleNet has capabilities that go beyond a purely symbolic system. We show how this architecture can be applied to the problem of case-role assignment in natural language processing, yielding a novel rule-based solution. 1 INTRODUCTION We believe that neural networks are capable of more than pattern recognition; they can also perform higher cognitive tasks which are fund",NLP,cf67355a3333e6e143439161adc2d82e-Paper.pdf,1991
A comparison between a neural network model for,"Recently, high resolution images of the simultaneous representation of orientation preference, orientation selectivity and ocular dominance have been obtained for large areas in monkey striate cortex by optical imaging [1-3]. These data allow for the first time a ""local"" as well as ""global"" description of the spatial patterns and provide strong evidence for corre lations between orientation selectivity and ocular dominance. A quantitative analysis reveals that these correlations arise when a five dimensional feature space (two dimensions for retinotopic space, one each for orientation preference, orientation specificity, and ocular dominance) is mapped into the two available dimensions of cortex while locally preserving topology. These results provide strong evidence for the concept of topology preserving maps which have been suggested as a basic design principle of striate cortex [4-7]. Monkey striate cortex contains a retinotopic map in which are embedded the highly repetitive patter",Computer Vision,cfee398643cbc3dc5eefc89334cacdc1-Paper.pdf,1991
A Connectionist Learning Approach to Analyzing,"We use connectionist modeling to develop an analysis of stress systems in terms of ease of learnability. In traditional linguistic analyses, learnability arguments determine default parameter settings based on the feasibilty of logicall y deducing correct settings from an initial state. Our approach provides an empirical alter native to such arguments. Based on perceptron learning experiments using data from nineteen human languages, we develop a novel characterization of stress patterns in terms of six parameters. These provide both a partial description of the stress pattern itself and a prediction of its learnability, without invoking abstract theoretical constructs such as metrical feet. This work demonstrates that ma chine learning methods can provide a fresh approach to understanding linguistic phenomena. 1 LINGUISTIC STRESS The domain of stress systems in language is considered to have a relatively good linguistic theory, called metrical phonologyl. In this theory, the stress pa",NLP,d07e70efcfab08731a97e7b91be644de-Paper.pdf,1991
Stationarity of Synaptic Coupling Strength Between,"Based on a general non-stationary point process model, we computed estimates of the synaptic coupling strength (efficacy) as a function of time after stimulus onset between an inhibitory interneuron and its target postsynaptic cell in the feline dorsal cochlear nucleus. The data consist of spike trains from pairs of neurons responding to brief tone bursts recorded in vivo. Our results suggest that the synaptic efficacy is non-stationary. Further. synaptic efficacy is shown to be inversely and approximately linearly related to average presynaptic spike rate. A second-order analysis suggests that the latter result is not due to non-linear interactions. Synaptic efficacy is less strongly correlated with postsynaptic rate and the correlation is not consistent across neural pairs. 1 INTRODUCTION The aim of this study was to investigate the dynamic properties of the inhibitory effect of type IT neurons on type IV neurons in the cat dorsal cochlear nucleus (DeN). Type IV cells are the princip",NLP,d18f655c3fce66ca401d5f38b48c89af-Paper.pdf,1991
Time-Warping Network:,"Recently. much interest has been generated regarding speech recognition systems based on Hidden Markov Models (HMMs) and neural network (NN) hybrids. Such systems attempt to combine the best features of both models: the temporal structure of HMMs and the discriminative power of neural networks. In this work we define a time-warping (1W) neuron that extends the operation of the fonnal neuron of a back-propagation network by warping the input pattern to match it optimally to its weights. We show that a single-layer network of TW neurons is equivalent to a Gaussian density HMM based recognition system. and we propose to improve the discriminative power of this system by using back-propagation discriminative training. and/or by generalizing the structure of the recognizer to a multi-layered net The performance of the proposed network was evaluated on a highly confusable, isolated word. multi speaker recognition task. The results indicate that not only does the recognition performance impro",NLP,d61e4bbd6393c9111e6526ea173a7c8b-Paper.pdf,1991
The Effective Number of Parameters:,"We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learn ing systems, such as multilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and tlaining set errors: (1) e, Here, n is the size of the training sample u;f f is the effective noise variance in the response variable( s), ,x is a regularization or weight decay parameter, and Peff(,x) is the effective number of parameters in the non linear model. The expectations ( ) of training set and test set errors are e e' taken over possible training sets and training and test sets respec tively. The effective number of parameters Peff(,x) usually differs from the true number of model parameters P for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments. In addition to the surprising result that Peff(,x)",Optimization & Theoretical ML,d64a340bcb633f536d56e51874281454-Paper.pdf,1991
Dual Inhibitory Mechanisms for Definition of,"In single cells of the cat striate cortex, lateral inhibition across orienta tion and/or spatial frequency is found to enhance pre-existing biases. A contrast-dependent but spatially non-selective inhibitory component is also found. Stimulation with ascending and descending contrasts reveals the latter as a response hysteresis that is sensitive, powerful and rapid, sug gesting that it is active in day-to-day vision. Both forms of inhibition are not recurrent but are rather network properties. These findings suggest two fundamental inhibitory mechanisms: a global mechanism that limits dynamic range and creates spatial selectivity through thresholding and a local mechanism that specifically refines spatial filter properties. Analysis of burst patterns in spike trains demonstrates that these two mechanisms have unique physiological origins. 1 INFORMATION PROCESSING IN STRIATE CORTICAL CELLS The most popular current model of single cells in the striate cortex casts them in terms of spatial",Computer Vision,db85e2590b6109813dafa101ceb2faeb-Paper.pdf,1991
Networks with Learned Unit Response Functions,"Feedforward networks composed of units which compute a sigmoidal func tion of a weighted sum of their inputs have been much investigated. We tested the approximation and estimation capabilities of networks using functions more complex than sigmoids. Three classes of functions were tested: polynomials, rational functions, and flexible Fourier series. Un like sigmoids, these classes can fit non-monotonic functions. They were compared on three problems: prediction of Boston housing prices, the sunspot count, and robot arm inverse dynamics. The complex units at tained clearly superior performance on the robot arm problem, which is a highly non-monotonic, pure approximation problem. On the noisy and only mildly nonlinear Boston housing and sunspot problems, differences among the complex units were revealed; polynomials did poorly, whereas rationals and flexible Fourier series were comparable to sigmoids. 1 Introduction A commonly studied neural architecture is the feedforward network in whi",NLP,dd458505749b2941217ddd59394240e8-Paper.pdf,1991
Adaptive Elastic Models for Hand-Printed,"Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit. the control points have preferred ., home"" locations, and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the defor mation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points.",Computer Vision,df877f3865752637daa540ea9cbc474f-Paper.pdf,1991
Modeling Applications with the Focused Gamma Net,"The focused gamma network is proposed as one of the possible implementations of the gamma neural model. The focused gamma network is compared with the focused backpropagation network and TDNN for a time series prediction problem, and with ADALINE in a system identification problem. 1 INTRODUCTION At NIPS-90 we introduced the gamma neural model, a real time neural net for temporal processing (de Vries and Principe, 1991). This model is characterized by a neural short term memory mechanism, the gamma memory structure, which is implemented as a tapped delay line of adaptive dispersive elements. The gamma model seems to provide an integrative framework to study the neural processing of time varying patterns (de Vries and Principe, 1992). In fact both the memory by delays as implemented in TDNN (Lang et aI, 1990) and memory by local feedback (self-recurrent loops) as proposed by Jordan (1986), and Elman (1990) are special cases of the gamma memory structure. The preprocessor utilized in Tan",Deep Learning,e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf,1991
Statistical Reliability of a Blowfly,"We develop a model-independent method for characterizing the reliability of neural responses to brief stimuli. This approach allows us to measure the discriminability of similar stimuli, based on the real-time response of a single neuron. Neurophysiological data were obtained from a movement sensitive neuron (HI) in the visual system of the blowfly Calliphom ery throcephala. Furthermore, recordings were made from blowfly photore ceptor cells to quantify the signal to noise ratios in the peripheral visual system. As photoreceptors form the input to the visual system, the reli ability of their signals ultimately determines the reliability of any visual discrimination task. For the case of movement detection, this limit can be computed, and compared to the HI neuron's reliability. Under favor able conditions, the performance of the HI neuron closely approaches the theoretical limit, which means that under these conditions the nervous system adds little noise in the process of computing mo",NLP,dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf,1991
Towards Faster Stochastic Gradient Search,"Stochastic gradient descent is a general algorithm which includes LMS, on-line backpropagation, and adaptive k-means clustering as special cases. The standard choices of the learning rate 1] (both adaptive and fixed func tions of time) often perform quite poorly. In contrast, our recently pro posed class of ""search then converge"" learning rate schedules (Darken and Moody, 1990) display the theoretically optimal asymptotic convergence rate and a superior ability to escape from poor local minima. However, the user is responsible for setting a key parameter. We propose here a new method ology for creating the first completely automatic adaptive learning rates which achieve the optimal rate of convergence. Intro d uction The stochastic gradient descent algorithm is =",Optimization & Theoretical ML,e2230b853516e7b05d79744fbd4c9c13-Paper.pdf,1991
Gradient Descent: Second-Order Momentum,"= Batch gradient descent, ~w(t) -7JdE/dw(t), conver~es to a minimum of quadratic form with a time constant no better than '4Amax/ Amin where Amin and Amax are the minimum and maximum eigenvalues of the Hessian matrix of E with respect to w. It was recently shown that adding a momentum term ~w(t) = -7JdE/dw(t) + Q'~w(t - 1) improves this to ~ VAmax/ Amin, although only in the batch case. Here we show that second = + + order momentum, ~w(t) -7JdE/dw(t) Q'~w(t -1) (3~w(t - 2), can lower this no further. We then regard gradient descent with momentum as a dynamic system and explore a non quadratic error surface, showing that saturation of the error accounts for a variety of effects observed in simulations and justifies some popular heuristics. 1 INTRODUCTION Gradient descent is the bread-and-butter optimization technique in neural networks. Some people build special purpose hardware to accelerate gradient descent optimiza tion of backpropagation networks. Understanding the dynamics of gradi",Optimization & Theoretical ML,e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf,1991
Active Exploration in Dynamic Environments,"\Vhenever an agent learns to control an unknown environment, two oppos ing principles have to be combined, namely: exploration (long-term opti mization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by ran domness in action selection. This might be disadvantageous when costs are assigned to ""negative experiences"". The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller's accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors - exploration and exploitation - depending on ex pected costs and knowledge gain. The appropriateness of this method is demonstrated by a simple robot navigation task. INTRODUCTION The need for exploration in adaptive control has been recognized by various au th",Reinforcement Learning,e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf,1991
Data Analysis using G/SPLINES,"G/SPLINES is an algorithm for building functional models of data. It uses genetic search to discover combinations of basis functions which are then used to build a least-squares regression model. Because it produces a population of models which evolve over time rather than a single model, it allows analysis not possible with other regression-based approaches. 1 INTRODUCTION G/SPLINES is a hybrid of Friedman's Multivariable Adaptive Regression Splines (MARS) algorithm (Friedman, 1990) with Holland's Genetic Algorithm (Holland, 1975). G/SPLINES has advantages over MARS in that it requires fewer least-squares computations, is easily extendable to non-spline basis functions, may discover models inaccessible to local-variable selection algorithms, and allows significantly larger problems to be considered. These issues are discussed in (Rogers, 1991). This paper begins with a discussion of linear regression models, followed by a description of the G/SPLINES algorithm, and finishes with a ser",Optimization & Theoretical ML,e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf,1991
A Contrast Sensitive Silicon Retina with,"The goal of perception is to extract invariant properties of the underly ing world. By computing contrast at edges, the retina reduces incident light intensities spanning twelve decades to a twentyfold variation. In one stroke, it solves the dynamic range problem and extracts relative reflec tivity, bringing us a step closer to the goal. We have built a contrast sensitive silicon retina that models all major synaptic interactions in the outer-plexiform layer of the vertebrate retina using current-mode CMOS circuits: namely, reciprocal synapses between cones and horizontal cells, which produce the antagonistic center/surround receptive field, and cone and horizontal cell gap junctions, which determine its size. The chip has 90 x 92 pixels on a 6.8 x 6.9mm die in 2/lm n-well technology and is fully functional. 1 INTRODUCTION Retinal cones use both intracellular and extracellular mechanisms to adapt their gain to the input intensity level and hence remain sensitive over a large dynamic ra",Computer Vision,e836d813fd184325132fca8edcdfb40e-Paper.pdf,1991
Markov Random Fields Can Bridge Levels of,"Paul R. Cooper Peter N. Prokopowicz Institute for the Learning Sciences Institute for the Learning Sciences Northwestern University Northwestern U ni versity Evanston, IL Evanston, IL cooper@ils.nwu.edu prokopowicz@ils.nwu.edu Network vision systems must make inferences from evidential informa tion across levels of representational abstraction, from low level invariants, through intermediate scene segments, to high level behaviorally relevant object descriptions. This paper shows that such networks can be realized as Markov Random Fields (MRFs). We show first how to construct an MRF functionally equivalent to a Hough transform parameter network, thus establishing a principled probabilistic basis for visual networks. Sec ond, we show that these MRF parameter networks are more capable and flexible than traditional methods. In particular, they have a well-defined probabilistic interpretation, intrinsically incorporate feedback, and offer richer representations and decision capabilities. 1",Computer Vision,e8c0653fea13f91bf3c48159f7c24f78-Paper.pdf,1991
Information Measure Based Skeletonisation,"Automatic determination of proper neural network topology by trimming over-sized networks is an important area of study, which has previously been addressed using a variety of techniques. In this paper, we present Information Measure Based Skeletonisation (IMBS), a new approach to this problem where superfluous hidden units are removed based on their information measure (1M). This measure, borrowed from decision tree in duction techniques, reflects the degree to which the hyperplane formed by a hidden unit discriminates between training data classes. We show the results of applying IMBS to three classification tasks and demonstrate that it removes a substantial number of hidden units without significantly affecting network performance. 1 INTRODUCTION Neural networks can be evaluated based on their learning speed, the space and time complexity of the learned network, and generalisation performance. Pruning over sized networks (skeletonisation) has the potential to improve networks along",Optimization & Theoretical ML,eba0dc302bcd9a273f8bbb72be3a687b-Paper.pdf,1991
Interpretation of Artificial Neural Networks:,"We propose and empirically evaluate a method for the extraction of expert comprehensible rules from trained neural networks. Our method operates in the context of a three-step process for learning that uses rule-based domain knowledge in combination with neural networks. Empirical tests using real worlds problems from molecular biology show that the rules our method extracts from trained neural networks: closely reproduce the accuracy of the network from which they came, are superior to the rules derived by a learning system that directly refines symbolic rules, and are expert-comprehensible. 1 Introduction Artificial neural networks (ANNs) have proven to be a powerful and general technique for machine learning [1, 11]. However, ANNs have several well-known shortcomings. Perhaps the most significant of these shortcomings is that determining why a trained ANN makes a particular decision is all but impossible. Without the ability to explain their decisions, it is hard to be confident in ",NLP,ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf,1991
Competitive Anti-Hebbian Learning of Invariants,"Although the detection of invariant structure in a given set of input patterns is vital to many recognition tasks, connectionist learning rules tend to focus on directions of high variance (principal components). The prediction paradigm is often used to reconcile this dichotomy; here we suggest a more direct approach to invariant learning based on an anti-Hebbian learning rule. An unsupervised tWO-layer network implementing this method in a competitive setting learns to extract coherent depth information from random-dot stereograms. 1 INTRODUCTION: LEARNING INVARIANT STRUCTURE Many connectionist learning algorithms share with principal component analysis (Jolliffe, 1986) the strategy of extracting the directions of highest variance from the input. A single Hebbian neuron, for instance, will come to encode the input's first principal component (Oja and Karhunen, 1985); various forms of lateral interaction can be used to force a layer of such nodes to differentiate and span the principal",Deep Learning,ef575e8837d065a1683c022d2077d342-Paper.pdf,1991
Information Processing to Create Eye Movements,"Because eye muscles never cocontract and do not deal with external loads, one can write an equation that relates motoneuron firing rate to eye position and velocity - a very uncommon situation in the CNS. The semicircular canals transduce head velocity in a linear manner by using a high background discharge rate, imparting linearity to the premotor circuits that generate eye movements. This has allowed deducing some of the signal processing involved, including a neural network that integrates. These ideas are often summarized by block diagrams. Unfortunately, they are of little value in describing the behavior of single neurons - a fmding supported by neural network models. 1 INTRODUCTION The neural networks in our studies are quite simple. They differ from other applications in that they attempt to model real neural subdivisions of the oculomotor system which have been extensively studied with microelectrodes. Thus, we can ask the extent to which neural networks succeed in describing ",Optimization & Theoretical ML,f1b6f2857fb6d44dd73c7041e0aa0f19-Paper.pdf,1991
Propagation Filters in PDS Networks for,"We present a Parallel Distributed Semantic (PDS) Network architecture that addresses the problems of sequencing and ambiguity resolution in natural language understanding. A PDS Network stores phrases and their meanings using multiple PDP networks, structured in the form of a se mantic net. A mechanism called Propagation Filters is employed: (1) to control communication between networks, (2) to properly sequence the components of a phrase, and (3) to resolve ambiguities. Simulation results indicate that PDS Networks and Propagation Filters can successfully rep resent high-level knowledge, can be trained relatively quickly, and provide for parallel inferencing at the knowledge level. 1 INTRODUCTION Backpropagation has shown considerable potential for addressing problems in nat ural language processing (NLP). However, the traditional PDP [Rumelhart and McClelland, 1986] approach of using one (or a small number) of backprop networks for NLP has been plagued by a number of problems: (1) it",NLP,f387624df552cea2f369918c5e1e12bc-Paper.pdf,1991
Neural Network Routing for Random Multistage,"A routing scheme that uses a neural network has been developed that can aid in establishing point-to-point communication routes through multi stage interconnection networks (MINs). The neural network is a network of the type that was examined by Hopfield (Hopfield, 1984 and 1985). In this work, the problem of establishing routes through random MINs (RMINs) in a shared-memory, distributed computing system is addressed. The performance of the neural network routing scheme is compared to two more traditional approaches - exhaustive search routing and greedy rout ing. The results suggest that a neural network router may be competitive for certain RMIN s. 1 INTRODUCTION A neural network has been developed that can aid in establishing point-to point communication routes through multistage interconnection networks (MINs) (Goudreau and Giles, 1991). Such interconnection networks have been widely stud ied (Huang, 1984; Siegel, 1990). The routing problem is of great interest due to its broad app",Computer Vision,f3f27a324736617f20abbf2ffd806f6d-Paper.pdf,1991
Feature Extraction,"Intrator (1990) proposed a feature extraction method that is related to recent statistical theory (Huber, 1985; Friedman, 1987), and is based on a biologically motivated model of neuronal plasticity (Bienenstock et al., 1982). This method has been recently applied to feature extraction in the context of recognizing 3D objects from single 2D views (Intrator and Gold, 1991). Here we describe experiments designed to analyze the nature of the extracted features, and their relevance to the theory and psychophysics of object recognition. 1 Introduction Results of recent computational studies of visual recognition (e.g., Poggio and Edel man, 1990) indicate that the problem of recognition of 3D objects can be effectively reformulated in terms of standard pattern classification theory. According to this approach, an object is represented by a few of its 2D views, encoded as clusters in multidimentional space. Recognition of a novel view is then carried out by interpo- 460 3D Object Recognition ",Computer Vision,f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf,1991
Direction Selective Silicon Retina,"Biological retinas extract spatial and temporal features in an attempt to reduce the complexity of performing visual tasks. We have built and tested a silicon retina which encodes several useful temporal features found in ver tebrate retinas. The cells in our silicon retina are selective to direction, highly sensitive to positive contrast changes around an ambient light level, and tuned to a particular velocity. Inhibitory connections in the null di rection perform the direction selectivity we desire. This silicon retina is on a 4.6 x 6.8mm die and consists of a 47 x 41 array of photoreceptors. 1 INTRODUCTION The ability to sense motion in the visual world is essential to survival in animals. Visual motion processing is indispensable; it tells us about predators and prey, our own motion and image stablization on the retina. Many algorithms for performing early visual motion processing have been proposed [HK87] [Nak85]. A key salient feature of motion is direction selectivity, ie the ab",Computer Vision,f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf,1991
Software for ANN training on a Ring Array Processor,"Experimental research on Artificial Neural Network (ANN) algorithms requires either writing variations on the same program or making one monolithic program with many parameters and options. By using an object-oriented library, the size of these experimental programs is reduced while making them easier to read, write and modify. An efficient and flexible realization of this idea is Connection ist Layered Object-oriented Network Simulator (CLONES). CLONES runs on UNIX1 workstations and on the 100-1000 MFLOP Ring Array Processor (RAP) that we built with ANN algorithms in mind. In this report we describe CLONES and show how it is implemented on the RAP. 1 Overview As we continue to experiment with Artificial Neural Networks (ANNs) to generate phoneme probabilities for speech recognition (Bourlard & Morgan, 1991), two things have become increasingly clear:",Optimization & Theoretical ML,f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf,1991
The Clusteron: Toward a Simple Abstraction for,"Are single neocortical neurons as powerful as multi-layered networks? A recent compartmental modeling study has shown that voltage-dependent membrane nonlinearities present in a complex dendritic tree can provide a virtual layer of local nonlinear processing elements between synaptic in puts and the final output at the cell body, analogous to a hidden layer in a multi-layer network. In this paper, an abstract model neuron is in troduced, called a clusteron, which incorporates aspects of the dendritic ""cluster-sensitivity"" phenomenon seen in these detailed biophysical mod eling studies. It is shown, using a clusteron, that a Hebb-type learning rule can be used to extract higher-order statistics from a set of train ing patterns, by manipulating the spatial ordering of synaptic connections onto the dendritic tree. The potential neurobiological relevance of these higher-order statistics for nonlinear pattern discrimination is then studied within a full compartmental model of a neocortical ",Optimization & Theoretical ML,f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf,1991
Application of Neural Network Methodology to,"In this paper, a tree based neural network viz. MARS (Friedman, 1991) for the modelling of the yield strength of a steel rolling plate mill is described. The inputs to the time series model are temperature, strain, strain rate, and interpass time and the output is the corresponding yield stress. It is found that the MARS-based model reveals which variable's functional dependence is nonlinear, and significant. The results are compared with those obta.ined by using a Kalman filter based online tuning method and other classification methods, e.g. CART, C4.5, Bayesian classification. It is found that the MARS-based method consistently outperforms the other methods. 1 Introduction Hot rolling of steel slabs into fiat plates is a common process in a steel mill. This technology has been in use for many years. The process of rolling hot slabs into plates is relatively well understood [see, e.g., Underwood, 1950]. But with the intense intrnational market competition, there is more and more dema",Optimization & Theoretical ML,f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf,1991
Reverse TDNN: An Architecture for Trajectory,"The backpropagation algorithm can be used for both recognition and gen eration of time trajectories. When used as a recognizer, it has been shown that the performance of a network can be greatly improved by adding structure to the architecture. The same is true in trajectory generation. In particular a new architecture corresponding to a ""reversed"" TDNN is proposed. Results show dramatic improvement of performance in the gen eration of hand-written characters. A combination of TDNN and reversed TDNN for compact encoding is also suggested. 1 INTRODUCTION Trajectory generation finds interesting applications in the field of robotics, automa tion, filtering, or time series prediction. Neural networks, with their ability to learn from examples, have been proposed very early on for solving non-linear control prob lems adaptively. Several neural net architectures have been proposed for trajectory generation, most notably recurrent networks, either with discrete time and exter nalloops (Jordan",Computer Vision,fde9264cf376fffe2ee4ddf4a988880d-Paper.pdf,1991
Principles of Risk Minimization,"Learning is posed as a problem of function estimation, for which two princi ples of solution are considered: empirical risk minimization and structural risk minimization. These two principles are applied to two different state ments of the function estimation problem: global and local. Systematic improvements in prediction power are illustrated in application to zip-code recognition. 1 INTRODUCTION The structure of the theory of learning differs from that of most other theories for applied problems. The search for a solution to an applied problem usually requires the three following steps:",Optimization & Theoretical ML,ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf,1991
Networks with Learned Unit Response Functions,"Feedforward networks composed of units which compute a sigmoidal func tion of a weighted sum of their inputs have been much investigated. We tested the approximation and estimation capabilities of networks using functions more complex than sigmoids. Three classes of functions were tested: polynomials, rational functions, and flexible Fourier series. Un like sigmoids, these classes can fit non-monotonic functions. They were compared on three problems: prediction of Boston housing prices, the sunspot count, and robot arm inverse dynamics. The complex units at tained clearly superior performance on the robot arm problem, which is a highly non-monotonic, pure approximation problem. On the noisy and only mildly nonlinear Boston housing and sunspot problems, differences among the complex units were revealed; polynomials did poorly, whereas rationals and flexible Fourier series were comparable to sigmoids. 1 Introduction A commonly studied neural architecture is the feedforward network in whi",Optimization & Theoretical ML,ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf,1991
Parameterising Feature Sensitive Cell,"This paper examines and extends the work of Linsker (1986) on self organising feature detectors. Linsker concentrates on the vi sual processing system, but infers that the weak assumptions made will allow the model to be used in the processing of other sensory information. This claim is examined here, with special attention paid to the auditory system, where there is much lower connec tivity and therefore more statistical variability. On-line training is utilised, to obtain an idea of training times. These are then com pared to the time available to pre-natal mammals for the formation of feature sensitive cells. 1 INTRODUCTION Within the last thirty years, a great deal of research has been carried out in an attempt to understand the development of cells in the pathways between the sensory apparatus and the cortex in mammals. For example, theories for the development of feature detectors were forwarded by Nass and Cooper (1975), by Grossberg (1976) and more recently Obermayer et al (199",Optimization & Theoretical ML,00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf,1992
Learning Spatio-Temporal Planning from,"Within a simple test-bed, application of feed-forward neurocontrol for short-term planning of robot trajectories in a dynamic environ ment is studied. The action network is embedded in a sensory motoric system architecture that contains a separate world model. It is continuously fed with short-term predicted spatio-temporal obstacle trajectories, and receives robot state feedback. The ac tion net allows for external switching between alternative plan ning tasks. It generates goal-directed motor actions - subject to the robot's kinematic and dynamic constraints - such that colli sions with moving obstacles are avoided. Using supervised learn ing, we distribute examples of the optimal planner mapping over a structure-level adapted parsimonious higher order network. The training database is generated by a Dynamic Programming algo rithm. Extensive simulations reveal, that the local planner map ping is highly nonlinear, but can be effectively and sparsely repre sented by the chosen powerful",Reinforcement Learning,04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf,1992
Hidden Markov Models in Molecular,"Hidden Markov Models (HMMs) can be applied to several impor tant problems in molecular biology. We introduce a new convergent learning algorithm for HMMs that, unlike the classical Baum-Welch algorithm is smooth and can be applied on-line or in batch mode, with or without the usual Viterbi most likely path approximation. Left-right HMMs with insertion and deletion states are then trained to represent several protein families including immunoglobulins and kinases. In all cases, the models derived capture all the important statistical properties of the families and can be used efficiently in a number of important tasks such as multiple alignment, motif de tection, and classification. *and Division of Biology, California Institute of Technology. t and Department of Psychology, Stanford University. 747 748 Baldi, Chauvin, Hunkapiller, and McClure 1 INTRODUCTION Hidden Markov Models (e.g., Rabiner, 1989) and the more general EM algorithm in statistics can be applied to the modeling and anal",NLP,051e4e127b92f5d98d3c79b195f2b291-Paper.pdf,1992
Statistical and Dynamical Interpretation of ISIH,"We interpret the time interval data obtained from periodically stimulated sensory neurons in terms of two simple dynamical systems driven by noise with an embedded weak periodic function called the signal: 1) a bistable system defined by two potential wells separated by a barrier, and 2) a Fit zHugh-Nagumo system. The implementation is by analog simulation: elec tronic circuits which mimic the dynamics. For a given signal frequency, our simulators have only two adjustable parameters, the signal and noise intensi ties. We show that experimental data obtained from the periodically stimu lated mechanoreceptor in the crayfish tail fan can be accurately approximated stochastic resonance by these simulations. Finally, we discuss in the two models. 1 INTRODUcnON It is well known that sensory information is transmitted to the brain using a code which must be based on the time intervals between neural firing events or the mean firing rate. However, in any collection of such data, and even when ",Optimization & Theoretical ML,076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf,1992
Spiral Waves in Integrate-and-Fire,"The formation of propagating spiral waves is studied in a randomly connected neural network composed of integrate-and-fire neurons with recovery period and excitatory connections using computer simulations. Network activity is initiated by periodic stimulation at a single point. The results suggest that spiral waves can arise in such a network via a sub-critical Hopf bifurcation. 1 Introduction In neural networks activity propagates through populations, or layers, of neurons. This propagation can be monitored as an evolution of spatial patterns of activity. Thirty years ago, computer simulations on the spread of activity through 2-D ran domly connected networks demonstrated that a variety of complex spatio-temporal patterns can be generated including target waves and spirals (Beurle, 1956, 1962; Farley and Clark, 1961; Farley, 1965). The networks studied by these investigators correspond to inhomogeneous excitable media in which the probability of interneu ronal connectivity decreases ",Neuroscience,07a96b1f61097ccb54be14d6a47439b0-Paper.pdf,1992
Adaptive Stimulus Representations:,"We present a theory of cortico-hippocampal interaction in discrimination learning. The hippocampal region is presumed to form new stimulus representations which facilitate learning by enhancing the discriminability of predictive stimuli and compressing stimulus-stimulus redundancies. The cortical and cerebellar regions, which are the sites of long-term memory. may acquire these new representations but are not assumed to be capable of forming new representations themselves. Instantiated as a connectionist model. this theory accounts for a wide range of trial-level classical conditioning phenomena in normal (intact) and hippocampal-Iesioned animals. It also makes several novel predictions which remain to be investigated empirically. The theory implies that the hippocampal region is involved in even the simplest learning tasks; although hippocampal-Iesioned animals may be able to use other strategies to learn these tasks. the theory predicts that they will show consistently different patt",NLP,07c5807d0d927dcd0980f86024e5208b-Paper.pdf,1992
Computing with Almost Optimal Size Neural,"Artificial neural networks are comprised of an interconnected collection of certain nonlinear devices; examples of commonly used devices include linear threshold elements, sigmoidal elements and radial-basis elements. We employ results from harmonic analysis and the theory of rational ap proximation to obtain almost tight lower bounds on the size (i.e. number of elements) of neural networks. The class of neural networks to which our techniques can be applied is quite general; it includes any feedforward network in which each element can be piecewise approximated by a low degree rational function. For example, we prove that any depth-( d + 1) network of sigmoidal units or linear threshold elements computing the par ity function of n variables must have O(dnl/d-£) size, for any fixed > O. i In addition, we prove that this lower bound is almost tight by showing that the parity function can be computed with O(dnl/d) sigmoidal units + or linear threshold elements in a depth-(d 1) network. T",Optimization & Theoretical ML,08b255a5d42b89b0585260b6f2360bdd-Paper.pdf,1992
Q-Learning with Hidden-Unit Restarting,"Platt's resource-allocation network (RAN) (Platt, 1991a, 1991b) is modified for a reinforcement-learning paradigm and to ""restart"" existing hidden units rather than adding new units. After restart ing, units continue to learn via back-propagation. The resulting restart algorithm is tested in a Q-Iearning network that learns to solve an inverted pendulum problem. Solutions are found faster on average with the restart algorithm than without it. 1 Introduction The goal of supervised learning is the discovery of a compact representation that generalizes well. Such representations are typically found by incremental, gradient based search, such as error back-propagation. However, in the early stages of learn ing a control task, we are more concerned with fast learning than a compact rep resentation. This implies a local representation with the extreme being the mem orization of each experience. An initially local representation is also advantageous when the learning component is operating in",Reinforcement Learning,08c5433a60135c32e34f46a71175850c-Paper.pdf,1992
Synaptic Weight Noise During MLP,"We analyse the effects of analog noise on the synaptic arithmetic during MultiLayer Perceptron training, by expanding the cost func tion to include noise-mediated penalty terms. Predictions are made in the light of these calculations which suggest that fault tolerance, generalisation ability and learning trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The re sults appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implica tions for all applications, particularly those involving ""inaccurate"" analog neural VLSI. 1 Introduction This paper demonstrates both by consjderatioll of the cost function and the learn ing equations, and by simulation experiments, that injection of random noise on to MLP weights during learning enhances fault-tolerance without additional super vision. We also show that the nature of the hidden n",Optimization & Theoretical ML,08d98638c6fcd194a4b1e6992063e944-Paper.pdf,1992
Rational Parametrizations of Neural,"A connection is drawn between rational functions, the realization theory of dynamical systems, and feedforward neural networks. This allows us to parametrize single hidden layer scalar neural networks with (almost) arbitrary analytic activation functions in terms of strictly proper rational functions. Hence, we can solve the uniqueness of parametrization problem for such networks. 1 INTRODUCTION Nonlinearly parametrized representations of functions ¢: IR -+- IR of the form Ln = (1.1) ¢(x) CiU(X - ai) x E IR, i=l have attracted considerable attention recently in the neural network literature. Here u: IR -+- IR is typically a sigmoidal function such as (1.2) but other choices than (1.2) are possible and of interest. Sometimes more complex representations such as Ln = (1.3) ¢(x) ciu(bix - ad i=l 623 624 Helmke and Williamson or even compositions of these are considered. The purpose of this paper is to explore some parametrization issues regarding (1.1) and in particular to show the close ",Optimization & Theoretical ML,0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf,1992
Directional-Unit Boltzmann Machines,"We present a general formulation for a network of stochastic di rectional units. This formulation is an extension of the Boltzmann machine in which the units are not binary, but take on values in a cyclic range, between 0 and 271' radians. The state of each unit in a Directional-Unit Boltzmann Machine (DUBM) is described by a complex variable, where the phase component specifies a direction; the weights are also complex variables. We associate a quadratic energy function, and corresponding probability, with each DUBM configuration. The conditional distribution of a unit's stochastic state is a circular version of the Gaussian probability distribution, known as the von Mises distribution. In a mean-field approxima tion to a stochastic DUBM, the phase component of a unit's state represents its mean direction, and the magnitude component spec ifies the degree of certainty associated with this direction. This combination of a value and a certainty provides additional repre sentational powe",Deep Learning,0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf,1992
STIMULUS ENCODING BY,"Multiple single neuron responses were recorded from a single electrode in VI of alert, behaving monkeys. Drifting sinusoidal gratings were presented in the cells' overlapping receptive fields, and the stimulus was varied along several visual dimensions. The degree of dimensional separability was calculated for a large population of neurons, and found to be a continuum. Several cells showed different temporal response dependencies to variation of different stimulus dimensions, i.e. the tuning of the modulated firing was not necessarily the same as that of the mean firing rate. We describe a multidimensional receptive field, and use simultaneously recorded responses to compute a multi-neuron receptive field, describing the information processing capabilities of a group of cells. Using dynamic correlation analysis, we propose several computational schemes for multidimensional spatiotemporal tuning for groups of cells. The implications for neuronal coding of stimuli are discussed. 377 378 ",Computer Vision,0f96613235062963ccde717b18f97592-Paper.pdf,1992
On the Use of Projection Pursuit Constraints for,"\Ve present a novel classifica t.ioll and regression met.hod that com bines exploratory projection pursuit. (unsupervised traiuing) with pro jection pursuit. regression (supervised t.raining), t.o yield a. nev,,' family of cost./complexity penalLy terms. Some improved generalization properties are demonstrat.ed on real \vorld problems. 1 Introduction Parameter estimat.ion becomes difficult. in high-dimensional spaces due t.o the in creasing sparseness of t.he dat.a. Therefore. when a low dimensional representation is embedded in t.he da.t.a. dimensionality l'eJuction methods become useful. One such met.hod - projection pursuit. regression (Friedman and St.uet.zle, 1981) (PPR) is capable of performing dimensionality reduct.ion by composit.ion, namely, it con structs an approximat.ion to the desired response function using a composition of lower dimensional smooth functions, These functions depend on low dimensional projections t.hrough t.he data. • Research was support.ed by the N at.io",Optimization & Theoretical ML,0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf,1992
Assessing and Improving Neural Network,The bootstrap algorithm is a computational intensive procedure to derive nonparametric confidence intervals of statistical estimators in situations where an analytic solution is intractable. It is ap plied to neural networks to estimate the predictive distribution for unseen inputs. The consistency of different bootstrap procedures and their convergence speed is discussed. A small scale simulation experiment shows the applicability of the bootstrap to practical problems and its potential use. 1 INTRODUCTION Bootstrapping is a strategy for estimating standard errors and confidence intervals for parameters when the form of the underlying distribution is unknown. It is particularly valuable when the parameter of interest is a complicated functional of the true distribution. The key idea first promoted by Efron (1979) is that the relationship between the true cumulative distribution function (cdf) F and the Fn sample of size n is similar to the relationship between the empirical cdf and a ,Optimization & Theoretical ML,0ff8033cf9437c213ee13937b1c4c455-Paper.pdf,1992
Self-Organizing Rules for Robust,No abstract found,Optimization & Theoretical ML,109a0ca3bc27f3e96597370d5c8cf03d-Paper.pdf,1992
Intersecting regions: The key to combinatorial,"Hidden units in multi-layer networks form a representation space in which each region can be identified with a class of equivalent outputs (Elman, 1989) or a logical state in a finite state machine (Cleeremans, Servan-Schreiber & McClelland, 1989; Giles, Sun, Chen, Lee, & Chen, 1990). We extend the analysis of the spatial structure of hidden unit space to a combinatorial task, based on binding features together in a visual scene. The logical structure requires a combinatorial number of states to represent all valid scenes. On analysing our networks, we find that the high dimensionality of hidden unit space is exploited by using the intersection of neighboring regions to represent conjunctions of features. These results show how combinatorial structure can be based on the spatial nature of networks, and not just on their emulation of logical structure. 1 TECHNIQUES FOR ANALYSING THE SPATIAL AND LOGICAL STRUCTURE OF HIDDEN UNIT SPACE In multi-layer networks, regions of hidden unit space ",NLP,10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf,1992
A Parallel Gradient Descent Method for Learning,"Typical methods for gradient descent in neural network learning involve calculation of derivatives based on a detailed knowledge of the network model. This requires extensive, time consuming calculations for each pat tern presentation and high precision that makes it difficult to implement in VLSI. We present here a perturbation technique that measures, not calculates, the gradient. Since the technique uses the actual network as a measuring device, errors in modeling neuron activation and synaptic weights do not cause errors in gradient descent. The method is parallel in nature and easy to implement in VLSI. We describe the theory of such an algorithm, an analysis of its domain of applicability, some simulations using it and an outline of a hardware implementation. 1 Introduction The most popular method for neural network learning is back-propagation (Rumel hart, 1986) and related algorithms that calculate gradients based on detailed knowl edge of the neural network model. These method",Optimization & Theoretical ML,1595af6435015c77a7149e92a551338e-Paper.pdf,1992
A Model of Feedback to the Lateral,"Simplified models of the lateral geniculate nucles (LGN) and stri ate cortex illustrate the possibility that feedback to the LG N may be used for robust, low-level pattern analysis. The information fed back to the LG N is rebroadcast to cortex using the LG N 's full fan-out, so the cortex-LGN-cortex pathway mediates extensive cortico-cortical communication while keeping the number of neces sary connections small. 1 INTRODUCTION The lateral geniculate nucleus (LGN) in the thalamus is often considered as just a relay station on the way from the retina to visual cortex, since receptive field prop erties of neurons in the LGN are very similar to retinal ganglion cell receptive field properties. However, there is a massive projection from cortex back to the LGN: it is estimated that 3-4 times more synapses in the LG N are due to corticogenicu late connections than those due to retinogeniculate connections [12]. This suggests some important processing role for the LGN, but the nature of the ",NLP,17c276c8e723eb46aef576537e9d56d0-Paper.pdf,1992
History-dependent Attractor Neural,"We present a methodological framework enabling a detailed de scription of the performance of Hopfield-like attractor neural net works (ANN) in the first two iterations. Using the Bayesian ap proach, we find that performance is improved when a history-based term is included in the neuron's dynamics. A further enhancement of the network's performance is achieved by judiciously choosing the censored neurons (those which become active in a given itera tion) on the basis of the magnitude of their post-synaptic poten tials. The contribution of biologically plausible, censored, history dependent dynamics is especially marked in conditions of low firing activity and sparse connectivity, two important characteristics of the mammalian cortex. In such networks, the performance at tained is higher than the performance of two 'independent' iter ations, which represents an upper bound on the performance of history-independent networks. 1 Introduction Associative Attractor Neural Network (ANN) models",NLP,185c29dc24325934ee377cfda20e414c-Paper.pdf,1992
Single-iteration Threshold Hamming,"We analyze in detail the performance of a Hamming network clas sifying inputs that are distorted versions of one of its m stored memory patterns. The activation function of the memory neurons in the original Hamming network is replaced by a simple threshold function. The resulting Threshold Hamming Network (THN) cor rectly classifies the input pattern, with probability approaching 1, using only O(mln m) connections, in a single iteration. The THN drastically reduces the time and space complexity of Hamming Net work classifiers. 1 Introduction Originally presented in (Steinbuch 1961, Taylor 1964) the Hamming network (HN) has received renewed attention in recent years (Lippmann et. al. 1987, Baum et. al. 1988). The HN calculates the Hamming distance between the input pattern and each memory pattern, and selects the memory with the smallest distance. It is composed of two subnets: The similarity subnet, consisting of an n-neuron input layer connected with an m-neuron memory layer, calcula",Optimization & Theoretical ML,192fc044e74dffea144f9ac5dc9f3395-Paper.pdf,1992
Reinforcement Learning Applied to,"Recent research on reinforcement learning has focused on algo rithms based on the principles of Dynamic Programming (DP). One of the most promising areas of application for these algo rithms is the control of dynamical systems, and some impressive results have been achieved. However, there are significant gaps between practice and theory. In particular, there are no con ver gence proofs for problems with continuous state and action spaces, or for systems involving non-linear function approximators (such as multilayer perceptrons). This paper presents research applying DP-based reinforcement learning theory to Linear Quadratic Reg ulation (LQR), an important class of control problems involving continuous state and action spaces and requiring a simple type of non-linear function approximator. We describe an algorithm based on Q-Iearning that is proven to converge to the optimal controller for a large class of LQR problems. We also describe a slightly different algorithm that is only loca",Reinforcement Learning,19bc916108fc6938f52cb96f7e087941-Paper.pdf,1992
The Computation of Stereo Disparity for,"The classical computational model for stereo vision incorporates a uniqueness inhibition constraint to enforce a one-to-one feature match, thereby sacrificing the ability to handle transparency. Crit ics of the model disregard the uniqueness constraint and argue that the smoothness constraint can provide the excitation support required for transparency computation. However, this modifica tion fails in neighborhoods with sparse features. We propose a Bayesian approach to stereo vision with priors favoring cohesive over transparent surfaces. The disparity and its segmentation into a multi-layer ""depth planes"" representation are simultaneously com puted. The smoothness constraint propagates support within each layer, providing mutual excitation for non-neighboring transparent or partially occluded regions. Test results for various random-dot and other stereograms are presented. 1 INTRODUCTION The horizontal disparity in the projection of a 3-D point in a parallel stereo imag ing system ca",Computer Vision,1ecfb463472ec9115b10c292ef8bc986-Paper.pdf,1992
Forecasting Demand for Electric Power,"We are developing a forecaster for daily extremes of demand for electric power encountered in the service area of a large midwest ern utility and using this application as a testbed for approaches to input dimension reduction and decomposition of network train ing. Projection pursuit regression representations and the ability of algorithms like SIR to quickly find reasonable weighting vectors enable us to confront the vexing architecture selection problem by reducing high-dimensional gradient searchs to fitting single-input single-output (SISO) subnets. We introduce dimension reduction algorithms, to select features or relevant subsets of a set of many variables, based on minimizing an index of level-set dispersions (closely related to a projection index and to SIR), and combine them with backfitting to implement a neural network version of projection pursuit. The performance achieved by our approach, when trained on 1989, 1990 data and tested on 1991 data, is com parable to that achie",Optimization & Theoretical ML,2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf,1992
Visual Motion Computation in Analog,"The real time computation of motion from real images using a single chip with integrated sensors is a hard prob lem. We present two analog VLSI schemes that use pulse domain neuromorphic circuits to compute motion. Pulses of variable width, rather than graded potentials, represent a natural medium for evaluating temporal relationships. Both algorithms measure speed by timing a moving edge in the image. Our first model is inspired by Reichardt's algorithm in the fiy and yields a non-monotonic response vs. velocity curve. We present data from a chip that implements this model. Our second algorithm yields a monotonic response vs. velocity curve and is currently being translated into silicon. 1 Introd uction Analog VLSI chips for the real time computation of visual motion have been the focus of much active research because of their importance as sensors for robotic applications. Correlation schemes such as those described in (Delbriick, 1993) have been found to be more robust than gradient",Computer Vision,233509073ed3432027d48b1a83f5fbd2-Paper.pdf,1992
Learning Control Under Extreme,"A peg-in-hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real-world conditions of uncertainty and noise. Task complexity due to the use of an unchamfered hole and a clearance of less than 0.2mm is compounded by the presence of positional uncertainty of magnitude exceeding 10 to 50 times the clearance. Despite this extreme degree of uncertainty, our results indicate that direct reinforcement learning can be used to learn a robust reactive control strategy that results in skillful peg-in-hole insertions. 1 INTRODUCTION Many control tasks of interest today involve controlling complex nonlinear systems under uncertainty and noise.1 Because traditional control design techniques are not very effective under such circumstances, methods for learning control are becoming increasingly popular. Unfortunately, in many of these control tasks, it is difficult to obtain training information in the form",Reinforcement Learning,24681928425f5a9133504de568f5f6df-Paper.pdf,1992
Efficient Pattern Recognition Using a,"Memory-based classification algorithms such as radial basis func tions or K-nearest neighbors typically rely on simple distances (Eu clidean, dot product ... ), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rota tion, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases. 1 INTRODUCTION Distance-based classification algorithms such as radial basis functions or K-nearest neighbors often rely on simple distances (such as Euclidean distance, Hamming distance, etc.). As a result, they suf",Computer Vision,26408ffa703a72e8ac0117e74ad46f33-Paper.pdf,1992
Planar Hidden Markov Modeling:,"We propose in this paper a statistical model (planar hidden Markov model - PHMM) describing statistical properties of images. The model generalizes the single-dimensional HMM, used for speech processing, to the planar case. For this model to be useful an efficient segmentation algorithm, similar to the Viterbi algorithm for HMM, must exist We present conditions in terms of the PHMM parameters that are sufficient to guarantee that the planar segmentation problem can be solved in polynomial time, and describe an algorithm for that. This algorithm aligns optimally the image with the model, and therefore is insensitive to elastic distortions of images. Using this algorithm a joint optima1 segmentation and recognition of the image can be performed, thus overcoming the weakness of traditional OCR systems where segmentation is performed independently before the recognition leading to unrecoverable recognition errors. Tbe PHMM approach was evaluated using a set of isolated band-written digits.",Computer Vision,26dd0dbc6e3f4c8043749885523d6a25-Paper.pdf,1992
Global Regularization of Inverse Kinematics for Redundant,"The inverse kinematics problem for redundant manipulators is ill-posed and nonlinear. There are two fundamentally different issues which result in the need for some form of regularization; the existence of multiple solution branches (global ill-posedness) and the existence of excess degrees of freedom (local ill posedness). For certain classes of manipulators, learning methods applied to input-output data generated from the forward function can be used to globally regularize the problem by partitioning the domain of the forward mapping into a finite set of regions over which the inverse problem is well-posed. Local regularization can be accomplished by an appropriate parameterization of the redundancy consistently over each region. As a result, the ill-posed problem can be transformed into a finite set of well-posed problems. Each can then be solved separately to construct approximate direct inverse functions. 1 INTRODUCTION The robot forward kinematics function maps a vector of joint ",Optimization & Theoretical ML,291597a100aadd814d197af4f4bab3a7-Paper.pdf,1992
Object-Based Analog VLSI Vision,"We describe two successfully working, analog VLSI vision circuits that move beyond pixel-based early vision algorithms. One circuit, implementing the dynamic wires model, provides for dedicated lines of communication among groups of pixels that share a common property. The chip uses the dynamic wires model to compute the arclength of visual contours. Another circuit labels all points inside a given contour with one voltage and all other with another volt age. Its behavior is very robust, since small breaks in contours are automatically sealed, providing for Figure-Ground segregation in a noisy environment. Both chips are implemented using networks of resistors and switches and represent a step towards object level processing since a single voltage value encodes the property of an ensemble of pixels. 1 CONTOUR-LENGTH CHIP Contour length computation is useful for further processing such as structural saliency (Shaashua and Ullman, 1988), which is thought to be an important stage before o",Computer Vision,2dea61eed4bceec564a00115c4d21334-Paper.pdf,1992
Perceiving Complex Visual Scenes:,"Which processes underly our ability to quickly recognize familiar objects within a complex visual input scene? In this paper an imple mented neural network model is described that attempts to specify how selective visual attention, perceptual organisation, and invari ance transformations might work together in order to segment, select, and recognize objects out of complex input scenes containing multi ple, possibly overlapping objects. Retinotopically organized feature maps serve as input for two main processing routes: the 'where pathway' dealing with location information and the 'what-pathway' computing the shape and attributes of objects. A location-based at tention mechanism operates on an early stage of visual processing selecting a contigous region of the visual field for preferential proces sing. Additionally, location-based attention plays an important role for invariant object recognition controling appropriate normalization processes within the what-pathway. Object recognitio",Computer Vision,2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf,1992
Second order derivatives for network,"We investigate the use of information from all second order derivatives of the error function to perfonn network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Sol1a, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-I from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weighL decay on three benchmark MONK's problems [Thrun et aI., 1991]. ",Optimization & Theoretical ML,303ed4c69846ab36c2904d3ba8573050-Paper.pdf,1992
Automatic Learning Rate Maximization,"We propose a very simple, and well principled way of computing the optimal step size in gradient descent algorithms. The on-line version is very efficient computationally, and is applicable to large backpropagation networks trained on large data sets. The main ingredient is a technique for estimating the principal eigenvalue(s) and eigenvector(s) of the objective function's second derivative ma trix (Hessian), which does not require to even calculate the Hes sian. Several other applications of this technique are proposed for speeding up learning, or for eliminating useless parameters. 1 INTRODUCTION Choosing the appropriate learning rate, or step size, in a gradient descent procedure such as backpropagation, is simultaneously one of the most crucial and expert intensive part of neural-network learning. We propose a method for computing the best step size which is both well-principled, simple, very cheap computationally, and, most of all, applicable to on-line training with large networ",Optimization & Theoretical ML,30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf,1992
Computation of Heading Direction From,"We have designed a neural network which detects the direction of ego motion from optic flow in the presence of eye movements (Lappe and Rauschecker, 1993). The performance of the network is consistent with human psychophysical data, and its output neurons show great similarity to ""triple component"" cells in area MSTd of monkey visual cortex. We now show that by using assumptions about the kind of eye movements that the obsenrer is likely to perform, our model can generate various other cell types found in MSTd as well. 1 INTRODUCTION Following the ideas of Gibson in the 1950's a number of studies in human psychophysics have demonstrated that optic flow can be used effectively for navigation in space (Rieger and Toet, 1985; Stone and Perrone, 1991; Warren et aI., 1988). In search for the neural basis of optic flow processing, an area in the cat's extrastriate visual cortex (PMLS) was described as having a centrifugal organization of neuronal direction preferences, which suggested an inv",Computer Vision,30ef30b64204a3088a26bc2e6ecf7602-Paper.pdf,1992
Statistical Modeling of Cell-Assemblies,"So far there has been no general method for relating extracellular electrophysiological measured activity of neurons in the associative cortex to underlying network or ""cognitive"" states. We propose to model such data using a multivariate Poisson Hidden Markov Model. We demonstrate the application of this approach for tem poral segmentation of the firing patterns, and for characterization of the cortical responses to external stimuli. Using such a statisti cal model we can significantly discriminate two behavioral modes of the monkey, and characterize them by the different firing pat terns, as well as by the level of coherency of their multi-unit firing activity. Our study utilized measurements carried out on behaving Rhesus monkeys by M. Abeles, E. Vaadia, and H. Bergman, of the Hadassa Medical School of the Hebrew University. 1 Introduction Hebb hypothesized in 1949 that the basic information processing unit in the cortex is a cell-assembly which may include thousands of cells in a h",Optimization & Theoretical ML,3328bdf9a4b9504b9398284244fe97c2-Paper.pdf,1992
Harmonic Grammars,"Basic connectionist principles imply that grammars should take the form of systems of parallel soft constraints defining an optimization problem the solutions to which are the well-formed structures in the language. Such Harmonic Grammars have been successfully applied to a number of problems in the theory of natural languages. Here it is shown that formal languages too can be specified by Harmonic Grammars, rather than by conventional serial re-write rule systems. 1 HARMONIC GRAMMARS In collaboration with Geraldine Legendre, Yoshiro Miyata, and Alan Prince, I have been studying how symbolic computation in human cognition can arise naturally as a higher-level virtual machine realized in appropriately designed lower-level con nectionist networks. The basic computational principles of the approach are these: (1) a. \Vhell analyzed at the lower level, mental representations are dis tributed patterns of connectionist activity; when analyzed at a higher level, these same representations con",NLP,3435c378bb76d4357324dd7e69f3cd18-Paper.pdf,1992
Silicon Auditory Processors,"Sever<tl resE'<lI'ch gl'Oups cue impl('lllt'lIt.ing allalog integrat.ed circuit. models of hiological audit.ory Pl""Occ'ssing. The outputs of these circuit models haV(~ takell sevel'al forms. includillg video [ormat. for monitor display, simple scanned Ollt.put [01' oscilloscope display anJ parallel analog out.put.s suitable ror dat.a-acquisition systems. In this pa.per, we describe an allel""llative out.put method for silicon auditory models, suit.able for din-'ct. interface to digital computers. * Present address: f\1. Mahowald, f\1H.C ,\natol1lical Ncmophamacology Unit, Mansfield TId, Oxfc)('d OXI :1'rl£ Ellgland. mam~vax.oxford.ac.uk t Present address: f\lass Siviloui. '1'(1111)(,1' H,csearrh, 180 Nort.h Vinedo Avenue, Pasadena, CA 9Il07. mass~tanner. corn :I: Present address: Dave Gill,>spiE', SYllapf,ics, :l()!)8 Orchard Parkway, San Jose CA, 95131. daveg~synaptics. com 820 Silicon Auditory Processors as Computer Peripherals 821",Computer Vision,3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf,1992
"Information, prediction, and query by","We analyze the ""query by committee"" algorithm, a method for fil tering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves infor mation gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of thresholded smooth functions. 1 Introduction For the most part, research on supervised learning has utilized a random input paradigm, in which the learner is both trained and tested on examples drawn at random from the same distribution. In contrast, in the query paradigm, the learner is given the power to ask questions, rather than just passively accept examples. What does the learner gain from this additional power? Can it attain the same prediction performance with fewer examples? Most work on query learning has been in the constructive paradigm, in which the 483 484 Freund, Seung, Shamir, and Tishby ",Optimization & Theoretical ML,3871bd64012152bfb53fdf04b401193f-Paper.pdf,1992
Improving Convergence in Hierarchical,"We are interested in the use of analog neural networks for recog nizing visual objects. Objects are described by the set of parts they are composed of and their structural relationship. Struc tural models are stored in a database and the recognition prob lem reduces to matching data to models in a structurally consis tent way. The object recognition problem is in general very diffi cult in that it involves coupled problems of grouping, segmentation and matching. We limit the problem here to the simultaneous la belling of the parts of a single object and the determination of analog parameters. This coupled problem reduces to a weighted match problem in which an optimizing neural network must min imize E(M, p) = LO'i MO'i WO'i(p), where the {MO'd are binary match variables for data parts i to model parts a and {Wai(P)} are weights dependent on parameters p. In this work we show that by first solving for estimates p without solving for Mai, we may obtain good initial parameter estimates t",Computer Vision,3a066bda8c96b9478bb0512f0a43028c-Paper.pdf,1992
A dynamical model of priming and,"We describe a model of visual word recognition that accounts for several aspects of the temporal processing of sequences of briefly presented words. The model utilizes a new representation for writ ten words, based on dynamic time warping and multidimensional scaling. The visual input passes through cascaded perceptual, com parison, and detection stages. We describe how these dynamical processes can account for several aspects of word recognition, in cluding repetition priming and repetition blindness. 1 INTRODUCTION Several psychological phenomena show that the construction of organized and mean ingful representations of the visual environment requires establishing separate repre sentations (termed episodic representations) for the different objects viewed. Three phenomena in the word recognition literature suggest that the segregation of the visual flow into separate episodic representations can be characterized in terms of specific temporal constraints. We developed a model to explo",Computer Vision,3d2d8ccb37df977cb6d9da15b76c3f3a-Paper.pdf,1992
Unsupervised Discrimination of Clustered Data,"We present the information-theoretic derivation of a learning algorithm that clusters unlabelled data with linear discriminants. In contrast to methods that try to preserve information about the input patterns, we maximize the information gained from observing the output of robust binary discriminators implemented with sigmoid nodes. We deri ve a local weight adaptation rule via gradient ascent in this objective, demonstrate its dynamics on some simple data sets, relate our approach to previous work and suggest directions in which it may be extended. 1 INTRODUCTION Unsupervised learning algorithms may perform useful preprocessing functions by pre serving some aspects of their input while discarding others. This can be quantified as maximization of the information the network's output carries about those aspects of the input that are deemed important. (Linsker, 1988) suggests maximal preservation of information about all aspects of the input. This In/omax principle provides for optimal ",Optimization & Theoretical ML,42e77b63637ab381e8be5f8318cc28a2-Paper.pdf,1992
On-Line Estimation of the Optimal Value,"In this paper, we discuss on-line estimation strategies that model the optimal value function of a typical optimal control problem. We present a general strategy that uses local corridor solutions obtained via dynamic programming to provide local optimal con trol sequence training data for a neural architecture model of the optimal value function. ION-LINE ESTIMATORS In this paper, the problems of adaptive control using neural architectures are ex plored in the setting of general on-line estimators. 'Ve will try to pay close attention to the underlying mathematical structure that arises in the on-line estimation pro cess. The complete effect of a control action Uk at a given time step t/.; is clouded by the fact that the state history depends on the control actions taken after time step tk' So the effect of a control action over all future time must be monitored. Hence, choice of control must inevitably involve knowledge of the future history of the state trajectory. In other words, th",Optimization & Theoretical ML,443cb001c138b2561a0d90720d6ce111-Paper.pdf,1992
Using Aperiodic Reinforcement for Directed,"We present a local learning rule in which Hebbian learning is conditional on an incorrect prediction of a reinforcement signal. We propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target. Three exam pIes are presented which illustrate how this framework can be applied to the development of the oculomotor system. 1 INTRODUCTION Activity-dependent accounts of the self-organization of the vertebrate brain have relied ubiquitously on correlational (mainly Hebbian) rules to drive synaptic learn ing. In the brain, a major problem for any such unsupervised rule is that many different kinds of correlations exist at approximately the same time scales and each is effectively noise to the next. For example, relationships within and between the retinae among variables such as color, motion, and topography may mask one another and disrupt their appropriate segregation",Reinforcement Learning,44c4c17332cace2124a1a836d9fc4b6f-Paper.pdf,1992
A Connectionist Symbol Manipulator,"We present a neural net architecture that can discover hierarchical and re cursive structure in symbol strings. To detect structure at multiple levels, the architecture has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory. In terms of formal languages, the architecture can learn to parse strings in an LR(O) context free grammar. Given training sets of positive and negative exemplars, the architecture has been trained to recognize many different grammars. The architecture has only one layer of modifiable weights, allowing for a straightforward interpretation of its behavior. Many cognitive domains involve complex sequences that contain hierarchical or recursive structure, e.g., music, natural language parsing, event perception. To il lustrate, ""the spider that ate the hairy fly"" is a noun phrase containing the embed ded noun phrase ""the hairy fly."" Understanding such multilevel structures requires forming reduced descriptions (Hi",NLP,45645a27c4f1adc8a7a835976064a86d-Paper.pdf,1992
Analogy--Watershed or Waterloo?,"Neural network models have been criticized for their inability to make use of compositional representations. In this paper, we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition. These findings suggest that people compare relational representations via a process of structural alignment. This process will have to be captured by any model of cognition, symbolic or subsymbolic. 1.0 INTRODUCTION Pattern recognition is central to cognition. At the perceptual level, we notice key features of the world (like symmetry), recognize objects in front of us and identify the letters on a printed page. At a higher level, we recognize problems we have solved before and determine similarities-including analogical similarities-between new situations and old ones. Neural network models have been successful at capturing sensory pattern recognition (e.g., Sabourin & Mitiche, 1992). In contrast, the determination of higher level similarities has",NLP,48ab2f9b45957ab574cf005eb8a76760-Paper.pdf,1992
A Recurrent Neural Network for,"This paper presents a neural network able to control saccadic movements. The input to the network is a specification of a stimulation site on the collicular motor map. The output is the time course of the eye position in the orbit (horizontal and vertical angles). The units in the network exhibit a one-to-one correspondance with neurons in the intermediate layer of the superior colliculus (collicular motor map), in the brainstem and with oculomotor neurons. Simulations carried out with this network demonstrate its ability to reproduce in a straightforward fashion many experimental observations.",NLP,4a47d2983c8bd392b120b627e0e1cab4-Paper.pdf,1992
Network Structuring And Training Using,"We demonstrate in this paper how certain forms of rule-based knowledge can be used to prestructure a neural network of nor malized basis functions and give a probabilistic interpretation of the network architecture. We describe several ways to assure that rule-based knowledge is preserved during training and present a method for complexity reduction that tries to minimize the num ber of rules and the number of conjuncts. After training the refined rules are extracted and analyzed. 1 INTRODUCTION Training a network to model a high dimensional input/output mapping with only a small amount of training data is only possible if the underlying map is of low com plexity and the network, therefore, can be oflow complexity as well. With increasing *Mail address: Siemens AG, Central Research, Otto-Hahn-Ring 6, 8000 Miinchen 83. 871 872 Tresp, Hollatz, and Ahmad network complexity, parameter variance increases and. the network prediction be comes less reliable. This predicament can be solved if w",Optimization & Theoretical ML,4c27cea8526af8cfee3be5e183ac9605-Paper.pdf,1992
Hybrid Circuits of Interacting Computer Model,"We demonstrate the use of a digital signal processing board to construct hybrid networks consisting of computer model neurons connected to a biological neural network. This system operates in real time. and the synaptic connections are realistic effective conductances. Therefore. the synapses made from the computer model neuron are integrated correctly by the postsynaptic biological neuron. This method provides us with the ability to add additional. completely known elements to a biological network and study their effect on network activity. Moreover. by changing the parameters of the model neuron. it is possible to assess the role of individual conductances in the activity of the neuron. and in the network in which it participates. ""Present address. lXL. Universit6 de Bordeaux l-Enserb. CNRSURA 846. 351 crs de la Liberation. 33405 Talence Cedex. France. 'Present address. LNPC. CNRS. Universite de Bordeaux 1. Place de Dr. Peyneau. 33120 Arcachon. France 813 814 Renaud-LeMasson, LeMasso",Optimization & Theoretical ML,4ffce04d92a4d6cb21c1494cdfcd6dc1-Paper.pdf,1992
A Knowledge-Based Model of Geometry Learning,"We propose a model of the development of geometric reasoning in children that explicitly involves learning. The model uses a neural network that is initialized with an understanding of geometry similar to that of second-grade children. Through the presentation of a series of examples, the model is shown to develop an understanding of geometry similar to that of fifth-grade children who were trained using similar materials. 1 Introduction One of the principal problems in instructing children is to develop sequences of examples that help children acquire useful concepts. In this endeavor it is often useful to have a model of how children learn the material, for a good model can guide an instructor towards particularly effective examples. In short, good models of learning help a teacher maximize the utility of the example presented. The particular problem with which we are concerned is learning about conventional concepts in geometry, like those involved in identifying, and recognizing si",Deep Learning,500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf,1992
Integration of Visual and Somatosensory,"The primate brain must solve two important problems in grasping move ments. The first problem concerns the recognition of grasped objects: specifically, how does the brain integrate visual and motor information on a grasped object? The second problem concerns hand shape planning: specifically, how does the brain design the hand configuration suited to the shape of the object and the manipulation task? A neural network model that solves these problems has been developed. The operations of the net work are divided into a learning phase and an optimization phase. In the learning phase, internal representations, which depend on the grasped ob jects and the task, are acquired by integrating visual and somatosensory information. In the optimization phase, the most suitable hand shape for grasping an object is determined by using a relaxation computation of the network. * Present Address: Parallel Distributed Processing Research Dept., Sony Corporation, 6-7-35 Kitashinagawa, Shinagawa-ku, Tok",Computer Vision,53e3a7161e428b65688f14b84d61c610-Paper.pdf,1992
Kohonen Feature Maps and Growing,"A performance comparison of two self-organizing networks, the Ko honen Feature Map and the recently proposed Growing Cell Struc tures is made. For this purpose several performance criteria for self-organizing networks are proposed and motivated. The models are tested with three example problems of increasing difficulty. The Kohonen Feature Map demonstrates slightly superior results only for the simplest problem. For the other more difficult and also more realistic problems the Growing Cell Structures exhibit significantly better performance by every criterion. Additional advantages of the new model are that all parameters are constant over time and that size as well as structure of the network are determined auto matically. 1 INTRODUCTION Self-organizing networks are able to generate interesting low-dimensional represen tations of high-dimensional input data. The most well-known of these models is the Kohonen Feature Map (Kohonen [1982)). So far it has been applied to a large variety o",Computer Vision,5487315b1286f907165907aa8fc96619-Paper.pdf,1992
A Neural Model of Descending Gain,"In the electrosensory system of weakly electric fish, descending pathways to a first-order sensory nucleus have been shown to influ ence the gain of its output neurons. The underlying neural mecha nisms that subserve this descending gain control capability are not yet fully understood. We suggest that one possible gain control mechanism could involve the regulation of total membrane conduc tance of the output neurons. In this paper, a neural model based on this idea is used to demonstrate how activity levels on descend ing pathways could control both the gain and baseline excitation of a target neuron. 1 INTRODUCTION Certain species of freshwater tropical fish, known as weakly electric fish, possess an active electric sense that allows them to detect and discriminate objects in their environment using a self-generated electric field (Bullock and Heiligenberg, 1986). They detect objects by sensing small perturbations in this electric field using an array of specialized receptors, known ",Optimization & Theoretical ML,556f391937dfd4398cbac35e050a2177-Paper.pdf,1992
Memory-based Reinforcement Learning: Efficient,"We present a new algorithm, Prioritized Sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Differencing and Q-Iearning have fast real time perfor mance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize impor tant dynamic programming sweeps and to guide the exploration of state space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have difficulty. 1 STOCHASTIC PREDICTION The paper introduces a memory-based technique, prioritized 6weeping, which is used both for stochastic prediction and reinforcement learning. A fuller version of this paper is in preparation [Moore and Atkeson, 1992]. Conside",Reinforcement Learning,55743cc0393b1cb4b8b37d09ae48d097-Paper.pdf,1992
Learning Fuzzy Rule-Based Neural,"A three-step method for function approximation with a fuzzy sys tem is proposed. First, the membership functions and an initial rule representation are learned; second, the rules are compressed as much as possible using information theory; and finally, a com putational network is constructed to compute the function value. This system is applied to two control examples: learning the truck and trailer backer-upper control system, and learning a cruise con trol system for a radio-controlled model car. 1 Introduction Function approximation is the problem of estimating a function from a set of ex amples of its independent variables and function value. If there is prior knowledge of the type of function being learned, a mathematical model of the function can be constructed and the parameters perturbed until the best match is achieved. How ever, if there is no prior knowledge of the function, a model-free system such as a neural network or a fuzzy system may be employed to approximate an arbi",Optimization & Theoretical ML,55b37c5c270e5d84c793e486d798c01d-Paper.pdf,1992
Neural Network On-Line Learning Control,"The overall goal is to reduce spacecraft weight. volume, and cost by on line adaptive non-linear control of flexible structural components. The objective of this effort is to develop an adaptive Neural Network (NN) controller for the Ball C-Side 1m x 3m antenna with embedded actuators and the RAMS sensor system. A traditional optimal controller for the major modes is provided perturbations by the NN to compensate for unknown residual modes. On-line training of recurrent and feed-forward NN architectures have achieved adaptive vibration control with unknown modal variations and noisy measurements. On-line training feedback to each actuator NN output is computed via Newton's method to reduce the difference between desired and achieved antenna positions. 1 ADAPTIVE CONTROL BACKGROUND The two traditional approaches to adaptive control are 1) direct control (such as perfonned in direct model reference adaptive controllers) and 2) indirect control (such as performed by explicit self-tuning r",Reinforcement Learning,58d4d1e7b1e97b258c9ed0b37e02d087-Paper.pdf,1992
Hidden Markov Model Induction by Bayesian,"This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge. 1 INTRODUCTION AND OVERVIEW Hidden Markov Models (HMMs) are a well-studied approach to the modelling of sequence data. HMMs can be viewed as a stochastic generalization of finite-state automata, where both the transitions between states and the generation of output symbols are governed by probability distributions. HMMs have been important in speech recognition (Rabiner & Juang, 1986),",NLP,5c04925674920eb58467fb52ce4ef728-Paper.pdf,1992
Statistical Mechanics of Learning I• n a,"We use statistical mechanics to study generalization in large com mittee machines. For an architecture with nonoverlapping recep tive fields a replica calculation yields the generalization error in the limit of a large number of hidden units. For continuous weights the generalization error falls off asymptotically inversely proportional to the number of training examples per weight. For binary Q, weights we find a discontinuous transition from poor to perfect generalization followed by a wide region of metastability. Broken replica symmetry is found within this region at low temperatures. For a fully connected architecture the generalization error is cal culated within the annealed approximation. For both binary and continuous weights we find transitions from a symmetric state to one with specialized hidden units, accompanied by discontinuous drops in the generalization error. 1 Introduction There has been a good deal of theoretical work on calcula.ting the generalization ability of ne",Optimization & Theoretical ML,5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf,1992
An Information-Theoretic Approach to,"Information theory is used to derive a simple formula for the amount of information conveyed by the firing rate of a neuron about any experimentally measured variable or combination of variables (e.g. running speed, head direction, location of the animal, etc.). The derivation treats the cell as a communication channel whose input is the measured variable and whose output is the cell's spike train. Applying the formula, we find systematic differences in the information content of hippocampal ""place cells"" in different ex perimental conditions. 1 INTRODUCTION Almost any neuron will respond to some manipulation or other by changing its firing rate, and this change in firing can convey information to downstream neurons. The aim of this article is to introduce a very simple formula for the average rate at which a cell conveys information in this way, and to show how the formula is helpful in the study of the firing properties of cells in the rat hippocampus. This is by no means the first a",Optimization & Theoretical ML,5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf,1992
Using hippocampal 'place cells' for,"A model of the hippocampus as a central element in rat naviga tion is presented. Simulations show both the behaviour of single cells and the resultant navigation of the rat. These are compared with single unit recordings and behavioural data. The firing of CAl place cells is simulated as the (artificial) rat moves in an en vironment. This is the input for a neuronal network whose output, at each theta (0) cycle, is the next direction of travel for the rat. Cells are characterised by the number of spikes fired and the time of firing with respect to hippocampal 0 rhythm. 'Learning' occurs in 'on-off' synapses that are switched on by simultaneous pre- and post-synaptic activity. The simulated rat navigates successfully to goals encountered one or more times during exploration in open fields. One minute of random exploration of a 1m2 environment allows navigation to a newly-presented goal from novel starting po sitions. A limited number of obstacles can be successfully avoided. 1 Backgroun",Reinforcement Learning,5e9f92a01c986bafcabbafd145520b13-Paper.pdf,1992
Weight Space Probability Densities,"The ensemble dynamics of stochastic learning algorithms can be studied using theoretical techniques from statistical physics. We develop the equations of motion for the weight space probability densities for stochastic learning algorithms. We discuss equilibria in the diffusion approximation and provide expressions for special cases of the LMS algorithm. The equilibrium densities are not in general thermal (Gibbs) distributions in the objective function be ing minimized, but rather depend upon an effective potential that includes diffusion effects. Finally we present an exact analytical expression for the time evolution of the density for a learning algo rithm with weight updates proportional to the sign of the gradient. 1 Introduction: Theoretical Framework Stochastic learning algorithms involve weight updates of the form + w(n+1) = w(n) /-l(n)H[w(n),x(n)] (1) where w E 7£m is the vector of m weights, /-l is the learning rate, H[.] E 7£m is the update function, and x(n) is the exempla",Optimization & Theoretical ML,6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf,1992
Discriminability-Based Transfer between,"Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm. called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly. 1 INTRODUCTION Neural networks are usually trained from scratch, relying only on the training data for guidance. However, as more and more networks are trained for various tasks, it becomes reasonable to seek out methods that. avoid ""'reinventing the wheel"" , and instead are able to build on previously trained networks' results. For example, con sider a speech recognition network that was only trained on Amer",Optimization & Theoretical ML,67e103b0761e60683e83c559be18d40c-Paper.pdf,1992
Learning Sequential Tasks by,"An incremental, higher-order, non-recurrent network combines two properties found to be useful for learning sequential tasks: higher order connections and incremental introduction of new units. The network adds higher orders when needed by adding new units that dynamically modify connection weights. Since the new units mod ify the weights at the next time-step with information from the previous step, temporal tasks can be learned without the use of feedback, thereby greatly simplifying training. Furthermore, a the oretically unlimited number of units can be added to reach into the arbitrarily distant past. Experiments with the Reber gram mar have demonstrated speedups of two orders of magnitude over recurrent networks. 1 INTRODUCTION Second-order recurrent networks have proven to be very powerful [8], especially when trained using complete back propagation through time [1, 6, 14]. It has also been demonstrated by Fahlman that a recurrent network that incrementally adds nodes during tra",NLP,68264bdb65b97eeae6788aa3348e553c-Paper.pdf,1992
Topography and Ocular Dominance,"Anewcomputationalmodelthataddressestheformationofbothtopog- raphy and ocular dominance is presented. This is motivated by exper- imental evidence thatthese phenomena may be subserved by the same mechanisms. An important aspect of this model is that ocular domi- nancesegregationcanoccurwheninputactivityisbothdistributed,and positively correlated betweenthe eyes. This allowsinvestigationof the dependence ofthe patternofocular dominance stripesonthe degree of correlationbetweentheeyes: itisfoundthatincreasingcorrelationleads tonarrowerstripes. Experimentsaresuggestedtotestwhethersuchbe- haviouroccursinthenaturalsystem. 1 INTRODUCTION Thedevelopmentoftopographicandinterdigitatedmappingsinthenervoussys- tem has been much studied experimentally, especially in the visual system (e.g. [8, 15]). Here, each eye projects in a topographic manner to more central brain structures: i.e. neighbouringpointsintheeyemaptoneighbouringpointsinthe brain. In addition, when fibres from the two eyes invade the",Computer Vision,6a10bbd480e4c5573d8f3af73ae0454b-Paper.pdf,1992
A Hybrid Neural Net System for,"Untill recently, state-of-the-art, large-vocabulary, continuous speech recognition (CSR) has employed Hidden Markov Modeling (HMM) to model speech sounds. In an attempt to improve over HMM we developed a hybrid system that integrates HMM technology with neu ral networks. We present the concept of a ""Segmental Neural Net"" (SNN) for phonetic modeling in CSR. By taking into account all the frames of a phonetic segment simultaneously, the SNN overcomes the well-known conditional-independence limitation of HMMs. In several speaker-independent experiments with the DARPA Resource Manage ment corpus, the hybrid system showed a consistent improvement in performance over the baseline HMM system. 1 INTRODUCTION The current state of the art in continuous speech recognition (CSR) is based on the use of hidden Markov models (HMM) to model phonemes in context. Two main reasons for the popularity of HMMs are their high performance, in terms of recognition accu racy, and their computational efficiency ",NLP,6aca97005c68f1206823815f66102863-Paper.pdf,1992
A Formal Model of the Insect Olfactory,It is known from biological data that the response patterns of interneurons in the olfactory macroglomerulus (MGC) of insects are of central importance for the coding of the olfactory signal. We propose an analytically tractable model of the MGC which allows us to relate the distribution of response patterns to the architecture of the network.,NLP,71a3cb155f8dc89bf3d0365288219936-Paper.pdf,1992
Using Prior Knowledge in a NNPDA to Learn,"Although considerable interest has been shown in language inference and automata induction using recurrent neural networks, success of these models has mostly been limited to regular languages. We have previ ously demonstrated that Neural Network Pushdown Automaton (NNPDA) model is capable of learning deterministic context-free languages (e.g., anbn and parenthesis languages) from examples. However, the learning task is computationally intensive. In this paper we discus some ways in which a priori knowledge about the task and data could be used for efficient learning. We also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages (eg. anbncbmam ). 1 INTRODUCTION Language inference and automata induction using recurrent neural networks has gained considerable interest in the recent years. Nevertheless, success of these mod els has been mostly limited to regular languages. Additional information in form of a priori knowledge has proved importa",NLP,766ebcd59621e305170616ba3d3dac32-Paper.pdf,1992
A Method for Learning from Hints,"We address the problem of learning an unknown function by pu tting together several pieces of information (hints) that we know about the function. We introduce a method that generalizes learn ing from examples to learning from hints. A canonical representa tion of hints is defined and illustrated for new types of hints. All the hints are represented to the learning process by examples, and examples of the function are treated on equal footing with the rest of the hints. During learning, examples from different hints are selected for processing according to a given schedule. We present two types of schedules; fixed schedules that specify the relative em phasis of each hint, and adaptive schedules that are based on how well each hint has been learned so far. Our learning method is compatible with any descent technique that we may choose to use. 1 INTRODUCTION The use of hints is coming to the surface in a number of research communities dealing with learning and adaptive systems. In the l",Optimization & Theoretical ML,7750ca3559e5b8e1f44210283368fc16-Paper.pdf,1992
Holographic Recurrent Networks,"Holographic Recurrent Networks (HRNs) are recurrent networks which incorporate associative memory techniques for storing se quential structure. HRNs can be easily and quickly trained using gradient descent techniques to generate sequences of discrete out puts and trajectories through continuous spaee. The performance of HRNs is found to be superior to that of ordinary recurrent net works on these sequence generation tasks. 1 INTRODUCTION The representation and processing of data with complex structure in neural networks remains a challenge. In a previous paper [Plate, 1991b] I described Holographic Re duced Representations (HRRs) which use circular-convolution associative-memory to embody sequential and recursive structure in fixed-width distributed represen tations. This paper introduces Holographic Recurrent Networks (HRNs), which are recurrent nets that incorporate these techniques for generating sequences of symbols or trajectories through continuous space. The recurrent component ",Deep Learning,7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf,1992
Generalization Abilities of,"In [5], a new incremental cascade network architecture has been presented. This paper discusses the properties of such cascade networks and investigates their generalization abilities under the particular constraint of small data sets. The evaluation is done for cascade networks consisting of local linear maps using the Mackey Glass time series prediction task as a benchmark. Our results in dicate that to bring the potential of large networks to bear on the problem of extracting information from small data sets without run ning the risk of overjitting, deeply cascaded network architectures are more favorable than shallow broad architectures that contain the same number of nodes. 1 Introduction For many real-world applications, a major constraint for the successful learning from examples is the limited number of examples available. Thus, methods are required, that can learn from small data sets. This constraint makes the problem of generalization particularly hard. If the number of adju",Optimization & Theoretical ML,84117275be999ff55a987b9381e01f96-Paper.pdf,1992
Explanation-Based Neural Network Learning,"How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization when learning the target function. Results are presented applying this approach to a simulated robot task ",Deep Learning,851ddf5058cf22df63d3344ad89919cf-Paper.pdf,1992
Some Solutions to the Missing Feature Problem,"In visual processing the ability to deal with missing and noisy informa tion is crucial. Occlusions and unreliable feature detectors often lead to situations where little or no direct information about features is availa ble. However the available information is usually sufficient to highly constrain the outputs. We discuss Bayesian techniques for extracting class probabilities given partial data. The optimal solution involves inte grating over the missing dimensions weighted by the local probability densities. We show how to obtain closed-form approximations to the Bayesian solution using Gaussian basis function networks. The frame work extends naturally to the case of noisy features. Simulations on a complex task (3D hand gesture recognition) validate the theory. When both integration and weighting by input densities are used, performance decreases gracefully with the number of missing or noisy features. Per formance is substantially degraded if either step is omitted. 1 INTRODUCTION",Computer Vision,85fc37b18c57097425b52fc7afbb6969-Paper.pdf,1992
A Note on Learning Vector Quantization,"Vector Quantization is useful for data compression. Competitive Learn ing which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data. Vector quantization of labelled data for classification has a different objective, to minimize the number of misclassifications, and a different algorithm is appropriate. We show that a variant of Kohonen's LVQ2.1 algorithm can be seen as a multi class extension of an algorithm which in a restricted 2 class case can be proven to converge to the Bayes optimal classification boundary. We compare the performance of the LVQ2.1 algorithm to that of a modified version having a decreasing window and normalized step size, on a ten class vowel classification problem. 1 Introduction Vector quantization is a form of data compression that represents data vectors by a smaller set of codebook vectors. Each data vector is then represented by its nearest codebook vector. The goal of vector quantization is to represent the",Reinforcement Learning,8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf,1992
Diffusion Approximations for the,"In this paper we discuss the asymptotic properties of the most com monly used variant of the backpropagation algorithm in which net work weights are trained by means of a local gradient descent on ex amples drawn randomly from a fixed training set, and the learning rate TJ of the gradient updates is held constant (simple backpropa gation). Using stochastic approximation results, we show that for TJ ~ 0 this training process approaches a batch training and pro vide results on the rate of convergence. Further, we show that for small TJ one can approximate simple back propagation by the sum of a batch training process and a Gaussian diffusion which is the unique solution to a linear stochastic differential equation. Using this approximation we indicate the reasons why simple backprop agation is less likely to get stuck in local minima than the batch training process and demonstrate this empirically on a number of examples. 1 INTRODUCTION The original (simple) backpropagation algorithm, in",Optimization & Theoretical ML,884d247c6f65a96a7da4d1105d584ddd-Paper.pdf,1992
Analog Cochlear Model for Multiresolution,"This paper discusses the parameterization of speech by an analog cochlear model. The tradeoff between time and frequency resolution is viewed as the fundamental difference between conventional spectrographic analysis and cochlear signal processing for broadband, rapid-changing signals. The model's response exhibits a wavelet-like analysis in the scale domain that preserves good temporal resolution; the frequency of each spectral compo nent in a broadband signal can be accurately determined from the inter peak intervals in the instantaneous firing rates of auditory fibers. Such properties of the cochlear model are demonstrated with natural speech and synthetic complex signals. 1 Introduction As a non-parametric tool, spectrogram, or short-term Fourier transform, is widely used in analyzing non-stationary signals, such speech. Usually a window is applied to the running signal and then the Fourier transform is performed. The specific window applied determines the tradeoff between temporal",Computer Vision,8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf,1992
Unknown Title,No abstract found,Unknown,8c7bbbba95c1025975e548cee86dfadc-Paper.pdf,1992
A Boundary Hunting Radial Basis Function,"A new boundary hunting radial basis function (BH-RBF) classifier which allocates RBF centers constructively near class boundaries is described. This classifier creates complex decision boundaries only in regions where confusions occur and corresponding RBF outputs are similar. A predicted square error measure is used to determine how many centers to add and to determine when to stop adding centers. Two experiments are presented which demonstrate the advantages of the BH RBF classifier. One uses artificial data with two classes and two input features where each class contains four clusters but only one cluster is near a decision region boundary. The other uses a large seismic database with seven classes and 14 input features. In both experiments the BH RBF classifier provides a lower error rate with fewer centers than are required by more conventional RBF, Gaussian mixture, or MLP classifiers. 1 INTRODUCTION Radial basis function (RBF) classifiers have been successfully applied to many ",Computer Vision,8df707a948fac1b4a0f97aa554886ec8-Paper.pdf,1992
Learning to See Where and What:,"This paper describes an approach to integrated segmentation and recognition of hand-printed characters. The approach, called Saccade, integrates ballistic and corrective saccades (eye movements) with character recognition. A single backpropagation net is trained to make a classification decision on a character centered in its input window, as well as to estimate the distance of the current and next character from the center of the input window. The net learns to accurately estimate these distances regardless of variations in character width, spacing between characters, writing style and other factors. During testing, the system uses the net~xtracted classification and distance information, along with a set of jumping rules, to jump from character to character. The ability to read rests on multiple foundation skills. In learning how to read, people learn how to recognize individual characters centered in the visual field. They also learn how to move their eyes along a line of text, sequ",Computer Vision,8ebda540cbcc4d7336496819a46a1b68-Paper.pdf,1992
How Oscillatory Neuronal Responses Reflect,"A switching between apparently coherent (oscillatory) and stochastic episodes of activity has been observed in responses from cat and monkey visual cortex. We describe the dynamics of these phenomena in two paral lel approaches, a phenomenological and a rather microscopic one. On the one hand we analyze neuronal responses in terms of a hidden state model (HSM). The parameters of this model are extracted directly from exper imental spike trains. They characterize the underlying dynamics as well as the coupling of individual neurons to the network. This phenomenolog ical model thus provides a new framework for the experimental analysis of network dynamics. The application of this method to multi unit ac tivities from the visual cortex of the cat substantiates the existence of oscillatory and stochastic states and quantifies the switching behaviour in the assembly dynamics. On the other hand we start from the single spiking neuron and derive a master equation for the time evolution of the",NLP,8fecb20817b3847419bb3de39a609afe-Paper.pdf,1992
Summed Weight Neuron Perturbation: An O(N),"The algorithm presented performs gradient descent on the weight space of an Artificial Neural Network (ANN), using a finite difference to approximate the gradient The method is novel in that it achieves a com putational complexity similar to that of Node Perturbation, O(N3), but does not require access to the activity of hidden or internal neurons. This is possible due to a stochastic relation between perturbations at the weights and the neurons of an ANN. The algorithm is also similar to Weight Perturbation in that it is optimal in terms of hardware require ments when used for the training ofVLSI implementations of ANN's. 1 INTRODUCTION Optimization of the weights of an ANN may be performed by, the application of a gradi ent descent teclmique. The gradient may be calculated directly as in Backpropagation, or it may be approximated by a Finite Difference Method which is what we concern ourselves with in this paper. These methods lend themselves to the task of training hardware imple me",Optimization & Theoretical ML,996a7fa078cc36c46d02f9af3bef918b-Paper.pdf,1992
Extended Regularization Methods for,"Many techniques for model selection in the field of neural networks correspond to well established statistical methods. The method of 'stopped training', on the other hand, in which an oversized network is trained until the error on a further validation set of ex amples deteriorates, then training is stopped, is a true innovation, since model selection doesn't require convergence of the training process. In this paper we show that this performance can be significantly enhanced by extending the 'non convergent model selection method' of stopped training to include dynamic topology modifications (dynamic weight pruning) and modified complexity penalty term methods in which the weighting of the penalty term is adjusted during the training process. 1 INTRODUCTION One of the central topics in the field of neural networks is that of model selection. Both the theoretical and practical side of this have been intensively investigated and a vast array of methods have been suggested to perform th",Optimization & Theoretical ML,9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf,1992
Synchronization and Grammatical Inference,"We have designed an architecture to span the gap between bio physics and cognitive science to address and explore issues of how a discrete symbol processing system can arise from the continuum, and how complex dynamics like oscillation and synchronization can then be employed in its operation and affect its learning. We show how a discrete-time recurrent ""Elman"" network architecture can be constructed from recurrently connected oscillatory associative memory modules described by continuous nonlinear ordinary dif ferential equations. The modules can learn connection weights be tween themselves which will cause the system to evolve under a clocked ""machine cycle"" by a sequence of transitions of attractors within the modules, much as a digital computer evolves by transi tions of its binary flip-flop attractors. The architecture thus em ploys the principle of ""computing with attractors"" used by macro scopic systems for reliable computation in the presence of noise. We have specifically con",NLP,9c82c7143c102b71c593d98d96093fde-Paper.pdf,1992
Attractor Neural Networks with Local,"Networks with local inhibition are shown to have enhanced compu tational performance with respect to the classical Hopfield-like net works. In particular the critical capacity of the network is increased as well as its capability to store correlated patterns. Chaotic dy namic behaviour (exponentially long transients) of the devices in dicates the overloading of the associative memory. An implementa tion based on a programmable logic device is here presented. A 16 neurons circuit is implemented whit a XILINK 4020 device. The peculiarity of this solution is the possibility to change parts of the project (weights, transfer function or the whole architecture) with a simple software download of the configuration into the XILINK chip. 1 INTRODUCTION Attractor Neural Networks endowed with local inhibitory feedbacks, have been shown to have interesting computational performances[I]. Past effort was con centrated in studying a variety of synaptic structures or learning algorithms, while less at",Optimization & Theoretical ML,9cc138f8dc04cbf16240daa92d8d50e2-Paper.pdf,1992
Metamorphosis Networks:,"Given a set oft raining examples, determining the appropriate num ber of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learn ing. We explore an alternative class of algorithms-called meta morphosis algorithms-in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lat tice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selec tion, robust parameter selection, multiresolution processing, and interpolation of sparse training data. 1 INTRODUCTION Generalization performance on a fixed-size training set is closely related to the number of free parameters in a network. Selecting either too many or too few parameters can lead to poor generalization. Geman et al. (1991) refer t",Optimization & Theoretical ML,9cf81d8026a9018052c429cc4e56739b-Paper.pdf,1992
A Hybrid Linear/Nonlinear Approach to Channel,"Channel equalization problem is an important problem in high-speed communications. The sequences of symbols transmitted are distorted by neighboring symbols. Traditionally, the channel equalization problem is considered as a channel-inversion operation. One problem of this approach is that there is no direct correspondence between error proba bility and residual error produced by the channel inversion operation. In this paper, the optimal equalizer design is formulated as a classification problem. The optimal classifier can be constructed by Bayes decision rule. In general it is nonlinear. An efficient hybrid linear/nonlinear equalizer approach has been proposed to train the equalizer. The error probability of new linear/nonlinear equalizer has been shown to be bet ter than a linear equalizer in an experimental channel. 1 INTRODUCTION {ld In a typical communication system, a sequence of symbols are transmitted though a linear time-dispersive channel h(t). Let x(t) be the received signa",Optimization & Theoretical ML,9f396fe44e7c05c16873b05ec425cbad-Paper.pdf,1992
Physiologically Based Speech Synthesis,"This study demonstrates a paradigm for modeling speech produc tion based on neural networks. Using physiological data from speech utterances, a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior that allows articulator trajectories to be generated from motor commands constrained by phoneme input strings and global performance parameters. From these movement trajectories, a sec ond neural network generates PARCOR parameters that are then used to synthesize the speech acoustics. 1 INTRODUCTION Our group has attempted to model speech production computationally as a process in which linguistic intentions are realized as speech through a causal succession of patterned behavior. Our aim is to gain insight into the cognitive and neurophysi ological mechanisms governing this complex skilled behavior as well as to provide plausible models of speech synthesis and possibly recognition based on the physi ology of speech production. It",NLP,9fe8593a8a330607d76796b35c64c600-Paper.pdf,1992
Weight Space Probability Densities,"In stochastic learning, weights are random variables whose time evolution is governed by a Markov process. At each time-step, n, the weights can be described by a probability density function pew, n). We summarize the theory of the time evolution of P, and give graphical examples of the time evolution that contrast the behavior of stochastic learning with true gradient descent (batch learning). Finally, we use the formalism to obtain predictions of the time required for noise-induced hopping between basins of different optima. We compare the theoretical predictions with simulations of large ensembles of networks for simple problems in supervised and unsupervised learning. 1 Weight-Space Probability Densities Despite the recent application of convergence theorems from stochastic approxima tion theory to neural network learning (Oja 1982, White 1989) there remain out standing questions about the search dynamics in stochastic learning. For example, the convergence theorems do not tell us ",Optimization & Theoretical ML,a532400ed62e772b9dc0b86f46e583ff-Paper.pdf,1992
Performance Through Consistency:,"Connectionist Rpeech recognition systems are often handicapped by an inconsistency between training and testing criteria. This prob lem is addressed by the Multi-State Time Delay Neural Network (MS-TDNN), a hierarchical phonf'mp and word classifier which uses DTW to modulate its connectivit.y pattern, and which is directly trained on word-level targets. The consistent use of word accu racy as a criterion during bot.h t.raining and testing leads to very high system performance, even wif II limited training dat.a. Until now, the MS-TDN N has been appli('d primarily to small vocabu lary recognition and word spotting tasks. In this papf'f we apply the architecture to large vocabulary continuous speech recognition, and demonstrate that our MS-TDNN outperforms all ot,hf'r sys tems that have been tested on tht' eMU Conference Registration database. 1 INTRODUCTION Neural networks hold great promise in the area of speech recognition. But in order to fulfill their promise, they must be used ""pro",NLP,a733fa9b25f33689e2adbe72199f0e62-Paper.pdf,1992
Generic Analog Neural Computation,"An analog CMOS VLSI neural processing chip has been designed and fabri cated. The device employs ""pulse-stream"" neural state signalljng, and is capa ble of computing some 360 million synaptic connections per secood. In addi tion to basic characterisation results. the performance of the chip in solving ""real-world"" problems is also demonstrated. 1 INTRODUCTION Inspired by biology. and borne out of a desire to perform analogue computation with fun damentally digital fabrication processes. the so-called ""pulse-stream"" arithmetic system has been steadily evolved and improved since its inception in 1986 (Murrayl990a. Mur ray 1989a). In addition to this continuous development at Edinburgh. many other research groups around the world (mast notably Meador et al (Meadorl990a» have experimented with their own pulse-firing neural circuits. In pulsed implementations. each neural state is represented by some variable attribute (e.g. the width of fixed frequency pulses. or the rate of fixed width pu",Computer Vision,ab233b682ec355648e7891e66c54191b-Paper.pdf,1992
Analog VLSI Implementation of,"We describe an analog VLSI implementation of a multi-dimensional gradient estimation and descent technique for minimizing an on fO. chip scalar function The implementation uses noise injec tion and multiplicative correlation to estimate derivatives, as in [Anderson, Kerns 92]. One intended application of this technique is setting circuit parameters on-chip automatically, rather than manually [Kirk 91]. Gradient descent optimization may be used to adjust synapse weights for a backpropagation or other on-chip learning implementation. The approach combines the features of continuous multi-dimensional gradient descent and the potential for an annealing style of optimization. We present data measured from our analog VLSI implementation. 1 Introduction This work is similar to [Anderson, Kerns 92], but represents two advances. First, we describe the extension of the technique to multiple dimensions. Second, we demon strate an implementation of the multi-dimensional technique in analog VLSI, a",Optimization & Theoretical ML,abd815286ba1007abfbb8415b83ae2cf-Paper.pdf,1992
Improving Performance in Neural Networks,"A boosting algorithm converts a learning machine with error rate less than 50% to one with an arbitrarily low error rate. However, the algorithm discussed here depends on having a large supply of independent training samples. We show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network. We report the effect of boosting on four databases (all handwritten) consisting of 12,000 digits from segmented ZIP codes from the United State Postal Service (USPS) and the following from the National Institute of Standards and Testing (NIST): 220,000 digits, 45,000 upper case alphas, and 45,000 lower case alphas. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance in some cases by a factor of three. 1 INTRODUCTION In this article we summa",Optimization & Theoretical ML,acc3e0404646c57502b480dc052c4fe1-Paper.pdf,1992
An Object-Oriented Framework for the,"The field of software simulators for neural networks has been ex panding very rapidly in the last years but their importance is still being underestimated. They must provide increasing levels of as sistance for the design, simulation and analysis of neural networks. With our object-oriented framework (SESAME) we intend to show that very high degrees of transparency, manageability and flexibil ity for complex experiments can be obtained. SESAME's basic de sign philosophy is inspired by the natural way in which researchers explain their computational models. Experiments are performed with networks of building blocks, which can be extended very eas ily. Mechanisms have been integrated to facilitate the construction and analysis of very complex architectures. Among these mech anisms are t.he automatic configuration of building blocks for an experiment and multiple inheritance at run-time. 1 Introduction In recent years a lot of work has been put into the development of simulation systems f",Computer Vision,ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf,1992
A Practice Strategy for Robot Learning,"""Trajectory Extension Learning"" is a new technique for Learning Control in Robots which assumes that there exists some parameter of the desired trajectory that can be smoothly varied from a region of easy solvability of the dynamics to a region of desired behavior which may have more difficult dynamics. By gradually varying the parameter, practice movements remain near the desired path while a Neural Network learns to approximate the inverse dynamics. For example, the average speed of motion might be varied, and the in verse dynamics can be ""bootstrapped"" from slow movements with simpler dynamics to fast movements. This provides an example of the more general concept of a ""Practice Strategy"" in which a se quence of intermediate tasks is used to simplify learning a complex task. I show an example of the application of this idea to a real 2-joint direct drive robot arm. 1 INTRODUCTION The most general definition of Adaptive Control is one which includes any controller whose behavior chan",Reinforcement Learning,afd4836712c5e77550897e25711e1d96-Paper.pdf,1992
Modeling Consistency in a Speaker Independent,"We would like to incorporate speaker-dependent consistencies, such as gender, in an otherwise speaker-independent speech recognition system. In this paper we discuss a Gender Dependent Neural Network (GDNN) which can be tuned for each gender, while sharing most of the speaker independent parameters. We use a classification network to help generate gender-dependent phonetic probabilities for a statistical (HMM) recogni tion system. The gender classification net predicts the gender with high accuracy, 98.3% on a Resource Management test set. However, the in tegration of the GDNN into our hybrid HMM-neural network recognizer provided an improvement in the recognition score that is not statistically significant on a Resource Management test set. 1 INTRODUCTION Earlier work [Bourlard and Morgan, 1991l has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMM). As shown in their report, with a few assumptions, an MLP may be viewed",NLP,b1eec33c726a60554bc78518d5f9b32c-Paper.pdf,1992
Learning to categorize objects using,"The invariance of an objects' identity as it transformed over time provides a powerful cue for perceptual learning. We present an un supervised learning procedure which maximizes the mutual infor mation between the representations adopted by a feed-forward net work at consecutive time steps. We demonstrate that the network can learn, entirely unsupervised, to classify an ensemble of several patterns by observing pattern trajectories, even though there are abrupt transitions from one object to another between trajecto ries. The same learning procedure should be widely applicable to a variety of perceptual learning tasks. 1 INTRODUCTION A promising approach to understanding human perception is to try to model its developmental stages. There is ample evidence that much of perception is learned. Even some very low level perceptual abilities such as stereopsis (Held, Birch and Gwiazda, 1980; Birch, Gwiazda and Held, 1982) are not present at birth, and appear to be learned. Once rudimentary ",Deep Learning,b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf,1992
"Learning Curves, Model Selection and","Learning curves show how a neural network is improved as the number of t.raiuing examples increases and how it is related to the network complexity. The present paper clarifies asymptotic properties and their relation of t.wo learning curves, one concerning the predictive loss or generalization loss and the other the training loss. The result gives a natural definition of the complexity of a neural network. Moreover, it provides a new criterion of model selection. 1 INTRODUCTION The leal'lI ing Cl1l've shows how well t hE' behavior of a neural network is improved as t.he nurnber of training examples increast""'s and how it is I'elated with the complexity of neural net.works. This provides liS with a criterion for choosing an adequate network ill relat.ion t.o the number of training examples. Some researchers have attacked this problem by using sta tistical mechanical met.hods (see Levin et al. [1990], Seung et al. [1991]' etc.) and some by informat.ion theory and algorithmic methods (se",Optimization & Theoretical ML,b2f627fff19fda463cb386442eac2b3d-Paper.pdf,1992
Optimal Depth Neural Networks for Multiplication,"An artificial neural network (ANN) is commonly modeled by a threshold circuit, a network of interconnected processing units called linear threshold gates. The depth of a network represents the number of unit delays or the time for parallel computation. The SIze of a circuit is the number of gates and measures the amount of hardware. It was known that traditional logic circuits consisting of only unbounded fan-in AND, OR, NOT gates would require at least O(log n/log log n) depth to compute common arithmetic functions such as the product or the quotient of two n-bit numbers, unless we allow the size (and fan-in) to increase exponentially (in n). We show in this paper that ANNs can be much more powerful than traditional logic circuits. In particular, we prove that that iterated addition can be com puted by depth-2 ANN, and multiplication and division can be computed by depth-3 ANNs with polynomial size and polynomially bounded integer weights, respectively. Moreover, it follows from known",Optimization & Theoretical ML,b4288d9c0ec0a1841b3b3728321e7088-Paper.pdf,1992
Time Warping Invariant Neural Networks,"We proposed a model of Time Warping Invariant Neural Networks (TWINN) to handle the time warped continuous signals. Although TWINN is a simple modifica tion of well known recurrent neural network, analysis has shown that TWINN com pletely removes time warping and is able to handle difficult classification problem. It is also shown that TWINN has certain advantages over the current available sequential processing schemes: Dynamic Programming(DP)[I], Hidden Markov Model( HMM)[2], Time Delayed Neural Networks(TDNN) [3] and Neural Network Finite Automata(NNFA)[4]. We also analyzed the time continuity employed in TWINN and pointed out that this kind of structure can memorize longer input history compared with Neural Net work Finite Automata (NNFA). This may help to understand the well accepted fact that for learning grammatical reference with NNF A one had to start with very short strings in training set. The numerical example we used is a trajectory classification problem. This problem, ma",NLP,b4a528955b84f584974e92d025a75d1f-Paper.pdf,1992
Deriving Receptive Fields Using An,"An information-theoretic optimization principle ('infomax') has previously been used for unsupervised learning of statistical reg ularities in an input ensemble. The principle states that the input output mapping implemented by a processing stage should be cho sen so as to maximize the average mutual information between input and output patterns, subject to constraints and in the pres ence of processing noise. In the present work I show how infomax, when applied to a class of nonlinear input-output mappings, can under certain conditions generate optimal filters that have addi tional useful properties: (1) Output activity (for each input pat tern) tends to be concentrated among a relatively small number of nodes. (2) The filters are sensitive to higher-order statistical structure (beyond pairwise correlations). If the input features are localized, the filters' receptive fields tend to be localized as well. (3) Multiresolution sets of filters with subsampling at low spatial frequencies -",Deep Learning,b5dc4e5d9b495d0196f61d45b26ef33e-Paper.pdf,1992
Input Reconstruction Reliability Estimation,"This paper describes a technique called Input Reconstruction Reliability Estimation (IRRE) for determining the response reliability of a restricted class of multi-layer perceptrons (MLPs). The technique uses a network's ability to accurately encode the input pattern in its internal representation as a measure of its reliability. The more accurately a network is able to reconstruct the input pattern from its internal representation, the more reliable the network is considered to be. IRRE is provides a good estimate of the reliability of MLPs trained for autonomous driving. Results are presented in which the reliability estimates provided by IRRE are used to select between networks trained for different driving situations. 1 Introduction In many real world domains it is important to know the reliability of a network's response since a single network cannot be expected to accurately handle all the possible inputs. Ideally, a network should not only provide a response to a given input patt",Deep Learning,b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf,1992
Mapping Between Neural and Physical,"A computer model of the musculoskeletal system of the lobster gastric mill was constructed in order to provide a behavioral in terpretation of the rhythmic patterns obtained from isolated stom atogastric ganglion. The model was based on Hill's muscle model and quasi-static approximation of the skeletal dynamics and could simulate the change of chewing patterns by the effect of neuromod ulators. 1 THE STOMATOGASTRIC NERVOUS SYSTEM The crustacean stomatogastric ganglion (STG) is a circuit of 30 neurons that con trols rhythmic movement of the foregut. It is one of the best elucidated neural circuits. All the neurons and the synaptic connections between them are identi fied and the effects of neuromodulators on the oscillation patterns and neuronal characteristics have been extensively studied (Selverston and Moulins 1987, H arris Warrick et al. 1992). However, STG's function as a controller of ingestive behavior is not fully understood in part because of our poor understanding of the cont",Optimization & Theoretical ML,be3159ad04564bfb90db9e32851ebf9c-Paper.pdf,1992
A Fast Stochastic Error-Descent,"A parallel stochastic algorithm is investigated for error-descent learning and optimization in deterministic networks of arbitrary topology. No explicit information about internal network struc ture is needed. The method is based on the model-free distributed learning mechanism of Dembo and Kailath. A modified parameter update rule is proposed by which each individual parameter vector perturbation contributes a decrease in error. A substantially faster learning speed is hence allowed. Furthermore, the modified algo rithm supports learning time-varying features in dynamical net works. We analyze the convergence and scaling properties of the algorithm, and present simulation results for dynamic trajectory learning in recurrent networks. 1 Background and Motivation We address general optimization tasks that require finding a set of constant param eter values Pi that minimize a given error functional £(p). For supervised learning, the error functional consists of some quantitative measure ",Optimization & Theoretical ML,c06d06da9666a219db15cf575aff2824-Paper.pdf,1992
Connected Letter Recognition with a,"The Multi-State Time Delay Neural Network (MS-TDNN) inte grates a nonlinear time alignment procedure (DTW) and the high accuracy phoneme spotting capabilities of a TDNN into a connec tionist speech recognition system with word-level classification and error backpropagation. We present an MS-TDNN for recognizing continuously spelled letters, a task characterized by a small but highly confusable vocabulary. Our MS-TDNN achieves 98.5/92.0% word accuracy on speaker dependent/independent tasks, outper forming previously reported results on the same databases. We pro pose training techniques aimed at improving sentence level perfor mance, including free alignment across word boundaries, word du ration modeling and error backpropagation on the sentence rather than the word level. Architectures integrating submodules special ized on a subset of speakers achieved further improvements. 1 INTRODUCTION The recognition of spelled strings of letters is essential for all applications involving proper",NLP,b73dfe25b4b8714c029b37a6ad3006fa-Paper.pdf,1992
Nets with Unreliable Hidden Nodes Learn,"In a multi-layered neural network, anyone of the hidden layers can be viewed as computing a distributed representation of the input. Several ""encoder"" experiments have shown that when the representation space is small it can be fully used. But computing with such a representation requires completely dependable nodes. In the case where the hidden nodes are noisy and unreliable, we find that error correcting schemes emerge simply by using noisy units during training; random errors in jected during backpropagation result in spreading representations apart. Average and minimum distances increase with misfire probability, as predicted by coding-theoretic considerations. Furthennore, the effect of this noise is to protect the machine against permanent node failure, thereby potentially extending the useful lifetime of the machine. 1 INTRODUCTION The encoder task described by Ackley, Hinton, and Sejnowski (1985) for the Boltzmann machine, and by Rumelhart, Hinton, and Williams (1986) for feed-",Optimization & Theoretical ML,c361bc7b2c033a83d663b8d9fb4be56e-Paper.pdf,1992
Unsmearing Vistlal Motion:,"Human VlSlon systems integrate information nonlocally, across long spatial ranges. For example, a moving stimulus appears smeared when viewed briefly (30 ms), yet sharp when viewed for a longer exposure (100 ms) (Burr, 1980). This suggests that visual systems combine information along a trajectory that matches the motion of the stimulus. Our self-organizing neural network model shows how developmental exposure to moving stimuli can direct the formation of horizontal trajectory-specific motion integration pathways that unsmear representations of moving stimuli. These results account for Burr's data and can potentially also model ot.her phenomena, such as visual inertia. 1 INTRODUCTION N onlocal interactions strongly influence the processing of visual motion information and the response characteristics of visual neurons. Examples include: attentional modulation of receptive field shape; modulation of neural response by stimuli beyond the classical receptive field; and neural response to ",Computer Vision,c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf,1992
Remote Sensing Image Analysis via a Texture,"In this work we apply a texture classification network to remote sensing im age analysis. The goal is to extract the characteristics of the area depicted in the input image, thus achieving a segmented map of the region. We have recently proposed a combined neural network and rule-based framework for texture recognition. The framework uses unsupervised and supervised learning, and provides probability estimates for the output classes. We describe the texture classification network and extend it to demonstrate its application to the Landsat and Aerial image analysis domain. 1 INTRODUCTION In this work we apply a texture classification network to remote sensing image analysis. The goal is to segment the input image into homogeneous textured regions and identify each region as one of a prelearned library of textures, e.g. tree area and urban area distinction. Classification f remote sensing imagery is of importance in 0 many applications, such as navigation, surveillance and exploration. I",Computer Vision,c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf,1992
Filter Selection Model for Generating,"Neurons in area MT of primate visual cortex encode the velocity of moving objects. We present a model of how MT cells aggregate responses from VI to form such a velocity representation. Two different sets of units, with local receptive fields, receive inputs from motion energy filters. One set of units forms estimates of local motion, while the second set computes the utility of these estimates. Outputs from this second set of units ""gate"" the outputs from the first set through a gain control mechanism. This active process for selecting only a subset of local motion responses to integrate into more global responses distinguishes our model from previous models of velocity estimation. The model yields accurate velocity estimates in synthetic images containing multiple moving targets of varying size, luminance, and spatial frequency profile and deals well with a number of transparency phenomena. 1 INTRODUCTION Humans, and primates in general, are very good at complex motion processing tas",Computer Vision,ca9c267dad0305d1a6308d2a0cf1c39c-Paper.pdf,1992
N on-Linear Dimensionality Reduction,"A method for creating a non-linear encoder-decoder for multidimensional data with compact representations is presented. The commonly used technique of autoassociation is extended to allow non-linear representations, and an objec tive function which penalizes activations of individual hidden units is shown to result in minimum dimensional encodings with respect to allowable error in reconstruction. 1 INTRODUCTION Reducing dimensionality of data with minimal information loss is important for feature extraction, compact coding and computational efficiency. The data can be tranformed into ""good"" representations for further processing, constraints among feature variables may be identified, and redundancy eliminated. Many algorithms are exponential in the dimensionality of the input, thus even reduction by a single dimension may provide valuable computational savings. Autoassociating feed forward networks with one hidden layer have been shown to extract the principal components of the data (",Optimization & Theoretical ML,cdc0d6e63aa8e41c89689f54970bb35f-Paper.pdf,1992
Feudal Reinforcement Learning,"One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-Iearning managerial hierarchy in which high level managers learn how to set tasks to their sub managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task .. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-Iearning and builds a more comprehensive map. 1 INTRODUCTION Straightforward reinforcement learning has been quite successful at some rela tively complex tasks like playing backgammon (Tesauro, 1992). However, the learning time does not scale well with the number of parameters. For agents solv ing rewarded Markovian decision tasks by lear",Reinforcement Learning,d14220ee66aeec73c49038385428ec4c-Paper.pdf,1992
A Neural Network that Learns to Interpret,"The planar thallium-201 myocardial perfusion scintigram is a widely used diagnostic technique for detecting and estimating the risk of coronary artery disease. Neural networks learned to interpret 100 thallium scinti grams as determined by individual expert ratings. Standard error back propagation was compared to standard LMS, and LMS combined with one layer of RBF units. Using the ""leave-one-out"" method, generaliza tion was tested on all 100 cases. Training time was determined automati cally from cross-validation perfonnance. Best perfonnance was attained by the RBF/LMS network with three hidden units per view and compares favorably with human experts. 1 Introduction Coronary artery disease (CAD) is one of the leading causes of death in the Western World. The planar thallium-201 is considered to be a reliable diagnostic tool in the detection of • Current address: Geriatrics, Research, Educational and Clinical Center, VA Medical Center, Salt Lake City, Utah. 755 756 Rosenberg, Erel, an",NLP,d490d7b4576290fa60eb31b5fc917ad1-Paper.pdf,1992
Learning Cellular Automaton Dynamics,"We have trained networks of E - II units with short-range connec tions to simulate simple cellular automata that exhibit complex or chaotic behaviour. Three levels of learning are possible (in decreas ing order of difficulty): learning the underlying automaton rule, learning asymptotic dynamical behaviour, and learning to extrap olate the training history. The levels of learning achieved with and without weight sharing for different automata provide new insight into their dynamics. 1 INTRODUCTION Neural networks have been shown to be capable of learning the dynamical behaviour exhibited by chaotic time series composed of measurements of a single variable among many in a complex system [1, 2, 3]. In this work we consider instead cellular automaton arrays (CA)[4], a class of many-degree-of-freedom systems which exhibits very complex dynamics, including universal computation. We would like to know whether neural nets can be taught to imitate these dynamics, both locally and globally. One ",Reinforcement Learning,d6c651ddcd97183b2e40bc464231c962-Paper.pdf,1992
Probability Estimation from a Database,"We present an algorithm for creating a neural network which pro duces accurate probability estimates as outputs. The network im plements a Gibbs probability distribution model of the training database. This model is created by a new transformation relating the joint probabilities of attributes in the database to the weights (Gibbs potentials) of the distributed network model. The theory of this transformation is presented together with experimental re sults. One advantage of this approach is the network weights are prescribed without iterative gradient descent. Used as a classifier the network tied or outperformed published results on a variety of databases. 1 INTRODUCTION This paper addresses the problem of modeling a discrete database. The database is viewed as a collection of independent samples from a probability distribution. This distribution is called the underlying distribution. In contrast, the empirical distribution is the distribution obtained if you take independent random ",Optimization & Theoretical ML,d7a728a67d909e714c0774e22cb806f2-Paper.pdf,1992
Word Space,"Representations for semantic information about words are neces sary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large num ber of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are success fully applied to word sense disambiguation using a nearest neighbor method. 1 Introduction Many tasks in natural language processing require access to semantic information about lexical items and text segments. For example, a system processing the sound sequence: /rE.k~maisbi:tJ/ needs to know the topic of the discourse in order to decide which of the plausible hypotheses for analysis is the right one: e.g. ""wreck a nice beach"" or ""recognize speech"". Similarly, a mail filtering program has to know the topical significance of words to do its job properly. Traditional semantic representations are i",NLP,d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf,1992
An Analog VLSI Chip for Radial Basis Functions,"We have designed, fabricated, and tested an analog VLSI chip which computes radial basis functions in parallel. We have de veloped a synapse circuit that approximates a quadratic function. We aggregate these circuits to form radial basis functions. These radial basis functions are then averaged together using a follower aggregator. 1 INTRODUCTION Radial basis functions (RBFs) are a mel hod for approximating a function from scattered training points [Powell, H)87]. RBFs have been used to solve recognition and prediction problems with a fair amonnt of success [Lee, 1991] [Moody, 1989] [Platt, 1991]. The first layer of an RBF network computes t.he distance of the input to the network to a set of stored memories. Each basis function is a non-linear function of a corresponding distance. Tht> basis functions are then added together with second-layer weights to produce the output of the network. The general form of an RBF is L Ii) , Yi = hii<l>i (Iii - Cj ( 1) i where Yi is the output of the ",Computer Vision,daca41214b39c5dc66674d09081940f0-Paper.pdf,1992
Predicting Complex Behavior,Recurrent networks of threshold elements have been studied inten sively as associative memories and pattern-recognition devices. While most research has concentrated on fully-connected symmetric net works. which relax to stable fixed points. asymmetric networks show richer dynamical behavior. and can be used as sequence generators or flexible pattern-recognition devices. In this paper. we approach the problem of predicting the complex global behavior of a class of ran dom asymmetric networks in terms of network parameters. These net works can show fixed-point. cyclical or effectively aperiodic behavior. depending on parameter values. and our approach can be used to set parameters. as necessary. to obtain a desired complexity of dynamics. The approach also provides qualitative insight into why the system behaves as it does and suggests possible applications. 1 INTRODUCTION Recurrent neural networks of threshold elements have been intensively investigated in recent years. in part because,Deep Learning,dbe272bab69f8e13f14b405e038deb64-Paper.pdf,1992
Destabilization and Route to Chaos,"The occurence of chaos in recurrent neural networks is supposed to depend on the architecture and on the synaptic coupling strength. It is studied here for a randomly diluted architecture. By normalizing the variance of synaptic weights, we produce a bifurcation parameter, dependent on this variance and on the slope of the transfer function but independent of the connectivity, that allows a sustained activity and the occurence of chaos when reaching a critical value. Even for weak connectivity and small size, we find numerical results in accordance with the theoretical ones previously established for fully connected infinite sized networks. Moreover the route towards chaos is numerically checked to be a quasi-periodic one, whatever the type of the first bifurcation is (Hopf bifurcation, pitchfork or flip). 549 550 Doyon, Cessac, Quoy, and Samuelides 1 INTRODUCTION Most part of studies on recurrent neural networks assume sufficient conditions of convergence. Models with symmetric synapt",Deep Learning,dc6a70712a252123c40d2adba6a11d84-Paper.pdf,1992
Recognition-based Segmentation,"This paper reports on the performance of two methods for recognition-based segmentation of strings of on-line hand-printed capital Latin characters. The input strings consist of a time ordered sequence of X-Y coordinates, punctuated by pen-lifts. The methods were designed to work in ""run-on mode"" where there is no constraint on the spacing between characters. While both methods use a neural network recognition engine and a graph-algorithmic post-processor, their approaches to segmentation are quite differ ent. The first method, which we call IN SEC (for input segmen tation), uses a combination of heuristics to identify particular pen lifts as tentative segmentation points. The second method, which we call OUTSEC (for output segmentation), relies on the empiri cally trained recognition engine for both recognizing characters and identifying relevant segmentation points. 1 INTRODUCTION We address the problem of writer independent recognition of hand-printed words from an 80,OOO-word Engli",NLP,dc82d632c9fcecb0778afbc7924494a6-Paper.pdf,1992
Some Estimates of Necessary Number of,"The feed-forward networks with fixed hidden units (FllU-networks) are compared against the category of remaining feed-forward net works with variable hidden units (VHU-networks). Two broad classes of tasks on a finite domain X C Rn are considered: ap proximation of every function from an open subset of functions on X and representation of every dichotomy of X. For the first task it is found that both network categories require the same minimal number of synaptic weights. For the second task and X in gen eral position it is shown that VHU-networks with threshold logic hidden units can have approximately lin times fewer hidden units than any FHU-network must have. 1 Introduction A good candidate artificial neural network for short term memory needs to be: (i) easy to train, (ii) able to support a broad range of tasks in a domain of interest and (iii) simple to implement. The class of feed-forward networks with fixed hidden units (HU) and adjustable synaptic weights at the top layer only ",Optimization & Theoretical ML,e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf,1992
The Power of Approximating: a,"We compare activation functions in terms of the approximation power of their feedforward nets. We consider the case of analog as well as boolean input. 1 Introduction We consider efficient approximationsofa given multivariate function I: [-1, l]m-+ n by feedforward neural networks. We first introduce the notion of a feedforward net. Let r be a class of real-valued functions, where each function is defined on some n. subset of A r-net C is an unbounded fan-in circuit whose edges and vertices are labeled by real numbers. The real number assigned to an edge (resp. vertex) is called its weight (resp. its threshold). Moreover, to each vertex v an activation function IV E r is assigned. Finally, we assume that C has a single sink w. The net C computes a = f unction Ie : [-1,11m --+ n as follows. The components of the input vector x (Xl, ... , x m ) E [-1, 11m are assigned to the sources of C. Let Vl, ••• , Vn be the immediate predecessors of a vertex v. The input for v is then sv(x) = E~=l W",Optimization & Theoretical ML,e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf,1992
Neural Network Model Selection Using,"Two theorems and a lemma are presented about the use of jackknife es timator and the cross-validation method for model selection. Theorem 1 gives the asymptotic form for the jackknife estimator. Combined with the model selection criterion, this asymptotic form can be used to obtain the fit of a model. The model selection criterion we used is the negative of the average predictive likehood, the choice of which is based on the idea of the cross-validation method. Lemma 1 provides a formula for further explo ration of the asymptotics of the model selection criterion. Theorem 2 gives an asymptotic form of the model selection criterion for the regression case, when the parameters optimization criterion has a penalty term. Theorem 2 also proves the asymptotic equivalence of Moody's model selection cri terion (Moody, 1992) and the cross-validation method, when the distance measure between response y and regression function takes the form of a squared difference. 1 INTRODUCTION Selecting a mod",Optimization & Theoretical ML,e5841df2166dd424a57127423d276bbe-Paper.pdf,1992
Context-Dependent Multiple,A number of hybrid multilayer perceptron (MLP)/hidden Markov model (HMM:) speech recognition systems have been developed in recent years (Morgan and Bourlard. 1990). In this paper. we present a new MLP architecture and training algorithm which allows the modeling of context-dependent phonetic classes in a hybrid MLP/HMM: framework. The new training procedure smooths MLPs trained at different degrees of context dependence in order to obtain a robust estimate of the cootext-dependent probabilities. Tests with the DARPA Resomce Management database have shown substantial advantages of the context-dependent MLPs over earlier cootext independent MLPs. and have shown substantial advantages of this hybrid approach over a pure HMM approach. 1 INTRODUCTION Bidden Markov models are used in most current state-of-the-art continuous-speech recognition systems. A hidden Markov model (HMM) is a stochastic finite state machine with two sets of probability distributions. Associated with each state is a ,NLP,e70611883d2760c8bbafb4acb29e3446-Paper.pdf,1992
On the Use of Evidence in Neural Networks,"The Bayesian ""evidence"" approximation has recently been employed to determine the noise and weight-penalty terms used in back-propagation. This paper shows that for neural nets it is far easier to use the exact result than it is to use the evidence approximation. Moreover, unlike the evi dence approximation, the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code (the exact result is closed form). In addition, it turns out that the evidence proce dure's MAP estimate for neural nets is, in toto, approximation error. An other advantage of the exact analysis is that it does not lead one to incor rect intuition, like the claim that using evidence one can ""evaluate differ ent priors in light of the data"". This paper also discusses sufficiency conditions for the evidence approximation to hold, why it can sometimes give ""reasonable"" results, etc. 1 THE EVIDENCE APPROXIMATION It has recently become popular to consider the problem of tr",Optimization & Theoretical ML,e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf,1992
Automatic Capacity Tuning,"Large VC-dimension classifiers can learn difficult tasks, but are usually impractical because they generalize well only if they are trained with huge quantities of data. In this paper we show that even high-order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller VC-dimension. This is achieved with a maximum margin algorithm (the Generalized Portrait). The technique is applicable to a wide variety of classifiers, including Perceptrons, polynomial classifiers (sigma-pi unit net works) and Radial Basis Functions. The effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem. It is shown to equal the number of those training patterns which are closest patterns to the decision boundary (supporting patterns). Bounds on the generalization error and the speed of convergence of the al gorithm are given. Experimental results",Optimization & Theoretical ML,eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf,1992
Interposing an ontogenic model between,"The relationships between learning, development and evolution in Nature is taken seriously, to suggest a model of the developmental process whereby the genotypes manipulated by the Genetic Algo rithm (GA) might be expressed to form phenotypic neural networks (NNet) that then go on to learn. ONTOL is a grammar for gener ating polynomial NN ets for time-series prediction. Genomes corre spond to an ordered sequence of ONTOL productions and define a grammar that is expressed to generate a NNet. The NNet's weights are then modified by learning, and the individual's prediction error is used to determine GA fitness. A new gene doubling operator appears critical to the formation of new genetic alternatives in the preliminary but encouraging results presented. 1 Introduction Two natural phenomena, the learning done by individuals' nervous systems and the evolution done by populations of individuals, have served as the basis of distinct classes of adaptive algorithms, neural networks (NNets) and",Optimization & Theoretical ML,eb6fdc36b281b7d5eabf33396c2683a2-Paper.pdf,1992
Bayesian Learning,"The attempt to find a single ""optimal"" weight vector in conven tional network training can lead to overfitting and poor generaliza tion. Bayesian methods avoid this, without the need for a valida tion set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution. 1 CONVENTIONAL AND BAYESIAN LEARNING I view neural networks as probabilistic models, and learning as statistical inference. Conventional network learning finds a single ""optimal"" set of network parameter values, corresponding to maximum likelihood or maximum penalized likelihood in ference. Bayesian inference instead integrates the predictions of the network over all possible values of the network parameters, weighting each parameter set by its posterior probability in light of the training data. 1.1 NEURAL NETWORKS AS PROBABILISTIC MODELS C",Optimization & Theoretical ML,f29c21d4897f78948b91f03172341b7b-Paper.pdf,1992
Biologically Plausible Local Learning Rules for,"The vestibulo-ocular reflex (VOR) is a compensatory eye movement that stabilizes images on the retina during head turns. Its magnitude, or gain, can be modified by visual experience during head movements. Possible learning mechanisms for this adaptation have been explored in a model of the oculomotor system based on anatomical and physiological con straints. The local correlational learning rules in our model reproduce the adaptation and behavior of the VOR under certain parameter conditions. From these conditions, predictions for the time course of adaptation at the learning sites are made. 1 INTRODUCTION The primate oculomotor system is capable of maintaining the image of an object on the fovea even when the head and object are moving simultaneously. The vestibular organs provide information about the head velocity with a short delay of 14 ms but visual Signals from the moving object are relatively slow and can take 100 ms to affect eye movemen.ts. a, The gain, of the VOR, defined as",Neuroscience,f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf,1992
Combining Neural and Symbolic,"This paper describes RAPTURE - a system for revising probabilis tic knowledge bases that combines neural and symbolic learning methods. RAPTURE uses a modified version of backpropagation to refine the certainty factors of a MYCIN-style rule base and uses ID3's information gain heuristic to add new rules. Results on re fining two actual expert knowledge bases demonstrate that this combined approach performs better than previous methods. 1 Introduction In complex domains, learning needs to be biased with prior knowledge in order to produce satisfactory results from limited training data. Recently, both connectionist and symbolic methods have been developed for biasing learning with prior knowl edge lFu, 1989; Towell et a/., 1990; Ourston and Mooney, 1990]. Most ofthese meth ods revise an imperfect knowledge base (usually obtained from a domain expert) to fit a set of empirical data. Some of these methods have been successfully applied to real-world tasks, such as recognizing promoter seq",NLP,f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf,1992
Transient Signal Detection with Neural Networks:,"Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (011 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background. 1 INTRODUCTION Detection of poorly defined waveshapes in a nonstationary high noise background is an important and difficult problem in signal processing. The matched filter is the optimal linear filter assuming stationary noise [Thomas, 1969]. The application area that we are going to disc",Deep Learning,fae0b27c451c728867a567e8c1bb4e53-Paper.pdf,1992
Information Theoretic Analysis of,"We have attempted to use information theoretic quantities for ana lyzing neuronal connection structure from spike trains. Two point mu tual information and its maximum value, channel capacity, be tween a pair of neurons were found to be useful for sensitive de tection of crosscorrelation and for estimation of synaptic strength, respectively. Three point mutual information among three neurons could give their interconnection structure. Therefore, our informa tion theoretic analysis was shown to be a very powerful technique for deducing neuronal connection structure. Some concrete exam ples of its application to simulated spike trains are presented. 1 INTRODUCTION The deduction of neuronal connection structure from spike trains, including synaptic strength estimation, has long been one of the central issues for understanding the structure and function of the neuronal circuit and thus the information processing ·corresponding author 515 516 Shiono, Yamada, Nakashima, and Matsumoto mechani",NLP,fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf,1992
Backpropagation without Multiplication,"The back propagation algorithm has been modified to work with out any multiplications and to tolerate comput.ations with a low resolution, which makes it. more attractive for a hardware imple mentatioll. Numbers are represented in float.ing point format with 1 bit mantissa and 3 bits in the exponent for the states, and 1 bit mantissa and 5 bit exponent. for the gradients, while the weights are 16 bit fixed-point numbers. In this way, all the computations can be executed with shift and add operations. Large nehvorks with over 100,000 weights were t.rained and demonstrat.ed the same per formance as networks comput.ed with full precision. An estimate of a circuit implementatioll shows that a large network can be placed on a single chip, reaching more t.han 1 billion weight updat.es pel' second. A speedup is also obtained on any machine where a mul tiplication is slower than a shift operat.ioJl. 1 INTRODUCTION One of the main problems for implement.ing the backpropagation algorithm in hard",Optimization & Theoretical ML,013a006f03dbc5392effeb8f18fda755-Paper.pdf,1993
Hoeffding Races: Accelerating Model,"Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Tech niques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models re duce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly dis carding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memory based learning algorithms, but we also argue that it is applicable to any class of model selection problems. 1 Introduction Model selection addresses ""high level"" decisions about how best to tune learning algorithm architectures for particular tasks. Such decisions include wh",Optimization & Theoretical ML,02a32ad2669e6fe298e607fe7cc0e1a0-Paper.pdf,1993
Pulling It All Together: Methods for,No abstract found,Optimization & Theoretical ML,0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf,1993
Cross-Validation Estimates IMSE,"Integrated Mean Squared Error (IMSE) is a version of the usual mean squared error criterion, averaged over all possible training sets of a given size. If it could be observed, it could be used to determine optimal network complexity or optimal data sub sets for efficient training. We show that two common methods of cross-validating average squared error deliver unbiased estimates of IMSE, converging to IMSE with probability one. These esti mates thus make possible approximate IMSE-based choice of net work complexity. We also show that two variants of cross validation measure provide unbiased IMSE-based estimates potentially useful for selecting optimal data subsets. 1 Summary To begin, assume we are given a fixed network architecture. (We dispense with this assumption later.) Let zN denote a given set of N training examples. Let QN(zN) denote the expected squared error (the expectation taken over all possible examples) of the network after being trained on zN. This measures the quality",Optimization & Theoretical ML,06997f04a7db92466a2baa6ebc8b872d-Paper.pdf,1993
The Power of Amnesia,"We propose a learning algorithm for a variable memory length Markov process. Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales. On short scales it is characterized mostly by the dynamics that gen erate the process, whereas on large scales, more syntactic and se mantic information is carried. For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures. On the other hand using long mem ory models uniformly is not practical even for as short memory as four. The algorithm we propose is based on minimizing the sta tistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small. We demonstrate the algorithm by learning the structure of natural En glish text and applying the learned model to the correction of cor rupted text. Using less than 3000 states the model's performance is far superior to that o",NLP,08419be897405321542838d77f855226-Paper.pdf,1993
On the Non-Existence of a Universal Learning,"We prove that the so called ""loading problem"" for (recurrent) neural net works is unsolvable. This extends several results which already demon strated that training and related design problems for neural networks are (at least) NP-complete. Our result also implies that it is impossible to find or to formulate a universal training algorithm, which for any neu ral network architecture could determine a correct set of weights. For the simple proof of this, we will just show that the loading problem is equivalent to ""Hilbert's tenth problem"" which is known to be unsolvable. 1 THE NEURAL NETWORK MODEL It seems that there are relatively few commonly accepted general formal definitions of the notion of a ""neural network"". Although our results also hold if based on other formal definitions we will try to stay here very close to the original setting in which Judd's NP completeness result was given [Judd, 1990]. But in contrast to [Judd, 1990] we will deal here with simple recurrent networks ins",Optimization & Theoretical ML,0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf,1993
Constructive Learning Using Internal,"We present an algorithm for the training of feedforward and recur rent neural networks. It detects internal representation conflicts and uses these conflicts in a constructive manner to add new neu rons to the network. The advantages are twofold: (1) starting with a small network neurons are only allocated when required; (2) by detecting and resolving internal conflicts at an early stage learning time is reduced. Empirical results on two real-world problems sub stantiate the faster learning speed; when applied to the training of a recurrent network on a well researched sequence recognition task (the Reber grammar), training times are significantly less than previously reported. 1 Introduction Selecting the optimal network architecture for a specific application is a nontrivial task, and several algorithms have been proposed to automate this process. The first class of network adaptation algorithms start out with a redundant architecture and proceed by pruning away seemingly unimportant",Deep Learning,1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf,1993
A Local Algorithm to Learn Trajectories,"This paper presents a simple algorithm to learn trajectories with a continuous time, continuous activation version of the Boltzmann machine. The algorithm takes advantage of intrinsic Brownian noise in the network to easily compute gradients using entirely local computations. The algorithm may be ideal for parallel hardware implementations. This paper presents a learning algorithm to train continuous stochastic networks to respond with desired trajectories in the output units to environmental input trajectories. This is a task, with potential applications to a variety of problems such as stochastic modeling of neural processes, artificial motor control, and continuous speech recognition. For example, in a continuous speech recognition problem, the input trajectory may be a sequence of fast Fourier transform coefficients, and the output a likely trajectory of phonemic patterns corresponding to the input. This paper was based on recent work on diffusion networks by Movellan and McClellan",Reinforcement Learning,185e65bc40581880c4f2c82958de8cfe-Paper.pdf,1993
Use of Bad Training Data For Better,"We show how randomly scrambling the output classes of various fractions of the training data may be used to improve predictive accuracy of a classification algorithm. We present a method for calculating the ""noise sensitivity signature"" of a learning algorithm which is based on scrambling the output classes. This signature can be used to indicate a good match between the complexity of the classifier and the complexity of the data. Use of noise sensitivity signatures is distinctly different from other schemes to avoid over training, such as cross-validation, which uses only part of the train ing data, or various penalty functions, which are not data-adaptive. Noise sensitivity signature methods use all of the training data and are manifestly data-adaptive and non-parametric. They are well suited for situations with limited training data. 1 INTRODUCTION A major problem of pattern recognition and classification algorithms that learn from a training set of examples is to select the complex",Optimization & Theoretical ML,1905aedab9bf2477edc068a355bba31a-Paper.pdf,1993
Non-Intrusive Gaze Tracking Using Artificial,"We have developed an artificial neural network based gaze tracking system which can be customized to individual users. Unlike other gaze trackers, which normally require the user to wear cumbersome headgear, or to use a chin rest to ensure head immobility, our system is entirely non-intrusive. Currently, the best intrusive gaze tracking systems are accurate to approxi mately 0.75 degrees. In our experiments, we have been able to achieve an accuracy of 1.5 degrees, while allowing head mobility. In this paper we present an empirical analysis of the performance of a large number of artifi cial neural network architectures for this task. 1 INTRODUCTION The goal of gaze tracking is to determine where a subject is looking from the appearance of the subject's eye. The interest in gaze tracking exists because of the large number of potential applications. Three of the most common uses of a gaze tracker are as an alterna tive to the mouse as an input modality [Ware & Mikaelian, 1987], as an ana",Computer Vision,19b650660b253761af189682e03501dd-Paper.pdf,1993
Learning Curves: Asymptotic Values and,"Training classifiers on large databases is computationally demand ing. It is desirable to develop efficient procedures for a reliable prediction of a classifier's suitability for implementing a given task, so that resources can be assigned to the most promising candidates or freed for exploring new classifier candidates. We propose such a practical and principled predictive method. Practical because it avoids the costly procedure of training poor classifiers on the whole training set, and principled because of its theoretical foundation. The effectiveness of the proposed procedure is demonstrated for both single- and multi-layer networks. 1 Introd uction Training classifiers on large data.bases is computationally demanding. It is desirable to develop efficient procedures for a reliable prediction of a classifier's suitability for implementing a given task. Here we describe such a practical and principled predictive method. The procedure applies to real-life situations with huge databas",Optimization & Theoretical ML,1aa48fc4880bb0c9b8a3bf979d3b917e-Paper.pdf,1993
How to Describe Neuronal Activity:,"What is the 'correct' theoretical description of neuronal activity? The analysis of the dynamics of a globally connected network of spiking neurons (the Spike Response Model) shows that a descrip tion by mean firing rates is possible only if active neurons fire in coherently. If firing occurs coherently or with spatio-temporal cor relations, the spike structure of the neural code becomes relevant. Alternatively, neurons can be gathered into local or distributed en sembles or 'assemblies'. A description based on the mean ensemble activity is, in principle, possible but the interaction between differ ent assemblies becomes highly nonlinear. A description with spikes should therefore be preferred. 1 INTRODUCTION Neurons communicate by sequences of short pulses, the so-called action potentials or spikes. One of the most important problems in theoretical neuroscience concerns the question of how information on the environment is encoded in such spike trains: Is the exact timing of spikes wi",NLP,1efa39bcaec6f3900149160693694536-Paper.pdf,1993
Digital Boltzmann VLSI for,"We built a high-speed, digital mean-field Boltzmann chip and SBus board for general problems in constraint satjsfaction and learning. Each chip has 32 neural processors and 4 weight update processors, supporting an arbitrary topology of up to 160 functional neurons. On-chip learning is at a theoretical maximum rate of 3.5 x 108 con nection updates/sec; recall is 12000 patterns/sec for typical condi tions. The chip's high speed is due to parallel computation of inner products, limited (but adequate) precision for weights and activa tions (5 bits), fast clock (125 MHz), and several design insights. 896 Digital Boltzmann VLSI for Constraint Satisfaction and Learning 897 1 INTRODUCTION A vast number of important problems can be cast into a form of constraint satisfac tion. A crucial difficulty when solving such problems is the fact that there are local minima in the solution space, and hence simple gradient descent methods rarely suf fice. Simulated annealing via the Boltzmann algorithm (B",Computer Vision,1fc214004c9481e4c8073e85323bfd4b-Paper.pdf,1993
Feature Densities are Required for,"The feature correspondence problem is a classic hurdle in visual object-recognition concerned with determining the correct mapping between the features measured from the image and the features ex pected by the model. In this paper we show that determining good correspondences requires information about the joint probability density over the image features. We propose ""likelihood based correspondence matching"" as a general principle for selecting op timal correspondences. The approach is applicable to non-rigid models, allows nonlinear perspective transformations, and can op timally deal with occlusions and missing features. Experiments with rigid and non-rigid 3D hand gesture recognition support the theory. The likelihood based techniques show almost no decrease in classification performance when compared to performance with perfect correspondence knowledge. 1 INTRODUCTION The ability to deal with missing information is crucial in model-based vision sys tems. The feature correspondence",Computer Vision,217eedd1ba8c592db97d0dbe54c7adfc-Paper.pdf,1993
Emergence of Global Structure from,"A variant of the encoder architecture, where units at the input and out put layers represent nodes on a graph. is applied to the task of mapping locations to sets of neighboring locations. The degree to which the re suIting internal (i.e. hidden unit) representations reflect global proper ties of the environment depends upon several parameters of the learning procedure. Architectural bottlenecks. noise. and incremental learning of landmarks are shown to be important factors in maintaining topograph ic relationships at a global scale. 1 INTRODUCTION The acquisition of spatial knowledge by exploration of an environment has been the sub ject of several recent experimental studies. investigating such phenomena as the relation ship between distance estimation and priming (e.g. McNamara et al .• 1989) and the influ ence of route infonnation (McNamara et al., 1984). Clayton and Habibi (1991) have gath ered data suggesting that temporal contiguity during exploration is an important factor in d",NLP,22ac3c5a5bf0b520d281c122d1490650-Paper.pdf,1993
Robot Learning: Exploration and,No abstract found,Reinforcement Learning,22fb0cee7e1f3bde58293de743871417-Paper.pdf,1993
Catastrophic interference in,No abstract found,Optimization & Theoretical ML,28267ab848bcf807b2ed53c3a8f8fc8a-Paper.pdf,1993
Lower Boundaries of Motoneuron,"Using a quasi-realistic model of the feedback inhibition ofmotoneurons (MNs) by Renshaw cells, we show that weak inhibition is sufficient to maximally desynchronize MNs, with negligible effects on total MN activity. MN synchrony can produce a 20 - 30 Hz peak in the force power spectrum, which may cause instability in feedback loops. 1 INTRODUCTION The structure of the recurrent inhibitory connections from Renshaw cells (RCs) onto motoneurons (MNs) (Figure 1) suggests that the RC forms a simple negative feedback * send mail to: Mitchell G. Maltenfort, SMPP room 1406, Rehabilitation Insitute of Chicago, 345 East Superior Street, Chicago, IT.. 60611. Email address is mgm@nwu.edu 535 536 Maltenfort, Druzinsky, Heckman, and Rymer loop. Past theoretical work has examined possible roles of this feedback in smoothing or gain regulation of motor output (e.g., Bullock and Contreras-Vidal, 1991; Graham and Redman, 1993), but has assumed relatively strong inhibitory effects from the RC. Experiment",Optimization & Theoretical ML,2823f4797102ce1a1aec05359cc16dd9-Paper.pdf,1993
"Signature Verification using a ""Siamese""","This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a ""Siamese"" neural network. This network consists of two identical sub-networks joined at their out puts. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance be tween the two feature vectors. Verification consists of comparing an extracted feature vector ~ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh old are accepted, all other signatures are rejected as forgeries. 1 INTRODUCTION The aim of the project was to make a signature verification system based on the NCR 5990 Signature Capture Device (a pen-input tablet) and to use 80 bytes or less for signature feature storage in order that the features can be stored on the magnetic strip of a credit-card. Verification using a digitizer such as th",Computer Vision,288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf,1993
Connectionism for Music and Audition,"This workshop explored machine learning approaches to 3 topics: (1) finding structure in music (analysis, continuation, and comple tion of an unfinished piece), (2) modeling perception of time (ex traction of musical meter, explanation of human data on timing), and (3) interpolation in timbre space. In recent years, NIPS has heard neural networks generate tunes and harmonize chorales. With a large amount of music becoming available in computer readable form, real data can be used to train connectionist models. At the beginning of this workshop, Andreas Weigend focused on architectures to capture structure on multiple time scales. J. S. Bach's last (unfinished) fugue from Die Kunst der Fuge served as an example (Dirst & Weigend, 1994).1 The prediction approach to continuation and completion, as well as to modeling expectations, can be charac terized by the question ""What's next?"". Moving to time as the primary medium of musical communication, the inquiry in music perception and cognitio",NLP,2a084e55c87b1ebcdaad1f62fdbbac8e-Paper.pdf,1993
Central and Pairwise Data Clustering by,"Data clustering amounts to a combinatorial optimization problem to re duce the complexity of a data representation and to increase its precision. Central and pairwise data clustering are studied in the maximum en tropy framework. For central clustering we derive a set of reestimation equations and a minimization procedure which yields an optimal num ber of clusters, their centers and their cluster probabilities. A meanfield approximation for pairwise clustering is used to estimate assignment probabilities. A se1fconsistent solution to multidimensional scaling and pairwise clustering is derived which yields an optimal embedding and clustering of data points in a d-dimensional Euclidian space. 1 Introduction A central problem in information processing is the reduction of the data complexity with minimal loss in precision to discard noise and to reveal basic structure of data sets. Data clustering addresses this tradeoff by optimizing a cost function which preserves the original data as c",Optimization & Theoretical ML,2afe4567e1bf64d32a5527244d104cea-Paper.pdf,1993
Processing of Visual and Auditory Space,No abstract found,Computer Vision,2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf,1993
A Connectionist Model of the Owl's,",,""'e do not have a good understanding of how theoretical principles of learning are realized in neural systems. To address this problem we built a computational model of development in the owl's sound localization system. The structure of the model is drawn from known experimental data while the learning principles come from recent work in the field of brain style computation. The model accounts for numerous properties of the owl's sound localization system, makes specific and testable predictions for future experi ments, and provides a theory of the developmental process. 1 INTRODUCTION The barn owl, Tyto Alba, has a remarkable ability to localize sounds in space. In complete darkness it catches mice with nearly flawless precision. The owl depends upon this skill for survival, for it is a nocturnal hunter who uses audition to guide ·Current address: Keck Center for Integrative Neuroscience, UCSF, 513 Parnassus Ave., San Francisco, CA 94143-0444. 606 A Connectionist Model of the Owl's",NLP,2dace78f80bc92e6d7493423d729448e-Paper.pdf,1993
Identifying Fault-Prone Software,"Functional complexity of a software module can be measured in terms of static complexity metrics of the program text. Classify ing software modules, based on their static complexity measures, into different fault-prone categories is a difficult problem in soft ware engineering. This research investigates the applicability of neural network classifiers for identifying fault-prone software mod ules using a data set from a commercial software system. A pre liminary empirical comparison is performed between a minimum distance based Gaussian classifier, a perceptron classifier and a multilayer layer feed-forward network classifier constructed using a modified Cascade-Correlation algorithm. The modified version of the Cascade-Correlation algorithm constrains the growth of the network size by incorporating a cross-validation check during the output layer training phase. Our preliminary results suggest that a multilayer feed-forward network can be used as a tool for iden tifying fault-prone so",Optimization & Theoretical ML,2e65f2f2fdaf6c699b223c61b1b5ab89-Paper.pdf,1993
Event-Driven Simulation of Networks of,"A fast event-driven software simulator has been developed for sim ulating large networks of spiking neurons and synapses. The prim itive network elements are designed to exhibit biologically realis tic behaviors, such as spiking, refractoriness, adaptation, axonal delays, summation of post-synaptic current pulses, and tonic cur rent inputs. The efficient event-driven representation allows large networks to be simulated in a fraction of the time that would be required for a full compartmental-model simulation. Correspond ing analog CMOS VLSI circuit primitives have been designed and characterized, so that large-scale circuits may be simulated prior to fabrication. 1 Introduction Artificial neural networks typically use an abstraction of real neuron behaviour, in which the continuously varying mean firing rate of the neuron is presumed to carry the information about the neuron's time-varying state of excitation [1]. This useful simplification allows the neuron's state to be represented a",Computer Vision,301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf,1993
Grammatical Inference by,"We show how an ""Elman"" network architecture, constructed from recurrently connected oscillatory associative memory network mod ules, can employ selective ""attentional"" control of synchronization to direct the flow of communication and computation within the architecture to solve a grammatical inference problem. Previously we have shown how the discrete time ""Elman"" network algorithm can be implemented in a network completely described by continuous ordinary differential equations. The time steps (ma chine cycles) of the system are implemented by rhythmic variation (clocking) of a bifurcation parameter. In this architecture, oscilla tion amplitude codes the information content or activity of a mod ule (unit), whereas phase and frequency are used to ""softwire"" the network. Only synchronized modules communicate by exchang ing amplitude information; the activity of non-resonating modules contributes incoherent crosstalk noise. Attentional control is modeled as a special subset of the hidde",NLP,31839b036f63806cba3f47b93af8ccb5-Paper.pdf,1993
Bayesian Self-Organization,"Recent work by Becker and Hinton (Becker and Hinton, 1992) shows a promising mechanism, based on maximizing mutual in formation assuming spatial coherence, by which a system can self organize itself to learn visual abilities such as binocular stereo. We introduce a more general criterion, based on Bayesian probability theory, and thereby demonstrate a connection to Bayesian theo ries of visual perception and to other organization principles for early vision (Atick and Redlich, 1990). Methods for implementa tion using variants of stochastic learning are described and, for the special case of linear filtering, we derive an analytic expression for the output. 1 Introduction The input intensity patterns received by the human visual system are typically complicated functions of the object surfaces and light sources in the world. It *Lei Xu was a research scholar in the Division of Applied Sciences at Harvard University while this work was performed. 1001 1002 Yuille, Smimakis, and Xu seems ",Computer Vision,32b30a250abd6331e03a2a1f16466346-Paper.pdf,1993
"The ""Softmax"" Nonlinearity:","We use mean-field theory methods from Statistical Mechanics to derive the ""softmax"" nonlinearity from the discontinuous winner take-all (WTA) mapping. We give two simple ways of implementing ""soft max"" as a multiterminal network element. One of these has a number of important network-theoretic properties. It is a recipro cal, passive, incrementally passive, nonlinear, resistive multitermi nal element with a content function having the form of information theoretic entropy. These properties should enable one to use this element in nonlinear RC networks with such other reciprocal el ements as resistive fuses and constraint boxes to implement very high speed analog optimization algorithms using a minimum of hardware. 1 Introduction In order to efficiently implement nonlinear optimization algorithms in analog VLSI hardware, maximum use should be made of the natural properties of the silicon medium. Reciprocal circuit elements facilitate such an implementation since they 882 The ""Softmax"" N",Optimization & Theoretical ML,352407221afb776e3143e8a1a0577885-Paper.pdf,1993
Robust Parameter Estimation And,"In this paper, it is shown that the conventional back-propagation (BPP) algorithm for neural network regression is robust to lever ages (data with :n corrupted), but not to outliers (data with y corrupted). A robust model is to model the error as a mixture of normal distribution. The influence function for this mixture model is calculated and the condition for the model to be robust to outliers is given. EM algorithm [5] is used to estimate the parameter. The usefulness of model selection criteria is also discussed. Illustrative simulations are performed. 1 Introduction In neural network research, the back-propagation (BPP) algorithm is the most popular algorithm. In the regression problem y = 7](:n, w) + £, in which 7](:n, 8) denote a neural network with weight 8, the algorithm is equivalent to modeling the error as identically independently normally distributed (i.i.d.), and using the maximum likelihood method to estimate the parameter [13]. Howerer, the training data set may contain",Optimization & Theoretical ML,35cf8659cfcb13224cbd47863a34fc58-Paper.pdf,1993
Computational Elements of the Adaptive,"We consider the problem of how the CNS learns to control dynam ics of a mechanical system. By using a paradigm where a subject's hand interacts with a virtual mechanical environment, we show that learning control is via composition of a model of the imposed dynamics. Some properties of the computational elements with which the CNS composes this model are inferred through the gen eralization capabilities of the subject outside the training data. 1 Introduction At about the age of three months, children become interested in tactile exploration of objects around them. They attempt to reach for an object, but often fail to properly control their arm and end up missing their target. In the ensuing weeks, they rapidly improve and soon they can not only reach accurately, they can also pick up the object and place it. Intriguingly, during this period of learning they tend to perform rapid, flailing-like movements of their arm, as if trying to ""excite"" the plant that they wish to control in ord",Computer Vision,3621f1454cacf995530ea53652ddf8fb-Paper.pdf,1993
Dopaminergic Neuromodulation Brings a,"The fovea of a mammal retina was simulated with its detailed bio logical properties to study the local preprocessing of images. The direct visual pathway (photoreceptors, bipolar and ganglion cells) and the horizontal units, as well as the D-amacrine cells were sim ulated. The computer program simulated the analog non-spiking transmission between photoreceptor and bipolar cells, and between bipolar and ganglion cells, as well as the gap-junctions between hor izontal cells, and the release of dopamine by D-amacrine cells and its diffusion in the extra-cellular space. A 64 x 64 photoreceptors retina, containing 16,448 units, was carried out. This retina dis played contour extraction with a Mach effect, and adaptation to brightness. The simulation showed that the dopaminergic amacrine cells were necessary to ensure adaptation to local brightness. 1 INTRODUCTION The retina is the first stage in visual information processing. One of its functions is to compress the information received from",Computer Vision,3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf,1993
Transition Point Dynamic Programming,"Transition point dynamic programming (TPDP) is a memory based, reinforcement learning, direct dynamic programming ap proach to adaptive optimal control that can reduce the learning time and memory usage required for the control of continuous stochastic dynamic systems. TPDP does so by determining an ideal set of transition points (TPs) which specify only the control action changes necessary for optimal control. TPDP converges to an ideal TP set by using a variation of Q-Iearning to assess the mer its of adding, swapping and removing TPs from states throughout the state space. When applied to a race track problem, TPDP learned the optimal control policy much sooner than conventional Q-Iearning, and was able to do so using less memory. 1 INTRODUCTION Dynamic programming (DP) approaches can be utilized to determine optimal con trol policies for continuous stochastic dynamic systems when the state spaces of those systems have been quantized with a resolution suitable for control (Barto et ",Reinforcement Learning,362e80d4df43b03ae6d3f8540cd63626-Paper.pdf,1993
Monte Carlo Matrix Inversion and,"We describe the relationship between certain reinforcement learn ing (RL) methods based on dynamic programming (DP) and a class of unorthodox Monte Carlo methods for solving systems of linear equations proposed in the 1950's. These methods recast the solu tion of the linear system as the expected value of a statistic suitably defined over sample paths of a Markov chain. The significance of our observations lies in arguments (Curtiss, 1954) that these Monte Carlo methods scale better with respect to state-space size than do standard, iterative techniques for solving systems of linear equa tions. This analysis also establishes convergence rate estimates. Because methods used in RL systems for approximating the evalu ation function of a fixed control policy also approximate solutions to systems of linear equations, the connection to these Monte Carlo methods establishes that algorithms very similar to TD algorithms (Sutton, 1988) are asymptotically more efficient in a precise sense than o",Reinforcement Learning,3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf,1993
Globally Trained Handwritten Word,"We introduce a new approach for on-line recognition of handwrit ten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution ""annotated images"" where each pixel contains informa tion about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors. 1 Introduction Natural handwriting is often a mixture of different ""styles"", lower case printed, upper case, and cursive. A reliable recognizer for such handwriting would greatly improve interaction with pen-based devices, but its implementation presents new *also, AT&T Bell Labs, Holmdel NJ 07733 937 938 Bengio, Le Cun, and Henderson technical challenges. Characters taken in isolation can be very ambiguous, but ",Computer Vision,3b5dca501ee1e6d8cd7b905f4e1bf723-Paper.pdf,1993
Robust Reinforcement Learning •,"While exploring to find better solutions, an agent performing on line reinforcement learning (RL) can perform worse than is accept able. In some cases, exploration might have unsafe, or even catas trophic, results, often modeled in terms of reaching 'failure' states of the agent's environment. This paper presents a method that uses domain knowledge to reduce the number of failures during explo ration. This method formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies. The resulting action set has a more abstract relationship to the task being solved than is common in many applications of RL. Although the cost of this added safety is that learning may result in a suboptimal solution, we argue that this is an appropri ate tradeoff in many problems. We illustrate this method in the domain of motion planning. ""'This work was done while the first author was fini",Reinforcement Learning,3d8e28caf901313a554cebc7d32e67e5-Paper.pdf,1993
Optimal Stopping and Effective Machine,"We study tltt' problem of when to stop If'arning a class of feedforward networks - networks with linear outputs I1PUrOIl and fixed input weights - when they are trained with a gradient descent algorithm on a finite number of examples. Under general regularity conditions, it is shown that there a.re in general three distinct phases in the generalization performance in the learning process, and in particular, the network has hetter gt'neralization pPTformance when learning is stopped at a certain time before til(' global miniIl111lu of the empirical error is reachert. A notion of effective size of a machine is rtefil1e<i and used to explain the trade-off betwf'en the complexity of the marhine and the training error ill the learning process. The study leads nat.urally to a network size selection critt'rion, which turns Ol1t to be a generalization of Akaike's Information Criterioll for the It'arning process. It if; shown that stopping Iparning before tiJt' global minimum of the empirical e",Optimization & Theoretical ML,43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf,1993
Exploiting Chaos to Control the Future,"Recently, Ott, Grebogi and Yorke (OGY) [6] found an effective method to control chaotic systems to unstable fixed points by us ing only small control forces; however, OGY's method is based on and limited to a linear theory and requires considerable knowledge of the dynamics of the system to be controlled. In this paper we use two radial basis function networks: one as a model of an unknown plant and the other as the controller. The controller is trained with a recurrent learning algorithm to minimize a novel objective function such that the controller can locate an unstable fixed point and drive the system into the fixed point with no a priori knowl edge of the system dynamics. Our results indicate that the neural controller offers many advantages over OGY's technique. 1 Introduction Recently, Ott, Grebogi and Yorke (OGY) [6] proposed a simple but very good idea. Since any small perturbation can cause a large change in a chaotic trajectory, it is possible to use a very small control fo",Optimization & Theoretical ML,43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf,1993
Backpropagation Convergence Via,"The fundamental backpropagation (BP) algorithm for training ar tificial neural networks is cast as a deterministic nonmonotone per turbed gradient method. Under certain natural assumptions, such as the series of learning rates diverging while the series of their squares converging, it is established that every accumulation point of the online BP iterates is a stationary point of the BP error func tion. The results presented cover serial and parallel online BP, modified BP with a momentum term, and BP with weight decay. 1 INTRODUCTION We regard training artificial neural networks as an unconstrained minimization problem N min f(x) := ~ h(x) (1) xERn ~ j=l where h : ~n --+ ~, j = 1, ... , N are continuously differentiable functions from the Ii n-dimensional real space ~n to the real numbers~. Each function represents the error associated with the j-th training example, and N is the number of examples in the training set. The n-dimensional variable space here is that of the weights associ",Optimization & Theoretical ML,4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf,1993
Fool.s Gold: Extracting Finite State Machines,"Several recurrent networks have been proposed as representations for the task of formal language learning. After training a recurrent network rec ognize a formal language or predict the next symbol of a sequence, the next logical step is to understand the information processing carried out by the network. Some researchers have begun to extracting finite state machines from the internal state trajectories of their recurrent networks. This paper describes how sensitivity to initial conditions and discrete measurements can trick these extraction methods to return illusory finite state descriptions. INTRODUCTION Formal language learning (Gold, 1969) has been a topic of concern for cognitive science and artificial intelligence. It is the task of inducing a computational description of a formal language from a sequence of positive and negative examples of strings in the target lan guage. Neural information processing approaches to this problem involve the use of recur rent networks that embo",NLP,470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf,1993
Two Iterative Algorithms for Computing,"The Singular Value Decomposition (SVD) is an important tool for linear algebra and can be used to invert or approximate matrices. Although many authors use ""SVD"" synonymously with ""Eigen vector Decomposition"" or ""Principal Components Transform"", it is important to realize that these other methods apply only to symmetric matrices, while the SVD can be applied to arbitrary nonsquare matrices. This property is important for applications to signal transmission and control. I propose two new algorithms for iterative computation of the SVD given only sample inputs and outputs from a matrix. Although there currently exist many algorithms for Eigenvector Decomposi tion (Sanger 1989, for example), these are the first true sample based SVD algorithms. 1 INTRODUCTION The Singular Value Decomposition (SVD) is a method for writing an arbitrary nons quare matrix as the product of two orthogonal matrices and a diagonal matrix. This technique is an important component of methods for approximating near",Optimization & Theoretical ML,49c9adb18e44be0711a94e827042f630-Paper.pdf,1993
Efficient Computation of Complex,"By their very nature, memory based algorithms such as KNN or Parzen windows require a computationally expensive search of a large database of prototypes. In this paper we optimize the search ing process for tangent distance (Simard, LeCun and Denker, 1993) to improve speed performance. The closest prototypes are found by recursively searching included subset.s of the database using dis tances of increasing complexit.y. This is done by using a hierarchy of tangent distances (increasing the Humber of tangent. vectors from o to its maximum) and multiresolution (using wavelets). At each stage, a confidence level of the classification is computed. If the confidence is high enough, the c.omputation of more complex dis tances is avoided. The resulting algorithm applied to character recognition is close to t.hree orders of magnitude faster than com puting the full tangent dist.ance on every prot.ot.ypes. 1 INTRODUCTION Memory based algorithms such as KNN or Parzen windows have been extensively",Computer Vision,4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf,1993
Statistics of Natural Images:,"In order to best understand a visual system one should attempt to characterize the natural images it processes. We gather images from the woods and find that these scenes possess an ensemble scale invariance. Further, they are highly non-Gaussian, and this non Gaussian character cannot be removed through local linear filter ing. We find that including a simple ""gain control"" nonlinearity in the filtering process makes the filter output quite Gaussian, mean ing information is maximized at fixed channel variance. Finally, we use the measured power spectrum to place an upper bound on the information conveyed about natural scenes by an array of receptors. 1 Introduction Natural stimuli are playing an increasingly important role in our understanding of sensory processing. This is because a sensory system's ability to perform a task is a statistical quantity which depends on the signal and noise characteristics. Recently several approaches have explored visual processing as it relates to nat",Computer Vision,4d5b995358e7798bc7e9d9db83c612a5-Paper.pdf,1993
GDS: Gradient Descent Generation of,"Imagine you have designed a neural network that successfully learns a complex classification task. What are the relevant input features the classifier relies on and how are these features combined to pro duce the classification decisions? There are applications where a deeper insight into the structure of an adaptive system and thus into the underlying classification problem may well be as important as the system's performance characteristics, e.g. in economics or medicine. GDSi is a backpropagation-based training scheme that produces networks transformable into an equivalent and concise set of IF-THEN rules. This is achieved by imposing penalty terms on the network parameters that adapt the network to the expressive power of this class of rules. Thus during training we simultaneously minimize classification and transformation error. Some real-world tasks demonstrate the viability of our approach. 1 Introduction This paper deals with backpropagation networks trained to perform a classi",NLP,4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf,1993
Packet Routing in Dynamically,"This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, Q-routing proves supe rior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation, such as the network load, are allowed to vary dy namically. The paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies. 1 INTRODUCTION The field of reinforcement learning has grown dramatically over the past several years, but with the exception of backgammon [8, 2], has had few successful appli cations to large-scale, practical tasks. This paper demonstrates that the practical task of routing packets throu",Reinforcement Learning,4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf,1993
How to Choose an Activation Function,"We study the complexity problem in artificial feedforward neural networks designed to approximate real valued functions of several real variables; i.e., we estimate the number of neurons in a network required to ensure a given degree of approximation to every function in a given function class. We indicate how to construct networks with the indicated number of neurons evaluating standard activation functions. Our general theorem shows that the smoother the activation function, the better the rate of approximation. 1 INTRODUCTION The approximation capabilities of feedforward neural networks with a single hidden layer has been studied by many authors, e.g., [1, 2, 5]. In [10], we have shown that such a network using practically any nonlinear activation function can approximate any continuous function of any number of real variables on any compact set to any desired degree of accuracy. A central question in this theory is the following. If one needs to approximate a function from a known ",Optimization & Theoretical ML,51ef186e18dc00c2d31982567235c559-Paper.pdf,1993
Efficient Simulation of Biological Neural,"We present a neural network simulation which we implemented on the massively parallel Connection Machine 2. In contrast to previous work, this simulator is based on biologically realistic neu rons with nontrivial single-cell dynamics, high connectivity with a structure modelled in agreement with biological data, and preser vation of the temporal dynamics of spike interactions. We simulate neural networks of 16,384 neurons coupled by about 1000 synapses per neuron, and estimate the performance for much larger systems. Communication between neurons is identified as the computation ally most demanding task and we present a novel method to over come this bottleneck. The simulator has already been used to study the primary visual system of the cat. 1 INTRODUCTION Neural networks have been implemented previously on massively parallel supercom puters (Fujimoto et al., 1992, Zhang et al., 1990). However, these are implemen tations of artificial, highly simplified neural networks, while our aim",Computer Vision,5751ec3e9a4feab575962e78e006250d-Paper.pdf,1993
Convergence of Stochastic Iterative,"Increasing attention has recently been paid to algorithms based on dynamic programming (DP) due to the suitability of DP for learn ing problems involving control. In stochastic environments where the system being controlled is only incompletely known, however, a unifying theoretical account of these methods has been missing. In this paper we relate DP-based learning algorithms to the pow erful techniques of stochastic approximation via a new convergence theorem, enabling us to establish a class of convergent algorithms to which both TD(""\) and Q-Iearning belong. 1 INTRODUCTION Learning to predict the future and to find an optimal way of controlling it are the basic goals of learning systems that interact with their environment. A variety of algorithms are currently being studied for the purposes of prediction and control in incompletely specified, stochastic environments. Here we consider learning algo rithms defined in Markov environments. There are actions or controls (u) available f",Reinforcement Learning,5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf,1993
High Performance Neural Net Simulation,"The performance requirements in experimental research on arti ficial neural nets often exceed the capability of workstations and PCs by a great amount. But speed is not the only requirement. Flexibility and implementation time for new algorithms are usually of equal importance. This paper describes the simulation of neural nets on the MUSIC parallel supercomputer, a system that shows a good balance between the three issues and therefore made many research projects possible that were unthinkable before. (MUSIC stands for Multiprocessor System with Intelligent Communication) 1 Overview of the MUSIC System The goal of the MUSIC project was to build a fast parallel system and to use it in real-world applications like neural net simulations, image processing or simulations in chemistry and physics [1, 2]. The system should be flexible, simple to program and the realization time should be short enough to not have an obsolete system by the time it is finished. Therefore, the fastest available",Optimization & Theoretical ML,59c33016884a62116be975a9bb8257e3-Paper.pdf,1993
A Massively-Parallel SIMD Processor for,"This paper describes the MM32k, a massively-parallel SIMD com puter which is easy to program, high in performance, low in cost and effective for implementing highly parallel neural network ar chitectures. The MM32k has 32768 bit serial processing elements, each of which has 512 bits of memory, and all of which are inter connected by a switching network. The entire system resides on a single PC-AT compatible card. It is programmed from the host computer using a C++ language class library which abstracts the parallel processor in terms of fast arithmetic operators for vectors of variable precision integers. 1 INTRODUCTION Many well known neural network techniques for adaptive pattern classification and function approximation are inherently highly parallel, and thus have proven dif ficult to implement for real-time applications at a reasonable cost. This includes 843 844 Glover and Miller a variety of learning systems such as radial basis function networks [Moody 1989], Kohonen self-organ",Computer Vision,50c3d7614917b24303ee6a220679dab3-Paper.pdf,1993
An Analog VLSI Model of Central Pattern,"I detail the design and construction of an analog VLSI model of the neural system responsible for swimming behaviors of the leech. Why the leech? The biological network is small and relatively well understood, and the silicon model can therefore span three levels of organization in the leech nervous system (neuron, ganglion, system); it represents one of the first comprehensive models of leech swimming operating in real-time. The circuit employs biophysically motivated analog neurons networked to form multiple biologically inspired silicon ganglia. These ganglia are coupled using known interganglionic connections. Thus the model retains the flavor of its biological counterpart, and though simplified, the output of the silicon circuit is similar to the output of the leech swim central pattern generator. The model operates on the same time- and spatial-scale as the leech nervous system and will provide an excellent platform with which to explore real-time adaptive locomotion in the leech",Computer Vision,5c572eca050594c7bc3c36e7e8ab9550-Paper.pdf,1993
Non-linear Statistical Analysis and,"Neurons learning under an unsupervised Hebbian learning rule can perform a nonlinear generalization of principal component analysis. This relationship between nonlinear PCA and nonlinear neurons is reviewed. The stable fixed points of the neuron learning dynamics correspond to the maxima of the statist,ic optimized under non linear PCA. However, in order to predict. what the neuron learns, knowledge of the basins of attractions of the neuron dynamics is required. Here the correspondence between nonlinear PCA and neural networks breaks down. This is shown for a simple model. Methods of statistical mechanics can be used to find the optima of the objective function of non-linear PCA. This determines what the neurons can learn. In order to find how the solutions are parti tioned amoung the neurons, however, one must solve the dynamics. 1 INTRODUCTION Linear neurons learning under an unsupervised Hebbian rule can learn to perform a linear statistical analysis ofthe input data. This was firs",Optimization & Theoretical ML,5ec91aac30eae62f4140325d09b9afd0-Paper.pdf,1993
Locally Adaptive Nearest Neighbor,"Four versions of a k-nearest neighbor algorithm with locally adap tive k are introduced and compared to the basic k-nearest neigh bor algorithm (kNN). Locally adaptive kNN algorithms choose the value of k that should be used to classify a query by consulting the results of cross-validation computations in the local neighborhood of the query. Local kNN methods are shown to perform similar to kNN in experiments with twelve commonly used data sets. Encour aging results in three constructed tasks show that local methods can significantly outperform kNN in specific applications. Local methods can be recommended for on-line learning and for appli cations where different regions of the input space are covered by patterns solving different sub-tasks. 1 Introduction The k-nearest neighbor algorithm (kNN, Dasarathy, 1991) is one of the most ven erable algorithms in machine learning. The entire training set is stored in memory. A new example is classified with the class of the majority of the k n",Optimization & Theoretical ML,5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf,1993
Bayesian Backprop in Action:,"MacKay's Bayesian framework for backpropagation is conceptually appealing as well as practical. It automatically adjusts the weight decay parameters during training, and computes the evidence for each trained network. The evidence is proportional to our belief in the model. The networks with highest evidence turn out to generalise well. In this paper, the framework is extended to pruned nets, leading to an Ockham Factor for ""tuning the architecture to the data"". A committee of networks, selected by their high evidence, is a natural Bayesian construction. The evidence of a committee is computed. The framework is illustrated on real-world data from a near infrared spectrometer used to determine the fat content in minced meat. Error bars are computed, including the contribution from the dissent of the committee members. 1 THE OCKHAM FACTOR William of Ockham's (1285-1349) principle of economy in explanations, can be formulated as follows: If several theories account for a phenomenon we sho",Optimization & Theoretical ML,5f2c22cb4a5380af7ca75622a6426917-Paper.pdf,1993
Solvable Models of Artificial Neural,"Solvable models of nonlinear learning machines are proposed, and learning in artificial neural networks is studied based on the theory of ordinary differential equations. A learning algorithm is con structed, by which the optimal parameter can be found without any recursive procedure. The solvable models enable us to analyze the reason why experimental results by the error backpropagation often contradict the statistical learning theory. 1 INTRODUCTION Recent studies have shown that learning in artificial neural networks can be under stood as statistical parametric estimation using t.he maximum likelihood method [1], and that their generalization abilities can be estimated using the statistical asymptotic theory [2]. However, as is often reported, even when the number of parameters is too large, the error for the test.ing sample is not so large as the theory predicts. The reason for such inconsistency has not yet been clarified, because it is difficult for the artificial neural network",Optimization & Theoretical ML,61b4a64be663682e8cb037d9719ad8cd-Paper.pdf,1993
A Comparative Study Of A Modified,"Bumptrees are geometric data structures introduced by Omohundro (1991) to provide efficient access to a collection of functions on a Euclidean space of interest. We describe a modified bumptree structure that has been employed as a neural network classifier, and compare its performance on several classification tasks against that of radial basis function networks and the standard mutIi-Iayer perceptron. 1 INTRODUCTION A number of neural network studies have demonstrated the utility of the multi-layer perceptron (MLP) and shown it to be a highly effective paradigm. Studies have also shown, however, that the MLP is not without its problems, in particular it requires an extensive training time, is susceptible to local minima problems and its perfonnance is dependent upon its internal network architecture. In an attempt to improve upon the generalisation performance and computational efficiency a number of studies have been undertaken principally concerned with investigating the parametris",NLP,632cee946db83e7a52ce5e8d6f0fed35-Paper.pdf,1993
Generalization Error and The Expected,"For two layer networks with n sigmoidal hidden units, the generalization error is shown to be bounded by O(E~ K) + O( (E NK )d l og N) , where d and N are the input dimension and the number of training samples, re spectively. E represents the expectation on random number K of hidden units = (1 :::; I\ :::; n). The probability Pr(I{ k) (1 :::; k :::; n) is (kt.erl11ined by a prior distribution of weights, which corresponds to a Gibbs distribtt! ion of a regularizeI'. This relationship makes it possible to characterize explicitly how a regularization term affects bias/variance of networks. The bound can be obtained analytically for a large class of commonly used priors. It can also be applied to estimate the expected net.work complexity Ef{ in practice. The result provides a quantitative explanation on how large networks can generalize well. 1 Introduction Regularization (or weight-decay) methods are widely used in supervised learning by adding a regularization term t.o an energy functio",Optimization & Theoretical ML,670e8a43b246801ca1eaca97b3e19189-Paper.pdf,1993
COMBINED NEURAL NETWORKS,"We propose a method for improving the performance of any net work designed to predict the next value of a time series. Vve advo cate analyzing the deviations of the network's predictions from the data in the training set. This can be carried out by a secondary net work trained on the time series of these residuals. The combined system of the two networks is viewed as the new predictor. We demonstrate the simplicity and success of this method, by apply ing it to the sunspots data. The small corrections of the secondary network can be regarded as resulting from a Taylor expansion of a complex network which includes the combined system. \\Te find that the complex network is more difficult to train and performs worse than the two-step procedure of the combined system. 1 INTRODUCTION The use of neural networks for computational tasks is based on the idea that the efficient way in which the nervous system handles memory and cognition is worth immitating. Artificial implementations are often ",Optimization & Theoretical ML,677e09724f0e2df9b6c000b75b5da10d-Paper.pdf,1993
Illumination-Invariant Face Recognition with a,"Changes in lighting conditions strongly effect the performance and reli ability of computer vision systems. We report face recognition results under drastically changing lighting conditions for a computer vision sys tem which concurrently uses a contrast sensitive silicon retina and a con ventional, gain controlled CCO camera. For both input devices the face recognition system employs an elastic matching algorithm with wavelet based features to classify unknown faces. To assess the effect of analog on -chip preprocessing by the silicon retina the CCO images have been ""digitally preprocessed"" with a bandpass filter to adjust the power spec trum. The silicon retina with its ability to adjust sensitivity increases the recognition rate up to 50 percent. These comparative experiments demonstrate that preprocessing with an analog VLSI silicon retina gen erates image data enriched with object-constant features. 1 Introdnction Neural computation as an information processing paradigm promises t",Computer Vision,67d16d00201083a2b118dd5128dd6f59-Paper.pdf,1993
Address Block Location with a Neural Net System,"We developed a system for finding address blocks on mail pieces that can process four images per second. Besides locating the address block, our system also determines the writing style, handwritten or machine printed, and moreover, it measures the skew angle of the text lines and cleans noisy images. A layout analysis of all the elements present in the image is performed in order to distinguish drawings and dirt from text and to separate text of advertisement from that of the destination address. A speed of more than four images per second is obtained on a modular hardware platform, containing a board with two of the NET32K neural net chips, a SP ARC2 processor board, and a board with 2 digital signal processors. The system has been tested with more than 100,000 images. Its performance depends on the quality of the images, and lies between 85% correct location in very noisy images to over 98% in cleaner images. 1 INTRODUCTION The system described here has been integrated into an addre",Computer Vision,67d96d458abdef21792e6d8e590244e7-Paper.pdf,1993
Assessing the Quality of Learned Local Models,"An approach is presented to learning high dimensional functions in the case where the learning algorithm can affect the generation of new data. A local modeling algorithm, locally weighted regression, is used to represent the learned function. Architectural parameters of the approach, such as distance metrics, are also localized and become a function of the query point instead of being global. Statistical tests are given for when a local model is good enough and sampling should be moved to a new area. Our methods explicitly deal with the case where prediction accuracy requirements exist during exploration: By gradually shifting a ""center of exploration"" and controlling the speed of the shift with local pre diction accuracy, a goal-directed exploration of state space takes place along the fringes of the current data support until the task goal is achieved. We illustrate this approach with simulation results and results from a real robot learning a complex juggling task. 1 INTRODUCTION E",Reinforcement Learning,68053af2923e00204c3ca7c6a3150cf7-Paper.pdf,1993
"Synchronization, oscillations, and","We investigate a model for neural activity that generates long range temporal correlations, 1/f noise, and oscillations in global activity. The model consists of a two-dimensional sheet of leaky integrate and-fire neurons with feedback connectivity consisting of local ex citation and surround inhibition. Each neuron is independently driven by homogeneous external noise. Spontaneous symmetry breaking occurs, resulting in the formation of ""hotspots"" of activ ity in the network. These localized patterns of excitation appear as clusters that coalesce, disintegrate, or fluctuate in size while si multaneously moving in a random walk constrained by the interac tion with other clusters. The emergent cross-correlation functions have a dual structure, with a sharp peak around zero on top of a much broader hill. The power spectrum associated with single units shows a 1/ f decay for small frequencies and is flat at higher frequencies, while the power spectrum of the spiking activity aver aged over",Computer Vision,6bc24fc1ab650b25b4114e93a98f1eba-Paper.pdf,1993
What Does the Hippocampus Compute?:,No abstract found,NLP,6c29793a140a811d0c45ce03c1c93a28-Paper.pdf,1993
Unsupervised Learning of Mixtures of,"This paper presents a formulation for unsupervised learning of clus ters reflecting multiple causal structure in binary data. Unlike the standard mixture model, a multiple cause model accounts for ob served data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observ able dimensions. A crucial issue is the mixing-function for combin ing beliefs from different cluster-centers in order to generate data reconstructions whose errors are minimized both during recognition and learning. We demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer an alterna tive form of the nonlinearity. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representat.ions of noisy test data and in images of printed characters. 1 Introduction The objective of unsupervised learning is to identify patterns or features reflecting underlying regularit",NLP,6cd67d9b6f0150c77bda2eda01ae484c-Paper.pdf,1993
A Hodgkin-Huxley Type Neuron Model,"A gradient descent algorithm for parameter estimation which is similar to those used for continuous-time recurrent neural networks was derived for Hodgkin-Huxley type neuron models. Using mem brane potential trajectories as targets, the parameters (maximal conductances, thresholds and slopes of activation curves, time con stants) were successfully estimated. The algorithm was applied to modeling slow non-spike oscillation of an identified neuron in the lobster stomatogastric ganglion. A model with three ionic currents was trained with experimental data. It revealed a novel role of A-current for slow oscillation below -50 mY. 1 INTRODUCTION Conductance-based neuron models, first formulated by Hodgkin and Huxley [10], are commonly used for describing biophysical mechanisms underlying neuronal be havior. Since the days of Hodgkin and Huxley, tens of new ionic channels have been identified [9]. Accordingly, recent H-H type models have tens of variables and hundreds of parameters [1, 2]. Id",NLP,6e0721b2c6977135b916ef286bcb49ec-Paper.pdf,1993
Memory-Based Methods for Regression,No abstract found,Optimization & Theoretical ML,6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf,1993
Neural Network Methods for,No abstract found,Deep Learning,6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,1993
A Comparison of Dynamic Reposing and,"In drug activity prediction (as in handwritten character recogni tion), the features extracted to describe a training example depend on the pose (location, orientation, etc.) of the example. In hand written character recognition, one of the best techniques for ad dressing this problem is the tangent distance method of Simard, LeCun and Denker (1993). Jain, et al. (1993a; 1993b) introduce a new technique-dynamic reposing-that also addresses this prob lem. Dynamic reposing iteratively learns a neural network and then reposes the examples in an effort to maximize the predicted out put values. New models are trained and new poses computed until models and poses converge. This paper compares dynamic reposing to the tangent distance method on the task of predicting the bio logical activity of musk compounds. In a 20-fold cross-validation, 216 A Comparison of Dynamic Reposing and Tangent Distance for Drug Activity Prediction 217 dynamic reposing attains 91 % correct compared to 79% for the ta",NLP,7143d7fbadfa4693b9eec507d9d37443-Paper.pdf,1993
Hoo Optimality Criteria for LMS and,"We have recently shown that the widely known LMS algorithm is an HOO optimal estimator. The HOO criterion has been introduced, initially in the control theory literature, as a means to ensure ro bust performance in the face of model uncertainties and lack of statistical information on the exogenous signals. We extend here our analysis to the nonlinear setting often encountered in neural networks, and show that the backpropagation algorithm is locally HOO optimal. This fact provides a theoretical justification of the widely observed excellent robustness properties of the LMS and backpropagation algorithms. We further discuss some implications of these results. 1 Introduction The LMS algorithm was originally conceived as an approximate recursive procedure that solves the following problem (Widrow and Hoff, 1960): given a sequence of n x 1 input column vectors {hd, and a corresponding sequence of desired scalar responses { d i }, find an estimate of an n x 1 column vector of weights w suc",Optimization & Theoretical ML,71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf,1993
Classifying Hand Gestures with a View-based,"We present a method for learning, tracking, and recognizing human hand gestures recorded by a conventional CCD camera without any special gloves or other sensors. A view-based representation is used to model aspects of the hand relevant to the trained gestures, and is found using an unsupervised clustering technique. We use normalized correlation net works, with dynamic time warping in the temporal domain, as a distance function for unsupervised clustering. Views are computed separably for space and time dimensions; the distributed response of the combination of these units characterizes the input data with a low dimensional repre sentation. A supervised classification stage uses labeled outputs of the spatio-temporal units as training data. Our system can correctly classify gestures in real time with a low-cost image processing accelerator. 1 INTRODUCTION Gesture recognition is an important aspect of human interaction, either interpersonally or in the context of man-machine interfaces",Computer Vision,7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf,1993
Optimal Unsupervised Motor Learning,"(Masino and Knudsen 1990) showed some remarkable results which suggest that head motion in the barn owl is controlled by distinct circuits coding for the horizontal and vertical components of move ment. This implies the existence of a set of orthogonal internal co ordinates that are related to meaningful coordinates of the external world. No coherent computational theory has yet been proposed to explain this finding. I have proposed a simple model which pro vides a framework for a theory of low-level motor learning. I show that the theory predicts the observed microstimulation results in the barn owl. The model rests on the concept of ""Optimal U n supervised Motor Learning"", which provides a set of criteria that predict optimal internal representations. I describe two iterative Neural Network algorithms which find the optimal solution and demonstrate possible mechanisms for the development of internal representations in animals. 1 INTRODUCTION In the sensory domain, many algorithms for",Reinforcement Learning,72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf,1993
Amplifying and Linearizing Apical,"Intradendritic electrophysiological recordings reveal a bewildering repertoire of complex electrical spikes and plateaus that are dif ficult to reconcile with conventional notions of neuronal function. In this paper we argue that such dendritic events are just an exu berant expression of a more important mechanism - a proportional current amplifier whose primary task is to offset electrotonic losses. Using the example of functionally important synaptic inputs to the superficial layers of an anatomically and electrophysiologically re constructed layer 5 pyramidal neuron, we derive and simulate the properties of conductances that linearize and amplify distal synap tic input current in a graded manner. The amplification depends on a potassium conductance in the apical tuft and calcium conduc tances in the apical trunk. ·To whom all correspondence should be addressed. 519 520 Bemander, Koch, and Douglas 1 INTRODUCTION About half the pyramidal neurons in layer 5 of neocortex have long apica",Optimization & Theoretical ML,788d986905533aba051261497ecffcbb-Paper.pdf,1993
Decoding Cursive Scripts,"Online cursive handwriting recognition is currently one of the most intriguing challenges in pattern recognition. This study presents a novel approach to this problem which is composed of two comple mentary phases. The first is dynamic encoding of the writing tra jectory into a compact sequence of discrete motor control symbols. In this compact representation we largely remove the redundancy of the script, while preserving most of its intelligible components. In the second phase these control sequences are used to train adaptive probabilistic acyclic automata (PAA) for the important ingredients of the writing trajectories, e.g. letters. We present a new and effi cient learning algorithm for such stochastic automata, and demon strate its utility for spotting and segmentation of cursive scripts. Our experiments show that over 90% of the letters are correctly spotted and identified, prior to any higher level language model. Moreover, both the training and recognition algorithms are very e",Computer Vision,795c7a7a5ec6b460ec00c5841019b9e9-Paper.pdf,1993
Foraging in an Uncertain Environment Using,"Survival is enhanced by an ability to predict the availability of food, the likelihood of predators, and the presence of mates. We present a concrete model that uses diffuse neurotransmitter systems to implement a predictive version of a Hebb learning rule embedded in a neural ar chitecture based on anatomical and physiological studies on bees. The model captured the strategies seen in the behavior of bees and a number of other animals when foraging in an uncertain environment. The predictive model suggests a unified way in which neuromodulatory influences can be used to bias actions and control synaptic plasticity. Successful predictions enhance adaptive behavior by allowing organisms to prepare for fu ture actions, rewards, or punishments. Moreover, it is possible to improve upon behavioral choices if the consequences of executing different actions can be reliably predicted. Al though classical and instrumental conditioning results from the psychological literature [1] demonstrate th",Optimization & Theoretical ML,7a53928fa4dd31e82c6ef826f341daec-Paper.pdf,1993
Analysis of Short Term Memories for Neural,"Short term memory is indispensable for the processing of time varying information with artificial neural networks. In this paper a model for linear memories is presented, and ways to include memories in connectionist topologies are discussed. A comparison is drawn among different memory types, with indication of what is the salient characteristic of each memory model. 1 INTRODUCTION An adaptive system that has to interact with the external world is faced with the problem of coping with the time varying nature of real world signals. Time varying signals, natural or man made, carry information in their time structure. The problem is then one of devising methods and topologies (in the case of interest here, neural topologies) that explore information along time.This problem can be appropriately called temporal pattern recognition, as opposed to the more traditional case of static pattern recognition. In static pattern recognition an input is represented by a point in a space with dimensio",NLP,7c590f01490190db0ed02a5070e20f01-Paper.pdf,1993
Credit Assignment through Time:,"Learning to recognize or predict sequences using long-term con text has many applications. However, practical and theoretical problems are found in training recurrent neural networks to per form tasks in which input/output dependencies span long intervals. Starting from a mathematical analysis of the problem, we consider and compare alternative algorithms and architectures on tasks for which the span of the input/output dependencies can be controlled. Results on the new algorithms show performance qualitatively su perior to that obtained with backpropagation. 1 Introduction Recurrent neural networks have been considered to learn to map input sequences to output sequences. Machines that could efficiently learn such tasks would be useful for many applications involving sequence prediction, recognition or production. However, practical difficulties have been reported in training recurrent neural net works to perform tasks in which the temporal contingencies present in the in put/output se",Deep Learning,7f1171a78ce0780a2142a6eb7bc4f3c8-Paper.pdf,1993
Analyzing Cross Connected Networks,"The non-linear complexities of neural networks make network solutions difficult to understand. Sanger's contribution analysis is here extended to the analysis of networks automatically generated by the cascade correlation learning algorithm. Because such networks have cross connections that supersede hidden layers, standard analyses of hidden unit activation patterns are insufficient. A contribution is defined as the product of an output weight and the associated activation on the sending unit, whether that sending unit is an input or a hidden unit, multiplied by the sign of the output target for the current input pattern. Intercorrelations among contributions, as gleaned from the matrix of contributions x input patterns, can be subjected to principal components analysis (PCA) to extract the main features of variation in the contributions. Such an analysis is applied to three problems, continuous XOR, arithmetic comparison, and distinguishing between two interlocking spirals. In all th",Deep Learning,81e74d678581a3bb7a720b019f4f1a93-Paper.pdf,1993
Recognition-based Segmentation of,"This paper introduces a new recognition-based segmentation ap proach to recognizing on-line cursive handwriting from a database of 10,000 English words. The original input stream of z, y pen coor dinates is encoded as a sequence of uniform stroke descriptions that are processed by six feed-forward neural-networks, each designed to recognize letters of different sizes. Words are then recognized by performing best-first search over the space of all possible segmen tations. Results demonstrate that the method is effective at both writer dependent recognition (1.7% to 15.5% error rate) and writer independent recognition (5.2% to 31.1% error rate). 1 Introduction With the advent of pen-based computers, the problem of automatically recognizing handwriting from the motions of a pen has gained much significance. Progress has been made in reading disjoint block letters [Weissman et. ai, 93]. However, cursive writing is much quicker and natural for humans, but poses a significant challenge to pa",NLP,82489c9737cc245530c7a6ebef3753ec-Paper.pdf,1993
Implementing Intelligence on Silicon,"We will present the implementation of intelligent electronic circuits realized for the first time using a new functional device called Neuron MOS Transistor (neuMOS or vMOS in short) simulating the behavior of biological neurons at a single transistor level. Search for the most resembling data in the memory cell array, for instance, can be automatically carried out on hardware without any software manipulation. Soft Hardware, which we named, can arbitrarily change its logic function in real time by external control signals without any hardware modification. Implementation of a neural network equipped with an on-chip self-learning capability is also described. Through the studies of vMOS intelligent circuit implementation, we noticed an interesting similarity in the architectures of vMOS logic circuitry and biological systems. 1 INTRODUCTION The motivation of this work has stemmed from the invention of a new functional transistor which simulates the behavior of biological neurons (Shiba",Optimization & Theoretical ML,82f2b308c3b01637c607ce05f52a2fed-Paper.pdf,1993
Probabilistic Anomaly Detection In,"This paper describes probabilistic methods for novelty detection when using pattern recognition methods for fault monitoring of dynamic systems. The problem of novelty detection is particular ly acute when prior knowledge and training data only allow one to construct an incomplete classification model. Allowance must be made in model design so that the classifier will be robust to data generated by classes not included in the training phase. For diagnosis applications one practical approach is to construct both an input density model and a discriminative class model. Using Bayes' rule and prior estimates of the relative likelihood of data of known and unknown origin the resulting classification equations are straightforward. The paper describes the application of this method in the context of hidden Markov models for online fault monitoring of large ground antennas for spacecraft tracking, with particular application to the detection of transient behaviour of unknown origin. 1 PROBLEM ",Optimization & Theoretical ML,846c260d715e5b854ffad5f70a516c88-Paper.pdf,1993
Learning Complex Boolean Functions:,"The most commonly used neural network models are not well suited to direct digital implementations because each node needs to per form a large number of operations between floating point values. Fortunately, the ability to learn from examples and to generalize is not restricted to networks ofthis type. Indeed, networks where each node implements a simple Boolean function (Boolean networks) can be designed in such a way as to exhibit similar properties. Two algorithms that generate Boolean networks from examples are pre sented. The results show that these algorithms generalize very well in a class of problems that accept compact Boolean network descriptions. The techniques described are general and can be ap plied to tasks that are not known to have that characteristic. Two examples of applications are presented: image reconstruction and hand-written character recognition. 1 Introduction The main objective of this research is the design of algorithms for empirical learning that generate",Computer Vision,847cc55b7032108eee6dd897f3bca8a5-Paper.pdf,1993
A Unified Gradient-Descent/Clustering,"Although recurrent neural nets have been moderately successful in learning to emulate finite-state machines (FSMs), the continu ous internal state dynamics of a neural net are not well matched to the discrete behavior of an FSM. We describe an architecture, called DOLCE, that allows discrete states to evolve in a net as learn ing progresses. DOLCE consists of a standard recurrent neural net trained by gradient descent and an adaptive clustering technique that quantizes the state space. DOLCE is based on the assumption that a finite set of discrete internal states is required for the task, and that the actual network state belongs to this set but has been corrupted by noise due to inaccuracy in the weights. DOLCE learns to recover the discrete state with maximum a posteriori probabil ity from the noisy state. Simulations show that DOLCE leads to a significant improvement in generalization performance over earlier neural net approaches to FSM induction. 1 INTRODUCTION Researchers often t",Optimization & Theoretical ML,84f7e69969dea92a925508f7c1f9579a-Paper.pdf,1993
Convergence of Indirect Adaptive,"Reinforcement Learning methods based on approximating dynamic programming (DP) are receiving increased attention due to their utility in forming reactive control policies for systems embedded in dynamic environments. Environments are usually modeled as controlled Markov processes, but when the environment model is not known a priori, adaptive methods are necessary. Adaptive con trol methods are often classified as being direct or indirect. Direct methods directly adapt the control policy from experience, whereas indirect methods adapt a model of the controlled process and com pute control policies based on the latest model. Our focus is on indirect adaptive DP-based methods in this paper. We present a convergence result for indirect adaptive asynchronous value itera tion algorithms for the case in which a look-up table is used to store the value function. Our result implies convergence of several ex isting reinforcement learning algorithms such as adaptive real-time dynamic programming",Reinforcement Learning,86b122d4358357d834a87ce618a55de0-Paper.pdf,1993
Fast Pruning Using Principal,"We present a new algorithm for eliminating excess parameters and improving network generalization after supervised training. The method, ""Principal Components Pruning (PCP)"", is based on prin cipal component analysis of the node activations of successive layers of the network. It is simple, cheap to implement, and effective. It requires no network retraining, and does not involve calculating the full Hessian of the cost function. Only the weight and the node activity correlation matrices for each layer of nodes are required. We demonstrate the efficacy of the method on a regression problem using polynomial basis functions, and on an economic time series prediction problem using a two-layer, feedforward network. 1 Introduction In supervised learning, a network is presented with a set of training exemplars = [u(k), y(k)), k 1 ... N where u(k) is the kth input and y(k) is the correspond ing output. The assumption is that there exists an underlying (possibly noisy) functional relationship ",Optimization & Theoretical ML,872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf,1993
Hidden Markov Models for Human,No abstract found,NLP,88ae6372cfdc5df69a976e893f4d554b-Paper.pdf,1993
Postal Address Block Location Using A,"This paper describes the use of a convolutional neural network to perform address block location on machine-printed mail pieces. Locating the address block is a difficult object recognition problem because there is often a large amount of extraneous printing on a mail piece and because address blocks vary dramatically in size and shape. We used a convolutional locator network with four outputs, each trained to find a different corner of the address block. A simple set of rules was used to generate ABL candidates from the network output. The system performs very well: when allowed five guesses, the network will tightly bound the address delivery information in 98.2% of the cases. 1 INTRODUCTION The U.S. Postal Service delivers about 350 million mail pieces a day. On this scale, even highly sophisticated and custom-built sorting equipment quickly pays for itself. Ideally, such equipment would be able to perform optical character recognition (OCR) over an image of the entire mail piece. H",Computer Vision,8c235f89a8143a28a1d6067e959dd858-Paper.pdf,1993
An Analog VLSI Saccadic Eye Movement,"In an effort to understand saccadic eye movements and their rela tion to visual attention and other forms of eye movements, we - in collaboration with a number of other laboratories - are carry ing out a large-scale effort to design and build a complete primate oculomotor system using analog CMOS VLSI technology. Using this technology, a low power, compact, multi-chip system has been built which works in real-time using real-world visual inputs. We describe in this paper the performance of an early version of such a system including a 1-D array of photoreceptors mimicking the retina, a circuit computing the mean location of activity represent ing the superior colliculus, a saccadic burst generator, and a one degree-of-freedom rotational platform which models the dynamic properties of the primate oculomotor plant. 1 Introduction When we look around our environment, we move our eyes to center and stabilize objects of interest onto our fovea. In order to achieve this, our eyes move in qui",Computer Vision,8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf,1993
An Optimization Method of Layered,"This paper proposes a practical optimization method for layered neural networks, by which the optimal model and parameter can be found simultaneously. 'i\Te modify the conventional information criterion into a differentiable function of parameters, and then, min imize it, while controlling it back to the ordinary form. Effective ness of this method is discussed theoretically and experimentally. 1 INTRODUCTION Learning in art.ificialneural networks has been studied based on a statist.ical frame work, because the statistical theory clarifies the quantitative relation between t.he empirical error and the prediction error. Let us consider a function <p( w; x) from the input space R/\ to the out.put space R L with a paramet.er 'lV. ""\i\Te assume that training samples {(.1:j, yd}~l are taken from t.he true probabilit.y density Q(x, y). Let us define the empirical error by (1) 293 294 Watanabe and the prediction error by JJ E(w) == lIy - ip(w; x)112Q(x, y)dxdy. (2) If we find a parameter w* w",Optimization & Theoretical ML,8d317bdcf4aafcfc22149d77babee96d-Paper.pdf,1993
A Computational Model,"We propose a trajectory planning and control theory for continuous movements such as connected cursive handwriting and continuous natural speech. Its hardware is based on our previously proposed forward-inverse-relaxation neural network (Wada & Kawato, 1993). Computationally, its optimization principle is the minimum torque change criterion. Regarding the representation level, hard constraints satisfied by a trajectory are represented as a set of via-points extracted from a handwritten character. Accordingly, we propose a via-point estimation algorithm that estimates via-points by repeating the trajectory formation of a character and the via-point extraction from the character. In experiments, good quantitative agreement is found between human handwriting data and the trajectories generated by the theory. Finally, we propose a recognition schema based on the movement generation. We show a result in which the recognition schema is applied to the handwritten character recognition and can",Reinforcement Learning,8e82ab7243b7c66d768f1b8ce1c967eb-Paper.pdf,1993
Discontinuous Generalization in Large,"The problem of learning from examples in multilayer networks is studied within the framework of statistical mechanics. Using the replica formalism we calculate the average generalization error of a fully connected committee machine in the limit of a large number of hidden units. If the number of training examples is proportional to the number of inputs in the network, the generalization error as a function of the training set size approaches a finite value. If the number of training examples is proportional to the number of weights in the network we find first-order phase transitions with a discontinuous drop in the generalization error for both binary and continuous weights. 1 INTRODUCTION Feedforward neural networks are widely used as nonlinear, parametric models for the solution of classification tasks and function approximation. Trained from examples of a given task, they are able to generalize, i.e. to compute the correct output for new, unknown inputs. Since the seminal work of G",Optimization & Theoretical ML,8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf,1993
Learning Temporal Dependencies in,"Hybrid connectionistfHMM systems model time both using a Markov chain and through properties of a connectionist network. In this paper, we discuss the nature of the time dependence currently employed in our systems using recurrent networks (RNs) and feed-forward multi-layer perceptrons (MLPs). In particular, we introduce local recurrences into a MLP to produce an enhanced input representation. This is in the form of an adaptive gamma filter and incorporates an automatic approach for learning temporal dependencies. We have experimented on a speaker independent phone recognition task using the TIMIT database. Results using the gamma filtered input representation have shown improvement over the baseline MLP system. Improvements have also been obtained through merging the baseline and gamma filter models. 1 INTRODUCTION The most common approach to large-vocabulary, talker-independent speech recognition has been statistical modelling with hidden Markov models (HMMs). The HMM has an explicit",NLP,92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf,1993
Inverse Dynamics,"Progress ha.s been made in comput.ational implementation of speech production based on physiological dat.a. An inverse dynamics model of the speech articulator's l1111sculo-skeletal system. which is the mapping from art.iculator t.rajectories to e\ectromyogl'aphic (EMG) signals, was modeled using the acquired forward dynamics model and temporal (smoot.hness of EMG activation) and range constraints. This inverse dynamics model allows the use of a faster speech mot.or control scheme, which can be applied to phoneme-to speech synthesis via musclo-skeletal system dynamics, or to future use in speech recognition. The forward acoustic model, which is the mapping from articulator trajectories t.o the acoustic parameters, was improved by adding velocity and voicing information inputs to distinguish acollst.ic paramet.er differences caused by changes in source characterist.ics. 1 INTRODUCTION Modeling speech articulator dynamics is important not only for speech science, but also for speech proc",NLP,912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf,1993
Surface Learning with Applications to,"Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition. 1 Surface Learning Mappings are an appropriate representation for systems whose variables naturally decompose into ""inputs"" and ""outputs)). To use a learned mapping, the input vari ables must be known and error-free and a single output value must be estimated for each input. Many tasks in vision, robotics, and control must maintain relationships between variabl",Deep Learning,96b9bff013acedfb1d140579e2fbeb63-Paper.pdf,1993
Odor Processing in the Bee: a Preliminary,"Based on precise anatomical data of the bee's olfactory system, we propose an investigation of the possible mechanisms of modulation and control between the two levels of olfactory information processing: the antennallobe glomeruli and the mushroom bodies. We use simplified neurons, but realistic architecture. As a first conclusion, we postulate that the feature extraction performed by the antennallobe (glomeruli and interneurons) necessitates central input from the mushroom bodies for fine tuning. The central input thus facilitates the evolution from fuzzy olfactory images in the glomerular layer towards more focussed images upon odor presentation.",NLP,959a557f5f6beb411fd954f3f34b21c3-Paper.pdf,1993
Figure of Merit Training for Detection and,"Spotting tasks require detection of target patterns from a background of richly varied non-target inputs. The performance measure of interest for these tasks, called the figure of merit (FOM), is the detection rate for target patterns when the false alarm rate is in an acceptable range. A new approach to training spotters is presented which computes the FOM gradient for each input pattern and then directly maximizes the FOM using b ackpropagati on. This eliminates the need for thresholds during training. It also uses network resources to model Bayesian a posteriori probability functions accurately only for patterns which have a significant effect on the detection accuracy over the false alarm rate of interest. FOM training increased detection accuracy by 5 percentage points for a hybrid radial basis function (RBF) - hidden Markov model (HMM) wordspotter on the credit-card speech corpus. 1 INTRODUCTION Spotting tasks require accurate detection of target patterns from a background of ric",Deep Learning,96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf,1993
Unknown Title,No abstract found,Cannot classify without abstract.,98d6f58ab0dafbb86b083a001561bb34-Paper.pdf,1993
"Autoencoders, Minimum Description Length","An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distri bution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this ",Deep Learning,9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf,1993
Dual Mechanisms for Neural Binding,"We propose that the binding and segmentation of visual features is mediated by two complementary mechanisms; a low resolu tion, spatial-based, resource-free process and a high resolution, temporal-based, resource-limited process. In the visual cortex, the former depends upon the orderly topographic organization in stri ate and extrastriate areas while the latter may be related to ob served temporal relationships between neuronal activities. Com puter simulations illustrate the role the two mechanisms play in figure/ ground discrimination, depth-from-occlusion, and the vivid ness of perceptual completion. 1 COMPLEMENTARY BINDING MECHANISMS The ""binding problem"" is a classic problem in computational neuroscience which considers how neuronal activities are grouped to create mental representations. For the case of visual processing, the binding of neuronal activities requires a mecha nism for selectively grouping fragmented visual features in order to construct the coherent representations",Computer Vision,a1d33d0dfec820b41b54430b50e96b5c-Paper.pdf,1993
The Statistical Mechanics of,"The satisfiability of random CNF formulae with precisely k vari ables per clause (""k-SAT"") is a popular testbed for the performance of search algorithms. Formulae have M clauses from N variables, randomly negated, keeping the ratio a = M / N fixed. For k = 2, = this model has been proven to have a sharp threshold at a 1 between formulae which are almost aways satisfiable and formulae which are almost never satisfiable as N Computer experi --jo 00. = ments for k 2, 3, 4, 5 and 6, (carried out in collaboration with B. Selman of ATT Bell Labs). show similar threshold behavior for each value of k. Finite-size scaling, a theory of the critical point phenomena used in statistical physics, is shown to characterize the size dependence near the threshold. Annealed and replica-based mean field theories give a good account of the results. ""Permanent address: IBM TJ Watson Research Center, Yorktown Heights, NY 10598 USA. (kirk@watson.ibm.com) Portions of this work were done while visiting the Salk",Optimization & Theoretical ML,a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf,1993
Lipreading by neural networks:,"We have developed visual preprocessing algorithms for extracting phonologically relevant features from the grayscale video image of a speaker, to provide speaker-independent inputs for an automat ic lipreading (""speechreading"") system. Visual features such as mouth open/closed, tongue visible/not-visible, teeth visible/not visible, and several shape descriptors of the mouth and its motion are all rapidly computable in a manner quite insensitive to lighting conditions. We formed a hybrid speechreading system consisting of two time delay neural networks (video and acoustic) and inte grated their responses by means of independent opinion pooling - the Bayesian optimal method given conditional independence, which seems to hold for our data. This hybrid system had an er ror rate 25% lower than that of the acoustic subsystem alone on a five-utterance speaker-independent task, indicating that video can be used to improve speech recognition. 1027 1028 Wolff, Prasad, Stork, and Hennecke 1 INTRO",Computer Vision,a67f096809415ca1c9f112d96d27689b-Paper.pdf,1993
Dynamic Modulation of Neurons and Networks,"Biological neurons have a variety of intrinsic properties because of the large number of voltage dependent currents that control their activity. Neuromodulatory substances modify both the balance of conductances that determine intrinsic properties and the strength of synapses. These mechanisms alter circuit dynamics, and suggest that functional circuits exist only in the modulatory environment in which they operate. 1 INTRODUCTION Many studies of artificial neural networks employ model neurons and synapses that are considerably simpler than their biological counterparts. A variety of motivations underly the use of simple models for neurons and synapses in artificial neural networks. Here, I discuss some of the properties of biological neurons and networks that are lost in overly simplified models of neurons and synapses. A fundamental principle in biological nervous systems is that neurons and networks operate over a wide range of time scales, and that these are modified by neuromodula",Optimization & Theoretical ML,a8e864d04c95572d1aece099af852d0a-Paper.pdf,1993
Training Neural Networks with,"We analyze how data with uncertain or missing input features can be incorporated into the training of a neural network. The gen eral solution requires a weighted integration over the unknown or uncertain input although computationally cheaper closed-form so lutions can be found for certain Gaussian Basis Function (GBF) networks. We also discuss cases in which heuristical solutions such as substituting the mean of an unknown input can be harmful. 1 INTRODUCTION The ability to learn from data with uncertain and missing information is a funda mental requirement for learning systems. In the ""real world"" , features are missing due to unrecorded information or due to occlusion in vision, and measurements are affected by noise. In some cases the experimenter might want to assign varying degrees of reliability to the data. In regression, uncertainty is typically attributed to the dependent variable which is assumed to be disturbed by additive noise. But there is no reason to assume that input ",Optimization & Theoretical ML,a8ecbabae151abacba7dbde04f761c37-Paper.pdf,1993
When Will a Genetic Algorithm,"We analyze a simple hill-climbing algorithm (RMHC) that was pre viously shown to outperform a genetic algorithm (GA) on a simple ""Royal Road"" function. We then analyze an ""idealized"" genetic algorithm (IGA) that is significantly faster than RMHC and that gives a lower bound for GA speed. We identify the features of the IGA that give rise to this speedup, and discuss how these features can be incorporated into a real GA. 1 INTRODUCTION Our goal is to understand the class of problems for which genetic algorithms (GA) are most suited, and in particular, for which they will outperform other search algorithms. Several studies have empirically compared GAs with other search and optimization methods such as simple hill-climbing (e.g., Davis, 1991), simulated annealing (e.g., Ingber & Rosen, 1992), linear, nonlinear, and integer programming techniques, and other traditional optimization techniques (e.g., De Jong, 1975). However, such comparisons typically compare one version of the GA with a s",Optimization & Theoretical ML,ab88b15733f543179858600245108dd8-Paper.pdf,1993
Unsupervised Parallel Feature Extraction,"We describe a number of learning rules that can be used to train un supervised parallel feature extraction systems. The learning rules are derived using gradient ascent of a quality function. We con sider a number of quality functions that are rational functions of higher order moments of the extracted feature values. We show that one system learns the principle components of the correla tion matrix. Principal component analysis systems are usually not optimal feature extractors for classification. Therefore we design quality functions which produce feature vectors that support unsu pervised classification. The properties of the different systems are compared with the help of different artificially designed datasets and a database consisting of all Munsell color spectra. 1 Introduction There are a number of unsupervised Hebbian learning algorithms (see Oja, 1992 and references therein) that perform some version of the Karhunen-Loeve expan sion. Our approach to unsupervised feature extr",Deep Learning,aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf,1993
Classification of Electroencephalogram using,"In this paper, we will consider the problem of classifying electroencephalo gram (EEG) signals of normal subjects, and subjects suffering from psychi atric disorder, e.g., obsessive compulsive disorder, schizophrenia, using a class of artificial neural networks, viz., multi-layer perceptron. It is shown that the multilayer perceptron is capable of classifying unseen test EEG signals to a high degree of accuracy. 1 Introduction The spontaneous electrical activity of the brain was first observed by Caton in 1875. Although considerable investigations on the electrical activity of the non-human brain have been undertaken, it was not until 1929 that a German neurologist Hans Berger first published studies on the electroencephalogram (EEG) recorded on the scalp of human. He lay the foundation of clinical and experimental applications of EEG between 1929 and 1938. Since then EEG signals have been used in both clinical and experimental work to discover the state which the brain is in (see e.g.",Deep Learning,addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf,1993
Comparisoll Training for a Resclleduling,"Airline companies usually schedule their flights and crews well in advance to optimize their crew pools activities. Many events such as flight delays or the absence of a member require the crew pool rescheduling team to change the initial schedule (rescheduling). In this paper, we show that the neural network comparison paradigm applied to the backgammon game by Tesauro (Tesauro and Se jnowski, 1989) can also be applied to the rescheduling problem of an aircrew pool. Indeed both problems correspond to choosing the best solut.ion from a set of possible ones without ranking them (called here best choice problem). The paper explains from a math ematical point of view the architecture and the learning strategy of the backpropagation neural network used for the best choice prob lem. We also show how the learning phase of the network can be accelerated. Finally we apply the neural network model to some real rescheduling problems for the Belgian Airline (Sabena). 1 Introduction Due to merges,",Reinforcement Learning,aeb3135b436aa55373822c010763dd54-Paper.pdf,1993
Bounds on the complexity of recurrent,"In this paper the efficiency of recurrent neural network implementa tions of m-state finite state machines will be explored. Specifically, it will be shown that the node complexity for the unrestricted case can be bounded above by 0 ( fo) . It will also be shown that the node complexity is 0 (y'm log m) when the weights and thresholds are restricted to the set {-I, I}, and 0 (m) when the fan-in is re stricted to two. Matching lower bounds will be provided for each of these upper bounds assuming that the state of the FSM can be 1. encoded in a subset of the nodes of size rlog m 1 Introduction The topic of this paper is understanding how efficiently neural networks scale to large problems. Although there are many ways to measure efficiency, we shall be concerned with node complexity, which as its name implies, is a calculation of the required number of nodes. Node complexity is a useful measure of efficiency since the amount of resources required to implement or even simulate a recurrent",Optimization & Theoretical ML,afda332245e2af431fb7b672a68b659d-Paper.pdf,1993
Classification of Multi-Spectral Pixels,"A new neural network, the Binary Diamond, is presented and its use as a classifier is demonstrated and evaluated. The network is of the feed-forward type. It learns from examples in the 'one shot' mode, and recruits new neurons as needed. It was tested on the problem of pixel classification, and performed well. Possible applications of the network in associative memories are outlined. 1 INTRODUCTION: CLASSIFICATION BY CLUES Classification is a process by which an item is assigned to a class. Classification is widely used in the animal kingdom. Identifying an item as food is classification. Assigning words to objects, actions, feelings, and situations is classification. The purpose of this work is to introduce a new neural network, the Binary Diamond, which can be used as a general purpose classification tool. The design and operational mode of the Binary Diamond are influenced by observations of the underlying mechanisms that take place in human classification processes. An item to be ",Computer Vision,aff1621254f7c1be92f64550478c56e6-Paper.pdf,1993
Optimal Brain Surgeon:,"We extend Optimal Brain Surgeon (OBS) - a second-order method for pruning networks - to allow for general error mea sures, and explore a reduced computational and storage implemen tation via a dominant eigenspace decomposition. Simulations on nonlinear, noisy pattern classification problems reveal that OBS does lead to improved generalization, and performs favorably in comparison with Optimal Brain Damage (OBD). We find that the required retraining steps in OBD may lead to inferior generaliza tion, a result that can be interpreted as due to injecting noise back into the system. A common technique is to stop training of a large network at the minimum validation error. We found that the test error could be reduced even further by means of OBS (but not OBD) pruning. Our results justify the t ~ 0 approximation used in OBS and indicate why retraining in a highly pruned network may lead to inferior performance. 263 264 Hassibi, Stork, Wolff, and Watanabe 1 INTRODUCTION The fundamental theory",Optimization & Theoretical ML,b056eb1587586b71e2da9acfe4fbd19e-Paper.pdf,1993
Agnostic PAC-Learning of Functions on,"There exist a number of negative results ([J), [BR), [KV]) about learning on neural nets in Valiant's model [V) for probably approx imately correct learning (""PAC-learning""). These negative results are based on an asymptotic analysis where one lets the number of nodes in the neural net go to infinit.y. Hence this analysis is less ad equate for the investigation of learning on a small fixed neural net. with relatively few analog inputs (e.g. the principal components of some sensory data). The latter type of learning problem gives rise to a different kind of asymptotic question: Can the true error of the neural net be brought arbitrarily close to that of a neural net with ""optimal"" weights through sufficiently long training? In this paper we employ some new arguments ill order to give a positive answer to this question in Haussler's rather realistic refinement of Valiant's model for PAC-learning ([H), [KSS)). In this more realistic model no a-priori assumptions are required about the ""le",Optimization & Theoretical ML,b0b183c207f46f0cca7dc63b2604f5cc-Paper.pdf,1993
Mixtures of Controllers for,"We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multi ple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior. 1 Introduction Many stationary dynamic systems exhibit significantly different behaviors under different operating conditions. To control such complex systems it is computation ally more efficient to decompose the problem into smaller subtasks, with different control strategies for different operating points. When detailed information about the plant is available, gain scheduling has proven a successful method for designing a global control (Shamma and Athans, 1992). The system is partitioned by choosing several operating points and a lin",Reinforcement Learning,b137fdd1f79d56c7edf3365fea7520f2-Paper.pdf,1993
A Hybrid Radial Basis Function Neurocomputer,"A neurocomputer was implemented using radial basis functions and a combination of analog and digital VLSI circuits. The hybrid system uses custom analog circuits for the input layer and a digital signal processing board for the hidden and output layers. The system combines the advantages of both analog and digital circuits. featuring low power consumption while minimizing overall system error. The analog circuits have been fabricated and tested, the system has been built, and several applications have been executed on the system. One application provides significantly better results for a remote sensing problem than have been previously obtained using conventional methods. 1.0 Introduction This paper describes a neurocomputer development system that uses a radial basis function as the transfer function of a neuron rather than the traditional sigmoid function. This neurocOOlputer is a hybrid system which has been implemented with a combination of analog and digital VLSI technologies. It",Optimization & Theoretical ML,b6edc1cd1f36e45daf6d7824d7bb2283-Paper.pdf,1993
Resolving motion ambiguities,"We address the problem of optical flow reconstruction and in par ticular the problem of resolving ambiguities near edges. They oc cur due to (i) the aperture problem and (ii) the occlusion problem, where pixels on both sides of an intensity edge are assigned the same velocity estimates (and confidence). However, these measurements are correct for just one side of the edge (the non occluded one). Our approach is to introduce an uncertamty field with respect to the estimates and confidence measures. We note that the confi dence measures are large at intensity edges and larger at the con vex sides of the edges, i.e. inside corners, than at the concave side. We resolve the ambiguities through local interactions via coupled Markov random fields (MRF). The result is the detection of motion for regions of images with large global convexity. 1 Introduction In this paper we discuss the problem of figure ground separation, via optical flow, for homogeneous images (textured images just provide mo",Computer Vision,b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf,1993
Developing Population Codes By,"The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representa tion that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center ofthis bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allow ing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs. Most existing unsupervised learning algor",Optimization & Theoretical ML,b86e8d03fe992d1b0e19656875ee557c-Paper.pdf,1993
Learning in Computer Vision and Image,No abstract found,Computer Vision,beb22fb694d513edcf5533cf006dfeae-Paper.pdf,1993
Neural Network Definitions of Highly,"We use two co-evolving neural networks to determine new classes of protein secondary structure which are significantly more pre dictable from local amino sequence than the conventional secondary structure classification. Accurate prediction of the conventional secondary structure classes: alpha helix, beta strand, and coil, from primary sequence has long been an important problem in compu tational molecular biology. Neural networks have been a popular method to attempt to predict these conventional secondary struc ture classes. Accuracy has been disappointingly low. The algo rithm presented here uses neural networks to similtaneously exam ine both sequence and structure data, and to evolve new classes of secondary structure that can be predicted from sequence with significantly higher accuracy than the conventional classes. These new classes have both similarities to, and differences with the con ventional alpha helix, beta strand and coil. 809 810 Lapedes, Steeg, and Farber The conven",NLP,bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf,1993
Coupled Dynamics of Fast Neurons and,"A simple model of coupled dynamics of fast neurons and slow inter actions, modelling self-organization in recurrent neural networks, leads naturally to an effective statistical mechanics characterized by a partition function which is an average over a replicated system. This is reminiscent of the replica trick used to study spin-glasses, but with the difference that the number of replicas has a physi cal meaning as the ratio of two temperatures and can be varied throughout the whole range of real values. The model has inter esting phase consequences as a function of varying this ratio and external stimuli, and can be extended to a range of other models. 1 A SIMPLE MODEL WITH FAST DYNAMIC NEURONS AND SLOW DYNAMIC INTERACTIONS As the basic archetypal model we consider a system of Ising spin neurons (J'i E {-I, I}, i E {I, ... , N}, interacting via continuous-valued symmetric interactions, Iij, which themselves evolve in response to the states of the neurons. The neurons are taken to have",Optimization & Theoretical ML,c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf,1993
U sing Local Trajectory Optimizers To,"Dynamic programming provides a methodology to develop planners and controllers for nonlinear systems. However, general dynamic programming is computationally intractable. We have developed procedures that allow more complex planning and control problems to be solved. We use second order local trajectory optimization to generate locally optimal plans and local models of the value function and its derivatives. We maintain global consistency of the local models of the value function, guaranteeing that our locally optimal plans are actually globally optimal, up to the resolution of our search procedures. Learning to do the right thing at each instant in situations that evolve over time is difficult, as the future cost of actions chosen now may not be obvious immediately, and may only become clear with time. Value functions are a representational tool that makes the consequences of actions explicit. Value functions are difficult to learn directly, but they can be built up from learned model",Optimization & Theoretical ML,c15da1f2b5e5ed6e6837a3802f0d1593-Paper.pdf,1993
Connectionist Modeling and,No abstract found,Optimization & Theoretical ML,c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf,1993
Structural and Behavioral Evolution,"This paper introduces GNARL, an evolutionary program which induces recurrent neural networks that are structurally unconstrained. In contrast to constructive and destructive algorithms, GNARL employs a popula tion of networks and uses a fitness function's unsupervised feedback to guide search through network space. Annealing is used in generating both gaussian weight changes and structural modifications. Applying GNARL to a complex search and collection task demonstrates that the system is capable of inducing networks with complex internal dynamics. 1 INTRODUCTION A variety of methods to induce network architecture exist. Some start with a very simple network and incrementally add nodes and links (Hanson 1990; Fahlman & Lebiere, 1990; Fahlman 1991; Chen, et aI., 1993); others start with a large network and then prune off superfluous pieces (Mozer & Smolensky, 1989; Cun, Denker, and SolI a, 1990; Hassibi & Stork, 1993; amlin & Giles, 1993). But these constructive and destructive algorit",Reinforcement Learning,c8ed21db4f678f3b13b9d5ee16489088-Paper.pdf,1993
Two-Dimensional Object Localization by,"We present a Mean Field Theory method for locating two dimensional objects that have undergone rigid transformations. The resulting algorithm is a form of coarse-to-fine correlation matching. We first consider problems of matching synthetic point data, and derive a point matching objective function. A tractable line segment matching objective function is derived by considering each line segment as a dense collection of points, and approximat ing it by a sum of Gaussians. The algorithm is tested on real images from which line segments are extracted and matched. 1 Introduction Assume that an object in a scene can be viewed as an instance of the model placed in space by some spatial transformation, and object recognition is achieved by dis covering an instance of the model in the scene. Two tightly coupled subproblems need to be solved for locating and recognizing the model: the correspondence prob lem (how are scene features put into correspondence with model features?), and the localiza",Computer Vision,ca8155f4d27f205953f9d3d7974bdd70-Paper.pdf,1993
"Neurobiology, Psychophysics, and",No abstract found,Optimization & Theoretical ML,ce78d1da254c0843eb23951ae077ff5f-Paper.pdf,1993
Rather Than Weights,"The conventional Bayesian justification of backprop is that it finds the MAP weight vector. As this paper shows, to find the MAP i-o function instead one must add a correction tenn to backprop. That tenn biases one towards i-o functions with small description lengths, and in particular fa vors (some kinds of) feature-selection, pruning, and weight-sharing. 1 INTRODUCTION In the conventional Bayesian view ofbackpropagation (BP) (Buntine and Weigend, 1991; Nowlan and Hinton,1994; MacKay,I992; Wolpert, 1993), one starts with the ""likelihood"" conditional distribution P(training set = t I weight vector w) and the ""prior"" distribution P(w). As an example, in regression one might have a ""Gaussian likelihood"", P(t I w) oc: exp[-x2(w, t)] == I1i exp [-(net(w, tx(i» - ty(i) )2/2c?] for some constant CJ. (tx(i) and ty(i) are the successive input and output values in the training set respectively, and net(w, .) is the function, induced by w, taking input neuron values to output neuron values.) As ",Optimization & Theoretical ML,d4c2e4a3297fe25a71d030b67eb83bfc-Paper.pdf,1993
Fast Non-Linear Dimension Reduction,"We present a fast algorithm for non-linear dimension reduction. The algorithm builds a local linear model of the data by merging PCA with clustering based on a new distortion measure. Exper iments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto-associative networks. The local linear algorithm is also more than an order of magnitude faster to train. 1 Introduction Feature sets can be more compact than the data they represent. Dimension reduc tion provides compact representations for storage, transmission, and classification. Dimension reduction algorithms operate by identifying and eliminating statistical redundancies in the data. The optimal linear technique for dimension reduction is principal component anal ysis (PCA). PCA performs dimension reduction by projecting the original n dimensional data onto the m < n dimensional linear subspace spanned by the leading eigenvectors of the data's ",Deep Learning,d554f7bb7be44a7267068a7df88ddd20-Paper.pdf,1993
WATTLE: A Trainable Gain Analogue,"This paper describes a low power analogue VLSI neural network called Wattle. Wattle is a 10:6:4 three layer perceptron with multi plying DAC synapses and on chip switched capacitor neurons fabri cated in 1.2um CMOS. The on chip neurons facillitate variable gain per neuron and lower energy/connection than for previous designs. The intended application of this chip is Intra Cardiac Electrogram classification as part of an implantable pacemaker / defibrillator sys tem. Measurements of t.he chip indicate that 10pJ per connection is achievable as part of an integrated system. Wattle has been suc cessfully trained in loop on parity 4 and ICEG morphology classi fication problems. 1 INTRODUCTION A three layer analogue VLSI perceptron has been previously developed by [Leong and Jabri, 1993]. This chip named Kakadu uses 6 bit digital weight storage, multiplying DACs in the synapses and fixed value off chip resistive neurons. The chip described in this paper called Wattle has the same synapse arr",Computer Vision,ccb0989662211f61edae2e26d58ea92f-Paper.pdf,1993
Observability of Neural Network,"We prove that except possibly for small exceptional sets, discrete time analog neural nets are globally observable, i.e. all their cor rupted pseudo-orbits on computer simulations actually reflect the true dynamical behavior of the network. Locally finite discrete (boolean) neural networks are observable without exception. 1 INTRODUCTION We address some aspects of the general problem of implementation and robustness of (mainly recurrent) autonomous discrete-time neural networks with continuous activation (herein referred to as analog networks) and discrete activation (herein, boolean networks). There are three main sources of perturbations from ideal oper ation in a neural network. First, the network's parameters may have been contam inated with noise from external sources. Second, the network is being implemented in optics or electronics (digital or analog) and inherent measurement limitations preclude the use of perfect information on the network's parameters. Third, as has been the ",Optimization & Theoretical ML,d58072be2820e8682c0a27c0518e805e-Paper.pdf,1993
Optimal signalling in Attractor Neural,"In [Meilijson and Ruppin, 1993] we presented a methodological framework describing the two-iteration performance of Hopfield like attractor neural networks with history-dependent, Bayesian dynamics. We now extend this analysis in a number of directions: input patterns applied to small subsets of neurons, general con nectivity architectures and more efficient use of history. We show that the optimal signal (activation) function has a slanted sigmQidal shape, and provide an intuitive account of activation functions with a non-monotone shape. This function endows the model with some properties characteristic of cortical neurons' firing. 1 Introduction It is well known that a given cortical neuron can respond with a different firing pat tern for the same synaptic input, depending on its firing history and on the effects of modulator transmitters (see [Connors and Gutnick, 1990] for a review). The time span of different channel conductances is very broad, and the influence of some ionic cur",Optimization & Theoretical ML,d5cfead94f5350c12c322b5b664544c1-Paper.pdf,1993
Neural Network Exploration Using,"Consider the problem of learning input/output mappings through exploration, e.g. learning the kinematics or dynamics of a robotic manipulator. If actions are expensive and computation is cheap, then we should explore by selecting a trajectory through the in put space which gives us the most amount of information in the fewest number of steps. I discuss how results from the field of opti mal experiment design may be used to guide such exploration, and demonstrate its use on a simple kinematics problem. 1 Introduction Most machine learning research treats the learner as a passive receptacle for data to be processed. This approach ignores the fact that, in many situations, a learner is able, and sometimes required, to act on its environment to gather data. Learning control inherently involves being active; the controller must act in order to learn the result of its action. When training a neural network to control a robotic arm, one may explore by allowing the controller to ""flail"" for a ",Optimization & Theoretical ML,d840cc5d906c3e9c84374c8919d2074e-Paper.pdf,1993
Directional Hearing by the Mauthner,"We provide a computational description of the function of the Mau thner system. This is the brainstem circuit which initiates fast start escapes in teleost fish in response to sounds. Our simula tions, using back propagation in a realistically constrained feedfor ward network, have generated hypotheses which are directly inter pretable in terms of the activity of the auditory nerve fibers, the principle cells of the system and their associated inhibitory neu rons. 1 INTRODUCTION 1.1 THE M.AUTHNER SYSTEM Much is known about the brainstem system that controls fast-start escapes in teleost fish. The most prominent feature of this network is the pair of large Mauthner cells whose axons cross the midline and descend down the spinal cord to synapse on primary motoneurons. The Mauthner system also includes inhibitory neurons, the PHP cells, which have a unique and intense field effect inhibition at the spike initiating zone of the Mauthner cells (Faber and Korn, 1978). The Mauthner system is ",NLP,da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf,1993
Asynchronous Dynamics of Continuous,"Motivated by mathematical modeling, analog implementation and distributed simulation of neural networks, we present a definition of asynchronous dynamics of general CT dynamical systems defined by ordinary differential equations, based on notions of local times and communication times. We provide some preliminary results on globally asymptotical convergence of asynchronous dynamics for contractive and monotone CT dynamical systems. When ap plying the results to neural networks, we obtain some conditions that ensure additive-type neural networks to be asynchronizable. 1 INTRODUCTION Neural networks are massively distributed computing systems. A major issue in par allel and distributed computation is synchronization versus asynchronization (Bert sekas and Tsitsiklis, 1989). To fix our idea, we consider a much studied additive-type model (Cohen and Grossberg, 1983; Hopfield, 1984; Hirsch, 1989) of a continuous time (CT) neural network of n neurons, whose dynamics is governed by Ln = + + =",Optimization & Theoretical ML,dc5689792e08eb2e219dce49e64c885b-Paper.pdf,1993
Development of Orientation and Ocular,"Maps of orientation preference and ocular dominance were recorded optically from the cortices of 5 infant macaque monkeys, ranging in age from 3.5 to 14 weeks. In agreement with previous observations, we found that basic features of orientation and ocular dominance maps, as well as correlations between them, are present and robust by 3.5 weeks of age. We did observe changes in the strength of ocular dominance signals, as well as in the spacing of ocular dom inance bands, both of which increased steadily between 3.5 and 14 weeks of age. The latter finding suggests that the adult spacing of ocular dominance bands depends on cortical growth in neonatal animals. Since we found no corresponding increase in the spacing of orientation preferences, however, there is a possibility that the orientation preferences of some cells change as the cortical surface expands. Since correlations between the patterns of orientation selectivity and ocular dominance are present at an age, when the visual sys",Computer Vision,dd45045f8c68db9f54e70c67048d32e8-Paper.pdf,1993
Counting function theorem for,"x We show that a randomly selected N-tuple of points ofRn with probability> 0 is such that any multi-layer percept ron with the first hidden layer composed of hi threshold logic units can imple- ment exactly 2 2:~~~ ( Nil) different dichotomies of x. If N > hin then such a perceptron must have all units of the first hidden layer fully connected to inputs. This implies the maximal capacities (in the sense of Cover) of 2n input patterns per hidden unit and 2 input patterns per synaptic weight of such networks (both capacities are achieved by networks with single hidden layer and are the same as for a single neuron). Comparing these results with recent estimates of VC-dimension we find that in contrast to the single neuron case, for sufficiently large nand hl, the VC-dimension exceeds Cover's capacity. 1 Introduction In the course of theoretical justification of many of the claims made about neural networks regarding their ability to learn a set of patterns and their ability to gen eralis",Optimization & Theoretical ML,df263d996281d984952c07998dc54358-Paper.pdf,1993
Supervised Learning with Growing Cell,"We present a new incremental radial basis function network suit able for classification and regression problems. Center positions are continuously updated through soft competitive learning. The width of the radial basis functions is derived from the distance to topological neighbors. During the training the observed error is accumulated locally and used to determine where to insert the next unit. This leads (in case of classification problems) to the placement of units near class borders rather than near frequency peaks as is done by most existing methods. The resulting networks need few training epochs and seem to generalize very well. This is demonstrated by examples. 1 INTRODUCTION Feed-forward networks of localized (e.g., Gaussian) units are an interesting alter native to the more frequently used networks of global (e.g., sigmoidal) units. It has been shown that with localized units one hidden layer suffices in principle to approximate any continuous function, whereas with sigmoida",Reinforcement Learning,df7f28ac89ca37bf1abd2f6c184fe1cf-Paper.pdf,1993
A Learning Analog Neural Network Chip,"We present experimental results on supervised learning of dynam ical features in an analog VLSI neural network chip. The recur rent network, containing six continuous-time analog neurons and 42 free parameters (connection strengths and thresholds), is trained to generate time-varying outputs approximating given periodic signals presented to the network. The chip implements a stochastic pertur bative algorithm, which observes the error gradient along random directions in the parameter space for error-descent learning. In ad dition to the integrated learning functions and the generation of pseudo-random perturbations, the chip provides for teacher forc ing and long-term storage of the volatile parameters. The network learns a 1 kHz circular trajectory in 100 sec. The chip occupies 2mm x 2mm in a 2JLm CMOS process, and dissipates 1.2 m W. 1 Introduction Exact gradient-descent algorithms for supervised learning in dynamic recurrent net works [1-3] are fairly complex and do not provide for ",Computer Vision,e07413354875be01a996dc560274708e-Paper.pdf,1993
Functional Models of Selective Attention,No abstract found,NLP,e0cf1f47118daebc5b16269099ad7347-Paper.pdf,1993
Learning Classification with Unlabeled Data,"One of the advantages of supervised learning is that the final error met ric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortu nately, when modeling human learning or constructing classifiers for au tonomous robots, supervisory labels are often not available or too ex pensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sen sory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding ap propriate placement for the codebook vectors particularly when the con fuseable classes are different for the two modalities. 1 INT",Deep Learning,e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf,1993
Temporal Difference Learning of,"The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spa tiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by their knowledge-intensive nature. We demonstrate a viable alternative by training networks to evaluate Go positions via tem poral difference (TD) learning. Our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play. These techniques yield far better performance than undifferentiated networks trained by self play alone. A network with less than 500 weights learned within 3,000 games of 9x9 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level. 1 INTRODUCTION Go was developed three to four millenia ",Reinforcement Learning,e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf,1993
Recovering a Feed-Forward Net,"We study feed-forward nets with arbitrarily many layers, using the stan dard sigmoid, tanh x. Aside from technicalities, our theorems are:",Optimization & Theoretical ML,e49b8b4053df9505e1f48c3a701c0682-Paper.pdf,1993
Optimal Stochastic Search and,"Stochastic optimization algorithms typically use learning rate schedules that behave asymptotically as J.t(t) = J.to/t. The ensem ble dynamics (Leen and Moody, 1993) for such algorithms provides an easy path to results on mean squared weight error and asymp totic normality. We apply this approach to stochastic gradient algorithms with momentum. We show that at late times, learning = is governed by an effective learning rate J.tejJ J.to/(l - f3) where f3 is the momentum parameter. We describe the behavior of the asymptotic weight error and give conditions on J.tejJ that insure optimal convergence speed. Finally, we use the results to develop an adaptive form of momentum that achieves optimal convergence speed independent of J.to. 1 Introduction The rate of convergence for gradient descent algorithms, both batch and stochastic, can be improved by including in the weight update a ""momentum"" term propor tional to the previous weight update. Several authors (Tugay and Tanik, 1989; Shynk and",Optimization & Theoretical ML,e57c6b956a6521b28495f2886ca0977a-Paper.pdf,1993
The Parti-game Algorithm for Variable,"Parti-game is a new algorithm for learning from delayed rewards in high dimensional real-valued state-spaces. In high dimensions it is essential that learning does not explore or plan over state space uniformly. Part i-game maintains a decision-tree partitioning of state-space and applies game-theory and computational geom etry techniques to efficiently and reactively concentrate high reso lution only on critical areas. Many simulated problems have been tested, ranging from 2-dimensional to 9-dimensional state-spaces, including mazes, path planning, non-linear dynamics, and uncurl ing snake robots in restricted spaces. In all cases, a good solution is found in less than twenty trials and a few minutes. 1 REINFORCEMENT LEARNING Reinforcement learning [Samuel, 1959, Sutton, 1984, Watkins, 1989, Barto et al., 1991] is a promising method for control systems to program and improve themselves. This paper addresses its biggest stumbling block: the curse of dimensionality [Bell man, 1957], in ",Reinforcement Learning,e94550c93cd70fe748e6982b3439ad3b-Paper.pdf,1993
Complexity Issues in Neural,No abstract found,Optimization & Theoretical ML,e97ee2054defb209c35fe4dc94599061-Paper.pdf,1993
Stability and Observability,No abstract found,Optimization & Theoretical ML,e995f98d56967d946471af29d7bf99f1-Paper.pdf,1993
The Role of MT Neuron Receptive Field,"The goal of this work was to investigate the role of primate MT neurons in solving the structure from motion (SFM) problem. Three types of receptive field (RF) surrounds found in area MT neurons (K.Tanaka et al.,1986; Allman et al.,1985) correspond, as our analysis suggests, to the oth, pt and 2nd order fuzzy space-differential operators. The large surround/center radius ratio (;::: 7) allows both differentiation of smooth velocity fields and discontinuity detection at boundaries of objects. The model is in agreement with recent psychophysical data on surface interpolation involvement in SFM. We suggest that area MT partially segregates information about object shape from information about spatial relations necessary for navigation and manipulation. 1 INTRODUCTION Both neurophysiological investigations [8] and lesioned human patients' data show that the Middle Temporal (MT) cortical area is crucial to perceiving three-dimensional shape in moving stimuli. On the other hand, 969 970 Bura",Computer Vision,ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf,1993
Estimating analogical similarity by dot-products,"Models of analog retrieval require a computationally cheap method of estimating similarity between a probe and the candidates in a large pool of memory items. The vector dot-product operation would be ideal for this purpose if it were possible to encode complex structures as vector representations in such a way that the superficial similarity of vector representations reflected underlying structural similarity. This paper de scribes how such an encoding is provided by Holographic Reduced Rep resentations (HRRs), which are a method for encoding nested relational structures as fixed-width distributed representations. The conditions un der which structural similarity is reflected in the dot-product rankings of HRRs are discussed. 1 INTRODUCTION Gentner and Markman (1992) suggested that the ability to deal with analogy will be a ""Watershed or Waterloo"" for connectionist models. They identified ""structural alignment"" as the central aspect of analogy making. They noted the apparent ease with",NLP,edfbe1afcf9246bb0d40eb4d8027d90f-Paper.pdf,1993
Segmental Neural Net Optimization for Continuous Speech,"Previously, we had developed the concept of a Segmental Neural Net (SNN) for phonetic modeling in continuous speech recognition (CSR). This kind of neu ral network technology advanced the state-of-the-art of large-vocabulary CSR, which employs Hidden Marlcov Models (HMM), for the ARPA 1oo0-word Re source Management corpus. More Recently, we started porting the neural net system to a larger, more challenging corpus - the ARPA 20,Ooo-word Wall Street Journal (WSJ) corpus. During the porting, we explored the following research directions to refine the system: i) training context-dependent models with a reg ularization method; ii) training SNN with projection pursuit; and ii) combining different models into a hybrid system. When tested on both a development set and an independent test set, the resulting neural net system alone yielded a per fonnance at the level of the HMM system, and the hybrid SNN/HMM system achieved a consistent 10-15% word error reduction over the HMM system. This pape",Deep Learning,eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf,1993
Grammatical Inference by,"We show how an ""Elman"" network architecture, constructed from recurrently connected oscillatory associative memory network mod ules, can employ selective ""attentional"" control of synchronization to direct the flow of communication and computation within the architecture to solve a grammatical inference problem. Previously we have shown how the discrete time ""Elman"" network algorithm can be implemented in a network completely described by continuous ordinary differential equations. The time steps (ma chine cycles) of the system are implemented by rhythmic variation (clocking) of a bifurcation parameter. In this architecture, oscilla tion amplitude codes the information content or activity of a mod ule (unit), whereas phase and frequency are used to ""softwire"" the network. Only synchronized modules communicate by exchang ing amplitude information; the activity of non-resonating modules contributes incoherent crosstalk noise. Attentional control is modeled as a special subset of the hidde",NLP,f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf,1993
Bayesian Modeling and Classification of,"Signal processing and classification algorithms often have limited applicability resulting from an inaccurate model of the signal's un derlying structure. We present here an efficient, Bayesian algo rithm for modeling a signal composed of the superposition of brief, Poisson-distributed functions. This methodology is applied to the specific problem of modeling and classifying extracellular neural waveforms which are composed of a superposition of an unknown number of action potentials CAPs). Previous approaches have had limited success due largely to the problems of determining the spike shapes, deciding how many are shapes distinct, and decomposing overlapping APs. A Bayesian solution to each of these problems is obtained by inferring a probabilistic model of the waveform. This approach quantifies the uncertainty of the form and number of the inferred AP shapes and is used to obtain an efficient method for decomposing complex overlaps. This algorithm can extract many times more informa",NLP,f1c1592588411002af340cbaedd6fc33-Paper.pdf,1993
Supervised learning from incomplete,"Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. VVe use mixture models for the den sity estimates and make two distinct appeals to the Expectation Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm-EM is used both for the estimation of mix ture components and for coping wit.h missing dat.a. The result ing algorithm is applicable t.o a wide range of supervised as well as unsupervised learning problems. Result.s from a classification benchmark-t.he iris data set-are presented. 1 Introduction Adaptive systems generally operate in environments t.hat are fraught with imper fections; nonet.heless they must cope with these imperfections and learn to extract as much relevant information as needed for their part.icular goals. One form of imperfection is incomplet.eness in sensing informa",'Optimization & Theoretical ML',f2201f5191c4e92cc5af043eebfd0946-Paper.pdf,1993
Correlation Functions in a Large,"Most theoretical investigations of large recurrent networks focus on the properties of the macroscopic order parameters such as popu lation averaged activities or average overlaps with memories. How ever, the statistics of the fluctuations in the local activities may be an important testing ground for comparison between models and observed cortical dynamics. We evaluated the neuronal cor relation functions in a stochastic network comprising of excitatory and inhibitory populations. We show that when the network is in a stationary state, the cross-correlations are relatively weak, i.e., their amplitude relative to that of the auto-correlations are of or der of 1/N , N being the size of the interacting population. This holds except in the neighborhoods of bifurcations to nonstationary states. As a bifurcation point is approached the amplitude of the cross-correlations grows and becomes of order 1 and the decay time constant diverges. This behavior is analogous to the phenomenon of critic",Optimization & Theoretical ML,f7e9050c92a851b0016442ab604b0488-Paper.pdf,1993
Clustering with a Domain-Specific,"With a point matching distance measure which is invariant under translation, rotation and permutation, we learn 2-D point-set ob jects, by clustering noisy point-set images. Unlike traditional clus tering methods which use distance measures that operate on feature vectors - a representation common to most problem domains - this object-based clustering technique employs a distance measure spe cific to a type of object within a problem domain. Formulating the clustering problem as two nested objective functions, we derive optimization dynamics similar to the Expectation-Maximization algorithm used in mixture models. 1 Introduction Clustering and related unsupervised learning techniques such as competitive learn ing and self-organizing maps have traditionally relied on measures of distance, like Euclidean or Mahalanobis distance, which are generic across most problem domains. Consequently, when working in complex domains like vision, extensive preprocess ing is required to produce feature",Computer Vision,f9028faec74be6ec9b852b0a542e2f39-Paper.pdf,1993
SPEAKER RECOGNITION USING,"A new classifier is presented for text-independent speaker recognition. The new classifier is called the modified neural tree network (MNTN). The NTN is a hierarchical classifier that combines the properties of decision trees and feed-forward neural networks. The MNTN differs from the stan dard NTN in that a new learning rule based on discriminant learning is used, which minimizes the classification error as opposed to a norm of the approximation error. The MNTN also uses leaf probability mea sures in addition to the class labels. The MNTN is evaluated for several speaker identification experiments and is compared to multilayer percep trons (MLPs), decision trees, and vector quantization (VQ) classifiers. The VQ classifier and MNTN demonstrate comparable performance and per form significantly better than the other classifiers for this task. Addition ally, the MNTN provides a logarithmic saving in retrieval time over that of the VQ classifier. The MNTN and VQ classifiers are also compar",NLP,f9a40a4780f5e1306c46f1c8daecee3b-Paper.pdf,1993
Tonal Music as a Componential Code:,"This study explores the extent to which a network that learns the temporal relationships within and between the component features of Western tonal music can account for music theoretic and psychological phenomena such as the tonal hierarchy and rhythmic expectancies. Predicted and generated sequences were recorded as the representation of a 153-note waltz melody was learnt by a predictive, recurrent network. The network learned transitions and relations between and within pitch and timing components: accent and duration values interacted in the development of rhythmic and metric structures and, with training, the network developed chordal expectancies in response to the activation of individual tones. Analysis of the hidden unit representation revealed that musical sequences are represented as transitions between states in hidden unit space. 1 INTRODUCTION The fundamental features of music, derivable from frequency, time and amplitude dimensions of the physical signal, can be describe",NLP,fa14d4fe2f19414de3ebd9f63d5c0169-Paper.pdf,1993
Connectionist Models for,"Although the visual and auditory systems share the same basic tasks of informing an organism about its environment, most con nectionist work on hearing to date has been devoted to the very different problem of speech recognition. VVe believe that the most fundamental task of the auditory system is the analysis of acoustic signals into components corresponding to individual sound sources, which Bregman has called auditory scene analysis. Computational and connectionist work on auditory scene analysis is reviewed, and the outline of a general model that includes these approaches is described. 1 INTRODUCTION The primary task of any perceptual system is to tell us about the external world. The primary problem is that the sensory inputs provide too much data and too little information. A perceptual system must glean from the flood of incomplete, noisy, redundant and constantly changing streams of data those invariant properties that reveal important objects and events in the environment. Fo",Computer Vision,fa3a3c407f82377f55c19c5d403335c7-Paper.pdf,1993
Learning in Compositional Hierarchies:,"I propose a learning algorithm for learning hierarchical models for ob ject recognition. The model architecture is a compositional hierarchy that represents part-whole relationships: parts are described in the lo cal context of substructures of the object. The focus of this report is learning hierarchical models from data, i.e. inducing the structure of model prototypes from observed exemplars of an object. At each node in the hierarchy, a probability distribution governing its parameters must be learned. The connections between nodes reflects the structure of the object. The formulation of substructures is encouraged such that their parts become conditionally independent. The resulting model can be interpreted as a Bayesian Belief Network and also is in many respects similar to the stochastic visual grammar described by Mjolsness. 1 INTRODUCTION Model-based object recognition solves the problem of invariant recognition by relying on stored prototypes at unit scale positioned at the or",Computer Vision,fa83a11a198d5a7f0bf77a1987bcd006-Paper.pdf,1993
VLSI Phase Locking Architectures for,"Recent physiological research has shown that synchronization of oscillatory responses in striate cortex may code for relationships between visual features of objects. A VLSI circuit has been de signed to provide rapid phase-locking synchronization of multiple oscillators to allow for further exploration of this neural mechanism. By exploiting the intrinsic random transistor mismatch of devices operated in subthreshold, large groups of phase-locked oscillators can be readily partitioned into smaller phase-locked groups. A mUltiple target tracker for binary images is described utilizing this phase-locking architecture. A VLSI chip has been fabricated and tested to verify the architecture. The chip employs Pulse Ampli tude Modulation (PAM) to encode the output at the periphery of the system. 1 Introduction In striate cortex, visual information coming from the retina (via the lateral genic ulate nuclei) is processed to extract retinotopic maps of visual features. Some cells in cortex are r",Computer Vision,fb89705ae6d743bf1e848c206e16a1d7-Paper.pdf,1993
A Network Mechanism for the Determination of,"We propose a computational model for how the cortex discriminates shape and depth from texture. The model consists of four stages: (1) extraction of local spatial frequency, (2) frequency characterization, (3) detection of texture compression by normalization, and (4) integration of the normalized frequency over space. The model accounts for a number of psychophysical observations including experiments based on novel random textures. These textures are generated from white noise and manipulated in Fourier domain in order to produce specific frequency spectra. Simulations with a range of stimuli, including real images, show qualitative and quantitative agreement with human perception. 1 INTRODUCTION There are several physical cues to shape and depth which arise from changes in projection as a surface curves away from view, or recedes in perspective. One major cue is the orderly change in the spatial frequency distribution of texture along the surface. In machine vision approaches, vario",Computer Vision,fc3cf452d3da8402bebb765225ce8c0e-Paper.pdf,1993
Encoding Labeled Graphs by Labeling,"In this paper we propose an extension to the RAAM by Pollack. This extension, the Labeling RAAM (LRAAM), can encode la beled graphs with cycles by representing pointers explicitly. Data encoded in an LRAAM can be accessed by pointer as well as by content. Direct access by content can be achieved by transform ing the encoder network of the LRAAM into an analog Hopfield network with hidden units. Different access procedures can be defined depending on the access key. Sufficient conditions on the asymptotical stability of the associated Hopfield network are briefly introduced. 1 INTRODUCTION In the last few years, several researchers have tried to demonstrate how symbolic structures such as lists, trees, and stacks can be represented and manipulated in a connectionist system, while still preserving all the computational characteristics of connectionism (and extending them to the symbolic representations) (Hinton, 1990; Plate, 1991; Pollack, 1990; Smolensky, 1990; Touretzky, 1990). The goa",NLP,fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf,1993
Mark Plutowski· Garrison Cottrell· Halbert White··,"We apply active exemplar selection (Plutowski &. White, 1991; 1993) to predicting a chaotic time series. Given a fixed set of ex amples, the method chooses a concise subset for training. Fitting these exemplars results in the entire set being fit as well as de sired. The algorithm incorporates a method for regulating network complexity, automatically adding exempla.rs and hidden units as needed. Fitting examples generated from the Mackey-Glass equa tion with fractal dimension 2.1 to an rmse of 0.01 required about 25 exemplars and 3 to 6 hidden units. The method requires an order of magnitude fewer floating point operations than training on the entire set of examples, is significantly cheaper than two contend ing exemplar selection techniques, and suggests a simpler active selection technique that performs comparably. 1 Introduction Plutowski &. White (1991; 1993), have developed a method of active selection of training exemplars for network learning. Active selection uses information a",Optimization & Theoretical ML,fc8001f834f6a5f0561080d134d53d29-Paper.pdf,1993
Structured Machine Learning For 'Soft',"We describe the use of smoothing spline analysis of variance (SS ANOVA) in the penalized log likelihood context, for learning (estimating) the probability p of a '1' outcome, given a train ing set with attribute vectors and outcomes. p is of the form pet) = eJ(t) /(1 + eJ(t)), where, if t is a vector of attributes, f is learned as a sum of smooth functions of one attribute plus a sum of smooth functions of two attributes, etc. The smoothing parameters governing f are obtained by an iterative unbiased risk or iterative GCV method. Confidence intervals for these estimates are available.",Optimization & Theoretical ML,fe8c15fed5f808006ce95eddb7366e35-Paper.pdf,1993
SIMPLIFYING NEURAL NETS BY,"We present a new algorithm for finding low complexity networks with high generalization capability. The algorithm searches for large connected regions of so-called ''fiat'' minima of the error func tion. In the weight-space environment of a ""flat"" minimum, the error remains approximately constant. Using an MDL-based ar gument, flat minima can be shown to correspond to low expected overfitting. Although our algorithm requires the computation of second order derivatives, it has backprop's order of complexity. Experiments with feedforward and recurrent nets are described. In an application to stock market prediction, the method outperforms conventional backprop, weight decay, and ""optimal brain surgeon"" . 1 INTRODUCTION Previous algorithms for finding low complexity networks with high generalization capability are based on significant prior assumptions. They can be broadly classified as follows: (1) Assumptions about the prior weight distribution. Hinton and van Camp [3] and Williams [17]",Optimization & Theoretical ML,01882513d5fa7c329e940dda99b12147-Paper.pdf,1994
A Model of the Neural Basis of the Rat's,"In the last decade the outlines of the neural structures subserving the sense of direction have begun to emerge. Several investigations have shed light on the effects of vestibular input and visual input on the head direction representation. In this paper, a model is formulated of the neural mechanisms underlying the head direction system. The model is built out of simple ingredients, depending on nothing more complicated than connectional specificity, attractor dynamics, Hebbian learning, and sigmoidal nonlinearities, but it behaves in a sophisticated way and is consistent with most of the observed properties ofreal head direction cells. In addition it makes a number of predictions that ought to be testable by reasonably straightforward experiments. 1 Head Direction Cells in the Rat There is quite a bit of behavioral evidence for an intrinsic sense of direction in many species of mammals, including rats and humans (e.g., Gallistel, 1990). The first specific information regarding the n",Optimization & Theoretical ML,024d7f84fff11dd7e8d9c510137a2381-Paper.pdf,1994
A Mixture Model System for Medical and,"Diagnosis of human disease or machine fault is a missing data problem since many variables are initially unknown. Additional information needs to be obtained. The j oint probability distribution of the data can be used to solve this problem. We model this with mixture models whose parameters are estimated by the EM algorithm. This gives the benefit that missing data in the database itself can also be handled correctly. The request for new information to refine the diagnosis is performed using the maximum utility principle. Since the system is based on learning it is domain independent and less labor intensive than expert systems or probabilistic networks. An example using a heart disease database is presented. 1 INTRODUCTION Diagnosis is the process of identifying diseases in patients or disorders in machines by considering history, symptoms and other signs through examination. Diagnosis is a common and important problem that has proven hard to automate and formalize. A procedural desc",Optimization & Theoretical ML,03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf,1994
Learning with Preknowledge:,"Prior constraints are imposed upon a learning problem in the form of distance measures. Prototypical 2-D point sets and graphs are learned by clustering with point matching and graph matching dis tance measures. The point matching distance measure is approx. invariant under affine transformations - translation, rotation, scale and shear - and permutations. It operates between noisy images with missing and spurious points. The graph matching distance measure operates on weighted graphs and is invariant under per mutations. Learning is formulated as an optimization problem. Large objectives so formulated ('"" million variables) are efficiently minimized using a combination of optimization techniques - alge braic transformations, iterative projective scaling, clocked objec tives, and deterministic annealing. 1 Introduction While few biologists today would subscribe to Locke's description of the nascent mind as a tabula rasa, the nature of the inherent constraints - Kant's preknowl- 1 E-mai",Computer Vision,043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf,1994
Learning Local Error Bars,"We present a new method for obtaining local error bars for nonlinear regression, i.e., estimates of the confidence in predicted values that de pend on the input. We approach this problem by applying a maximum likelihood framework to an assumed distribution of errors. We demon strate our method first on computer-generated data with locally varying, normally distributed target noise. We then apply it to laser data from the Santa Fe Time Series Competition where the underlying system noise is known quantization error and the error bars give local estimates of model misspecification. In both cases, the method also provides a weighted regression effect that improves generalization performance. 1 Learning Local Error Bars Using a Maximum Likelihood Framework: Motivation, Concept, and Mechanics Feed-forward artificial neural networks used for nonlinear regression can be interpreted as predicting the mean of the target distribution as a function of (conditioned on) the input pattern (e.g., Bun",Optimization & Theoretical ML,061412e4a03c02f9902576ec55ebbe77-Paper.pdf,1994
Reinforcement Learning Methods for,"Semi-Markov Decision Problems are continuous time generaliza tions of discrete time Markov Decision Problems. A number of reinforcement learning algorithms have been developed recently for the solution of Markov Decision Problems, based on the ideas of asynchronous dynamic programming and stochastic approxima tion. Among these are TD(,x), Q-Iearning, and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problems and Bellman's optimality equation in that context, we propose al gorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems. We demonstrate these algorithms by applying them to the problem of determining the optimal con trol for a simple queueing system. We conclude with a discussion of circumstances under which these algorithms may be usefully ap plied. 1 Introduction A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recentl",Reinforcement Learning,07871915a8107172b3b5dc15a6574ad3-Paper.pdf,1994
Connectionist Speaker Normalization,"The paper presents a rapid speaker-normalization technique based on neural network spectral mapping. The neural network is used as a front-end of a continuous speech recognition system (speaker dependent, HMM-based) to normalize the input acoustic data from a new speaker. The spectral difference between speakers can be reduced using a limited amount of new acoustic data (40 phonet ically rich sentences). Recognition error of phone units from the acoustic-phonetic continuous speech corpus APASCI is decreased with an adaptability ratio of 25%. We used local basis networks of elliptical Gaussian kernels, with recursive allocation of units and on-line optimization of parameters (GRAN model). For this ap plication, the model included a linear term. The results compare favorably with multivariate linear mapping based on constrained orthonormal transformations. 1 INTRODUCTION Speaker normalization methods are designed to minimize inter-speaker variations, one of the principal error sources in",NLP,08fe2621d8e716b02ec0da35256a998d-Paper.pdf,1994
A Novel Reinforcement Model of,"Songbirds learn to imitate a tutor song through auditory and motor learn ing. We have developed a theoretical framework for song learning that accounts for response properties of neurons that have been observed in many of the nuclei that are involved in song learning. Specifically, we suggest that the anteriorforebrain pathway, which is not needed for song production in the adult but is essential for song acquisition, provides synaptic perturbations and adaptive evaluations for syllable vocalization learning. A computer model based on reinforcement learning was con structed that could replicate a real zebra finch song with 90% accuracy based on a spectrographic measure. The second generation of the bird song model replicated the tutor song with 96% accuracy. 1 INTRODUCTION Studies of motor pattern generation have generally focussed on innate motor behaviors that are genetically preprogrammed and fine-tuned by adaptive mechanisms (Harris-Warrick et al., 1992). Birdsong learning provides",Reinforcement Learning,0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf,1994
"Bias, Variance and the Combination of","We consider the effect of combining several least squares estimators on the expected performance of a regression problem. Computing the exact bias and variance curves as a function of the sample size we are able to quantitatively compare the effect of the combination on the bias and variance separately, and thus on the expected error which is the sum of the two. Our exact calculations, demonstrate that the combination of estimators is particularly useful in the case where the data set is small and noisy and the function to be learned is unrealizable. For large data sets the single estimator produces superior results. Finally, we show that by splitting the data set into several independent parts and training each estimator on a different subset, the performance can in some cases be significantly improved. Key words: Bias, Variance, Least Squares, Combination. 1 INTRODUCTION Many of the problems related to supervised learning can be boiled down to the question of balancing bias and varia",Optimization & Theoretical ML,0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf,1994
Hierarchical Mixtures of Experts Methodology Applied to,"In this paper, we incorporate the Hierarchical Mixtures of Experts (HME) method of probability estimation, developed by Jordan [1], into an HMM based continuous speech recognition system. The resulting system can be thought of as a continuous-density HMM system, but instead of using gaussian mixtures, the HME system employs a large set of hierarchically organized but relatively small neural networks to perform the probability density estimation. The hierarchical structure is reminiscent of a decision tree except for two important differences: each ""expert"" or neural net performs a ""soft"" decision rather than a hard decision, and, unlike ordinary decision trees, the parameters of all the neural nets in the HME are automatically trainable using the EM algorithm. We report results on the ARPA 5,OOO-word and 4O,OOO-word Wall Street Journal corpus using HME models. 1 Introduction Recent research has shown that a continuous-density HMM (CD-HMM) system can out perform a more constrained tied-",NLP,0d0871f0806eae32d30983b62252da50-Paper.pdf,1994
A Comparison of Discrete-Time Operator Models,"We present a unifying view of discrete-time operator models used in the context of finite word length linear signal processing. Comparisons are made between the recently presented gamma operator model, and the delta and rho operator models for performing nonlinear system identification and prediction using neural networks. A new model based on an adaptive bilinear transformation which generalizes all of the above models is presented. 1 INTRODUCTION The shift operator, defined as qx(t) ~ x(t + 1), is frequently used to provide time-domain signals to neural network models. Using the shift operator, a discrete-time model for system identification or time series prediction problems may be constructed. A common method of developing nonlinear system identification models is to use a neural network architecture as an estimator F(Y(t), X(t); 0) of F(Y(t), X(t», where 0 represents the parameter vector of the network. Shift operators at the input of the network provide the regression vectors = =",Optimization & Theoretical ML,0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf,1994
Learning Many Related Tasks at the,"Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domain specific information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multi task backprop generalizes better in real domains. 1 INTRODUCTION You and I rarely learn things one at a time, yet we often ask our programs to-it must be easier to learn things one at a time than to learn many things at once. Maybe not. The things you and I learn are related in many ways. They are processed by the same sensory apparatus, controlled by the same physical laws, derived from the same cu",Deep Learning,0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf,1994
Multidimensional Scaling and Data Clustering,"Visualizing and structuring pairwise dissimilarity data are difficult combinatorial op timization problems known as multidimensional scaling or pairwise data clustering. Algorithms for embedding dissimilarity data set in a Euclidian space, for clustering these data and for actively selecting data to support the clustering process are discussed in the maximum entropy framework. Active data selection provides a strategy to discover structure in a data set efficiently with partially unknown data. 1 Introduction Grouping experimental data into compact clusters arises as a data analysis problem in psy chology, linguistics, genetics and other experimental sciences. The data which are supposed to be clustered are either given by an explicit coordinate representation (central clustering) or, in the non-metric case, they are characterized by dissimilarity values for pairs of data points (pairwise clustering). In this paper we study algorithms (i) for embedding non-metric data in a D-dimensional",Optimization & Theoretical ML,1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf,1994
Predicting the Risk of Complications in Coronary,"Experiments demonstrated that sigmoid multilayer perceptron (MLP) networks provide slightly better risk prediction than conventional logistic regression when used to predict the risk of death, stroke, and renal failure on 1257 patients who underwent coronary artery bypass operations at the Lahey Clinic. MLP networks with no hidden layer and networks with one hidden layer were trained using stochastic gradient descent with early stopping. MLP networks and logistic regression used the same input features and were evaluated using bootstrap sampling with 50 replications. ROC areas for predicting mortality using preoperative input features were 70.5% for logistic regression and 76.0% for MLP networks. Regularization provided by early stopping was an important component of improved perfonnance. A simplified approach to generating confidence intervals for MLP risk predictions using an auxiliary ""confidence MLP"" was developed. The confidence MLP is trained to reproduce confidence intervals tha",Optimization & Theoretical ML,168908dd3227b8358eababa07fcaf091-Paper.pdf,1994
Spatial Representations in the Parietal,"The parietal cortex is thought to represent the egocentric posi tions of objects in particular coordinate systems. We propose an alternative approach to spatial perception of objects in the pari etal cortex from the perspective of sensorimotor transformations. The responses of single parietal neurons can be modeled as a gaus sian function of retinal position multiplied by a sigmoid function of eye position, which form a set of basis functions. We show here how these basis functions can be used to generate receptive fields in either retinotopic or head-centered coordinates by simple linear transformations. This raises the possibility that the parietal cortex does not attempt to compute the positions of objects in a partic ular frame of reference but instead computes a general purpose representation of the retinal location and eye position from which any transformation can be synthesized by direct projection. This representation predicts that hemineglect, a neurological syndrome produced",Neuroscience,170c944978496731ba71f34c25826a34-Paper.pdf,1994
Reinforcement Learning Algorithm for,"Increasing attention has been paid to reinforcement learning algo rithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments. If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable. We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems. Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information. The algorithm involves a Monte-Carlo pol icy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum. The algorithm operates in the space of stochastic policies, a space which can yield a policy that per forms considerably better than any deterministic policy. Although the space of stochastic policies is continuous-even for a discrete action space-our algorithm",Reinforcement Learning,1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf,1994
FINANCIAL APPLICATIONS OF,"The basic paradigm for learning in neural networks is 'learning from examples' where a training set of input-output examples is used to teach the network the target function. Learning from hints is a gen eralization of learning from examples where additional information about the target function can be incorporated in the same learning process. Such information can come from common sense rules or special expertise. In financial market applications where the train ing data is very noisy, the use of such hints can have a decisive advantage. We demonstrate the use of hints in foreign-exchange trading of the U.S. Dollar versus the British Pound, the German Mark, the Japanese Yen, and the Swiss Franc, over a period of 32 months. We explain the general method of learning from hints and how it can be applied to other markets. The learning model for this method is not restricted to neural networks. 1 INTRODUCTION When a neural network learns its target function from examples (training data), i",Optimization & Theoretical ML,1cc3633c579a90cfdd895e64021e2163-Paper.pdf,1994
An Auditory Localization,"The localization and orientation to various novel or interesting events in the environment is a critical sensorimotor ability in all animals, predator or prey. In mammals, the superior colliculus (SC) plays a major role in this behavior, the deeper layers ex hibiting topographically mapped responses to visual, auditory, and somatosensory stimuli. Sensory information arriving from differ ent modalities should then be represented in the same coordinate frame. Auditory cues, in particular, are thought to be computed in head-based coordinates which must then be transformed to reti nal coordinates. In this paper, an analog VLSI implementation for auditory localization in the azimuthal plane is described which ex tends the architecture proposed for the barn owl to a primate eye movement system where further transformation is required. This transformation is intended to model the projection in primates from auditory cortical areas to the deeper layers of the primate superior colliculus. This ",Computer Vision,1ce927f875864094e3906a4a0b5ece68-Paper.pdf,1994
Limits on Learning Machine Accuracy,"Random errors and insufficiencies in databases limit the perfor mance of any classifier trained from and applied to the database. In this paper we propose a method to estimate the limiting perfor mance of classifiers imposed by the database. We demonstrate this technique on the task of predicting failure in telecommunication paths. 1 Introduction Data collection for a classification or regression task is prone to random errors, e.g. inaccuracies in the measurements of the input or mis-labeling of the output. Missing or insufficient data are other sources that may complicate a learning task and hinder accurate performance of the trained machine. These insufficiencies of the data limit the performance of any learning machine or other statistical tool constructed from and applied to the data collection - no matter how complex the machine or how much data is used to train it. In this paper we propose a method for estimating the limiting performance of learn ing machines imposed by the qual",Optimization & Theoretical ML,1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf,1994
Interference in Learning Internal,"Experiments were performed to reveal some of the computational properties of the human motor memory system. We show that as humans practice reaching movements while interacting with a novel mechanical environment, they learn an internal model of the inverse dynamics of that environment. Subjects show recall of this model at testing sessions 24 hours after the initial practice. The representation of the internal model in memory is such that there is interference when there is an attempt to learn a new inverse dynamics map immediately after an anticorrelated mapping was learned. We suggest that this interference is an indication that the same computational elements used to encode the first inverse dynamics map are being used to learn the second mapping. We predict that this leads to a forgetting of the initially learned skill. 1 Introduction In tasks where we use our hands to interact with a tool, our motor system develops a model of the dynamics of that tool and uses this model to contr",Optimization & Theoretical ML,1e48c4420b7073bc11916c6c1de226bb-Paper.pdf,1994
Optimal Movement Primitives,"The theory of Optimal Unsupervised Motor Learning shows how a network can discover a reduced-order controller for an unknown nonlinear system by representing only the most significant modes. Here, I extend the theory to apply to command sequences, so that the most significant components discovered by the network corre spond to motion ""primitives"". Combinations of these primitives can be used to produce a wide variety of different movements. I demonstrate applications to human handwriting decomposition and synthesis, as well as to the analysis of electrophysiological experiments on movements resulting from stimulation of the frog spinal cord. 1 INTRODUCTION There is much debate within the neuroscience community concerning the inter nal representation of movement, and current neurophysiological investigations are aimed at uncovering these representations. In this paper, I propose a different approach that attempts to define the optimal internal representation in terms of ""movement primit",Reinforcement Learning,1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf,1994
Using a Saliency Map for Active Spatial Selective,"In many vision based tasks, the ability to focus attention on the important portions of a scene is crucial for good performance on the tasks. In this paper we present a simple method of achieving spatial selective attention through the use of a saliency map. The saliency map indicates which regions of the input retina are important for performing the task. The saliency map is cre ated through predictive auto-encoding. The performance of this method is demonstrated on two simple tasks which have multiple very strong distract ing features in the input retina. Architectural extensions and application directions for this model are presented. 1 MOTIVATION Many real world tasks have the property that only a small fraction of the available input is important at any particular time. On some tasks this extra input can easily be ignored. Nonetheless, often the similarity between the important input features and the irrelevant features is great enough to interfere with task performance. Two examp",Computer Vision,1f50893f80d6830d62765ffad7721742-Paper.pdf,1994
Factorial Learning and the EM Algorithm,"Many real world learning problems are best characterized by an interaction of multiple independent causes or factors. Discover ing such causal structure from the data is the focus of this paper. Based on Zemel and Hinton's cooperative vector quantizer (CVQ) architecture, an unsupervised learning algorithm is derived from the Expectation-Maximization (EM) framework. Due to the com binatorial nature of the data generation process, the exact E-step is computationally intractable. Two alternative methods for com puting the E-step are proposed: Gibbs sampling and mean-field approximation, and some promising empirical results are presented. 1 Introduction Many unsupervised learning problems fall under the rubric of factorial learning- that is, the goal of the learning algorithm is to discover multiple independent causes, or factors, that can well characterize the observed data (Barlow, 1989; Redlich, 1993; Hinton and Zemel, 1994; Saund, 1995). Such learning problems often arise naturally in ",Optimization & Theoretical ML,20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf,1994
Pairwise Neural Network Classifiers with,"Multi-class classification problems can be efficiently solved by partitioning the original problem into sub-problems involving only two classes: for each pair of classes, a (potentially small) neural network is trained using only the data of these two classes. We show how to combine the outputs of the two-class neural networks in order to obtain posterior probabilities for the class decisions. The resulting probabilistic pairwise classifier is part of a handwriting recognition system which is currently applied to check reading. We present results on real world data bases and show that, from a practical point of view, these results compare favorably to other neural network approaches. 1 Introduction Generally, a pattern classifier consists of two main parts: a feature extractor and a classification algorithm. Both parts have the same ultimate goal, namely to transform a given input pattern into a representation that is easily interpretable as a class decision. In the case of feedforward",NLP,210f760a89db30aa72ca258a3483cc7f-Paper.pdf,1994
Real-Time Control of a Tokamak Plasma,"This paper presents results from the first use of neural networks for the real-time feedback control of high temperature plasmas in a tokamak fusion experiment. The tokamak is currently the prin cipal experimental device for research into the magnetic confine ment approach to controlled fusion. In the tokamak, hydrogen plasmas, at temperatures of up to 100 Million K, are confined by strong magnetic fields. Accurate control of the position and shape of the plasma boundary requires real-time feedback control of the magnetic field structure on a time-scale of a few tens of mi croseconds. Software simulations have demonstrated that a neural network approach can give significantly better performance than the linear technique currently used on most tokamak experiments. The practical application of the neural network approach requires high-speed hardware, for which a fully parallel implementation of the multilayer perceptron, using a hybrid of digital and analogue technology, has been develop",Optimization & Theoretical ML,2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf,1994
An Actor/Critic Algorithm that •,"We prove the convergence of an actor/critic algorithm that is equiv alent to Q-Iearning by construction. Its equivalence is achieved by encoding Q-values within the policy and value function of the ac tor and critic. The resultant actor/critic algorithm is novel in two ways: it updates the critic only when the most probable action is executed from any given state, and it rewards the actor using cri teria that depend on the relative probability of the action that was executed. 1 INTRODUCTION In actor/critic learning systems, the actor implements a stochastic policy that maps states to action probability vectors, and the critic attempts to estimate the value of each state in order to provide more useful reinforcement feedback to the actor. The result is two interacting adaptive processes: the actor adapts to the critic, while the critic adapts to the actor. The foundations of actor/critic learning systems date back at least to Samuel's checker program in the late 1950s (Samuel,1963). Exa",Reinforcement Learning,23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf,1994
Template-Based Algorithms for,"Casting neural network weights in symbolic terms is crucial for interpreting and explaining the behavior of a network. Additionally, in some domains, a symbolic description may lead to more robust generalization. We present a principled approach to symbolic rule extraction based on the notion of weight templates, parameterized regions of weight space corresponding to specific symbolic expressions. With an appropriate choice of representation, we show how template parameters may be efficiently identified and instantiated to yield the optimal match to a unit's actual weights. Depending on the requirements of the application domain, our method can accommodate arbitrary disjunctions and conjunctions with O(k) complexity, simple n-of-m expressions with O( k!) complexity, or a more general class of recursive n-of-m expressions with O(k!) complexity, where k is the number of inputs to a unit. Our method of rule extraction offers several benefits over alternative approaches in the literature, ",Optimization & Theoretical ML,24896ee4c6526356cc127852413ea3b4-Paper.pdf,1994
Reinforcement Learning with Soft State,"It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems. Unfortunately almost all of the theory of reinforcement learning assumes lookup table representa tions. In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approx imator based on a simple extension to state aggregation (a com monly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation. Preliminary empirical results are also presented. 1 INTRODUCTION The strong theory of convergence available for reinforcement le",Reinforcement Learning,287e03db1d99e0ec2edb90d079e142f3-Paper.pdf,1994
A Connectionist Technique for Accelerated,"Each year people spend a huge amount of time typing. The text people type typically contains a tremendous amount of redundancy due to predictable word usage patterns and the text's structure. This paper describes a neural network system call AutoTypist that monitors a person's typing and predicts what will be entered next. AutoTypist displays the most likely subsequent word to the typist, who can accept it with a single keystroke, instead of typing it in its entirety. The multi-layer perceptron at the heart of Auto'JYpist adapts its predictions of likely subsequent text to the user's word usage pattern, and to the characteristics of the text currently being typed. Increases in typing speed of 2-3% when typing English prose and 10-20% when typing C code have been demonstrated using the system, suggesting a potential time savings of more than 20 hours per user per year. In addition to increasing typing speed, AutoTypist reduces the number of keystrokes a user must type by a similar amoun",NLP,298923c8190045e91288b430794814c4-Paper.pdf,1994
Advantage Updating Applied to,"An application of reinforcement learning to a linear-quadratic, differential game is presented. The reinforcement learning system uses a recently developed algorithm, the residual gradient form of advantage updating. The game is a Markov Decision Process (MDP) with continuous time, states, and actions, linear dynamics, and a quadratic cost function. The game consists of two players, a missile and a plane; the missile pursues the plane and the plane evades the missile. The reinforcement learning algorithm for optimal control is modified for differential games in order to find the minimax point, rather than the maximum. Simulation results are compared to the optimal solution, demonstrating that the simulated reinforcement learning system converges to the optimal answer. The performance of both the residual gradient and non-residual gradient forms of advantage updating and Q-learning are compared. The results show that advantage updating converges faster than Q-learning in all simulations",Reinforcement Learning,2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf,1994
Malcolm Slaney,"Deciding the appropriate representation to use for modeling human auditory processing is a critical issue in auditory science. While engi neers have successfully performed many single-speaker tasks with LPC and spectrogram methods, more difficult problems will need a richer representation. This paper describes a powerful auditory representation known as the correlogram and shows how this non-linear representation can be converted back into sound, with no loss of perceptually impor tant information. The correlogram is interesting because it is a neuro physiologically plausible representation of sound. This paper shows improved methods for spectrogram inversion (conventional pattern playback), inversion of a cochlear model, and inversion of the correlo gram representation. 1 INTRODUCTIONl My interest in auditory models and perceptual displays [2] is motivated by the problem of sound understanding, especially the separation of speech from noisy backgrounds and interfering speakers. The co",NLP,2ab56412b1163ee131e1246da0955bd1-Paper.pdf,1994
An Analog Neural Network Inspired by,"We consider the problem of decoding block coded data, using a physical dynamical system. We sketch out a decompression algorithm for fractal block codes and then show how to implement a recurrent neural network using physically simple but highly-nonlinear, analog circuit models of neurons and synapses. The nonlinear system has many fixed points, but we have at our disposal a procedure to choose the parameters in such a way that only one solution, the desired solution, is stable. As a partial proof of the concept, we present experimental data from a small system a 16-neuron analog CMOS chip fabricated in a 2m analog p-well process. This chip operates in the subthreshold regime and, for each choice of parameters, converges to a unique stable state. Each state exhibits a qualitatively fractal shape.",Optimization & Theoretical ML,2ba596643cbbbc20318224181fa46b28-Paper.pdf,1994
Phase-Space Learning,"Existing recurrent net learning algorithms are inadequate. We in troduce the conceptual framework of viewing recurrent training as matching vector fields of dynamical systems in phase space. Phase space reconstruction techniques make the hidden states explicit, reducing temporal learning to a feed-forward problem. In short, we propose viewing iterated prediction [LF88] as the best way of training recurrent networks on deterministic signals. Using this framework, we can train multiple trajectories, insure their stabil ity, and design arbitrary dynamical systems. 1 INTRODUCTION Existing general-purpose recurrent algorithms are capable of rich dynamical be havior. Unfortunately, straightforward applications of these algorithms to training fully-recurrent networks on complex temporal tasks have had much less success than their feedforward counterparts. For example, to train a recurrent network to oscillate like a sine wave (the ""hydrogen atom"" of recurrent learning), existing techniques su",Deep Learning,2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf,1994
An experimental comparison,"Many different discrete-time recurrent neural network architec tures have been proposed. However, there has been virtually no effort to compare these arch:tectures experimentally. In this paper we review and categorize many of these architectures and compare how they perform on various classes of simple problems including grammatical inference and nonlinear system identification. 1 Introduction In the past few years several recurrent neural network architectures have emerged. In this paper we categorize various discrete-time recurrent neural network architec tures, and perform a quantitative comparison of these architectures on two prob lems: grammatical inference and nonlinear system identification. 2 RNN Architectures We broadly divide these networks into two groups depending on whether or not the states of the network are guaranteed to be observable. A network with observable states has the property that the states of the system can always be determined from observations of the inpu",NLP,31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf,1994
Inferring Ground Truth from Subjective,"In remote sensing applications ""ground-truth"" data is often used as the basis for training pattern recognition algorithms to gener ate thematic maps or to detect objects of interest. In practical situations, experts may visually examine the images and provide a subjective noisy estimate of the truth. Calibrating the reliability and bias of expert labellers is a non-trivial problem. In this paper we discuss some of our recent work on this topic in the context of detecting small volcanoes in Magellan SAR images of Venus. Empirical results (using the Expectation-Maximization procedure) suggest that accounting for subjective noise can be quite signifi cant in terms of quantifying both human and algorithm detection performance. 1 Introduction In certain pattern recognition applications, particularly in remote-sensing and med ical diagnosis, the standard assumption that the labelling of the data has been * and Division of Biology, California Institute of Technology 1086 Padhraic Smyth. Usama",Computer Vision,3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf,1994
Learning Prototype Models for Tangent,"Simard, LeCun & Denker (1993) showed that the performance of nearest-neighbor classification schemes for handwritten character recognition can be improved by incorporating invariance to spe cific transformations in the underlying distance metric - the so called tangent distance. The resulting classifier, however, can be prohibitively slow and memory intensive due to the large amount of prototypes that need to be stored and used in the distance compar isons. In this paper we develop rich models for representing large subsets of the prototypes. These models are either used singly per class, or as basic building blocks in conjunction with the K-means clustering algorithm. *This work was performed while Trevor Hastie was a member of the Statistics and Data Analysis Research Group, AT&T Bell Laboratories, Murray Hill, NJ 07974. J 000 Trevor Hastie, Patrice Simard, Eduard Siickinger 1 INTRODUCTION Local algorithms such as K-nearest neighbor (NN) perform well in pattern recogni tion, even tho",Deep Learning,3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf,1994
Diffusion of Credit in Markovian Models,"This paper studies the problem of diffusion in Markovian models, such as hidden Markov models (HMMs) and how it makes very difficult the task of learning of long-term dependencies in sequences. Using results from Markov chain theory, we show that the problem of diffusion is reduced if the transition probabilities approach 0 or 1. Under this condition, standard HMMs have very limited modeling capabilities, but input/output HMMs can still perform interesting computations. 1 Introduction This paper presents an important new element in our research on the problem of learning long-term dependencies in sequences. In our previous work [4J we found theoretical reasons for the difficulty in training recurrent networks (or more gen erally parametric non-linear dynamical systems) to learn long-term dependencies. The main result stated that either long-term storing or gradient propagation would be harmed, depending on whether the norm of the Jacobian of the state to state function was greater or l",Optimization & Theoretical ML,3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf,1994
The NilOOO: High Speed Parallel VLSI,"In this paper we present a new version of the standard multilayer perceptron (MLP) algorithm for the state-of-the-art in neural net work VLSI implementations: the Intel Ni1000. This new version of the MLP uses a fundamental property of high dimensional spaces which allows the 1-norm to be accurately approximated by the 2 It -norm. This approach enables the standard MLP to utilize the parallel architecture of the Ni1000 to achieve on the order of 40000, 256-dimensional classifications per second. 1 The Intel NilOOO VLSI Chip The Nestor/Intel radial basis function neural chip (Ni1000) contains the equivalent of 1024 256-dimensional artificial digital neurons and can perform at least 40000 classifications per second [Sullivan, 1993]. To attain this great speed, the Ni1000 was designed to calculate ""city block"" distances (Le. the II-norm) and thus to avoid the large number of multiplication units that would be required to calculate Euclidean dot products in parallel. Each neuron calculates",Optimization & Theoretical ML,430c3626b879b4005d41b8a46172e0c0-Paper.pdf,1994
Model of a Biological Neuron as a Temporal,"A biological neuron can be viewed as a device that maps a multidimen sional temporal event signal (dendritic postsynaptic activations) into a unidimensional temporal event signal (action potentials). We have designed a network, the Spatio-Temporal Event Mapping (STEM) architecture, which can learn to perform this mapping for arbitrary bio physical models of neurons. Such a network appropriately trained, called a STEM cell, can be used in place of a conventional compartmen tal model in simulations where only the transfer function is important, such as network simulations. The STEM cell offers advantages over compartmental models in terms of computational efficiency, analytical tractabili1ty, and as a framework for VLSI implementations of biologi cal neurons. 1 INTRODUCTION Discovery of the mechanisms by which the mammalian cerebral cortex processes and stores information is the greatest remaining challenge in the brain sciences. Numerous modeling studies have attempted to describe corti",Optimization & Theoretical ML,4311359ed4969e8401880e3c1836fbe1-Paper.pdf,1994
Non-linear Prediction of Acoustic Vectors,"In this paper we consider speech coding as a problem of speech modelling. In particular, prediction of parameterised speech over short time segments is performed using the Hierarchical Mixture of Experts (HME) (Jordan & Jacobs 1994). The HME gives two ad vantages over traditional non-linear function approximators such as the Multi-Layer Percept ron (MLP); a statistical understand ing of the operation of the predictor and provision of information about the performance of the predictor in the form of likelihood information and local error bars. These two issues are examined on both toy and real world problems of regression and time series prediction. In the speech coding context, we extend the principle of combining local predictions via the HME to a Vector Quantiza tion scheme in which fixed local codebooks are combined on-line for each observation. 1 INTRODUCTION We are concerned in this paper with the application of multiple models, specifi cally the Hierarchical Mixtures of Experts, ",Deep Learning,437d7d1d97917cd627a34a6a0fb41136-Paper.pdf,1994
JPMAX: Learning to Recognize Moving,"Unsupervised learning procedures have been successful at low-level feature extraction and preprocessing of raw sensor data. So far, however, they have had limited success in learning higher-order representations, e.g., of objects in visual images. A promising ap proach is to maximize some measure of agreement between the outputs of two groups of units which receive inputs physically sep arated in space, time or modality, as in (Becker and Hinton, 1992; Becker, 1993; de Sa, 1993). Using the same approach, a much sim pler learning procedure is proposed here which discovers features in a single-layer network consisting of several populations of units, and can be applied to multi-layer networks trained one layer at a time. When trained with this algorithm on image sequences of moving geometric objects a two-layer network can learn to perform accurate position-invariant object classification. 1 LEARNING COHERENT CLASSIFICATIONS A powerful constraint in sensory data is coherence over time, i",Computer Vision,4b0250793549726d5c1ea3906726ebfe-Paper.pdf,1994
The Electrotonic Transformation:,"The spatial distribution and time course of electrical signals in neurons have important theoretical and practical consequences. Because it is difficult to infer how neuronal form affects electrical signaling, we have developed a quantitative yet intuitive approach to the analysis of electrotonus. This approach transforms the architecture of the cell from anatomical to electrotonic space, using the logarithm of voltage attenuation as the distance metric. We describe the theory behind this approach and illustrate its use. 1 INTRODUCTION The fields of computational neuroscience and artificial neural nets have enjoyed a mutually beneficial exchange of ideas. This has been most evident at the network level, where concepts such as massive parallelism, lateral inhibition, and recurrent excitation have inspired both the analysis of brain circuits and the design of artificial neural net architectures. Less attention has been given to how properties of the individual neurons or processing eleme",Optimization & Theoretical ML,4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf,1994
Reinforcement Learning Predicts the Site,"The auditory system of the barn owl contains several spatial maps. In young barn owls raised with optical prisms over their eyes, these auditory maps are shifted to stay in register with the visual map, suggesting that the visual input imposes a frame of reference on the auditory maps. However, the optic tectum, the first site of convergence of visual with auditory information, is not the site of plasticity for the shift of the auditory maps; the plasticity occurs instead in the inferior colliculus, which contains an auditory map and projects into the optic tectum. We explored a model of the owl remapping in which a global reinforcement signal whose delivery is controlled by visual foveation. A hebb learning rule gated by rein forcement learned to appropriately adjust auditory maps. In addi tion, reinforcement learning preferentially adjusted the weights in the inferior colliculus, as in the owl brain, even though the weights were allowed to change throughout the auditory system. This ",Reinforcement Learning,4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf,1994
Boltzmann Chains and Hidden,"We propose a statistical mechanical framework for the modeling of discrete time series. Maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights. We call these networks Boltzmann chains and show that they contain hidden Markov models (HMMs) as a special case. Our framework also motivates new architectures that address partic ular shortcomings of HMMs. We look at two such architectures: parallel chains that model feature sets with disparate time scales, and looped networks that model long-term dependencies between hidden states. For these networks, we show how to implement the Boltzmann learning rule exactly, in polynomial time, without resort to simulated or mean-field annealing. The necessary com putations are done by exact decimation procedures from statistical mechanics. 1 INTRODUCTION AND SUMMARY Statistical models of discrete time series have a wide range of applications, most notably to problems in speech recognition (Juang & Rabi",Optimization & Theoretical ML,4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf,1994
Comparing the prediction accuracy of,"The TNM staging system has been used since the early 1960's to predict breast cancer patient outcome. In an attempt to in crease prognostic accuracy, many putative prognostic factors have been identified. Because the TNM stage model can not accom modate these new factors, the proliferation of factors in breast cancer has lead to clinical confusion. What is required is a new computerized prognostic system that can test putative prognostic factors and integrate the predictive factors with the TNM vari ables in order to increase prognostic accuracy. Using the area un der the curve of the receiver operating characteristic, we compare the accuracy of the following predictive models in terms of five year breast cancer-specific survival: pTNM staging system, princi pal component analysis, classification and regression trees, logistic regression, cascade correlation neural network, conjugate gradient descent neural, probabilistic neural network, and backpropagation neural network. Several stat",NLP,4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf,1994
Efficient Methods for Dealing with,"We present efficient algorithms for dealing with the problem of mis sing inputs (incomplete feature vectors) during training and recall. Our approach is based on the approximation of the input data dis tribution using Parzen windows. For recall, we obtain closed form solutions for arbitrary feedforward networks. For training, we show how the backpropagation step for an incomplete pattern can be approximated by a weighted averaged backpropagation step. The complexity of the solutions for training and recall is independent of the number of missing features. We verify our theoretical results using one classification and one regression problem. 1 Introduction The problem of missing data (incomplete feature vectors) is of great practical and theoretical interest. In many applications it is important to know how to react if the available information is incomplete, if sensors fail or if sources of information become A.t the time of the research for this paper, a visiting researcher at the Cen",Optimization & Theoretical ML,54a367d629152b720749e187b3eaa11b-Paper.pdf,1994
PREDICTIVE CODING WITH,"To compress text files, a neural predictor network P is used to ap proximate the conditional probability distribution of possible ""next characters"", given n previous characters. P's outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpre dictable characters. Tested on short German newspaper articles, our method outperforms widely used Lempel-Ziv algorithms (used in UNIX functions such as ""compress"" and ""gzip""). 1048 liirgen Schmidhuber, Stefan Heil 1 INTRODUCTION The method presented in this paper is an instance of a strategy known as ""predic tive coding"" or ""model-based coding"". To compress text files, a neural predictor network P approximates the conditional probability distribution of possible ""next characters"", given n previous characters. P's outputs are fed into algorithms that generate short codes for characters with low information content (characters with high predicted probability",NLP,5705e1164a8394aace6018e27d20d237-Paper.pdf,1994
Computational structure of coordinate,"One of the fundamental properties that both neural networks and the central nervous system share is the ability to learn and gener alize from examples. While this property has been studied exten sively in the neural network literature it has not been thoroughly explored in human perceptual and motor learning. We have chosen a coordinate transformation system-the visuomotor map which transforms visual coordinates into motor coordinates-to study the generalization effects of learning new input-output pairs. Using a paradigm of computer controlled altered visual feedback, we have studied the generalization of the visuomotor map subsequent to both local and context-dependent remappings. A local remapping of one or two input-output pairs induced a significant global, yet decaying, change in the visuomotor map, suggesting a representa tion for the map composed of units with large functional receptive fields. Our study of context-dependent remappings indicated that a single point in visual sp",Computer Vision,58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf,1994
Recognizing Handwritten Digits Using Mixtures of Linear Models,"We construct a mixture of locally linear generative models of a col lection of pixel-based images of digits, and use them for recogni tion. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane informa tion [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance. 1 Introduction The usual way of using a neural network for digit recognition is to train it to output one of the ten classes. When the training data is limited to N examples equally distributed among the classes, there are only N log2 10 bits of constraint in the class labels so the number of free parameters that can be allowed in a discriminative neural net mod",Computer Vision,5c936263f3428a40227908d5a3847c0b-Paper.pdf,1994
A Critical Comparison of Models for,"More than ten of the most prominent models for the structure and for the activity dependent formation of orientation and ocu lar dominance columns in the striate cort(>x have been evaluated. We implemented those models on parallel machines, we extensively explored parameter space, and we quantitatively compared model predictions with experimental data which were recorded optically from macaque striate cortex. In our contribution we present a summary of our results to date. Briefly, we find that (i) despite apparent differences, many models are based on similar principles and, consequently, make similar pre dictions, (ii) certain ""pattern models"" as well as the developmental ""correlation-based learning"" models disagree with the experimen tal data, and (iii) of the models we have investigated, ""competitive Hebbian"" models and the recent model of Swindale provide the best match with experimental data. 1 Models and Data The models for the formation and structure of orientation and ocular d",Optimization & Theoretical ML,5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf,1994
Classifying with Gaussian Mixtures and,"In this paper, we derive classifiers which are winner-take-all (WTA) approximations to a Bayes classifier with Gaussian mixtures for class conditional densities. The derived classifiers include clustering based algorithms like LVQ and k-Means. We propose a constrained rank Gaussian mixtures model and derive a WTA algorithm for it. Our experiments with two speech classification tasks indicate that the constrained rank model and the WTA approximations improve the performance over the unconstrained models. 1 Introduction A classifier assigns vectors from Rn (n dimensional feature space) to one of K classes, partitioning the feature space into a set of K disjoint regions. A Bayesian classifier builds the partition based on a model of the class conditional probability densities of the inputs (the partition is optimal for the given model). In this paper, we assume that the class conditional densities are modeled by mixtures of Gaussians. Based on Nowlan's work relating Gaussian mixtures and ",NLP,621461af90cadfdaf0e8d4cc25129f91-Paper.pdf,1994
Anatomical origin and computational role,"The maximization of diversity of neuronal response properties has been recently suggested as an organizing principle for the formation of such prominent features of the functional architecture of the brain as the corti cal columns and the associated patchy projection patterns (Malach, 1994). We show that (1) maximal diversity is attained when the ratio of dendritic and axonal arbor sizes is equal to one, as found in many cortical areas and across species (Lund et al., 1993; Malach, 1994), and (2) that maxi mization of diversity leads to better performance in systems of receptive fields implementing steerable/shiftable filters, and in matching spatially distributed signals, a problem that arises in many high-level visual tasks. 1 Anatomical substrate for sampling diversity A fundamental feature of cortical architecture is its columnar organization, mani fested in the tendency of neurons with similar properties to be organized in columns that run perpendicular to the cortical surface. Th",Optimization & Theoretical ML,6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf,1994
Synchrony and Desynchrony,"An novel class of locally excitatory, globally inhibitory oscillator networks is proposed. The model of each oscillator corresponds to a standard relaxation oscillator with two time scales. The network exhibits a mechanism of selective gating, whereby an oscillator jumping up to its active phase rapidly recruits the oscillators stimulated by the same pattern, while preventing others from jumping up. We show analytically that with the selective gating mechanism the network rapidly achieves both synchronization within blocks of oscillators that are stimulated by connected regions and desynchronization between different blocks. Computer simulations demonstrate the network's promising ability for segmenting multiple input patterns in real time. This model lays a physical foundation for the oscillatory correlation theory of feature binding, and may provide an effective computational framework for scene segmentation and figure/ground segregation. 1 INTRODUCTION A basic attribute of perceptio",Optimization & Theoretical ML,64223ccf70bbb65a3a4aceac37e21016-Paper.pdf,1994
A Computational Model of Prefrontal,"Accumulating data from neurophysiology and neuropsychology have suggested two information processing roles for prefrontal cor tex (PFC): 1) short-term active memory; and 2) inhibition. We present a new behavioral task and a computational model which were developed in parallel. The task was developed to probe both of these prefrontal functions simultaneously, and produces a rich set of behavioral data that act as constraints on the model. The model is implemented in continuous-time, thus providing a natural framework in which to study the temporal dynamics of processing in the task. We show how the model can be used to examine the be havioral consequences of neuromodulation in PFC. Specifically, we use the model to make novel and testable predictions regarding the behavioral performance of schizophrenics, who are hypothesized to suffer from reduced dopaminergic tone in this brain area. 1 Introduction Prefrontal cortex (PFC) is an area of the human brain which is significantly ex panded ",NLP,65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf,1994
Combining Estimators Using,"This paper discusses the linearly weighted combination of estima tors in which the weighting functions are dependent on the input. We show that the weighting functions can be derived either by evaluating the input dependent variance of each estimator or by estimating how likely it is that a given estimator has seen data in the region of the input space close to the input pattern. The lat ter solution is closely related to the mixture of experts approach and we show how learning rules for the mixture of experts can be derived from the theory about learning with missing features. The presented approaches are modular since the weighting functions can easily be modified (no retraining) if more estimators are ad ded. Furthermore, it is easy to incorporate estimators which were not derived from data such as expert systems or algorithms. 1 Introduction Instead of modeling the global dependency between input x E ~D and output y E ~ using a single estimator, it is often very useful to decompose",Optimization & Theoretical ML,6602294be910b1e3c4571bd98c4d5484-Paper.pdf,1994
Stochastic Dynamics of Three-State,"We present here an analysis of the stochastic neurodynamics of a neural network composed of three-state neurons described by a master equation. An outer-product representation of the mas ter equation is employed. In this representation, an extension of the analysis from two to three-state neurons is easily performed. We apply this formalism with approximation schemes to a sim ple three-state network and compare the results with Monte Carlo simulations. 1 INTRODUCTION Studies of single neurons or networks under the influence of noise have been a con tinuing item in neural network modelling. In particular, the analogy with spin systems at finite temprature has produced many important results on networks of two-state neurons. However, studies of networks of three-state neurons have been rather limited (Meunier, Hansel and Verga, 1989). A master equation was intro duced by Cowan (1991) to study stochastic neural networks. The equation uses the formalism of ""second quantization"" for classic",Optimization & Theoretical ML,692f93be8c7a41525c0baf2076aecfb4-Paper.pdf,1994
On the Computational Utility of,"We propose a computational framework for understanding and modeling human consciousness. This framework integrates many existing theoretical perspectives, yet is sufficiently concrete to allow simulation experiments. We do not attempt to explain qualia (sub jective experience), but instead ask what differences exist within the cognitive information processing system when a person is con scious of mentally-represented information versus when that infor mation is unconscious. The central idea we explore is that the con tents of consciousness correspond to temporally persistent states in a network of computational modules. Three simulations are de scribed illustrating that the behavior of persistent states in the models corresponds roughly to the behavior of conscious states people experience when performing similar tasks. Our simulations show that periodic settling to persistent (i.e., conscious) states im proves performance by cleaning up inaccuracies and noise, forcing decisions, and h",Optimization & Theoretical ML,6aab1270668d8cac7cef2566a1c5f569-Paper.pdf,1994
Ocular Dominance and Patterned Lateral,"A neural network model for the self-organization of ocular dominance and lateral connections from binocular input is presented. The self-organizing process results in a network where (1) afferent weights of each neuron or ganize into smooth hill-shaped receptive fields primarily on one of the reti nas, (2) neurons with common eye preference form connected, intertwined patches, and (3) lateral connections primarily link regions of the same eye preference. Similar self-organization of cortical structures has been ob served experimentally in strabismic kittens. The model shows how pat terned lateral connections in the cortex may develop based on correlated activity and explains why lateral connection patterns follow receptive field properties such as ocular dominance. 1 Introduction Lateral connections in the primary visual cortex have a patterned structure that closely matches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.1993). For example, in the norm",Computer Vision,6b180037abbebea991d8b1232f8a8ca9-Paper.pdf,1994
Effects of Noise on Convergence and,"We introduce and study methods of inserting synaptic noise into dynamically-driven recurrent neural networks and show that ap plying a controlled amount of noise during training may improve convergence and generalization. In addition, we analyze the effects of each noise parameter (additive vs. multiplicative, cumulative vs. non-cumulative, per time step vs. per string) and predict that best overall performance can be achieved by injecting additive noise at each time step. Extensive simulations on learning the dual parity grammar from temporal strings substantiate these predictions. 1 INTRODUCTION There has been much research in applying noise to neural networks to improve net work performance. It has been shown that using noisy hidden nodes during training can result in error-correcting codes which increase the tolerance of feedforward nets to unreliable nodes (Judd and Munro, 1992). Also, randomly disabling hidden nodes during the training phase increases the tolerance of MLP's to no",Deep Learning,6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf,1994
An Integrated Architecture of Adaptive Neural Network,"In this study, an integrated neural network control architecture for nonlinear dynamic systems is presented. Most of the recent emphasis in the neural network control field has no error feedback as the control input, which rises the lack of adaptation problem. The integrated architecture in this paper combines feed forward control and error feedback adaptive control using neural networks. The paper reveals the different internal functionality of these two kinds of neural network controllers for certain input styles, e.g., state feedback and error feedback. With error feedback, neural network controllers learn the slopes or the gains with respect to the error feedback, producing an error driven adaptive control systems. The results demonstrate that the two kinds of control scheme can be combined to realize their individual advantages. Testing with disturbances added to the plant shows good tracking and adaptation with the integrated neural control architecture. 1 INTRODUCTION Neural net",Optimization & Theoretical ML,6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf,1994
Implementation of Neural Hardware with,"This paper describes a way of neural hardware implementation with the analog-digital mixed mode neural chip. The full custom neural VLSI of Universally Reconstructible Artificial Neural network (URAN) is used to implement Korean speech recognition system. A multi-layer perceptron with linear neurons is trained successfully under the limited accuracy in computations. The network with a large frame input layer is tested to recognize spoken korean words at a forward retrieval. Multichip hardware module is suggested with eight chips or more for the extended performance and capacity. 812 ll-Song Han, Hwang-Soo Lee, Ki-Chul Kim 1 INTRODUCTION In general, the neural network hardware or VLSI has been preferred in respects of its relatively fast speed, huge network size and effective cost comparing to software simulation. Universally Reconstructible Artificial Neural-network(URAN), the new analog-digital mixed VLSI neural network, can be used for the implementation of the real world neural netw",Computer Vision,704afe073992cbe4813cae2f7715336f-Paper.pdf,1994
Estimating Conditional Probability,"Most of the common techniques for estimating conditional prob ability densities are inappropriate for applications involving peri odic variables. In this paper we introduce three novel techniques for tackling such problems, and investigate their performance us ing synthetic data. We then apply these techniques to the problem of extracting the distribution of wind vector directions from radar scatterometer data gathered by a remote-sensing satellite. 1 INTRODUCTION Many applications of neural networks can be formulated in terms of a multi-variate non-linear mapping from an input vector x to a target vector t. A conventional neural network approach, based on least squares for example, leads to a network mapping which approximates the regression of t on x. A more complete description of the data can be obtained by estimating the conditional probability density of t, conditioned on x, which we write as p(tlx). Various techniques exist for modelling such densities when the target variables ",Optimization & Theoretical ML,74bba22728b6185eec06286af6bec36d-Paper.pdf,1994
Analysis of Unstandardized Contributions,"Understanding knowledge representations in neural nets has been a difficult problem. Principal components analysis (PCA) of contributions (products of sending activations and connection weights) has yielded valuable insights into knowledge representations, but much of this work has focused on the correlation matrix of contributions. The present work shows that analyzing the variance-covariance matrix of contributions yields more valid insights by taking account of weights. 1 INTRODUCTION The knowledge representations learned by neural networks are usually difficult to understand because of the non-linear properties of these nets and the fact that knowledge is often distributed across many units. Standard network analysis techniques, based on a network's connection weights or on its hidden unit activations, have been limited. Weight diagrams are typically complex and weights vary across mUltiple networks trained on the same problem. Analysis of activation patterns on hidden units is lim",Optimization & Theoretical ML,7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf,1994
A Rigorous Analysis Of,"We propose a novel rigorous approach for the analysis of Linsker's unsupervised Hebbian learning network. The behavior of this model is determined by the underlying nonlinear dynamics which are parameterized by a set of parameters originating from the Heb bian rule and the arbor density of the synapses. These parameters determine the presence or absence of a specific receptive field (also referred to as a 'connection pattern') as a saturated fixed point attractor of the model. In this paper, we perform a qualitative analysis of the underlying nonlinear dynamics over the parameter space, determine the effects of the system parameters on the emer gence of various receptive fields, and predict precisely within which parameter regime the network will have the potential to develop a specially designated connection pattern. In particular, this ap proach exposes, for the first time, the crucial role played by the synaptic density functions, and provides a complete precise picture of the param",Optimization & Theoretical ML,7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf,1994
Associative Decorrelation Dynamics:,"This paper outlines a dynamic theory of development and adap tation in neural networks with feedback connections. Given in put ensemble, the connections change in strength according to an associative learning rule and approach a stable state where the neuronal outputs are decorrelated. We apply this theory to pri mary visual cortex and examine the implications of the dynamical decorrelation of the activities of orientation selective cells by the intracortical connections. The theory gives a unified and quantita tive explanation of the psychophysical experiments on orientation contrast and orientation adaptation. Using only one parameter, we achieve good agreements between the theoretical predictions and the experimental data. 1 Introduction The mammalian visual system is very effective in detecting the orientations of lines and most neurons in primary visual cortex selectively respond to oriented lines and form orientation columns [1). Why is the visual system organized as such? We *Pr",Computer Vision,766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf,1994
Visual Speech Recognition with,"This paper presents ongoing work on a speaker independent visual speech recognition system. The work presented here builds on previous research efforts in this area and explores the potential use of simple hidden Markov models for limited vocabulary, speaker independent visual speech recognition. The task at hand is recognition of the first four English digits, a task with possible applications in car-phone dialing. The images were modeled as mixtures of independent Gaussian distributions, and the temporal dependencies were captured with standard left-to-right hidden Markov models. The results indicate that simple hidden Markov models may be used to successfully recognize relatively unprocessed image sequences. The system achieved performance levels equivalent to untrained humans when asked to recognize the fIrst four English digits. 1 INTRODUCTION Visual articulation is an important source of information in face to face speech perception. Laboratory studies have shown that visual info",Computer Vision,7b13b2203029ed80337f27127a9f1d28-Paper.pdf,1994
Finding Structure in Reinforcement Learning,"Reinforcement learning addresses the problem of learning to select actions in order to maximize one's performance in unknown environments. To scale reinforcement learning to complex real-world tasks, such as typically studied in AI, one must ultimately be able to discover the structure in the world, in order to abstract away the myriad of details and to operate in more tractable problem spaces. This paper presents the SKILLS algorithm. SKILLS discovers skills, which are partially defined action policies that arise in the context of multiple, related tasks. Skills collapse whole action sequences into single operators. They are learned by minimizing the com pactness of action policies, using a description length argument on their representation. Empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning. 1 Introduction Reinforcement learning comprises a family of incremental planning algorithms that construct reactive cont",Reinforcement Learning,7ce3284b743aefde80ffd9aec500e085-Paper.pdf,1994
Active Learning with Statistical Models,"For many types of learners one can compute the statistically ""op timal"" way to select data. We review how these techniques have been used with feedforward neural networks [MacKay, 1992; Cohn, 1994]. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regres sion are both efficient and accurate. 1 ACTIVE LEARNING - BACKGROUND An active learning problem is one where the learner has the ability or need to influence or select its own training data. Many problems of great practical interest allow active learning, and many even require it. We consider the problem of actively learning a mapping X - Y based on a set of training examples {(Xi,Yi)}~l' where Xi E X and Yi E Y. The learner is allowed x to iteratively select new inputs (",Optimization & Theoretical ML,7f975a56c761db6506eca0b37ce6ec87-Paper.pdf,1994
From Data Distributions to,"Ideally pattern recognition machines provide constant output when the inputs are transformed under a group 9 of desired invariances. These invariances can be achieved by enhancing the training data to include examples of inputs transformed by elements of g, while leaving the corresponding targets unchanged. Alternatively the cost function for training can include a regularization term that penalizes changes in the output when the input is transformed un der the group. This paper relates the two approaches, showing precisely the sense in which the regularized cost function approximates the result of adding transformed (or distorted) examples to the training data. The cost function for the enhanced training set is equivalent to the sum of the original cost function plus a regularizer. For unbiased models, the regularizer reduces to the intuitively obvious choice - a term that penalizes changes in the output when the inputs are transformed under the group. For infinitesimal transformation",Optimization & Theoretical ML,7fa732b517cbed14a48843d74526c11a-Paper.pdf,1994
An Input Output HMM Architecture,"We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation. 1 INTRODUCTION Learning problems involving sequentially structured data cannot be effectively dealt with static models such as feedforward networks. Recurrent networks allow to model complex dynamical systems and can store and retrieve contextual information in a flexible way. Up until the present time, research efforts of supervised learning for recurrent networks have almost exclusively focused on error minimization by gradient descent methods. Although effective for learning short term memories, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/out",NLP,8065d07da4a77621450aa84fee5656d9-Paper.pdf,1994
Grouping Components of,"Many cells in the dorsal part of the medial superior temporal (MST) area of visual cortex respond selectively to spiral flow patterns-specific combinations of expansion/ contraction and ro tation motions. Previous investigators have suggested that these cells may represent self-motion. Spiral patterns can also be gener ated by the relative motion of the observer and a particular object. An MST cell may then account for some portion of the complex flow field, and the set of active cells could encode the entire flow; in this manner, MST effectively segments moving objects. Such a grouping operation is essential in interpreting scenes containing several independent moving objects and observer motion. We de scribe a model based on the hypothesis that the selective tuning of MST cells reflects the grouping of object components undergo ing coherent motion. Inputs to the model were generated from sequences of ray-traced images that simulated realistic motion sit uations, combining observer mo",Computer Vision,860320be12a1c050cd7731794e231bd3-Paper.pdf,1994
Higher Order Statistical Decorrelation without,"A neural network learning paradigm based on information theory is pro posed as a way to perform in an unsupervised fashion, redundancy reduction among the elements of the output layer without loss of infor mation from the sensory input. The model developed performs nonlin ear decorrelation up to higher orders of the cumulant tensors and results in probabilistic ally independent components of the output layer. This means that we don't need to assume Gaussian distribution neither at the input nor at the output. The theory presented is related to the unsuper vised-learning theory of Barlow, which proposes redundancy reduction as the goal of cognition. When nonlinear units are used nonlinear princi pal component analysis is obtained. In this case nonlinear manifolds can be reduced to minimum dimension manifolds. If such units are used the network performs a generalized principal component analysis in the sense that non-Gaussian distributions can be linearly decorrelated and higher orders o",Deep Learning,892c91e0a653ba19df81a90f89d99bcd-Paper.pdf,1994
Sample Size Requirements For,"We estimate the number of training samples required to ensure that the performance of a neural network on its training data matches that obtained when fresh data is applied to the network. Existing estimates are higher by orders of magnitude than practice indicates. This work seeks to narrow the gap between theory and practice by transforming the problem into determining the distribution of the supremum of a random field in the space of weight vectors, which in turn is attacked by application of a recent technique called the Poisson clumping heuristic. 1 INTRODUCTION AND KNOWN RESULTS We investigate the tradeofi""s among network complexity, training set size, and sta tistical performance of feedforward neural networks so as to allow a reasoned choice of network architecture in the face of limited training data. Nets are functions 7](x; w), parameterized by their weight vector w E W ~ Rd, which take as input points x E Rk. For classifiers, network output is restricted to {a, 1} while for",Optimization & Theoretical ML,89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf,1994
Generalisation in Feedforward Networks,"We discuss a model of consistent learning with an additional re striction on the probability distribution of training samples, the target concept and hypothesis class. We show that the model pro vides a significant improvement on the upper bounds of sample complexity, i.e. the minimal number of random training samples allowing a selection of the hypothesis with a predefined accuracy and confidence. Further, we show that the model has the poten tial for providing a finite sample complexity even in the case of infinite VC-dimension as well as for a sample complexity below VC-dimension. This is achieved by linking sample complexity to an ""average"" number of implement able dichotomies of a training sample rather than the maximal size of a shattered sample, i.e. VC-dimension. 1 Introduction A number offundamental results in computational learning theory [1, 2, 11] links the generalisation error achievable by a set of hypotheses with its Vapnik-Chervonenkis dimension (VC-dimension, for short",Optimization & Theoretical ML,8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf,1994
The Use of Dynamic Writing Information,"In this paper we present NPen+ +, a connectionist system for writer independent, large vocabulary on-line cursive handwriting recognition. This system combines a robust input representation, which preserves the dynamic writing information, with a neural network architecture, a so called Multi-State Time Delay Neural Network (MS-TDNN), which integrates rec.ognition and segmen tation in a single framework. Our preprocessing transforms the original coordinate sequence into a (still temporal) sequence offea ture vectors, which combine strictly local features, like curvature or writing direction, with a bitmap-like representation of the co ordinate's proximity. The MS-TDNN architecture is well suited for handling temporal sequences as provided by this input rep resentation. Our system is tested both on writer dependent and writer independent tasks with vocabulary sizes ranging from 400 up to 20,000 words. For example, on a 20,000 word vocabulary we achieve word recognition rates up to 88.9%",Computer Vision,8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf,1994
Capacity and Information Efficiency of a,"We have determined the capacity and information efficiency of an associative net configured in a brain-like way with partial connec tivity and noisy input cues. Recall theory was used to calculate the capacity when pattern recall is achieved using a winners-take all strategy. Transforming the dendritic sum according to input activity and unit usage can greatly increase the capacity of the associative net under these conditions. For moderately sparse pat terns, maximum information efficiency is achieved with very low connectivity levels (~ 10%). This corresponds to the level of con nectivity commonly seen in the brain and invites speculation that the brain is connected in the most information efficient way. 1 INTRODUCTION Standard network associative memories become more plausible as models of asso ciative memory in the brain if they incorporate (1) partial connectivity, (2) sparse activity and (3) recall from noisy cues. In this paper we consider the capacity of a binary associative ne",Theoretical ML,8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf,1994
SARDNET: A Self-Organizing Feature,"A self-organizing neural network for sequence classification called SARDNET is described and analyzed experimentally. SARDNET extends the Kohonen Feature Map architecture with activation re tention and decay in order to create unique distributed response patterns for different sequences. SARDNET yields extremely dense yet descriptive representations of sequential input in very few train ing iterations. The network has proven successful on mapping ar bitrary sequences of binary and real numbers, as well as phonemic representations of English words. Potential applications include isolated spoken word recognition and cognitive science models of sequence processing. 1 INTRODUCTION While neural networks have proved a good tool for processing static patterns, classi fying sequential information has remained a challenging task. The problem involves recognizing patterns in a time series of vectors, which requires forming a good inter nal representation for the sequences. Several researchers ha",Deep Learning,90794e3b050f815354e3e29e977a88ab-Paper.pdf,1994
Deterministic Annealing Variant,"We present a deterministic annealing variant of the EM algorithm for maximum likelihood parameter estimation problems. In our approach, the EM process is reformulated as the problem of min imizing the thermodynamic free energy by using the principle of maximum entropy and statistical mechanics analogy. Unlike simu lated annealing approaches, this minimization is deterministically performed. Moreover, the derived algorithm, unlike the conven tional EM algorithm, can obtain better estimates free of the initial parameter values. 1 INTRODUCTION The Expectation-Maximization (EM) algorithm (Dempster, Laird & Rubin, 1977) is an iterative statistical technique for computing maximum likelihood parameter estimates from incomplete data. It has generally been employed to a wide variety of parameter estimation problems. Recently, the EM algorithm has also been suc cessfully employed as the learning algorithm of the hierarchical mixture of experts (Jordan & Jacobs, 1993). In addition, it has been fo",Optimization & Theoretical ML,92262bf907af914b95a0fc33c3f33bf6-Paper.pdf,1994
A Non-linear Information Maximisation,"A new learning algorithm is derived which performs online stochas tic gradient ascent in the mutual information between outputs and inputs of a network. In the absence of a priori knowledge about the 'signal' and 'noise' components of the input, propagation of information depends on calibrating network non-linearities to the detailed higher-order moments of the input density functions. By incidentally minimising mutual information between outputs, as well as maximising their individual entropies, the network 'fac torises' the input into independent components. As an example application, we have achieved near-perfect separation of ten digi tally mixed speech signals. Our simulations lead us to believe that our network performs better at blind separation than the Herault J utten network, reflecting the fact that it is derived rigorously from the mutual information objective. 468 Anthony J. Bell, Terrence J. Sejnowski 1 Introduction Unsupervised learning algorithms based on information th",Optimization & Theoretical ML,9232fe81225bcaef853ae32870a2b0fe-Paper.pdf,1994
Real-Time Control of a Tokamak Plasma,"This paper presents results from the first use of neural networks for the real-time feedback control of high temperature plasmas in a tokamak fusion experiment. The tokamak is currently the prin cipal experimental device for research into the magnetic confine ment approach to controlled fusion. In the tokamak, hydrogen plasmas, at temperatures of up to 100 Million K, are confined by strong magnetic fields. Accurate control of the position and shape of the plasma boundary requires real-time feedback control of the magnetic field structure on a time-scale of a few tens of mi croseconds. Software simulations have demonstrated that a neural network approach can give significantly better performance than the linear technique currently used on most tokamak experiments. The practical application of the neural network approach requires high-speed hardware, for which a fully parallel implementation of the multilayer perceptron, using a hybrid of digital and analogue technology, has been develop",Optimization & Theoretical ML,9246444d94f081e3549803b928260f56-Paper.pdf,1994
Dynamic Cell Structures,"Dynamic Cell Structures (DCS) represent a family of artificial neural architectures suited both for unsupervised and supervised learning. They belong to the recently [Martinetz94] introduced class of Topology Representing Networks (TRN) which build perlectly topology pre serving feature maps. DCS empI'oy a modified Kohonen learning rule in conjunction with competitive Hebbian learning. The Kohonen type learning rule serves to adjust the synaptic weight vectors while Hebbian learning establishes a dynamic lateral connection structure between the units reflecting the topology of the feature manifold. In case of super vised learning, i.e. function approximation, each neural unit implements a Radial Basis Function, and an additional layer of linear output units adjusts according to a delta-rule. DCS is the first RBF-based approxima tion scheme attempting to concurrently learn and utilize a perfectly to pology preserving map for improved performance. Simulations on a selection of CMU-Benchm",Deep Learning,92977ae4d2ba21425a59afb269c2a14e-Paper.pdf,1994
Single Transistor Learning Synapses,"We describe single-transistor silicon synapses that compute, learn, and provide non-volatile memory retention. The single transistor synapses simultaneously perform long term weight storage, com pute the product of the input and the weight value, and update the weight value according to a Hebbian or a backpropagation learning rule. Memory is accomplished via charge storage on polysilicon floating gates, providing long-term retention without refresh. The synapses efficiently use the physics of silicon to perform weight up dates; the weight value is increased using tunneling and the weight value decreases using hot electron injection. The small size and low power operation of single transistor synapses allows the devel opment of dense synaptic arrays. We describe the design, fabri cation, characterization, and modeling of an array of single tran sistor synapses. When the steady state source current is used as the representation of the weight value, both the incrementing and decrementing ",Optimization & Theoretical ML,934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf,1994
Comparing the prediction accuracy of,"The TNM staging system has been used since the early 1960's to predict breast cancer patient outcome. In an attempt to in crease prognostic accuracy, many putative prognostic factors have been identified. Because the TNM stage model can not accom modate these new factors, the proliferation of factors in breast cancer has lead to clinical confusion. What is required is a new computerized prognostic system that can test putative prognostic factors and integrate the predictive factors with the TNM vari ables in order to increase prognostic accuracy. Using the area un der the curve of the receiver operating characteristic, we compare the accuracy of the following predictive models in terms of five year breast cancer-specific survival: pTNM staging system, princi pal component analysis, classification and regression trees, logistic regression, cascade correlation neural network, conjugate gradient descent neural, probabilistic neural network, and backpropagation neural network. Several stat",Computer Vision,9908279ebbf1f9b250ba689db6a0222b-Paper.pdf,1994
Learning direction in global motion: two,"Perceptual learning is defined as fast improvement in performance and retention of the learned ability over a period of time. In a set of psy chophysical experiments we demonstrated that perceptual learning oc curs for the discrimination of direction in stochastic motion stimuli. Here we model this learning using two approaches: a clustering model that learns to accommodate the motion noise, and an averaging model that learns to ignore the noise. Simulations of the models show performance similar to the psychophysical results. 1 Introduction Global motion perception is critical to many visual tasks: to perceive self-motion, to identify objects in motion, to determine the structure of the environment, and to make judgements for safe navigation. In the presence of noise, as in random dot kinematograms, efficient extraction of global motion involves considerable spatial integration. Newsome and Colleagues (1989) showed that neurons in the macaque middle temporal area (MT) are motion direc",NLP,9ab0d88431732957a618d4a469a0d4c3-Paper.pdf,1994
On-line Learning of Dichotomies,"The performance of on-line algorithms for learning dichotomies is studied. In on-line learn ing, the number of examples P is equivalent to the learning time, since each example is presented only once. The learning curve, or generalization error as a function of P, depends on the schedule at which the learning rate is lowered. For a target that is a perceptron rule, the learning curve of the perceptron algorithm can decrease as fast as p-1, if the sched ule is optimized. If the target is not realizable by a perceptron, the perceptron algorithm does not generally converge to the solution with lowest generalization error. For the case of unrealizability due to a simple output noise, we propose a new on-line algorithm for a perceptron yielding a learning curve that can approach the optimal generalization error as fast as p-l/2. We then generalize the perceptron algorithm to any class of thresholded smooth functions learning a target from that class. For ""well-behaved"" input distributions, ",Optimization & Theoretical ML,9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf,1994
Asymptotics of Gradient-based,"We study the asymptotic properties of the sequence of iterates of weight-vector estimates obtained by training a multilayer feed for ward neural network with a basic gradient-descent method using a fixed learning constant and no batch-processing. In the one dimensional case, an exact analysis establishes the existence of a limiting distribution that is not Gaussian in general. For the gen eral case and small learning constant, a linearization approximation permits the application of results from the theory of random ma trices to again establish the existence of a limiting distribution. We study the first few moments of this distribution to compare and contrast the results of our analysis with those of techniques of stochastic approximation. 1 INTRODUCTION The wide applicability of neural networks to problems in pattern classification and signal processing has been due to the development of efficient gradient-descent al gorithms for the supervised training of multilayer feedforward neur",Optimization & Theoretical ML,9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf,1994
Convergence Properties of the K-Means,"This paper studies the convergence properties of the well known K-Means clustering algorithm. The K-Means algorithm can be de scribed either as a gradient descent algorithm or by slightly extend ing the mathematics of the EM algorithm to this hard threshold case. We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm. 1 INTRODUCTION K-Means is a popular clustering algorithm used in many applications, including the initialization of more computationally expensive algorithms (Gaussian mixtures, Radial Basis Functions, Learning Vector Quantization and some Hidden Markov Models). The practice of this initialization procedure often gives the frustrating feeling that K-Means performs most of the task in a small fraction of the overall time. This motivated us to better understand this convergence speed. A second reason lies in the traditional debate between hard threshold (e.g. K Means, Viterbi Training) and soft threshold (e.g. Gauss",Optimization & Theoretical ML,a1140a3d0df1c81e24ae954d935e8926-Paper.pdf,1994
Using Voice Transformations to Create,"Speech recognizers provide good performance for most users but the error rate often increases dramatically for a small percentage of talkers who are ""different"" from those talkers used for training. One expensive solution to this problem is to gather more training data in an attempt to sample these outlier users. A second solution, explored in this paper, is to artificially enlarge the number of training talkers by transforming the speech of existing training talkers. This approach is similar to enlarging the training set for OCR digit recognition by warping the training digit images, but is more difficult because continuous speech has a much larger number of dimensions (e.g. linguistic, phonetic, style, temporal, spectral) that differ across talkers. We explored the use of simple linear spectral warping to enlarge a 48-talker training data base used for word spotting. The average detection rate overall was increased by 2.9 percentage points (from 68.3% to 71.2%) for male speakers and ",Speech Recognition,a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf,1994
Forward dynamic models in human,"Based on computational principles, with as yet no direct experi mental validation, it has been proposed that the central nervous system (CNS) uses an internal model to simulate the dynamic be havior of the motor system in planning, control and learning (Sut ton and Barto, 1981; Ito, 1984; Kawato et aI., 1987; Jordan and Rumelhart, 1992; Miall et aI., 1993). We present experimental re sults and simulations based on a novel approach that investigates the temporal propagation of errors in the sensorimotor integration process. Our results provide direct support for the existence of an internal model. 1 Introduction The notion of an internal model, a system which mimics the behavior of a natural process, has emerged as an important theoretical concept in motor control (Jordan, 1995). There are two varieties of internal models-""forward models,"" which mimic the causal flow of a process by predicting its next state given the current state and the motor command, and ""inverse models,"" which are ",Reinforcement Learning,a4300b002bcfb71f291dac175d52df94-Paper.pdf,1994
Direction Selectivity In Primary Visual,"Almost all models of orientation and direction selectivity in visual cortex are based on feedforward connection schemes, where genicu late input provides all excitation to both pyramidal and inhibitory neurons. The latter neurons then suppress the response of the for mer for non-optimal stimuli. However, anatomical studies show that up to 90 % of the excitatory synaptic input onto any corti cal cell is provided by other cortical cells. The massive excitatory feedback nature of cortical circuits is embedded in the canonical microcircuit of Douglas &. Martin (1991). We here investigate ana lytically and through biologically realistic simulations the function ing of a detailed model of this circuitry, operating in a hysteretic mode. In the model, weak geniculate input is dramatically ampli fied by intracortical excitation, while inhibition has a dual role: (i) to prevent the early geniculate-induced excitation in the null di rection and (ii) to restrain excitation and ensure that the neur",Computer Vision,a64c94baaf368e1840a1324e839230de-Paper.pdf,1994
Bayesian Query Construction for Neural,"If data collection is costly, there is much to be gained by actively se lecting particularly informative data points in a sequential way. In a Bayesian decision-theoretic framework we develop a query selec tion criterion which explicitly takes into account the intended use of the model predictions. By Markov Chain Monte Carlo methods the necessary quantities can be approximated to a desired preci sion. As the number of data points grows, the model complexity is modified by a Bayesian model selection strategy. The proper ties of two versions of the criterion ate demonstrated in numerical experiments. 1 INTRODUCTION In this paper we consider the situation where data collection is costly, as when for example, real measurements or technical experiments have to be performed. In this situation the approach of query learning ('active data selection', 'sequential experimental design', etc.) has a potential benefit. Depending on the previously seen examples, a new input value ('query') is selec",Optimization & Theoretical ML,a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf,1994
A Silicon Axon,"We present a silicon model of an axon which shows promise as a building block for pulse-based neural computations involving cor relations of pulses across both space and time. The circuit shares a number of features with its biological counterpart including an excitation threshold, a brief refractory period after pulse comple tion, pulse amplitude restoration, and pulse width restoration. We provide a simple explanation of circuit operation and present data from a chip fabricated in a standard 2Jlm CMOS process through the MOS Implementation Service (MOSIS). We emphasize the ne cessity of the restoration of the width of the pulse in time for stable propagation in axons. 1 INTRODUCTION It is well known that axons are neural processes specialized for transmitting infor mation over relatively long distances in the nervous system. Impulsive electrical disturbances known as action potentials are normally initiated near the cell body of a neuron when the voltage across the cell membrane cros",Optimization & Theoretical ML,aa169b49b583a2b5af89203c2b78c67c-Paper.pdf,1994
Plasticity-Mediated Competitive Learning,"Differentiation between the nodes of a competitive learning net work is conventionally achieved through competition on the ba sis of neural activity. Simple inhibitory mechanisms are limited to sparse representations, while decorrelation and factorization schemes that support distributed representations are computation ally unattractive. By letting neural plasticity mediate the compet itive interaction instead, we obtain diffuse, nonadaptive alterna tives for fully distributed representations. We use this technique to Simplify and improve our binary information gain optimiza tion algorithm for feature extraction (Schraudolph and Sejnowski, 1993); the same approach could be used to improve other learning algorithms. 1 INTRODUCTION Unsupervised neural networks frequently employ sets of nodes or subnetworks with identical architecture and objective function. Some form of competitive inter action is then needed for these nodes to differentiate and efficiently complement each other in their",Deep Learning,aa68c75c4a77c87f97fb686b2f068676-Paper.pdf,1994
Active Learning for Function,"We develop a principled strategy to sample a function optimally for function approximation tasks within a Bayesian framework. Using ideas from optimal experiment design, we introduce an objective function (incorporating both bias and variance) to measure the de gree of approximation, and the potential utility of the data points towards optimizing this objective. We show how the general strat egy can be used to derive precise algorithms to select data for two cases: learning unit step functions and polynomial functions. In particular, we investigate whether such active algorithms can learn the target with fewer examples. We obtain theoretical and empir ical results to suggest that this is the case. 1 INTRODUCTION AND MOTIVATION Learning from examples is a common supervised learning paradigm that hypothe sizes a target concept given a stream of training examples that describes the concept. In function approximation, example-based learning can be formulated as synthesiz ing an approximati",Optimization & Theoretical ML,acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf,1994
Patterns of damage in neural networks:,"Current understanding of the effects of damage on neural networks is rudimentary, even though such understanding could lead to im portant insights concerning neurological and psychiatric disorders. Motivated by this consideration, we present a simple analytical framework for estimating the functional damage resulting from fo cal structural lesions to a neural network. The effects of focal le sions of varying area, shape and number on the retrieval capacities of a spatially-organized associative memory. Although our analyti cal results are based on some approximations, they correspond well with simulation results. This study sheds light on some important features characterizing the clinical manifestations of multi-infarct dementia, including the strong association between the number of infarcts and the prevalence of dementia after stroke, and the 'mul tiplicative' interaction that has been postulated to occur between Alzheimer's disease and multi-infarct dementia. *Dr. Reggia is also wi",Optimization & Theoretical ML,b55ec28c52d5f6205684a473a2193564-Paper.pdf,1994
A Study of Parallel Perturbative,"We have continued our study of a parallel perturbative learning method [Alspector et al., 1993] and implications for its implemen tation in analog VLSI. Our new results indicate that, in most cases, a single parallel perturbation (per pattern presentation) of the func tion parameters (weights in a neural network) is theoretically the best course. This is not true, however, for certain problems and may not generally be true when faced with issues of implemen tation such as limited precision. In these cases, multiple parallel perturbations may be best as indicated in our previous results. 1 INTRODUCTION Motivated by difficulties in analog VLSI implementation of back-propagation [Rumelhart et al., 1986] and related algorithms that calculate gradients based on detailed knowledge of the neural network model, there were several similar re cent papers proposing to use a parallel [Alspector et al., 1993, Cauwenberghs, 1993, Kirk et al., 1993] or a semi-parallel [Flower and Jabri, 1993] perturb",Optimization & Theoretical ML,b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf,1994
A Neural Model of Delusions and,"We implement and study a computational model of Stevens' [19921 theory of the pathogenesis of schizophrenia. This theory hypoth esizes that the onset of schizophrenia is associated with reactive synaptic regeneration occurring in brain regions receiving degener ating temporal lobe projections. Concentrating on one such area, the frontal cortex, we model a frontal module as an associative memory neural network whose input synapses represent incoming temporal projections. We analyze how, in the face of weakened external input projections, compensatory strengthening of internal synaptic connections and increased noise levels can maintain mem ory capacities (which are generally preserved in schizophrenia). However, These compensatory changes adversely lead to sponta neous, biased retrieval of stored memories, which corresponds to the occurrence of schizophrenic delusions and hallucinations with out any apparent external trigger, and for their tendency to con centrate on just few central th",NLP,b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf,1994
Correlation and Interpolation Networks for,"We describe a framework for real-time tracking of facial expressions that uses neurally-inspired correlation and interpolation methods. A distributed view-based representation is used to characterize facial state, and is computed using a replicated correlation network. The ensemble response of the set of view correlation scores is input to a network based interpolation method, which maps perceptual state to motor control states for a simulated 3-D face model. Activation levels of the motor state correspond to muscle activations in an anatomically derived model. By integrating fast and robust 2-D processing with 3-D models, we obtain a system that is able to quickly track and interpret complex facial motions in real-time. 1 INTRODUCTION An important task for natural and artificial vision systems is the analysis and interpretation of faces. To be useful in interactive systems and in other settings where the information conveyed is of a time critical nature, analysis of facial expressions",Computer Vision,b706835de79a2b4e80506f582af3676a-Paper.pdf,1994
"Neural Network Ensembles, Cross","Learning of continuous valued functions using neural network en sembles (committees) can give improved accuracy, reliable estima tion of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members aver aged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combina tion with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1 INTRODUCTION It is well known that a combination of many different predictors can improve predic tions. In the neural networks community ""ensembles"" of neural networks",Optimization & Theoretical ML,b8c37e33defde51cf91e1e03e51657da-Paper.pdf,1994
Extracting Rules from Artificial Neural Networks,"Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neu ral networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illus trate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations. 1 Introduction In the last few years artificial neural networks have been applied successfully to a variety of real-world problems. For ",Optimization & Theoretical ML,bea5955b308361a1b07bc55042e25e54-Paper.pdf,1994
A model of the hippocampus combining self,"A model of the hippocampus is presented which forms rapid self- orga nized representations of input arriving via the perforant path, performs recall of previous associations in region CA3, and performs comparison of this recall with afferent input in region CA 1. This comparison drives feedback regulation of cholinergic modulation to set appropriate dynamics for learning of new representations in region CA3 and CA 1. The network responds to novel patterns with increased cholinergic mod ulation, allowing storage of new self-organized representations, but responds to familiar patterns with a decrease in acetylcholine, allowing recall based on previous representations. This requires selectivity of the cholinergic suppression of synaptic transmission in stratum radiatum of regions CA3 and CAl, which has been demonstrated experimentally. 1 INTRODUCTION A number of models of hippocampal function have been developed (Burgess et aI., 1994; Myers and Gluck, 1994; Touretzky et al., 1994), but re",NLP,c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf,1994
Glove-TalkII: Mapping Hand Gestures to,"Glove-TaikII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped contin uously to 10 control parameters of a parallel formant speech syn thesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses sev eral input devices (including a CyberGlove, a ContactGlove, a 3- space tracker, and a foot-pedal), a parallel formant speech synthe sizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any tra",Computer Vision,c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf,1994
Learning in large linear perceptrons and,"We present a new method for obtaining the response function 9 and its average G from which most of the properties of learning and generalization in linear perceptrons can be derived. We first rederive the known results for the 'thermodynamic limit' of infinite perceptron size N and show explicitly that 9 is self-averaging in this limit. We then discuss extensions of our method to more gen eral learning scenarios with anisotropic teacher space priors, input distributions, and weight decay terms. Finally, we use our method to calculate the finite N corrections of order 1/N to G and discuss the corresponding finite size effects on generalization and learning dynamics. An important spin-off is the observation that results obtained in the thermodynamic limit are often directly relevant to systems of fairly modest, 'real-world' sizes. 1 INTRODUCTION One of the main areas of research within the Neural Networks community is the issue of learning and generalization. Starting from a set of train",Optimization & Theoretical ML,c32d9bf27a3da7ec8163957080c8628e-Paper.pdf,1994
Learning Saccadic Eye Movements,We describe a framework for learning saccadic eye movements using a photometric representation of target points in natural scenes. The rep resentation takes the form of a high-dimensional vector comprised of the responses of spatial filters at different orientations and scales. We first demonstrate the use of this response vector in the task of locating pre viously foveated points in a scene and subsequently use this property in a multisaccade strategy to derive an adaptive motor map for delivering accurate saccades. 1 Introduction There has been recent interest in the use of space-variant sensors in active vision systems for tasks such as visual search and object tracking [14]. Such sensors realize the simultane ous need for wide field-of-view and good visual acuity. One popular class of space-variant sensors is formed by log-polar sensors which have a small area near the optical axis of greatly increased resolution (the fovea) coupled with a peripheral region that witnesses a gradual,Computer Vision,c4015b7f368e6b4871809f49debe0579-Paper.pdf,1994
A Charge-Based CMOS Parallel Analog,"We present an analog VLSI chip for parallel analog vector quantiza tion. The MOSIS 2.0 J..Lm double-poly CMOS Tiny chip contains an array of 16 x 16 charge-based distance estimation cells, implementing a mean absolute difference (MAD) metric operating on a 16-input analog vector field and 16 analog template vectors. The distance cell includ ing dynamic template storage measures 60 x 78 J..Lm2• Additionally, the chip features a winner-take-all (WTA) output circuit of linear com plexity, with global positive feedback for fast and decisive settling of a single winner output. Experimental results on the complete 16 x 16 VQ system demonstrate correct operation with 34 dB analog input dynamic range and 3 J..Lsec cycle time at 0.7 mW power dissipation. 1 Introduction Vector quantization (VQ) [1] is a common ingredient in signal processing, for applications of pattern recognition and data compression in vision, speech and beyond. Certain neural network models for pattern recognition, such as K",Optimization & Theoretical ML,c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf,1994
Boosting the Performance,"Radial Basis Function (RBF) Networks, also known as networks of locally-tuned processing units (see [6]) are well known for their ease of use. Most algorithms used to train these types of net works, however, require a fixed architecture, in which the number of units in the hidden layer must be determined before training starts. The RCE training algorithm, introduced by Reilly, Cooper and Elbaum (see [8]), and its probabilistic extension, the P-RCE algorithm, take advantage of a growing structure in which hidden units are only introduced when necessary. The nature of these al gorithms allows training to reach stability much faster than is the case for gradient-descent based methods. Unfortunately P-RCE networks do not adjust the standard deviation of their prototypes individually, using only one global value for this parameter. This paper introduces the Dynamic Decay Adjustment (DDA) al gorithm which utilizes the constructive nature of the P-RCE al gorithm together with independent adap",Deep Learning,c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf,1994
An Alternative Model for Mixtures of,"We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models-trained by either EM or gradient ascent-there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers. 1 INTRODUCTION For the mixtures of experts architecture (Jacobs, Jordan, Nowlan & Hinton, 1991), the EM algorithm decouples the learning process in a manner that fits well with the modular structure and yields a considerably improved rate of convergence (Jordan & Jacobs, 1994). The favorable properties of EM have also been shown by theoretical analyses (Jordan & Xu, in press; Xu & Jordan, 1994). It is difficult to apply EM to some parts of the mixt",Deep Learning,c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf,1994
Catastrophic Interference in Human,"Biological sensorimotor systems are not static maps that transform input (sensory information) into output (motor behavior). Evi dence from many lines of research suggests that their representa tions are plastic, experience-dependent entities. While this plastic ity is essential for flexible behavior, it presents the nervous system with difficult organizational challenges. If the sensorimotor system adapts itself to perform well under one set of circumstances, will it then perform poorly when placed in an environment with different demands (negative transfer)? Will a later experience-dependent change undo the benefits of previous learning (catastrophic inter ference)? We explore the first question in a separate paper in this volume (Shadmehr et al. 1995). Here we present psychophysical and computational results that explore the question of catastrophic interference in the context of a dynamic motor learning task. Un der some conditions, subjects show evidence of catastrophic inter fere",Neuroscience,ca75910166da03ff9d4655a0338e6b09-Paper.pdf,1994
On the Computational Complexity of Networks of,"We investigate the computational power of a formal model for net works of spiking neurons, both for the assumption of an unlimited timing precision, and for the case of a limited timing precision. We also prove upper and lower bounds for the number of examples that are needed to train such networks. 1 Introduction and Basic Definitions There exists substantial evidence that timing phenomena such as temporal differ ences between spikes and frequencies of oscillating subsystems are integral parts of various information processing mechanisms in biological neural systems (for a survey and references see e.g. Abeles, 1991; Churchland and Sejnowski, 1992; Aert sen, 1993). Furthermore simulations of a variety of specific mathematical models for networks of spiking neurons have shown that temporal coding offers interesting possibilities for solving classical benchmark-problems such as associative memory, binding, and pattern segmentation (for an overview see Gerstner et al., 1992). Some aspect",Optimization & Theoretical ML,cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf,1994
New Algorithms for,"A fundamental open problem in computer vision-determining pose and correspondence between two sets of points in space is solved with a novel, robust and easily implementable algorithm. The technique works on noisy point sets that may be of unequal sizes and may differ by non-rigid transformations. A 2D varia tion calculates the pose between point sets related by an affine transformation-translation, rotation, scale and shear. A 3D to 3D variation calculates translation and rotation. An objective describ ing the problem is derived from Mean field theory. The objective is minimized with clocked (EM-like) dynamics. Experiments with both handwritten and synthetic data provide empirical evidence for the method. 1 Introduction Matching the representations of two images has long been the focus of much research in Computer Vision, forming an essential component of many machine-based ob- 1 E-mail address of authors: lastname-firstname@cs.yale.edu 2Department of Computer Science and Engineering,",Computer Vision,cc1aa436277138f61cda703991069eaf-Paper.pdf,1994
PCA-Pyramids for Image Compression*,"This paper presents a new method for image compression by neural networks. First, we show that we can use neural networks in a py ramidal framework, yielding the so-called PCA pyramids. Then we present an image compression method based on the PCA pyramid, which is similar to the Laplace pyramid and wavelet transform. Some experimental results with real images are reported. Finally, we present a method to combine the quantization step with the learning of the PCA pyramid. 1 Introduction In the past few years, a lot of work has been done on using neural networks for image compression, d . e.g. (Cottrell et al., 1987; Sanger, 1989; Mougeot et al., 1991; Schweizer et al., 1991)). Typically, networks which perform a Principal Component Analysis (PCA) were employed; for a recent overview of PCA networks, see (Baldi and Hornik, 1995). A well studied and thoroughly understood PCA network architecture is the linear autoassociative network, see (Baldi and Hornik, 1989; Bourlard and Kamp, 1988). ",Computer Vision,ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf,1994
Morphogenesis of the Lateral Geniculate,"The macaque lateral geniculate nucleus (LGN) exhibits an intricate lamination pattern, which changes midway through the nucleus at a point coincident with small gaps due to the blind spot in the retina. We present a three-dimensional model of morphogenesis in which local cell interactions cause a wave of development of neuronal re ceptive fields to propagate through the nucleus and establish two distinct lamination patterns. We examine the interactions between the wave and the localized singularities due to the gaps, and find that the gaps induce the change in lamination pattern. We explore critical factors which determine general LGN organization. 1 INTRODUCTION Each side of the mammalian brain contains a structure called the lateral geniculate nucleus (LGN), which receives visual input from both eyes and sends projections to 134 Svilen Tzonev, Klaus Schulten, Joseph G. Malpeli the primary visual cortex. In primates the LG N consists of several distinct layers of neurons separated by ",Computer Vision,cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf,1994
Reinforcement Learning Predicts the Site,"The auditory system of the barn owl contains several spatial maps. In young barn owls raised with optical prisms over their eyes, these auditory maps are shifted to stay in register with the visual map, suggesting that the visual input imposes a frame of reference on the auditory maps. However, the optic tectum, the first site of convergence of visual with auditory information, is not the site of plasticity for the shift of the auditory maps; the plasticity occurs instead in the inferior colliculus, which contains an auditory map and projects into the optic tectum. We explored a model of the owl remapping in which a global reinforcement signal whose delivery is controlled by visual foveation. A hebb learning rule gated by rein forcement learned to appropriately adjust auditory maps. In addi tion, reinforcement learning preferentially adjusted the weights in the inferior colliculus, as in the owl brain, even though the weights were allowed to change throughout the auditory system. This ",Reinforcement Learning,d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf,1994
A Lagrangian Formulation For,"A training method based on a form of continuous spatially distributed optical error back-propagation is presented for an all optical network composed of nondiscrete neurons and weighted interconnections. The all optical network is feed-forward and is composed of thin layers of a Kerr type self focusing/defocusing nonlinear optical material. The training method is derived from a Lagrangian formulation of the constrained minimization of the network error at the output. This leads to a formulation that describes training as a calculation of the distributed error of the optical signal at the output which is then reflected back through the device to assign a spatially distributed error to the internal layers. This error is then used to modify the internal weighting values. Results from several computer simulations of the training are presented, and a simple optical table demonstration of the network is discussed. 772 Elizabeth C. Behrman 1 KERR TYPE MATERIALS Kerr-type optical networks util",Computer Vision,d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf,1994
Instance-Based State Identification for,"This paper presents instance-based state identification, an approach to reinforcement learning and hidden state that builds disambiguat ing amounts of short-term memory on-line, and also learns with an order of magnitude fewer training steps than several previous ap proaches. Inspired by a key similarity between learning with hidden state and learning in continuous geometrical spaces, this approach uses instance-based (or ""memory-based"") learning, a method that has worked well in continuous spaces. 1 BACKGROUND AND RELATED WORK When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, the robot suffers from hidden state. More formally, we say a reinforcement learning agent suffers from the hidden state problem if the agent's state representation is non-Markovian with respect to actions and utility. The hidden state problem arises as a case of perceptua",Reinforcement Learning,d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf,1994
Nonlinear Image Interpolation using,"The problem of interpolating between specified images in an image sequence is a simple, but important task in model-based vision. We describe an approach based on the abstract task of ""manifold learning"" and present results on both synthetic and real image se quences. This problem arose in the development of a combined lip-reading and speech recognition system. 1 Introduction Perception may be viewed as the task of combining impoverished sensory input with stored world knowledge to predict aspects of the state of the world which are not directly sensed. In this paper we consider the task of image interpolation by which we mean hypothesizing the structure of images which occurred between given images in a temporal sequence. This task arose during the development of a combined lip reading and speech recognition system [3], because the time windows for auditory and visual information are different (30 frames per second for the camera vs. 100 feature vectors per second for the acoustic inf",Computer Vision,d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf,1994
A Growing Neural Gas Network Learns,"An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the ""neural gas"" method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation. 1 INTRODUCTION In unsupervised learning settings only input data is available but no information on the desired output. What can the goal of learning be in this situation? One possible objective is dimensionality reduction: finding a low-dimensional sub space of the input vector space containing most or all of the input data. Linear subspaces with this property can be computed directly by principal component anal ysis or iteratively with a number of network models (",Optimization & Theoretical ML,d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf,1994
Transformation Invariant Autoassociation,"When training neural networks by the classical backpropagation algo rithm the whole problem to learn must be expressed by a set of inputs and desired outputs. However, we often have high-level knowledge about the learning problem. In optical character recognition (OCR), for in stance, we know that the classification should be invariant under a set of transformations like rotation or translation. We propose a new modular classification system based on several autoassociative multilayer percep trons which allows the efficient incorporation of such knowledge. Results are reported on the NIST database of upper case handwritten letters and compared to other approaches to the invariance problem. 1 INCORPORATION OF EXPLICIT KNOWLEDGE The aim of supervised learning is to learn a mapping between the input and the output space from a set of example pairs (input, desired output). The classical implementation in the domain of neural networks is the backpropagation algorithm. If this learning set i",Computer Vision,d707329bece455a462b58ce00d1194c9-Paper.pdf,1994
Learning To Play the Game of Chess,"This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach. 1 Introduction Throughout the last decades, the game of chess has been a major testbed for research on artificial intelligence and computer science. Most oftoday's chess programs rely on intensive search to generate moves. To evaluate boards, fast evaluation functions are employed which are usually carefully designed by hand, sometimes augmented by automatic parameter tuning methods [1]. Building a chess machine that learns to play solely from the final outcome of games (win/loss/draw) is a challenging open problem in AI. In this paper, we are interested in learning to play chess from the fina",Reinforcement Learning,d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf,1994
Interior Point Implementations of,"This paper presents an alternating minimization (AM) algorithm used in the training of radial basis function and linear regressor networks. The algorithm is a modification of a small-step interior point method used in solving primal linear programs. The algo rithm has a convergence rate of O( fo,L) iterations where n is a measure of the network size and L is a measure of the resulting solution's accuracy. Two results are presented that specify how aggressively the two steps of the AM may be pursued to ensure convergence of each step of the alternating minimization. 1 Introduction In recent years, considerable research has investigated the use of alternating min imization (AM) techniques in the supervised training of radial basis function networks. AM techniques were first introduced in soft-competitive learning al gorithms[l]. This training procedure was later shown to be closely related to Expectation-Maximization algorithms used by the statistical estimation commu nity[2]. Alternatin",Optimization & Theoretical ML,d79aac075930c83c2f1e369a511148fe-Paper.pdf,1994
A Convolutional Neural Network,"We describe a system that can track a hand in a sequence of video frames and recognize hand gestures in a user-independent manner. The system locates the hand in each video frame and determines if the hand is open or closed. The tracking system is able to track the hand to within ±10 pixels of its correct location in 99.7% of the frames from a test set containing video sequences from 18 dif ferent individuals captured in 18 different room environments. The gesture recognition network correctly determines if the hand being tracked is open or closed in 99.1 % of the frames in this test set. The system has been designed to operate in real time with existing hardware. 1 Introduction We describe an image processing system that uses convolutional neural networks to locate the position of a (moving) hand in a video frame, and to track the position of this hand across a sequence of video frames. In addition, for each frame, the system determines if the hand is currently open or closed. The inp",Computer Vision,d93ed5b6db83be78efb0d05ae420158e-Paper.pdf,1994
Temporal Dynamics of Generalization •,"This paper presents a rigorous characterization of how a general nonlinear learning machine generalizes during the training process when it is trained on a random sample using a gradient descent algorithm based on reduction of training error. It is shown, in particular, that best generalization performance occurs, in general, before the global minimum of the training error is achieved. The different roles played by the complexity of the machine class and the complexity of the specific machine in the class during learning are also precisely demarcated. 1 INTRODUCTION In learning machines such as neural networks, two major factors that affect the 'goodness of fit' of the examples are network size (complexity) and training time. These are also the major factors that affect the generalization performance of the network. Many theoretical studies exploring the relation between generalization performance and machine complexity support the parsimony heuristics suggested by Occam's ra zor, to w",Optimization & Theoretical ML,da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf,1994
Learning Stochastic Perceptrons Under,"We present a statistical method that PAC learns the class of stochastic perceptrons with arbitrary monotonic activation func tion and weights Wi E {-I, 0, + I} when the probability distribution that generates the input examples is member of a family that we call k-blocking distributions. Such distributions represent an impor tant step beyond the case where each input variable is statistically independent since the 2k-blocking family contains all the Markov distributions of order k. By stochastic percept ron we mean a per ceptron which, upon presentation of input vector x, outputs 1 with probability fCLJi WiXi - B). Because the same algorithm works for any monotonic (nondecreasing or nonincreasing) activation func tion f on Boolean domain, it handles the well studied cases of sigmolds and the ""usual"" radial basis functions. 1 INTRODUCTION Within recent years, the field of computational learning theory has emerged to pro vide a rigorous framework for the design and analysis of learning a",Optimization & Theoretical ML,dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf,1994
Recurrent Networks:,"Second order properties of cost functions for recurrent networks are investigated. We analyze a layered fully recurrent architecture, the virtue of this architecture is that it features the conventional feedforward architecture as a special case. A detailed description of recursive computation of the full Hessian of the network cost func tion is provided. We discuss the possibility of invoking simplifying approximations of the Hessian and show how weight decays iron the cost function and thereby greatly assist training. We present tenta tive pruning results, using Hassibi et al.'s Optimal Brain Surgeon, demonstrating that recurrent networks can construct an efficient internal memory. 1 LEARNING IN RECURRENT NETWORKS Time series processing is an important application area for neural networks and numerous architectures have been suggested, see e.g. (Weigend and Gershenfeld, 94). The most general structure is a fully recurrent network and it may be adapted using Real Time Recurrent Learni",Deep Learning,df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf,1994
Satoshi Suzuki Hiroshi Ando,"This paper presents an unsupervised learning scheme for categorizing 3D objects from their 2D projected images. The scheme exploits an auto-associative network's ability to encode each view of a single object into a representation that indicates its view direction. We propose two models that employ different classification mechanisms; the first model selects an auto-associative network whose recovered view best matches the input view, and the second model is based on a modular architecture whose additional network classifies the views by splitting the input space nonlinearly. We demonstrate the effectiveness of the proposed classification models through simulations using 3D wire-frame objects. 1 INTRODUCTION The human visual system can recognize various 3D (three-dimensional) objects from their 2D (two-dimensional) retinal images although the images vary significantly as the viewpoint changes. Recent computational models have explored how to learn to recognize 3D objects from their pro",Computer Vision,e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf,1994
"Hyperparameters, Evidence and","Using a statistical mechanical formalism we calculate the evidence, generalisation error and consistency measure for a linear percep tron trained and tested on a set of examples generated by a non linear teacher. The teacher is said to be unrealisable because the student can never model it without error. Our model allows us to interpolate between the known case of a linear teacher, and an un realisable, nonlinear teacher. A comparison of the hyperparameters which maximise the evidence with those that optimise the perfor mance measures reveals that, in the non-linear case, the evidence procedure is a misleading guide to optimising performance. Finally, we explore the extent to which the evidence procedure is unreliable and find that, despite being sub-optimal, in some circumstances it might be a useful method for fixing the hyperparameters. 1 INTRODUCTION The analysis of supervised learning or learning from examples is a major field of research within neural networks. In general, we hav",Optimization & Theoretical ML,e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf,1994
Adaptive Elastic Input Field for,"For machines to perform classification tasks, such as speech and character recognition, appropriately handling deformed patterns is a key to achieving high performance. The authors presents a new type of classification system, an Adaptive Input Field Neu ral Network (AIFNN), which includes a simple pre-trained neural network and an elastic input field attached to an input layer. By using an iterative method, AIFNN can determine an optimal affine translation for an elastic input field to compensate for the original deformations. The convergence of the AIFNN algorithm is shown. AIFNN is applied for handwritten numerals recognition. Conse quently, 10.83% of originally misclassified patterns are correctly categorized and total performance is improved, without modifying the neural network. 1 Introduction For machines to accomplish classification tasks, such as speech and character recog nition, appropriately handling deformed patterns is a key to achieving high perfor mance [Simard 92] [Sim",Computer Vision,e744f91c29ec99f0e662c9177946c627-Paper.pdf,1994
A Model for Chemosensory Reception,"A new model for chemosensory reception is presented. It models reacti ons between odor molecules and receptor proteins and the activation of second messenger by receptor proteins. The mathematical formulation of the reaction kinetics is transformed into an artificial neural network (ANN). The resulting feed-forward network provides a powerful means for parameter fitting by applying learning algorithms. The weights of the network corresponding to chemical parameters can be trained by presen ting experimental data. We demonstrate the simulation capabilities of the model with experimental data from honey bee chemosensory neurons. It can be shown that our model is sufficient to rebuild the observed data and that simpler models are not able to do this task. 1 INTRODUCTION Terrestrial animals, vertebrates and invertebrates, have developed very similar solutions for the problem of recognizing volatile substances [Vogt et ai., 1989]. Odor molecules bind to receptor proteins (receptor sites) at",Deep Learning,e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf,1994
A Real Time Clustering CMOS,"We describe an analog VLSI implementation of the ARTI algorithm (Carpenter, 1987). A prototype chip has been fabricated in a standard low cost 1.5~m double-metal single-poly CMOS process. It has a die area of lcm2 and is mounted in a 12O-pins PGA package. The chip realizes a modified version of the original ARTI architecture. Such modification has been shown to preserve all computational properties of the original algorithm (Serrano, 1994a), while being more appropriate for VLSI realizations. The chip implements an ARTI network with 100 F1 nodes and 18 F2 nodes. It can therefore cluster 100 binary pixels input patterns into up to 18 different categories. Modular expansibility of the system is possible by assembling an NxM array of chips without any extra interfacing circuitry, resulting in an F 1 layer with l00xN nodes, and an F2 layer with 18xM nodes. Pattern classification is performed in less than 1.8~s, which means an equivalent computing power of 2.2x109 connections and connection",Optimization & Theoretical ML,ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf,1994
Learning from queries for maximum,"In supervised learning, learning from queries rather than from random examples can improve generalization performance signif icantly. We study the performance of query learning for problems where the student cannot learn the teacher perfectly, which occur frequently in practice. As a prototypical scenario of this kind, we consider a linear perceptron student learning a binary perceptron teacher. Two kinds of queries for maximum information gain, i.e., minimum entropy, are investigated: Minimum student space en tropy (MSSE) queries, which are appropriate if the teacher space is unknown, and minimum teacher space entropy (MTSE) queries, which can be used if the teacher space is assumed to be known, but a student of a simpler form has deliberately been chosen. We find that for MSSE queries, the structure of the student space deter mines the efficacy of query learning, whereas MTSE queries lead to a higher generalization error than random examples, due to a lack of feedback about the progr",Optimization & Theoretical ML,eeb69a3cb92300456b6a5f4162093851-Paper.pdf,1994
Factorial Learning by Clustering Features,"We introduce a novel algorithm for factorial learning, motivated by segmentation problems in computational vision, in which the underlying factors correspond to clusters of highly correlated input features. The algorithm derives from a new kind of competitive clustering model, in which the cluster generators compete to ex plain each feature of the data set and cooperate to explain each input example, rather than competing for examples and cooper ating on features, as in traditional clustering algorithms. A natu ral extension of the algorithm recovers hierarchical models of data generated from multiple unknown categories, each with a differ ent, multiple causal structure. Several simulations demonstrate the power of this approach. 1 INTRODUCTION Unsupervised learning is the search for structure in data. Most unsupervised learn ing systems can be viewed as trying to invert a particular generative model of the data in order to recover the underlying causal structure of their world. Differ",Computer Vision,ef4e3b775c934dada217712d76f3d51f-Paper.pdf,1994
Generalization in Reinforcement Learning:,"A straightforward approach to the curse of dimensionality in re inforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neu ral net. Although this has been successful in the domain of backgam mon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approx imation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization. 1 INTRODUCTION Reinforcement learning-the problem of getting an agent to learn to act from sparse, delayed rewards-has been advanced by techniques based on dynamic programming (DP). These algorithms compute a value function which gives, for each state, the min imum possible long-term cost commencing in that state. For the high-dimensional and continuous state spaces chara",Reinforcement Learning,ef50c335cca9f340bde656363ebd02fd-Paper.pdf,1994
A Rapid Graph-based Method for,"We present a graph-based method for rapid, accurate search through prototypes for transformation-invariant pattern classifica tion. Our method has in theory the same recognition accuracy as other recent methods based on ''tangent distance"" [Simard et al., 1994], since it uses the same categorization rule. Nevertheless ours is significantly faster during classification because far fewer tan gent distances need be computed. Crucial to the success of our system are 1) a novel graph architecture in which transformation constraints and geometric relationships among prototypes are en coded during learning, and 2) an improved graph search criterion, used during classification. These architectural insights are applica ble to a wide range of problem domains. Here we demonstrate that on a handwriting recognition task, a basic implementation of our system requires less than half the computation of the Euclidean sorting method. 1 INTRODUCTION In recent years, the crucial issue of incorporating inv",Computer Vision,f33ba15effa5c10e873bf3842afb46a6-Paper.pdf,1994
A solvable connectionist model of,"A model of short-term memory for serially ordered lists of verbal stimuli is proposed as an implementation of the 'articulatory loop' thought to mediate this type of memory (Baddeley, 1986). The model predicts the presence of a repeatable time-varying 'context' signal coding the timing of items' presentation in addition to a store of phonological information and a process of serial rehearsal. Items are associated with context nodes and phonemes by Hebbian connections showing both short and long term plasticity. Items are activated by phonemic input during presentation and reactivated by context and phonemic feedback during output. Serial selection of items occurs via a winner-take-all interaction amongst items, with the winner subsequently receiving decaying inhibition. An approximate analysis of error probabilities due to Gaussian noise during output is presented. The model provides an explanatory account of the probability of error as a function of serial position, list length, word ",NLP,f47d0ad31c4c49061b9e505593e3db98-Paper.pdf,1994
oq p r s t u,No abstract found,Optimization & Theoretical ML,f57a2f557b098c43f11ab969efe1504b-Paper.pdf,1994
Grammar Learning by a Self-Organizing,"This paper presents the design and simulation results of a self organizing neural network which induces a grammar from exam ple sentences. Input sentences are generated from a simple phrase structure grammar including number agreement, verb transitiv ity, and recursive noun phrase construction rules. The network induces a grammar explicitly in the form of symbol categorization rules and phrase structure rules. 1 Purpose and related works The purpose of this research is to show that a self-organizing network with a certain structure can acquire syntactic knowledge from only positive (i.e. grammatical) data, without requiring any initial knowledge or external teachers that correct errors. There has been research on supervised neural network models of language acquisi tion tasks [Elman, 1991, Miikkulainen and Dyer, 1988, John and McClelland, 1988]. Unlike these supervised models, the current model self-organizes word and phrasal categories and phrase construction rules through mere exposu",NLP,fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf,1994
U sing a neural net to instantiate a,"Deformable models are an attractive approach to recognizing non rigid objects which have considerable within class variability. How ever, there are severe search problems associated with fitting the models to data. We show that by using neural networks to provide better starting points, the search time can be significantly reduced. The method is demonstrated on a character recognition task. In previous work we have developed an approach to handwritten character recogni tion based on the use of deformable models (Hinton, Williams and Revow, 1992a; Revow, Williams and Hinton, 1993). We have obtained good performance with this method, but a major problem is that the search procedure for fitting each model to an image is very computationally intensive, because there is no efficient algorithm (like dynamic programming) for this task. In this paper we demonstrate that it is possible to ""compile down"" some of the knowledge gained while fitting models to data to obtain better starting points t",Computer Vision,fba9d88164f3e2d9109ee770223212a0-Paper.pdf,1994
ICEG Morphology Classification using an,"An analogue VLSI neural network has been designed and tested to perform cardiac morphology classification tasks. Analogue tech niques were chosen to meet the strict power and area requirements of an Implantable Cardioverter Defibrillator (ICD) system. The ro bustness of the neural network architecture reduces the impact of noise, drift and offsets inherent in analogue approaches. The net work is a 10:6:3 multi-layer percept ron with on chip digital weight storage, a bucket brigade input to feed the Intracardiac Electro gram (ICEG) to the network and has a winner take all circuit at the output. The network was trained in loop and included a commercial ICD in the signal processing path. The system has suc cessfully distinguished arrhythmia for different patients with better than 90% true positive and true negative detections for dangerous rhythms which cannot be detected by present ICDs. The chip was implemented in 1.2um CMOS and consumes less than 200n W max imum average power in an are",Computer Vision,fed33392d3a48aa149a87a38b875ba4a-Paper.pdf,1994
Coarse-to-Fine Image Search Using Neural,"The efficiency of image search can be greatly improved by using a coarse-to-fine search strategy with a multi-resolution image representa tion. However, if the resolution is so low that the objects have few dis tinguishing features, search becomes difficult. We show that the performance of search at such low resolutions can be improved by using context information, i.e., objects visible at low-resolution which are not the objects of interest but are associated with them. The networks can be given explicit context information as inputs, or they can learn to detect the context objects, in which case the user does not have to be aware of their existence. We also use Integrated Feature Pyramids, which repre sent high-frequency information at low resolutions. The use of multi resolution search techniques allows us to combine information about the appearance of the objects on many scales in an efficient way. A natural fOlm of exemplar selection also arises from these techniques. We illus tra",Computer Vision,fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf,1994
Learning to Predict,"Visual occlusion events constitute a major source of depth information. This paper presents a self-organizing neural network that learns to detect, represent, and predict the visibility and invisibility relationships that arise during occlusion events, after a period of exposure to motion sequences containing occlusion and disocclusion events. The network develops two parallel opponent channels or ""chains"" of lateral excitatory connections for every resolvable motion trajectory. One channel, the ""On"" chain or ""visible"" chain, is activated when a moving stimulus is visible. The other channel, the ""Off"" chain or ""invisible"" chain, carries a persistent, amodal representation that predicts the motion of a formerly visible stimulus that becomes invisible due to occlusion. The learning rule uses disinhibition from the On chain to trigger learning in the Off chain. The On and Off chain neurons can learn separate associations with object depth or dering. The results are closely related to the ",Computer Vision,00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf,1995
Onset-based Sound Segmentation,"A technique for segmenting sounds using processing based on mam malian early auditory processing is presented. The technique is based on features in sound which neuron spike recording suggests are detected in the cochlear nucleus. The sound signal is band passed and each signal processed to enhance onsets and offsets. The onset and offset signals are compressed, then clustered both in time and across frequency channels using a network of integrate and-fire neurons. Onsets and offsets are signalled by spikes, and the timing of these spikes used to segment the sound. 1 Background Traditional speech interpretation techniques based on Fourier transforms, spectrum recoding, and a hidden Markov model or neural network interpretation stage have limitations both in continuous speech and in interpreting speech in the presence of noise, and this has led to interest in front ends modelling biological auditory systems for speech interpretation systems (Ainsworth and Meyer 92; Cosi 93; Cole et al 9",Computer Vision,021bbc7ee20b71134d53e20206bd6feb-Paper.pdf,1995
Beating a Defender in Robotic Soccer:,"Learning how to adjust to an opponent's position is critical to the success of having intelligent agents collaborating towards the achievement of specific tasks in unfriendly environments. This pa per describes our work on a Memory-based technique for to choose an action based on a continuous-valued state attribute indicating the position of an opponent. We investigate the question of how an agent performs in nondeterministic variations of the training situ ations. Our experiments indicate that when the random variations fall within some bound of the initial training, the agent performs better with some initial training rather than from a tabula-rasa. 1 Introduction One of the ultimate goals subjacent to the development of intelligent agents is to have multiple agents collaborating in the achievement of tasks in the presence of hostile opponents. Our research works towards this broad goal from a Machine Learning perspective. We are particularly interested in investigating how an intel ",Reinforcement Learning,03f544613917945245041ea1581df0c2-Paper.pdf,1995
A Neural Network Autoassociator for,"We present results on the use of neural network based autoassociators which act as novelty or anomaly detectors to detect imminent motor failures. The autoassociator is trained to reconstruct spectra obtained from the healthy motor. In laboratory tests, we have demonstrated that the trained autoassociator has a small reconstruction error on measurements recorded from healthy motors but a larger error on those recorded from a motor with a fault. We have designed and built a motor monitoring system using an autoassociator for anomaly detection and are in the process of testing the system at three industrial and commercial sites. 1 Introduction An unexpected breakdown of an electric induction motor can cause financial loss signif icantly in excess of the cost of the motor. For example, the breakdown of a motor in a production line during a production run can cause the loss of work in progress as well as loss of production time. When a motor does fail, it is not uncommon to replace it with",Computer Vision,062ddb6c727310e76b6200b7c71f63b5-Paper.pdf,1995
Laterally Interconnected Self-Organizing,"An application of laterally interconnected self-organizing maps (LISSOM) to handwritten digit recognition is presented. The lat eral connections learn the correlations of activity between units on the map. The resulting excitatory connections focus the activity into local patches and the inhibitory connections decorrelate redun dant activity on the map. The map thus forms internal representa tions that are easy to recognize with e.g. a perceptron network. The recognition rate on a subset of NIST database 3 is 4.0% higher with LISSOM than with a regular Self-Organizing Map (SOM) as the front end, and 15.8% higher than recognition of raw input bitmaps directly. These results form a promising starting point for building pattern recognition systems with a LISSOM map as a front end. 1 Introduction Hand-written digit recognition has become one of the touchstone problems in neural networks recently. Large databases of training examples such as the NIST (National Institute of Standards and Tec",Computer Vision,09c6c3783b4a70054da74f2538ed47c6-Paper.pdf,1995
The Gamma MLP for Speech Phoneme,"We define a Gamma multi-layer perceptron (MLP) as an MLP with the usual synaptic weights replaced by gamma filters (as pro posed by de Vries and Principe (de Vries and Principe, 1992)) and associated gain terms throughout all layers. We derive gradient descent update equations and apply the model to the recognition of speech phonemes. We find that both the inclusion of gamma filters in all layers, and the inclusion of synaptic gains, improves the performance of the Gamma MLP. We compare the Gamma MLP with TDNN, Back-Tsoi FIR MLP, and Back-Tsoi I1R MLP architectures, and a local approximation scheme. We find that the Gamma MLP results in an substantial reduction in error rates. 1 INTRODUCTION 1.1 THE GAMMA FILTER Infinite Impulse Response (I1R) filters have a significant advantage over Finite Im pulse Response (FIR) filters in signal processing: the length of the impulse response is uncoupled from the number of filter parameters. The length of the impulse re sponse is related to the mem",NLP,0768281a05da9f27df178b5c39a51263-Paper.pdf,1995
hnproved Silicon Cochlea,"Analog electronic cochlear models need exponentially scaled filters. CMOS Compatible Lateral Bipolar Transistors (CLBTs) can create exponentially scaled currents when biased using a resistive line with a voltage difference between both ends of the line. Since these CLBTs are independent of the CMOS threshold voltage, current sources implemented with CLBTs are much better matched than current sources created with MOS transistors operated in weak inversion. Measurements from integrated test chips are shown to verify the improved matching.",Optimization & Theoretical ML,0a0a0c8aaa00ade50f74a3f0ca981ed7-Paper.pdf,1995
On the Computational Power of Noisy,"It has remained unknown whether one can in principle carry out reliable digital computations with networks of biologically realistic models for neurons. This article presents rigorous constructions for simulating in real-time arbitrary given boolean circuits and fi nite automata with arbitrarily high reliability by networks of noisy spiking neurons. In addition we show that with the help of ""shunting inhibition"" even networks of very unreliable spiking neurons can simulate in real-time any McCulloch-Pitts neuron (or ""threshold gate""), and therefore any multilayer perceptron (or ""threshold circuit"") in a reliable manner. These constructions provide a possible explana tion for the fact that biological neural systems can carry out quite complex computations within 100 msec. It turns out that the assumption that these constructions require about the shape of the EPSP's and the behaviour of the noise are surprisingly weak. 1 Introduction We consider networks that consist of a finite set V o",Reinforcement Learning,0c048b3a434e49e655c1247efb389cec-Paper.pdf,1995
Parallel Optimization of Motion,"This paper describes a policy iteration algorithm for optimizing the performance of a harmonic function-based controller with respect to a user-defined index. Value functions are represented as poten tial distributions over the problem domain, being control policies represented as gradient fields over the same domain. All interme diate policies are intrinsically safe, i.e. collisions are not promoted during the adaptation process. The algorithm has efficient imple mentation in parallel SIMD architectures. One potential applica tion - travel distance minimization - illustrates its usefulness. 1 INTRODUCTION Harmonic functions have been proposed as a uniform framework for the solu tion of several versions of the motion planning problem. Connolly and Gru pen [Connolly and Grupen, 1993] have demonstrated how harmonic functions can be used to construct smooth, complete artificial potentials with no lo cal minima. In addition, these potentials meet the criteria established in [Rimon and Kodi",Optimization & Theoretical ML,0eec27c419d0fe24e53c90338cdc8bc6-Paper.pdf,1995
Model Matching and SFMD,"In systems that process sensory data there is frequently a model matching stage where class hypotheses are combined to recognize a complex entity. We introduce a new model of parallelism, the Single Function Multiple Data (SFMD) model, appropriate to this stage. SFMD functionality can be added with small hardware expense to certain existing SIMD architectures, and as an incremental addition to the programming model. Adding SFMD to an SIMD machine will not only allow faster model matching, but also increase its flexibility as a general purpose machine and its scope in performing the initial stages of sensory processing. 1 INTRODUCTION In systems that process sensory data there is frequently a post-classification stage where several independent class hypotheses are combined into the recognition of a more complex entity. Examples include matching word models with a string of observation probabilities, and matching visual object models with collections of edges or other features. Current p",Computer Vision,0f2c9a93eea6f38fabb3acb1c31488c6-Paper.pdf,1995
Learning with ensembles: How,"We study the characteristics of learning with ensembles. Solving exactly the simple model of an ensemble of linear students, we find surprisingly rich behaviour. For learning in large ensembles, it is advantageous to use under-regularized students, which actu ally over-fit the training data. Globally optimal performance can be obtained by choosing the training set sizes of the students ap propriately. For smaller ensembles, optimization of the ensemble weights can yield significant improvements in ensemble generaliza tion performance, in particular if the individual students are sub ject to noise in the training process. Choosing students with a wide range of regularization parameters makes this improvement robust against changes in the unknown level of noise in the training data. 1 INTRODUCTION An ensemble is a collection of a (finite) number of neural networks or other types of predictors that are trained for the same task. A combination of many differ ent predictors can often improv",Optimization & Theoretical ML,1019c8091693ef5c5f55970346633f92-Paper.pdf,1995
Analog VLSI Processor Implementing the,"We present an integrated analog processor for real-time wavelet decom position and reconstruction of continuous temporal signals covering the audio frequency range. The processor performs complex harmonic modu lation and Gaussian lowpass filtering in 16 parallel channels, each clocked at a different rate, producing a multiresolution mapping on a logarithmic frequency scale. Our implementation uses mixed-mode analog and dig ital circuits, oversampling techniques, and switched-capacitor filters to achieve a wide linear dynamic range while maintaining compact circuit size and low power consumption. We include experimental results on the processor and characterize its components separately from measurements on a single-channel test chip. 1 Introduction An effective mathematical tool for multiresolution analysis [Kais94], the wavelet transform has found widespread use in various signal processing applications involving characteristic patterns that cover multiple scales of resolution, such a",Optimization & Theoretical ML,1579779b98ce9edb98dd85606f2c119d-Paper.pdf,1995
A Novel Channel Selection System in,"State-of-the-art speech processors in cochlear implants perform channel selection using a spectral maxima strategy. This strategy can lead to confusions when high frequency features are needed to discriminate between sounds. We present in this paper a novel channel selection strategy based upon pattern recognition which al lows ""smart"" channel selections to be made. The proposed strategy is implemented using multi-layer perceptrons trained on a multi speaker labelled speech database. The input to the network are the energy coefficients of N energy channels. The output of the system are the indices of the M selected channels. We compare the performance of our proposed system to that of spectral maxima strategy, and show that our strategy can produce significantly better results. 1 INTRODUCTION A cochlear implant is a device used to provide the sensation of sound to those who are profoundly deaf by means of electrical stimulation of residual auditory neurons. It generally consists of a d",NLP,16e6a3326dd7d868cbc926602a61e4d0-Paper.pdf,1995
Rapid Quality Estimation of Neural,"The choice of an input representation for a neural network can have a profound impact on its accuracy in classifying novel instances. However, neural networks are typically computationally expensive to train, making it difficult to test large numbers of alternative representations. This paper introduces fast quality measures for neural network representations, allowing one to quickly and ac curately estimate which of a collection of possible representations for a problem is the best. We show that our measures for ranking representations are more accurate than a previously published mea sure, based on experiments with three difficult, real-world pattern recognition problems. 1 Introduction A key component of successful artificial neural network (ANN) applications is an input representation that suits the problem. However, ANNs are usually costly to train, preventing one from trying many different representations. In this paper, we address this problem by introducing and evaluating three",Optimization & Theoretical ML,184260348236f9554fe9375772ff966e-Paper.pdf,1995
Generating Accurate and Diverse,"Neural-network ensembles have been shown to be very accurate classification techniques. Previous work has shown that an effec tive ensemble should consist of networks that are not only highly correct, but ones that make their errors on different parts of the input space as well. Most existing techniques, however, only in directly address the problem of creating such a set of networks. In this paper we present a technique called ADDEMUP that uses genetic algorithms to directly search for an accurate and diverse set of trained networks. ADDEMUP works by first creating an ini tial population, then uses genetic operators to continually create new networks, keeping the set of networks that are as accurate as possible while disagreeing with each other as much as possible. Ex periments on three DNA problems show that ADDEMUP is able to generate a set of trained networks that is more accurate than sev eral existing approaches. Experiments also show that ADDEMUP is able to effectively incorpora",Optimization & Theoretical ML,1896a3bf730516dd643ba67b4c447d36-Paper.pdf,1995
A Model of Spatial Representations in,"We have recently developed a theory of spatial representations in which the position of an object is not encoded in a particular frame of reference but, instead, involves neurons computing basis func tions of their sensory inputs. This type of representation is able to perform nonlinear sensorimotor transformations and is consis tent with the response properties of parietal neurons. We now ask whether the same theory could account for the behavior of human patients with parietal lesions. These lesions induce a deficit known as hemineglect that is characterized by a lack of reaction to stimuli located in the hemispace contralateral to the lesion. A simulated lesion in a basis function representation was found to replicate three of the most important aspects of hemineglect: i) The models failed to cross the leftmost lines in line cancellation experiments, ii) the deficit affected multiple frames of reference and, iii) it could be object centered. These results strongly support the basis ",NLP,1b0114c51cc532ed34e1954b5b9e4b58-Paper.pdf,1995
Temporal Difference Learning in,"A continuous-time, continuous-state version of the temporal differ ence (TD) algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobi ological modeling. An optimal nonlinear feedback control law was also derived using the derivatives of the value function. The per formance of the algorithms was tested in a task of swinging up a pendulum with limited torque. Both the ""critic"" that specifies the paths to the upright position and the ""actor"" that works as a non linear feedback controller were successfully implemented by radial basis function (RBF) networks. 1 INTRODUCTION The temporal-difference (TD) algorithm (Sutton, 1988) for delayed reinforcement learning has been applied to a variety of tasks, such as robot navigation, board games, and biological modeling (Houk et al., 1994). Elucidation of the relationship between TD learning and dynamic programming (DP) has provided good theoretical insights (Barto et al., 1995). ",Reinforcement Learning,1e1d184167ca7676cf665225e236a3d2-Paper.pdf,1995
Improving Policies without Measuring,"Performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions (Werbos, 1991) - what Baird (1993) calls the ad vantages of actions at states. Nevertheless, most existing methods in dynamic programming (including Baird's) compute some form of absolute utility function. For smooth problems, advantages satisfy two differential consistency conditions (including the requirement that they be free of curl), and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages. 1 Introd uction In deciding how to change a policy at a state, an agent only needs to know the differences (called advantages) between the total return based on taking each action a for one step and then following the policy forever after, and the total return based on always following the policy (the conventional value of the state under the policy). The advantages are like differentials - they do n",Optimization & Theoretical ML,208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf,1995
Selective Attention for Handwritten,"Completely parallel object recognition is NP-complete. Achieving a recognizer with feasible complexity requires a compromise be tween parallel and sequential processing where a system selectively focuses on parts of a given image, one after another. Successive fixations are generated to sample the image and these samples are processed and abstracted to generate a temporal context in which results are integrated over time. A computational model based on a partially recurrent feedforward network is proposed and made cred ible by testing on the real-world problem of recognition of hand written digits with encouraging results. 1 INTRODUCTION For all-parallel bottom-up recognition, allocating one separate unit for each possible feature combination, i.e., conjunctive encoding, implies combinatorial explosion. It has been shown that completely parallel, bottom-up visual object recognition is NP-complete (Tsotsos, 1990). By exchanging space with time, systems with much less complexity may be d",Computer Vision,20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf,1995
Learning Model Bias,"In this paper the problem of learning appropriate domain-specific bias is addressed. It is shown that this can be achieved by learning many related tasks from the same domain, and a theorem is given bounding the number tasks that must be learnt. A corollary of the theorem is that if the tasks are known to possess a common inter nal representation or preprocessing then the number of examples required per task for good generalisation when learning n tasks si multaneously scales like O(a + ~), where O(a) is a bound on the minimum number of examples requred to learn a single task, and O( a + b) is a bound on the number of examples required to learn each task independently. An experiment providing strong qualita tive support for the theoretical results is reported. 1 Introduction It has been argued (see [6]) that the main problem in machine learning is the biasing of a learner's hypothesis space sufficiently well to ensure good generalisation from a small number of examples. Once suitable b",Optimization & Theoretical ML,20d135f0f28185b84a4cf7aa51f29500-Paper.pdf,1995
Tempering Backpropagation Networks:,"Backpropagation learning algorithms typically collapse the network's structure into a single vector of weight parameters to be optimized. We suggest that their performance may be improved by utilizing the struc tural information instead of discarding it, and introduce a framework for ''tempering'' each weight accordingly. In the tempering model, activation and error signals are treated as approx imately independent random variables. The characteristic scale of weight changes is then matched to that ofthe residuals, allowing structural prop erties such as a node's fan-in and fan-out to affect the local learning rate and backpropagated error. The model also permits calculation of an upper bound on the global learning rate for batch updates, which in turn leads to different update rules for bias vs. non-bias weights. This approach yields hitherto unparalleled performance on the family re lations benchmark, a deep multi-layer network: for both batch learning with momentum and the delta-bar",Deep Learning,1e6e0a04d20f50967c64dac2d639a577-Paper.pdf,1995
Estimating the Bayes Risk from Sample Data,"A new nearest-neighbor method is described for estimating the Bayes risk of a multiclass pattern claSSification problem from sample data (e.g., a classified training set). Although it is assumed that the classification prob lem can be accurately described by sufficiently smooth class-conditional distributions, neither these distributions, nor the corresponding prior prob abilities of the classes are required. Thus this method can be applied to practical problems where the underlying probabilities are not known. This method is illustrated using two different pattern recognition problems. 1 INTRODUCTION An important application of artificial neural networks is to obtain accurate solutions to pattern classification problems. In this setting, each pattern, represented as an n-dimensional feature vector, is associated with a discrete pattern class, or state of nature (Duda and Hart, 1973). Using available information, (e.g., a statistically representative set of labeled feature vectors {(Xi",Optimization & Theoretical ML,2290a7385ed77cc5592dc2153229f082-Paper.pdf,1995
A MODEL OF AUDITORY STREAMING,"An essential feature of intelligent sensory processing is the ability to focus on the part of the signal of interest against a background of distracting signals, and to be able to direct this focus at will. In this paper the problem of auditory scene segmentation is considered and a model of the early stages of the process is proposed. The behaviour of the model is shown to be in agreement with a number of well known psychophysical results. The principal contribution of this model lies in demonstrating how streaming might result from interactions between the tonotopic patterns of activity of input signals and traces of previous activity which feedback and influence the way in which subsequent signals are processed. 1 INTRODUCTION The appropriate segmentation and grouping of incoming sensory signals is important in enabling an organism to interact effectively with its environment (Llinas, 1991). The formation of associations between signals, which are considered to arise from the same e",NLP,24146db4eb48c718b84cae0a0799dcfc-Paper.pdf,1995
Context-Dependent Classes in a Hybrid,"A method for incorporating context-dependent phone classes in a connectionist-HMM hybrid speech recognition system is intro duced. A modular approach is adopted, where single-layer networks discriminate between different context classes given the phone class and the acoustic data. The context networks are combined with a context-independent (CI) network to generate context-dependent (CD) phone probability estimates. Experiments show an average reduction in word error rate of 16% and 13% from the CI system on ARPA 5,000 word and SQALE 20,000 word tasks respectively. Due to improved modelling, the decoding speed of the CD system is more than twice as fast as the CI system. INTRODUCTION The ABBOT hybrid connectionist-HMM system performed competitively with many conventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations of speech recognition systems (Hochberg, Cook, Renals, Robinson & Schechtman 1995). This hybrid framework is attractive because it is compact, having far f",NLP,27ed0fb950b856b06e1273989422e7d3-Paper.pdf,1995
Exploiting Tractable Substructures,"We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory. 1 INTRODUCTION Learning the parameters in a probabilistic neural network may be viewed as a problem in statistical estimation. In networks with sparse connectivity (e.g. trees and chains), there exist efficient algorithms for the exact probabilistic calculations that support inference and learning. In general, however, these calculations are intractable, and approximations are required. Mean field theory provides a framework for appr",Optimization & Theoretical ML,285f89b802bcb2651801455c86d78f2a-Paper.pdf,1995
Statistical Theory of Overtraining - Is,"A statistical theory for overtraining is proposed. The analysis treats realizable stochastic neural networks, trained with Kullback Leibler loss in the asymptotic case. It is shown that the asymptotic gain in the generalization error is small if we perform early stop ping, even if we have access to the optimal stopping time. Consider ing cross-validation stopping we answer the question: In what ratio the examples should be divided into training and testing sets in or der to obtain the optimum performance. In the non-asymptotic region cross-validated early stopping always decreases the general ization error. Our large scale simulations done on a CM5 are in nice agreement with our analytical findings. 1 Introduction Training multilayer neural feed-forward networks, there is a folklore that the gen eralization error decreases in an early period of training, reaches the minimum and then increases as training goes on, while the training error monotonically decreases. Therefore, it is consid",Optimization & Theoretical ML,299a23a2291e2126b91d54f3601ec162-Paper.pdf,1995
Primitive Manipulation Learning with,"Infants' manipulative exploratory behavior within the environment is a vehicle of cognitive stimulation[McCall 1974]. During this time, infants practice and perfect sensorimotor patterns that become be havioral modules which will be seriated and imbedded in more com plex actions. This paper explores the development of such primitive learning systems using an embodied light-weight hand which will be used for a humanoid being developed at the MIT Artificial In telligence Laboratory[Brooks and Stein 1993]. Primitive grasping procedures are learned from sensory inputs using a connectionist reinforcement algorithm while two submodules preprocess sensory data to recognize the hardness of objects and detect shear using competitive learning and back-propagation algorithm strategies, respectively. This system is not only consistent and quick dur ing the initial learning stage, but also adaptable to new situations after training is completed. 1 INTRODUCTION Learning manipulation in an unpredicta",Reinforcement Learning,2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf,1995
Silicon Models,"We are developing special-purpose, low-power analog-to-digital converters for speech and music applications, that feature analog circuit models of biological audition to process the audio signal before conversion. This paper describes our most recent converter design, and a working system that uses several copies ofthe chip to compute multiple representations of sound from an analog input. This multi-representation system demonstrates the plausibility of inexpensively implementing an auditory scene analysis approach to sound processing.",Computer Vision,2cfd4560539f887a5e420412b370b361-Paper.pdf,1995
Human Face Detection in Visual Scenes,"We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates. 1 INTRODUCTION In this paper, we present a neural network-based algorithm to detect frontal views of faces in gray-scale images. The algorithms and training methods are general, and can be applied to other views of faces, as well as to similar object and pattern recognition problems. Training a ne",Computer Vision,2f29b6e3abc6ebdefb55456ea6ca5dc8-Paper.pdf,1995
SPERT-II: A Vector Microprocessor,"We report on our development of a high-performance system for neural network and other signal processing applications. We have designed and implemented a vector microprocessor and pack aged it as an attached processor for a conventional workstation. We present performance comparisons with commercial worksta tions on neural network backpropagation training. The SPERT-II system demonstrates significant speedups over extensively hand optimization code running on the workstations. 1 Introduction We are working on pattern recognition problems using neural networks with a large number of parameters. Because of the large computational requirements of our area of research, we set out to design an integrated circuit that would serve as a good building block for our systems. Initially we considered designing extremely special ized chips, as this would maximize performance for a particular algorithm. However, the algorithms we use undergo considerable change as our research progresses. Still, we ",Optimization & Theoretical ML,31857b449c407203749ae32dd0e7d64a-Paper.pdf,1995
"+ .%( ’* ) + , - )’",No abstract found,NLP,3473decccb0509fb264818a7512a8b9b-Paper.pdf,1995
Stochastic Hillclimbing as a Baseline,"We investigate the effectiveness of stochastic hillclimbing as a baseline for evaluating the performance of genetic algorithms (GAs) as combinato rial function optimizers. In particular, we address two problems to which GAs have been applied in the literature: Koza's ll-multiplexer problem and the jobshop problem. We demonstrate that simple stochastic hill climbing methods are able to achieve results comparable or superior to those obtained by the GAs designed to address these two problems. We further illustrate, in the case of the jobshop problem, how insights ob tained in the formulation of a stochastic hillclimbing algorithm can lead to improvements in the encoding used by a GA. 1 Introduction Genetic algorithms (GAs) are a class of randomized optimization heuristics based loosely on the biological paradigm of natural selection. Among other proposed ap plications, they have been widely advocated in recent years as a general method for obtaining approximate solutions to hard combinat",Optimization & Theoretical ML,36a1694bce9815b7e38a9dad05ad42e0-Paper.pdf,1995
"Using the Future to ""Sort Out"" the","A patient visits the doctor; the doctor reviews the patient's history, asks questions, makes basic measurements (blood pressure, ... ), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk-patients at higher risk are given more and faster attention. It is also sequential- it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1 Background There are 3,000,000 cases of pneumonia each year in the U.S., 900,000 of which are admitted to the hospital for treatment and testing. Most pneumonia patients recover",Deep Learning,36a16a2505369e0c922b6ea7a23a56d2-Paper.pdf,1995
Exponentially many local minima for single,"We show that for a single neuron with the logistic function as the transfer function the number of local minima of the error function based on the square loss can grow exponentially in the dimension. 1 INTRODUCTION Consider a single artificial neuron with d inputs. The neuron has d weights w E Rd. The output of the neuron for an input pattern x E Rd is y = ¢(x· w), where ¢ : R -+ R is a transfer function. For a given sequence of training examples ((Xt, Yt))I<t<m, each consisting of a pattern Xt E R d and a desired output Yt E R, the goal of the training phase for neural networks consists of minimizing the error function with respect to the weight vector w E Rd. This function is the sum of the losses between outputs of the neuron and the desired outputs summed over all training examples. In notation, the error function is Lm E(w) = L(Yt, ¢(Xt . w)) , t=1 where L : R x R -+ [0,00) is the loss function. A common example of a transfer function is the logistic function logistic( z) = I+!-' ",Optimization & Theoretical ML,3806734b256c27e41ec2c6bffa26d9e7-Paper.pdf,1995
An Information-theoretic Learning,"A new learning algorithm is developed for the design of statistical classifiers minimizing the rate of misclassification. The method, which is based on ideas from information theory and analogies to statistical physics, assigns data to classes in probability. The dis tributions are chosen to minimize the expected classification error while simultaneously enforcing the classifier's structure and a level of ""randomness"" measured by Shannon's entropy. Achievement of the classifier structure is quantified by an associated cost. The con strained optimization problem is equivalent to the minimization of a Helmholtz free energy, and the resulting optimization method is a basic extension of the deterministic annealing algorithm that explicitly enforces structural constraints on assignments while re ducing the entropy and expected cost with temperature. In the limit of low temperature, the error rate is minimized directly and a hard classifier with the requisite structure is obtained. This lear",Optimization & Theoretical ML,38ca89564b2259401518960f7a06f94b-Paper.pdf,1995
Improving Elevator Performance Using,"This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The el evator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one ele vator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demon strate the power of RL on a very large scale stochastic dynamic opti",Reinforcement Learning,390e982518a50e280d8e2b535462ec1f-Paper.pdf,1995
Optimal Asset Allocation,"In recent years, the interest of investors has shifted to computer ized asset allocation (portfolio management) to exploit the growing dynamics of the capital markets. In this paper, asset allocation is formalized as a Markovian Decision Problem which can be opti mized by applying dynamic programming or reinforcement learning based algorithms. Using an artificial exchange rate, the asset allo cation strategy optimized with reinforcement learning (Q-Learning) is shown to be equivalent to a policy computed by dynamic pro gramming. The approach is then tested on the task to invest liquid capital in the German stock market. Here, neural networks are used as value function approximators. The resulting asset alloca tion strategy is superior to a heuristic benchmark policy. This is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space. 1 Introduction Billions of dollars are daily pushed th",Reinforcement Learning,3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf,1995
Recursive Estimation of Dynamic,"In this paper, recursive estimation algorithms for dynamic modular networks are developed. The models are based on Gaussian RBF networks and the gating network is considered in two stages: At first, it is simply a time-varying scalar and in the second, it is based on the state, as in the mixture of local experts scheme. The resulting algorithm uses Kalman filter estimation for the model estimation and the gating probability estimation. Both, 'hard' and 'soft' competition based estimation schemes are developed where in the former, the most probable network is adapted and in the latter all networks are adapted by appropriate weighting of the data. 1 INTRODUCTION The problem of learning multiple modes in a complex nonlinear system is increas ingly being studied by various researchers [2, 3, 4, 5, 6], The use of a mixture of local experts [5, 6], and a conditional mixture density network [3] have been devel oped to model various modes of a system. The development has mainly been on model e",Optimization & Theoretical ML,3b712de48137572f3849aabd5666a4e3-Paper.pdf,1995
A Neural Network Classifier for,"This paper describes a neural network classifier for the 11000 chip, which optically reads the E13B font characters at the bottom of checks. The first layer of the neural network is a hardware linear classifier which recognizes the characters in this font. A second software neural layer is implemented on an inexpensive microprocessor to clean up the re sults of the first layer. The hardware linear classifier is mathematically specified using constraints and an optimization principle. The weights of the classifier are found using the active set method, similar to Vap nik's separating hyperplane algorithm. In 7.5 minutes ofSPARC 2 time, the method solves for 1523 Lagrange mUltipliers, which is equivalent to training on a data set of approximately 128,000 examples. The result ing network performs quite well: when tested on a test set of 1500 real checks, it has a 99.995% character accuracy rate. 1 A BRIEF OVERVIEW OF THE 11000 CHIP At Synaptics, we have created the 11000, an analog VLSI c",Computer Vision,3eb71f6293a2a31f3569e10af6552658-Paper.pdf,1995
Modeling Saccadic Targeting in Visual Search,"Visual cognition depends criticalIy on the ability to make rapid eye movements known as saccades that orient the fovea over targets of interest in a visual scene. Saccades are known to be ballistic: the pattern of muscle activation for foveating a prespecified target location is computed prior to the movement and visual feedback is precluded. Despite these distinctive properties, there has been no general model of the saccadic targeting strategy employed by the human visual system during visual search in natural scenes. This paper proposes a model for saccadic targeting that uses iconic scene representations derived from oriented spatial filters at multiple scales. Visual search proceeds in a coarse-to-fine fashion with the largest scale filter responses being compared first. The model was empirically tested by comparing its perfonnance with actual eye movement data from human subjects in a natural visual search task; preliminary results indicate substantial agreement between eye movem",Computer Vision,3fe78a8acf5fda99de95303940a2420c-Paper.pdf,1995
Plasticity of Center-Surround Opponent,"Despite the phylogenic and structural differences, the visual sys tems of different species, whether vertebrate or invertebrate, share certain functional properties. The center-surround opponent recep tive field (CSRF) mechanism represents one such example. Here, analogous CSRFs are shown to be formed in an artificial neural network which learns to localize contours (edges) of the luminance difference. Furthermore, when the input pattern is corrupted by a background noise, the CSRFs of the hidden units becomes shal lower and broader with decrease of the signal-to-noise ratio (SNR). The same kind of SNR-dependent plasticity is present in the CSRF of real visual neurons; in bipolar cells of the carp retina as is shown here experimentally, as well as in large monopolar cells of the fly compound eye as was described by others. Also, analogous SNR dependent plasticity is shown to be present in the biphasic flash responses (BPFR) of these artificial and biological visual systems. Thus, the s",Computer Vision,41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf,1995
Active Gesture Recognition using,"We have developed a foveated gesture recognition system that runs in an unconstrained office environment with an active camera. Us ing vision routines previously implemented for an interactive envi ronment, we determine the spatial location of salient body parts of a user and guide an active camera to obtain images of gestures or expressions. A hidden-state reinforcement learning paradigm is used to implement visual attention. The attention module selects targets to foveate based on the goal of successful recognition, and uses a new multiple-model Q-Iearning formulation. Given a set of target and distractor gestures, our system can learn where to foveate to maximally discriminate a particular gesture. 1 INTRODUCTION Vision has numerous uses in the natural world. It is used by many organisms in navigation and object recognition tasks, for finding resources or avoiding predators. Often overlooked in computational models of vision, however, and particularly rel evant for humans, is the us",Reinforcement Learning,43baa6762fa81bb43b39c62553b2970d-Paper.pdf,1995
On Neural Networks with Minimal,"Linear threshold elements are the basic building blocks of artificial neural networks. A linear threshold element computes a function that is a sign of a weighted sum of the input variables. The weights are arbitrary integers; actually, they can be very big integers- exponential in the number of the input variables. However, in practice, it is difficult to implement big weights. In the present literature a distinction is made between the two extreme cases: linear threshold functions with polynomial-size weights as opposed to those with exponential-size weights. The main contribution of this paper is to fill up the gap by further refining that separation. Namely, we prove that the class of linear threshold functions with polynomial-size weights can be divided into subclasses according to the degree of the polynomial. In fact, we prove a more general result- that there exists a minimal weight linear threshold function for any arbitrary number of inputs and any weight size. To prove those",Optimization & Theoretical ML,43dd49b4fdb9bede653e94468ff8df1e-Paper.pdf,1995
Neural Networks with Quadratic VC,"This paper shows that neural networks which use continuous acti vation functions have VC dimension at least as large as the square of the number of weights w. This result settles a long-standing open question, namely whether the well-known O( w log w) bound, known for hard-threshold nets, also held for more general sigmoidal nets. Implications for the number of samples needed for valid gen eralization are discussed. 1 Introduction One of the main applications of artificial neural networks is to pattern classification tasks. A set of labeled training samples is provided, and a network must be obtained which is then expected to correctly classify previously unseen inputs. In this context, a central problem is to estimate the amount of training data needed to guarantee satisfactory learning performance. To study this question, it is necessary to first formalize the notion of learning from examples. One such formalization is based on the paradigm of probably approximately correct (PAC) lea",Optimization & Theoretical ML,456ac9b0d15a8b7f1e71073221059886-Paper.pdf,1995
Factorial Hidden Markov Models,"We present a framework for learning in hidden Markov models with distributed state representations. Within this framework, we de rive a learning algorithm based on the Expectation-Maximization (EM) procedure for maximum likelihood estimation. Analogous to the standard Baum-Welch update rules, the M-step of our algo rithm is exact and can be solved analytically. However, due to the combinatorial nature of the hidden state representation, the exact E-step is intractable. A simple and tractable mean field approxima tion is derived. Empirical results on a set of problems suggest that both the mean field approximation and Gibbs sampling are viable alternatives to the computationally expensive exact algorithm. 1 Introduction A problem of fundamental interest to machine learning is time series modeling. Due to the simplicity and efficiency of its parameter estimation algorithm, the hidden Markov model (HMM) has emerged as one of the basic statistical tools for modeling discrete time series, f",NLP,4588e674d3f0faf985047d4c3f13ed0d-Paper.pdf,1995
Extracting Thee-Structured,"A significant limitation of neural networks is that the represen tations they learn are usually incomprehensible to humans. We present a novel algorithm, TREPAN, for extracting comprehensible, symbolic representations from trained neural networks. Our algo rithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demon strate that TREPAN is able to produce decision trees that maintain a high level of fidelity to their respective networks while being com prehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large net works and problems with high-dimensional input spaces. 1 Introduction For many learning tasks, it is important to produce classifiers that are not only highly accurate, but also easily understood by humans. Neural networks are lim ited in this respect, since they are usually difficult to interpret after training. In contrast to neural netw",NLP,45f31d16b1058d586fc3be7207b58053-Paper.pdf,1995
Improving Committee Diagnosis with,"Central to the performance improvement of a committee relative to individual networks is the error correlation between networks in the committee. We investigated methods of achieving error indepen dence between the networks by training the networks with different resampling sets from the original training set. The methods were tested on the sinwave artificial task and the real-world problems of hepatoma (liver cancer) and breast cancer diagnoses. 1 INTRODUCTION The idea of a neural net committee is to combine several neural net predictors to perform collective decision making, instead of using a single network (Perrone, 1993). The potential of a committee in improving classification performance has been well documented. Central to this improvement is the extent to which the errors tend to coincide. Committee errors occur where the misclassification sets of individual networks overlap. On the one hand, if all errors of committee members coincide, using a committee does not improve perfo",NLP,46072631582fc240dd2674a7d063b040-Paper.pdf,1995
The Capacity of a Bump,"Recently, several researchers have reported encouraging experimental re sults when using Gaussian or bump-like activation functions in multilayer perceptrons. Networks of this type usually require fewer hidden layers and units and often learn much faster than typical sigmoidal networks. To explain these results we consider a hyper-ridge network, which is a simple perceptron with no hidden units and a rid¥e activation function. If we are interested in partitioningp points in d dimensions into two classes then in the limit as d approaches infinity the capacity of a hyper-ridge and a perceptron is identical. However, we show that for p ~ d, which is the usual case in practice, the ratio of hyper-ridge to perceptron dichotomies approaches pl2(d + 1). 1 Introduction A hyper-ridge network is a simple perceptron with no hidden units and a ridge activation = = function. With one output this is conveniently described as y g(h) g(w . x - b) where g(h) = sgn(1 - h2). Instead of dividing an input-",Optimization & Theoretical ML,47a658229eb2368a99f1d032c8848542-Paper.pdf,1995
From Isolation to Cooperation:,"We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predic tions. Each expert is trained by minimizing a penalized local cross vali dation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the recep tive field in which its predictions are valid, and also to detect relevant in put features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning.",Optimization & Theoretical ML,4a08142c38dbe374195d41c04562d9f8-Paper.pdf,1995
Some results on convergent unlearning,"In this paper we consider probabilities of different asymptotics of convergent unlearning algorithm for the Hopfield-type neural net work (Plakhov & Semenov, 1994) treating the case of unbiased random patterns. We show also that failed unlearning results in total memory breakdown. 1 INTRODUCTION In the past years the unsupervised learning schemes arose strong interest among researchers but for the time being a little is known about underlying learning mech anisms, as well as still less rigorous results like convergence theorems were obtained in this field. One of promising concepts along this line is so called ""unlearning"" for the Hopfield-type neural networks (Hopfield et ai, 1983, van Hemmen & Klem mer, 1992, Wimbauer et ai, 1994). Elaborating that elegant ideas the convergent unlearning algorithm has recently been proposed (Plakhov & Semenov, 1994), ex ecuting without patterns presentation. It is aimed at to correct initial Hebbian connectivity in order to provide extensive storage ",Optimization & Theoretical ML,4a213d37242bdcad8e7300e202e7caa4-Paper.pdf,1995
Forward-backward retraining of recurrent,"This paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden Markov model, off-line handwriting recognition system. The network esti mates posterior distributions for each of a series of frames repre senting sections of a handwritten word. The supervised training algorithm, backpropagation through time, requires target outputs to be provided for each frame. Three methods for deriving these targets are presented. A novel method based upon the forward backward algorithm is found to result in the recognizer with the lowest error rate. 1 Introduction In the field of off-line handwriting recognition, the goal is to read a handwritten document and produce a machine transcription. Such a system could be used for a variety of purposes, from cheque processing and postal sorting to personal correspondence reading for the blind or historical document reading. In a previous publication (Senior 1994) we have described a system based on a",NLP,4ca82782c5372a547c104929f03fe7a9-Paper.pdf,1995
KODAK lMAGELINK™ OCR,"This paper describes the Kodak Imageliok OCR alphanumeric TM handprint module. There are two neural network algorithms at its cme: the first network is trained to find individual characters in an alphamuneric field, while the second one perfmns the classification. Both networks were trained on Gabor projections of the ociginal pixel images, which resulted in higher recognition rates and greater noise immunity. Compared to its purely numeric counterpart (Shusurovich and Thrasher, 1995), this version of the system has a significant applicatim specific postprocessing module. The system has been implemented in specialized parallel hardware, which allows it to run at 80 char/sec/board. It has been installed at the Driver and Vehicle Licensing Agency (DVLA) in the United Kingdom. and its overall success rate exceeds 96% (character level without rejects). which translates into 85% field rate. If approximately 20% of the fields are rejected. the system achieves 99.8% character and 99.5% field ",Computer Vision,4da04049a062f5adfe81b67dd755cecc-Paper.pdf,1995
Predictive Q-Routing: A Memory-based,"In this paper, we propose a memory-based Q-Iearning algorithm called predictive Q-routing (PQ-routing) for adaptive traffic con trol. We attempt to address two problems encountered in Q-routing (Boyan & Littman, 1994), namely, the inability to fine-tune rout ing policies under low network load and the inability to learn new optimal policies under decreasing load conditions. Unlike other memory-based reinforcement learning algorithms in which mem ory is used to keep past experiences to increase learning speed, PQ-routing keeps the best experiences learned and reuses them by predicting the traffic trend. The effectiveness of PQ-routing has been verified under various network topologies and traffic con ditions. Simulation results show that PQ-routing is superior to Q-routing in terms of both learning speed and adaptability. 1 INTRODUCTION The adaptive traffic control problem is to devise routing policies for controllers (i.e. routers) operating in a non-stationary environment to minimize ",Reinforcement Learning,4e2545f819e67f0615003dd7e04a6087-Paper.pdf,1995
Unknown Title,No abstract found,Cannot be classified,4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf,1995
Family Discovery,"""Family discovery"" is the task of learning the dimension and struc ture of a parameterized family of stochastic models. It is espe cially appropriate when the training examples are partitioned into ""episodes"" of samples drawn from a single parameter value. We present three family discovery algorithms based on surface learn ing and show that they significantly improve performance over two alternatives on a parameterized classification task. 1 INTRODUCTION Human listeners improve their ability to recognize speech by identifying the accent of the speaker. ""Might"" in an American accent is similar to ""mate"" in an Australian accent. By first identifying the accent, discrimination between these two words is improved. We can imagine locating a speaker in a ""space of accents"" parameterized by features like pitch, vowel formants, ""r"" -strength, etc. This paper considers the task of learning such parameterized models from data. Most speech recognition systems train hidden Markov models on labelle",Optimization & Theoretical ML,5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf,1995
Modeling Interactions of the Rat's Place and,"We have developed a computational theory of rodent navigation that includes analogs of the place cell system, the head direction system, and path integration. In this paper we present simulation results showing how interactions between the place and head direction systems can account for recent observations about hippocampal place cell responses to doubling and/or rotation of cue cards in a cylindrical arena (Sharp et at., 1990). Rodents have multiple internal representations of their relationship to their environment. They have, for example, a representation of their location (place cells in the hippocampal formation, see Muller et at., 1991), and a location-independent representation of their heading (head direction cells in the postsubiculum and the anterior thalamic nuclei, see Taube et at., 1990; Taube, 1995). If these representations are to be used for navigation, they must be aligned consistently whenever the animal reenters a familiar environment. This process was examined in a",NLP,522a9ae9a99880d39e5daec35375e999-Paper.pdf,1995
Empirical Entropy Manipulation for,"No finite sample is sufficient to determine the density, and therefore the entropy, of a signal directly. Some assumption about either the functional form of the density or about its smoothness is necessary. Both amount to a prior over the space of possible density functions. By far the most common approach is to assume that the density has a parametric form. By contrast we derive a differential learning rule called EMMA that optimizes entropy by way of kernel density estimation. En tropy and its derivative can then be calculated by sampling from this density estimate. The resulting parameter update rule is sur prisingly simple and efficient. We will show how EMMA can be used to detect and correct cor ruption in magnetic resonance images (MRI). This application is beyond the scope of existing parametric entropy models. 1 Introduction Information theory is playing an increasing role in unsupervised learning and visual processing. For example, Linsker has used the concept of information ",Optimization & Theoretical ML,537d9b6c927223c796cac288cced29df-Paper.pdf,1995
Lightness Perception,"A neural network model of 3-D lightness perception is presented which builds upon the FACADE Theory Boundary Contour Sys tem/Feature Contour System of Grossberg and colleagues. Early ratio encoding by retinal ganglion neurons as well as psychophysi cal results on constancy across different backgrounds (background constancy) are used to provide functional constraints to the theory and suggest a contrast negation hypothesis which states that ratio measures between coplanar regions are given more weight in the determination of lightness of the respective regions. Simulations of the model address data on lightness perception, including the coplanar ratio hypothesis, the Benary cross, and White's illusion. 1 INTRODUCTION Our everyday visual experience includes surface color constancy. That is, despite 1) variations in scene lighting and 2) movement or displacement across visual contexts, the color of an object appears to a large extent to be the same. Color constancy refers, then, to the fa",NLP,53adaf494dc89ef7196d73636eb2451b-Paper.pdf,1995
Adaptive Retina with Center-Surround,Both vertebrate and invertebrate retinas are highly efficient in ex tracting contrast independent of the background intensity over five or more decades. This efficiency has been rendered possible by the adaptation of the DC operating point to the background inten sity while maintaining high gain transient responses. The center surround properties of the retina allows the system to extract in formation at the edges in the image. This silicon retina models the adaptation properties of the receptors and the antagonistic center surround properties of the laminar cells of the invertebrate retina and the outer-plexiform layer of the vertebrate retina. We also illus trate the spatio-temporal responses of the silicon retina on moving bars. The chip has 59x64 pixels on a 6.9x6.8mm2 die and it is fabricated in 2 J-tm n-well technology. 1 Introduction It has been observed previously that the initial layers of the vertebrate and inver tebrate retina systems perform very similar processing function,Computer Vision,53c04118df112c13a8c34b38343b9c10-Paper.pdf,1995
Does the Wake-sleep Algorithm,"The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a rel atively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connec tions in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the per formance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit. 1 INTRODUCTION Neural networks are often used as bottom-up recognition devices that transform input vec tors into representations of those vectors in one or more hidden layers. But multilayer net works of stochastic neurons can also be used as top-down generative models that produce patterns with complica",Deep Learning,55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf,1995
EM Optimization of Latent-Variable,"There is currently considerable interest in developing general non linear density models based on latent, or hidden, variables. Such models have the ability to discover the presence of a relatively small number of underlying 'causes' which, acting in combination, give rise to the apparent complexity of the observed data set. Unfortu nately, to train such models generally requires large computational effort. In this paper we introduce a novel latent variable algorithm which retains the general non-linear capabilities of previous models but which uses a training procedure based on the EM algorithm. We demonstrate the performance of the model on a toy problem and on data from flow diagnostics for a multi-phase oil pipeline. 1 INTRODUCTION Many conventional approaches to density estimation, such as mixture models, rely on linear superpositions of basis functions to represent the data density. Such approaches are unable to discover structure within the data whereby a relatively small number",Optimization & Theoretical ML,571e0f7e2d992e738adff8b1bd43a521-Paper.pdf,1995
Modern Analytic Techniques to Solve the,"We describe the use of modern analytical techniques in solving the dynamics of symmetric and nonsymmetric recurrent neural net works near saturation. These explicitly take into account the cor relations between the post-synaptic potentials, and thereby allow for a reliable prediction of transients. 1 INTRODUCTION Recurrent neural networks have been rather popular in the physics community, because they lend themselves so naturally to analysis with tools from equilibrium statistical mechanics. This was the main theme of physicists between, say, 1985 and 1990. Less familiar to the neural network community is a subsequent wave of theoretical physical studies, dealing with the dynamics of symmetric and nonsym metric recurrent networks. The strategy here is to try to describe the processes at a reduced level of an appropriate small set of dynamic macroscopic observables. At first, progress was made in solving the dynamics of extremely diluted models (Derrida et al, 1987) and of fully connect",NLP,58c54802a9fb9526cd0923353a34a7ae-Paper.pdf,1995
A Realizable Learning Task which,"In this paper we examine a perceptron learning task. The task is realizable since it is provided by another perceptron with identi cal architecture. Both perceptrons have nonlinear sigmoid output functions. The gain of the output function determines the level of nonlinearity of the learning task. It is observed that a high level of nonlinearity leads to overfitting. We give an explanation for this rather surprising observation and develop a method to avoid the overfitting. This method has two possible interpretations, one is learning with noise, the other cross-validated early stopping. 1 Learning Rules from Examples The property which makes feedforward neural nets interesting for many practical applications is their ability to approximate functions, which are given only by ex amples. Feed-forward networks with at least one hidden layer of nonlinear units are able to approximate each continuous function on a N-dimensional hypercube arbitrarily well. While the existence of neural functi",Optimization & Theoretical ML,678a1491514b7f1006d605e9161946b1-Paper.pdf,1995
Simulation of a Thalamocortical Circuit for,"Several regions of the rat brain contain neurons known as head-direc tion celis, which encode the animal's directional heading during spatial navigation. This paper presents a biophysical model of head-direction cell acti vity, which suggests that a thalamocortical circuit might com pute the rat's head direction by integrating the angular velocity of the head over time. The model was implemented using the neural simulator NEURON, and makes testable predictions about the structure and func tion of the rat head-direction circuit. 1 HEAD-DIRECTION CELLS As a rat navigates through space, neurons called head-direction celis encode the animal's directional heading in the horizontal plane (Ranck, 1984; Taube, Muller, & Ranck, 1990). Head-direction cells have been recorded in several brain areas, including the postsubicu lum (Ranck, 1984) and anterior thalamus (Taube, 1995). A variety of theories have pro posed that head-direction cells might play an important role in spatial learning and navi",NLP,69a5b5995110b36a9a347898d97a610e-Paper.pdf,1995
Clustering data through an analogy to,"A new approach for clustering is proposed. This method is based on an analogy to a physical model; the ferromagnetic Potts model at thermal equilibrium is used as an analog computer for this hard optimization problem. We do not assume any structure of the un derlying distribution of the data. Phase space of the Potts model is divided into three regions; ferromagnetic, super-paramagnetic and paramagnetic phases. The region of interest is that corresponding to the super-paramagnetic one, where domains of aligned spins ap pear. The range of temperatures where these structures are stable is indicated by a non-vanishing magnetic susceptibility. We use a very efficient Monte Carlo algorithm to measure the susceptibil ity and the spin spin correlation function. The values of the spin spin correlation function, at the super-paramagnetic phase, serve to identify the partition of the data points into clusters. Many natural phenomena can be viewed as optimization processes, and the drive to under",Optimization & Theoretical ML,6a2feef8ed6a9fe76d6b3f30f02150b4-Paper.pdf,1995
Learning Fine Motion by Markov,"Compliant control is a standard method for performing fine manip ulation tasks, like grasping and assembly, but it requires estimation of the state of contact (s.o.c.) between the robot arm and the ob jects involved. Here we present a method to learn a model of the movement from measured data. The method requires little or no prior knowledge and the resulting model explicitly estimates the s.o.c. The current s.o.c. is viewed as the hidden state variable of a discrete HMM. The control dependent transition probabilities between states are modeled as parametrized functions of the mea surement. We show that their parameters can be estimated from measurements at the same time as the parameters of the movement in each s.o.c. The learning algorithm is a variant of the EM proce dure. The E step is computed exactly; solving the M step exactly is not possible in general. Here, gradient ascent is used to produce an increase in likelihood. 1 INTRODUCTION For a large class of robotics tasks, such a",Optimization & Theoretical ML,6c340f25839e6acdc73414517203f5f0-Paper.pdf,1995
Stable Linear Approximations to,"We consider the solution to large stochastic control problems by means of methods that rely on compact representations and a vari ant of the value iteration algorithm to compute approximate cost to-go functions. While such methods are known to be unstable in general, we identify a new class of problems for which convergence, as well as graceful error bounds, are guaranteed. This class in volves linear parameterizations of the cost-to-go function together with an assumption that the dynamic programming operator is a contraction with respect to the Euclidean norm when applied to functions in the parameterized class. We provide a special case where this assumption is satisfied, which relies on the locality of transitions in a state space. Other cases will be discussed in a full length version of this paper. 1 INTRODUCTION Neural networks are well established in the domains of pattern recognition and function approximation, where their properties and training algorithms have been well stud",Reinforcement Learning,6d70cb65d15211726dcce4c0e971e21c-Paper.pdf,1995
Generalisation of A Class of Continuous,We propose a way of using boolean circuits to perform real valued computation in a way that naturally extends their boolean func tionality. The functionality of multiple fan in threshold gates in this model is shown to mimic that of a hardware implementation of continuous Neural Networks. A Vapnik-Chervonenkis dimension and sample size analysis for the systems is performed giving best known sample sizes for a real valued Neural Network. Experimen tal results confirm the conclusion that the sample sizes required for the networks are significantly smaller than for sigmoidal networks. 1 Introduction Recent developments in complexity theory have addressed the question of com plexity of computation over the real numbers. More recently attempts have been made to introduce some computational cost related to the accuracy of the compu tations [5]. The model proposed in this paper weakens the computational power still further by relying on classical boolean circuits to perform the computation us,Optimization & Theoretical ML,6e7d2da6d3953058db75714ac400b584-Paper.pdf,1995
Visual gesture-based robot guidance,"We report on the development of the modular neural system ""SEE EAGLE"" for the visual guidance of robot pick-and-place actions. Several neural networks are integrated to a single system that vi sually recognizes human hand pointing gestures from stereo pairs of color video images. The output of the hand recognition stage is processed by a set of color-sensitive neural networks to determine the cartesian location of the target object that is referenced by the pointing gesture. Finally, this information is used to guide a robot to grab the target object and put it at another location that can be specified by a second pointing gesture. The accuracy of the cur rent system allows to identify the location of the referenced target object to an accuracy of 1 cm in a workspace area of 50x50 cm. In our current environment, this is sufficient to pick and place arbi trarily positioned target objects within the workspace. The system consists of neural networks that perform the tasks of image seg men",Computer Vision,708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf,1995
Symplectic Nonlinear Component,"Statistically independent features can be extracted by finding a fac torial representation of a signal distribution. Principal Component Analysis (PCA) accomplishes this for linear correlated and Gaus sian distributed signals. Independent Component Analysis (ICA), formalized by Comon (1994), extracts features in the case of lin ear statistical dependent but not necessarily Gaussian distributed signals. Nonlinear Component Analysis finally should find a facto rial representation for nonlinear statistical dependent distributed signals. This paper proposes for this task a novel feed-forward, information conserving, nonlinear map - the explicit symplectic transformations. It also solves the problem of non-Gaussian output distributions by considering single coordinate higher order statis tics. 1 Introduction In previous papers Deco and Brauer (1994) and Parra, Deco, and Miesbach (1995) suggest volume conserving transformations and factorization as the key elements for a nonlinear version of",Optimization & Theoretical ML,731c83db8d2ff01bdc000083fd3c3740-Paper.pdf,1995
Independent Component Analysis,"Because of the distance between the skull and brain and their differ ent resistivities, electroencephalographic (EEG) data collected from any point on the human scalp includes activity generated within a large brain area. This spatial smearing of EEG data by volume conduction does not involve significant time delays, however, sug gesting that the Independent Component Analysis (ICA) algorithm of Bell and Sejnowski [1] is suitable for performing blind source sep aration on EEG data. The ICA algorithm separates the problem of source identification from that of source localization. First results of applying the ICA algorithm to EEG and event-related potential (ERP) data collected during a sustained auditory detection task show: (1) ICA training is insensitive to different random seeds. (2) ICA may be used to segregate obvious artifactual EEG components (line and muscle noise, eye movements) from other sources. (3) ICA is capable of isolating overlapping EEG phenomena, including al pha and",NLP,754dda4b1ba34c6fa89716b85d68532b-Paper.pdf,1995
Competence Acquisition in an,"In this paper we examine the practical use of hardware neural networks in an autonomous mobile robot. We have developed a hardware neural system based around a custom VLSI chip, EP SILON III, designed specifically for embedded hardware neural applications. We present here a demonstration application of an autonomous mobile robot that highlights the flexibility of this sys tem. This robot gains basic mobility competence in very few train ing epochs using an ""instinct-rule"" training methodology. 1 INTRODUCTION Though neural networks have been shown as an effective solution for a diverse range of real-world problems, applications and especially hardware implementations have been few and slow to emerge. For example in the DARPA neural networks study of 1988; of the 77 neural network applications investigated only 4 had resulted in field tested systems [Widrow, 1988]. Furthermore, none of these used dedicated neural network hardware. It is our view that this lack of tangible successes can b",Computer Vision,7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf,1995
Reorganisation of Somatosensory Cortex after,"Topographic maps in primary areas of mammalian cerebral cortex reor ganise as a result of behavioural training. The nature of this reorgani sation seems consistent with the behaviour of competitive neural net works, as has been demonstrated in the past by computer simulation. We model tactile training on the hand representation in primate somato sensory cortex, using the Neural Field Theory of Amari and his col leagues. Expressions for changes in both receptive field size and mag nification factor are derived, which are consistent with owl monkey ex periments and make a prediction which goes beyond them.",Computer Vision,7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf,1995
Prediction of Beta Sheets in Proteins,"Most current methods for prediction of protein secondary structure use a small window of the protein sequence to predict the structure of the central amino acid. We describe a new method for prediction of the non-local structure called ,8-sheet, which consists of two or more ,8-strands that are connected by hydrogen bonds. Since,8- strands are often widely separated in the protein chain, a network with two windows is introduced. After training on a set of proteins the network predicts the sheets well, but there are many false pos itives. By using a global energy function the ,8-sheet prediction is combined with a local prediction of the three secondary structures a-helix, ,8-strand and coil. The energy function is minimized using simulated annealing to give a final prediction. 1 INTRODUCTION Proteins are long sequences of amino acids. There are 20 different amino acids with varying chemical properties, e. g., some are hydrophobic (dislikes water) and some are hydrophilic [1]. It is con",Deep Learning,818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf,1995
Active Learning in Multilayer,"We propose an active learning method with hidden-unit reduction. which is devised specially for multilayer perceptrons (MLP). First, we review our active learning method, and point out that many Fisher-information-based methods applied to MLP have a critical problem: the information matrix may be singular. To solve this problem, we derive the singularity condition of an information ma trix, and propose an active learning technique that is applicable to MLP. Its effectiveness is verified through experiments. 1 INTRODUCTION When one trains a learning machine using a set of data given by the true system, its ability can be improved if one selects the training data actively. In this paper, we consider the problem of active learning in multilayer perceptrons (MLP). First, we review our method of active learning (Fukumizu el al., 1994), in which we prepare a probability distribution and obtain training data as samples from the distribution. This methodology leads us to an information-matrix-",Optimization & Theoretical ML,8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf,1995
Gaussian Processes for Regression,"The Bayesian analysis of neural networks is difficult because a sim ple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian anal ysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and av eraging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results. 1 INTRODUCTION In the Bayesian approach to neural networks a prior distribution over the weights induces a prior distribution over functions. This prior is combined with a noise model, which specifies the probability of observing the targets t given function values y, to yield a posterior over functions which can then be used for predictions. For neural networks the prior over functions has a complex form which means that implementations must either make ",Unknown,7cce53cf90577442771720a370c3c723-Paper.pdf,1995
A model of transparent motion and,"A model of human motion perception is presented. The model contains two stages of direction selective units. The first stage con tains broadly tuned units, while the second stage contains units that are narrowly tuned. The model accounts for the motion af tereffect through adapting units at the first stage and inhibitory interactions at the second stage. The model explains how two pop ulations of dots moving in slightly different directions are perceived as a single population moving in the direction of the vector sum, and how two populations moving in strongly different directions are perceived as transparent motion. The model also explains why the motion aftereffect in both cases appears as non-transparent motion. 1 INTRODUCTION Transparent motion can be studied using displays which contain two populations of moving dots. The dots within each population have the same direction of motion, but directions can differ between the two populations. When the two directions are very similar, ",Computer Vision,82b8a3434904411a9fdc43ca87cee70c-Paper.pdf,1995
Improved Gaussian Mixture Density,"We compare two regularization methods which can be used to im prove the generalization capabilities of Gaussian mixture density estimates. The first method uses a Bayesian prior on the parame ter space. We derive EM (Expectation Maximization) update rules which maximize the a posterior parameter probability. In the sec ond approach we apply ensemble averaging to density estimation. This includes Breiman's ""bagging"" , which recently has been found to produce impressive results for classification networks. 1 Introduction Gaussian mixture models have recently attracted wide attention in the neural net work community. Important examples of their application include the training of radial basis function classifiers, learning from patterns with missing features, and active learning. The appeal of Gaussian mixtures is based to a high degree on the applicability of the EM (Expectation Maximization) learning algorithm, which may be implemented as a fast neural network learning rule ([Now91], [O",Optimization & Theoretical ML,83fa5a432ae55c253d0e60dbfa716723-Paper.pdf,1995
A Practical Monte Carlo Implementation,"A practical method for Bayesian training of feed-forward neural networks using sophisticated Monte Carlo methods is presented and evaluated. In reasonably small amounts of computer time this approach outperforms other state-of-the-art methods on 5 data limited tasks from real world domains. 1 INTRODUCTION Bayesian learning uses a prior on model parameters, combines this with information from a training set, and then integrates over the resulting posterior to make pre dictions. With this approach, we can use large networks without fear of overfitting, allowing us to capture more structure in the data, thus improving prediction accu racy and eliminating the tedious search (often performed using cross validation) for the model complexity that optimises the bias/variance tradeoff. In this approach the size of the model is limited only by computational considerations. The application of Bayesian learning to neural networks has been pioneered by MacKay (1992), who uses a Gaussian approximati",Deep Learning,84d2004bf28a2095230e8e14993d398d-Paper.pdf,1995
A Smoothing Regularizer for Recurrent,"We derive a smoothing regularizer for recurrent network models by requiring robustness in prediction performance to perturbations of the training data. The regularizer can be viewed as a generaliza tion of the first order Tikhonov stabilizer to dynamic models. The closed-form expression of the regularizer covers both time-lagged and simultaneous recurrent nets, with feedforward nets and one layer linear nets as special cases. We have successfully tested this regularizer in a number of case studies and found that it performs better than standard quadratic weight decay. 1 Introd uction One technique for preventing a neural network from overfitting noisy data is to add a regularizer to the error function being minimized. Regularizers typically smooth the fit to noisy data. Well-established techniques include ridge regression, see (Ho erl & Kennard 1970), and more generally spline smoothing functions or Tikhonov stabilizers that penalize the mth-order squared derivatives of the function be",Optimization & Theoretical ML,8597a6cfa74defcbde3047c891d78f90-Paper.pdf,1995
REMAP: Recursive Estimation and,"In this paper, we introduce REMAP, an approach for the training and estimation of posterior probabilities using a recursive algorithm that is reminiscent of the EM-based Forward-Backward (Liporace 1982) algorithm for the estimation of sequence likelihoods. Al though very general, the method is developed in the context of a statistical model for transition-based speech recognition using Ar tificial Neural Networks (ANN) to generate probabilities for Hid den Markov Models (HMMs). In the new approach, we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences. Although we still use ANNs to estimate posterior probabilities, the network is trained with targets that are themselves estimates of local posterior proba bilities. An initial experimental result shows a significant decrease in error-rate in comparison to a baseline system. 1 INTRODUCTION The ultimate goal in speech recognition is to determine the sequence of words th",NLP,883e881bb4d22a7add958f2d6b052c9f-Paper.pdf,1995
Harmony Networks Do Not Work,"Harmony networks have been proposed as a means by which con nectionist models can perform symbolic computation. Indeed, pro ponents claim that a harmony network can be built that constructs parse trees for strings in a context free language. This paper shows that harmony networks do not work in the following sense: they construct many outputs that are not valid parse trees. In order to show that the notion of systematicity is compatible with connectionism, Paul Smolensky, Geraldine Legendre and Yoshiro Miyata (Smolensky, Legendre, and Miyata 1992; Smolen sky 1993; Smolen sky, Legendre, and Miyata 1994) pro posed a mechanism, ""Harmony Theory,"" by which connectionist models purportedly perform structure sensitive operations without implementing classical algorithms. Harmony theory describes a ""harmony network"" which, in the course of reaching a stable equilibrium, apparently computes parse trees that are valid according to the rules of a particular context-free grammar. Harmony networks ",NLP,884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf,1995
Learning Sparse Perceptrons,"We introduce a new algorithm designed to learn sparse percep trons over input representations which include high-order features. Our algorithm, which is based on a hypothesis-boosting method, is able to PAC-learn a relatively natural class of target concepts. Moreover, the algorithm appears to work well in practice: on a set of three problem domains, the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance. Perhaps most importantly, our algorithm generates concept descriptions that are easy for humans to understand. 1 Introd uction Multi-layer perceptron (MLP) learning is a powerful method for tasks such as con cept classification. However, in many applications, such as those that may involve scientific discovery, it is crucial to be able to explain predictions. Multi-layer percep trons are limited in this regard, since their representations are notoriously difficult for humans to understand. We present an approach to learnin",Optimization & Theoretical ML,8a1e808b55fde9455cb3d8857ed88389-Paper.pdf,1995
The Role of Activity in Synaptic,"An extended version of the dual constraint model of motor end plate morphogenesis is presented that includes activity dependent and independent competition. It is supported by a wide range of recent neurophysiological evidence that indicates a strong relation ship between synaptic efficacy and survival. The computational model is justified at the molecular level and its predictions match the developmental and regenerative behaviour of real synapses. 1 INTRODUCTION The neuromuscular junction (NMJ) of mammalian skeletal muscle is one of the most extensively studied areas of the nervous system. One aspect of its develop ment that it shares with many other parts of the nervous system is its achievement of single innervation, one axon terminal connecting to one muscle fibre, after an initial state of polyinnervation. The presence of electrical activity is associated with this transition, but the exact relationship is far from clear. Understanding how activity interacts with the morphogenesi",Optimization & Theoretical ML,8a3363abe792db2d8761d6403605aeb7-Paper.pdf,1995
Explorations with the Dynamic Wave,"Following Shrager and Johnson (1995) we study growth of logi cal function complexity in a network swept by two overlapping waves: one of pruning, and the other of Hebbian reinforcement of connections. Results indicate a significant spatial gradient in the appearance of both linearly separable and non linearly separable functions of the two inputs of the network; the n.l.s. cells are much sparser and their slope of appearance is sensitive to parameters in a highly non-linear way. 1 INTRODUCTION Both the complexity of the brain (and concomittant difficulty encoding that C0111- plexity through any direct genetic mapping). as well as the apparently high degree of cortical plasticity suggest that a great deal of cortical structure is emergent rather than pre-specified. Several neural models have explored the emergence of complexity. Von der Marlsburg (1973) studied the grouping of orientation selec tivity by competitive Hebbian synaptic modification. Linsker (1986.a, 1986.b and",Optimization & Theoretical ML,8ce6790cc6a94e65f17f908f462fae85-Paper.pdf,1995
Generalization in Reinforcement,"On large problems, reinforcement learning systems must use parame terized function approximators such as neural networks in order to gen eralize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and com putational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcome",Reinforcement Learning,8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf,1995
A Multiscale Attentional Framework for,"We investigate the optimization of neural networks governed by general objective functions. Practical formulations of such objec tives are notoriously difficult to solve; a common problem is the poor local extrema that result by any of the applied methods. In this paper, a novel framework is introduced for the solution oflarge scale optimization problems. It assumes little about the objective function and can be applied to general nonlinear, non-convex func tions; objectives in thousand of variables are thus efficiently min imized by a combination of techniques - deterministic annealing, multiscale optimization, attention mechanisms and trust region op timization methods. 1 INTRODUCTION Many practical problems in computer vision, pattern recognition, robotics and other areas can be described in terms of constrained optimization. In the past decade, researchers have proposed means of solving such problems with the use of neural networks [Hopfield & Tank, 1985; Koch et ai., 1986], which ",Optimization & Theoretical ML,93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf,1995
VLSI Model of Primate Visual Smooth Pursuit,"A one dimensional model of primate smooth pursuit mechanism has been implemented in 2 11m CMOS VLSI. The model consolidates Robinson's negative feedback model with Wyatt and Pola's positive feedback scheme, to produce a smooth pursuit system which zero's the velocity of a target on the retina. Furthermore, the system uses the current eye motion as a predictor for future target motion. Analysis, stability and biological correspondence of the system are discussed. For implementation at the focal plane, a local correlation based visual motion detection technique is used. Velocity measurements, ranging over 4 orders of magnitude with < 15% variation, provides the input to the smooth pursuit system. The system performed successful velocity tracking for high contrast scenes. Circuit design and performance of the complete smooth pursuit system is presented. 1 INTRODUCTION The smooth pursuit mechanism of primate visual systems is vital for stabilizing a region of the visual field on the retina",Computer Vision,995e1fda4a2b5f55ef0df50868bf2a8f-Paper.pdf,1995
Reinforcement Learning by Probability,"We present a new algorithm for associative reinforcement learn ing. The algorithm is based upon the idea of matching a network's output probability with a probability distribution derived from the environment's reward signal. This Probability Matching algorithm is shown to perform faster and be less susceptible to local minima than previously existing algorithms. We use Probability Match ing to train mixture of experts networks, an architecture for which other reinforcement learning rules fail to converge reliably on even simple problems. This architecture is particularly well suited for our algorithm as it can compute arbitrarily complex functions yet calculation of the output probability is simple. 1 INTRODUCTION The problem of learning associative networks from scalar reinforcement signals is notoriously difficult. Although general purpose algorithms such as REINFORCE (Williams, 1992) and Generalized Learning Automata (Phansalkar, 1991) exist, they are generally slow and have troubl",Reinforcement Learning,9ac403da7947a183884c18a67d3aa8de-Paper.pdf,1995
Generalized Learning Vector,"We propose a new learning method, ""Generalized Learning Vec tor Quantization (GLVQ),"" in which reference vectors are updated based on the steepest descent method in order to minimize the cost function. The cost function is determined so that the obtained learning rule satisfies the convergence condition. We prove that Kohonen's rule as used in LVQ does not satisfy the convergence condition and thus degrades recognition ability. Experimental re sults for printed Chinese character recognition reveal that GLVQ is superior to LVQ in recognition ability. 1 INTRODUCTION Artificial neural network models have been applied to character recognition with good results for small-set characters such as alphanumerics (Le Cun et aI., 1989) (Yamada et al., 1989). However, applying the models to large-set characters such as Japanese or Chinese characters is difficult because most of the models are based on Multi-Layer Perceptron (MLP) with the back propagation algorithm, which has a problem in regard to",Optimization & Theoretical ML,9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf,1995
Bayesian Methods for Mixtures of Experts,"We present a Bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by varia tional free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate these methods on artificial problems and sunspot time series prediction. INTRODUCTION The task of estimating the parameters of adaptive models such as artificial neural networks using Maximum Likelihood (ML) is well documented ego Geman, Bienen stock & Doursat (1992). ML estimates typically lead to models with high variance, a process known as ""over-fitting"". ML also yields over-confident predictions; in regression problems for example, ML underestimates the noise level. This problem is particularly dominant in models where the ratio of the number of data points in the training set to the number of parameters in the model is low. In this paper we consider inference of the parameters ",Optimization & Theoretical ML,9da187a7a191431db943a9a5a6fec6f4-Paper.pdf,1995
High-Speed Airborne Particle Monitoring,"Current environmental monitoring systems assume particles to be spherical, and do not attempt to classify them. A laser-based sys tem developed at the University of Hertfordshire aims at classify ing airborne particles through the generation of two-dimensional scattering profiles. The pedormances of template matching, and two types of neural network (HyperNet and semi-linear units) are compared for image classification. The neural network approach is shown to be capable of comparable recognition pedormance, while offering a number of advantages over template matching. 1 Introduction Reliable identification of low concentrations of airborne particles requires high speed monitoring of large volumes of air, and incurs heavy computational overheads. An instrument to detect particle shape and size from spatial light scattering profiles has High-speed Airborne Particle Monitoring Using Artificial Neural Networks 981 previously been described [6]. The system constrains individual particles to",Computer Vision,9f36407ead0629fc166f14dde7970f68-Paper.pdf,1995
Adaptive Mixture of Probabilistic Transducers,"We introduce and analyze a mixture model for supervised learning of probabilistic transducers. We devise an online learning algorithm that efficiently infers the structure and estimates the parameters of each model in the mixture. Theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best model from an arbitrarily large (possibly infinite) pool of models. We also present an application of the model for inducing a noun phrase recognizer. 1 Introduction Supervised learning of a probabilistic mapping between temporal sequences is an important goal of natural sequences analysis and classification with a broad range of applications such as handwriting and speech recognition, natural language processing and DNA analysis. Re search efforts in supervised learning of probabilistic mappings have been almost exclusively focused on estimating the parameters of a predefined model. For example, in [5] a second order recurrent neural network was used to indu",NLP,a0160709701140704575d499c997b6ca-Paper.pdf,1995
SEEMORE: A View-Based Approach to,"A neurally-inspired visual object recognition system is described called SEEMORE, whose goal is to identify common objects from a large known set-independent of 3-D viewiag angle, distance, and non-rigid distortion. SEEMORE's database consists of 100 ob jects that are rigid (shovel), non-rigid (telephone cord), articu lated (book), statistical (shrubbery), and complex (photographs of scenes). Recognition results were obtained using a set of 102 color and shape feature channels within a simple feedforward network ar chitecture. In response to a test set of 600 novel test views (6 of each object) presented individually in color video images, SEEMORE identified the object correctly 97% of the time (chance is 1%) using a nearest neighbor classifier. Similar levels of performance were obtained for the subset of 15 non-rigid objects. Generalization be havior reveals emergence of striking natural category structure not explicit in the input feature dimensions. 1 INTRODUCTION In natural contex",Computer Vision,a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf,1995
Using Pairs of Data-Points to Define,"Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a compu tationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points, there is one hyperplane that is orthog onal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods, particularly when the training sets were small. 1 Introduction Binary decision trees come in many flavours, but they all rely on splitting the set of k-dimensional data-points at each internal node into two disjoint sets. Each split is usually performed by projecting the data onto some direction in the k-dimensional space and then thresholding t",Optimization & Theoretical ML,a113c1ecd3cace2237256f4c712f61b5-Paper.pdf,1995
Dynamics of On-Line Gradient Descent,"We consider the problem of on-line gradient descent learning for general two-layer neural networks. An analytic solution is pre sented and used to investigate the role of the learning rate in con trolling the evolution and convergence of the learning process. Learning in layered neural networks refers to the modification of internal parameters {J} which specify the strength of the interneuron couplings, so as to bring the map 1. fJ implemented by the network as close as possible to a desired map The degree of success is monitored through the generalization error, a measure of the 1. dissimilarity between fJ and e Consider maps from an N-dimensional input space onto a scalar (, as arise in the formulation of classification and regression tasks. Two-layer networks with an arbitrary number of hidden units have been shown to be universal approximators i [1] for such N-to-one dimensional maps. Information about the desired map is provided through independent examples (e, (1'), with (I' = i(",Optimization & Theoretical ML,a1519de5b5d44b31a01de013b9b51a80-Paper.pdf,1995
Geometry of Early Stopping in Linear,"A theory of early stopping as applied to linear models is presented. The backpropagation learning algorithm is modeled as gradient descent in continuous time. Given a training set and a validation set, all weight vectors found by early stopping must lie on a cer tain quadric surface, usually an ellipsoid. Given a training set and a candidate early stopping weight vector, all validation sets have least-squares weights lying on a certain plane. This latter fact can be exploited to estimate the probability of stopping at any given point along the trajectory from the initial weight vector to the least squares weights derived from the training set, and to estimate the probability that training goes on indefinitely. The prospects for extending this theory to nonlinear models are discussed. 1 INTRODUCTION 'Early stopping' is the following training procedure: Split the available data into a training set and a ""validation"" set. Start with initial weights close to zero. Apply gradient descent (b",Optimization & Theoretical ML,a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf,1995
A Unified Learning Scheme:,No abstract found,Optimization & Theoretical ML,a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf,1995
Using Feedforward Neural Networks to,"We report here that changes in the normalized electroencephalo graphic (EEG) cross-spectrum can be used in conjunction with feedforward neural networks to monitor changes in alertness of op erators continuously and in near-real time. Previously, we have shown that EEG spectral amplitudes covary with changes in alert ness as indexed by changes in behavioral error rate on an auditory detection task [6,4]. Here, we report for the first time that increases in the frequency of detection errors in this task are also accompa nied by patterns of increased and decreased spectral coherence in several frequency bands and EEG channel pairs. Relationships between EEG coherence and performance vary between subjects, but within subjects, their topographic and spectral profiles appear stable from session to session. Changes in alertness also covary with changes in correlations among EEG waveforms recorded at different scalp sites, and neural networks can also estimate alert ness from correlation chang",Deep Learning,a26398dca6f47b49876cbaffbc9954f9-Paper.pdf,1995
A Dynamical Model of Context Dependencies for the,"The vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid head motions. The gain of the VOR (the ratio of eye to head rotation velocity) is typically around -1 when the eyes are focused on a distant target. However, to stabilize images accurately, the VOR gain must vary with context (eye position, eye vergence and head translation). We first describe a kinematic model of the VOR which relies solely on sensory information available from the semicircular canals (head rotation), the otoliths (head translation), and neural correlates of eye position and vergence angle. We then propose a dynamical model and compare it to the eye velocity responses measured in monkeys. The dynamical model repro duces the observed amplitude and time course of the modulation of the VOR and suggests one way to combine the required neural signals within the cerebellum and the brain stem. It also makes predictions for the responses of neurons to multiple inputs (head rotation and translation,",NLP,a34bacf839b923770b2c360eefa26748-Paper.pdf,1995
Constructive Algorithms for Hierarchical,"We present two additions to the hierarchical mixture of experts (HME) architecture. By applying a likelihood splitting criteria to each expert in the HME we ""grow"" the tree adaptively during train ing. Secondly, by considering only the most probable path through the tree we may ""prune"" branches away, either temporarily, or per manently if they become redundant. We demonstrate results for the growing and path pruning algorithms which show significant speed ups and more efficient use of parameters over the standard fixed structure in discriminating between two interlocking spirals and classifying 8-bit parity patterns. INTRODUCTION The HME (Jordan & Jacobs 1994) is a tree structured network whose terminal nodes are simple function approximators in the case of regression or classifiers in the case of classification. The outputs of the terminal nodes or experts are recursively combined upwards towards the root node, to form the overall output of the network, by ""gates"" which are situated a",Deep Learning,a3fb4fbf9a6f9cf09166aa9c20cbc1ad-Paper.pdf,1995
Discovering Structure in Continuous,"We study Bayesian networks for continuous variables using non linear conditional density estimators. We demonstrate that use ful structures can be extracted from a data set in a self-organized way and we present sampling techniques for belief update based on Markov blanket conditional density models. 1 Introduction One of the strongest types of information that can be learned about an unknown process is the discovery of dependencies and -even more important- of indepen dencies. A superior example is medical epidemiology where the goal is to find the causes of a disease and exclude factors which are irrelevant. Whereas complete independence between two variables in a domain might be rare in reality (which would mean that the joint probability density of variables A and B can be factored: = p(A, B) p(A)p(B)), conditional independence is more common and is often a result from true or apparent causality: consider the case that A is the cause of B = and B is the cause of C, then p(CIA, B) p",Bayesian networks for continuous variables using non linear conditional density estimators,a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf,1995
Temporal coding,"Binaural coincidence detection is essential for the localization of external sounds and requires auditory signal processing with high temporal precision. We present an integrate-and-fire model of spike processing in the auditory pathway of the barn owl. It is shown that a temporal precision in the microsecond range can be achieved with neuronal time constants which are at least one magnitude longer. An important feature of our model is an unsupervised Hebbian learning rule which leads to a temporal fine tuning of the neuronal connections. ·email: kempter.wgerst.lvh@physik.tu-muenchen.de Temporal Coding in the Submillisecond Range: Model of Bam Owl Auditory Pathway 125 1 Introduction Owls are able to locate acoustic signals based on extraction of interaural time dif ference by coincidence detection [1, 2]. The spatial resolution of sound localization found in experiments corresponds to a temporal resolution of auditory signal pro cessing well below one millisecond. It follows that both ",Optimization & Theoretical ML,a8240cb8235e9c493a0c30607586166c-Paper.pdf,1995
Stable Dynamic Parameter Adaptation,"A stability criterion for dynamic parameter adaptation is given. In the case of the learning rate of backpropagation, a class of stable algorithms is presented and studied, including a convergence proof. 1 INTRODUCTION All but a few learning algorithms employ one or more parameters that control the quality of learning. Backpropagation has its learning rate and momentum param eter; Boltzmann learning uses a simulated annealing schedule; Kohonen learning a learning rate and a decay parameter; genetic algorithms probabilities, etc. The investigator always has to set the parameters to specific values when trying to solve a certain problem. Traditionally, the metaproblem of adjusting the parameters is solved by relying on a set of well-tested values of other problems or an intensive search for good parameter regions by restarting the experiment with different val ues. In this situation, a great deal of expertise and/or time for experiment design is required (as well as a huge amount of comp",Optimization & Theoretical ML,a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf,1995
Experiments with Neural Networks for Real,"This paper describes a neural network based controller for allocating capacity in a telecommunications network. This system was proposed in order to overcome a ""real time"" response constraint. Two basic architectures are evaluated: 1) a feedforward network-heuristic and; 2) a feedforward network-recurrent network. These architectures are compared against a linear programming (LP) optimiser as a benchmark. This LP optimiser was also used as a teacher to label the data samples for the feedforward neural network training algorithm. It is found that the systems are able to provide a traffic throughput of 99% and 95%, respectively, of the throughput obtained by the linear programming solution. Once trained, the neural network based solutions are found in a fraction of the time required by the LP optimiser. 1 Introduction Among the many virtues of neural networks are their efficiency, in terms of both execution time and required memory for storing a structure, and their practical ability to ",Optimization & Theoretical ML,abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf,1995
Cholinergic suppression of transmission may,"Selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feed back with self-organization of feed forward synapses. Experimental data demonstrates cholinergic suppression of synaptic transmission in layer I (feedback synapses), and a lack of suppression in layer IV (feed forward synapses). A network with this feature uses local rules to learn mappings which are not linearly separable. During learning, sensory stimuli and desired response are simultaneously presented as input. Feedforward connections form self-organized representations of input, while suppressed feedback connections learn the transpose of feedfor ward connectivity. During recall, suppression is removed, sensory input activates the self-organized representation, and activity generates the learned response. 1 INTRODUCTION The synaptic connections in most models of the cortex can be defined as either associative or self-organizing on the basis of a sing",Reinforcement Learning,af21d0c97db2e27e13572cbf59eb343d-Paper.pdf,1995
Dynamics of Attention as Near,"In consideration of attention as a means for goal-directed behav ior in non-stationary environments, we argue that the dynamics of attention should satisfy two opposing demands: long-term main tenance and quick transition. These two characteristics are con tradictory within the linear domain. We propose the near saddle node bifurcation behavior of a sigmoidal unit with self-connection as a candidate of dynamical mechanism that satisfies both of these demands. We further show in simulations of the 'bug-eat-food' tasks that the near saddle-node bifurcation behavior of recurrent networks can emerge as a functional property for survival in non stationary environments. 1 INTRODUCTION Most studies of attention have focused on the selection process of incoming sensory cues (Posner et al., 1980; Koch et al., 1985; Desimone et al., 1995). Emphasis was placed on the phenomena of causing different percepts for the same sensory stimuli. However, the selection of sensory input itself is not the fin",Reinforcement Learning,afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf,1995
Optimizing Cortical Mappings,"""Topographic"" mappings occur frequently in the brain. A pop ular approach to understanding the structure of such mappings is to map points representing input features in a space of a few dimensions to points in a 2 dimensional space using some self organizing algorithm. We argue that a more general approach may be useful, where similarities between features are not con strained to be geometric distances, and the objective function for topographic matching is chosen explicitly rather than being spec ified implicitly by the self-organizing algorithm. We investigate analytically an example of this more general approach applied to the structure of interdigitated mappings, such as the pattern of ocular dominance columns in primary visual cortex. 1 INTRODUCTION A prevalent feature of mappings in the brain is that they are often ""topographic"". In the most straightforward case this simply means that neighbouring points on a two-dimensional sheet (e.g. the retina) are mapped to neighbouring poi",Optimization & Theoretical ML,aace49c7d80767cffec0e513ae886df0-Paper.pdf,1995
Softassign versus Softmax: Benchmarks,"A new technique, termed soft assign, is applied for the first time to two classic combinatorial optimization problems, the travel ing salesman problem and graph partitioning. Soft assign , which has emerged from the recurrent neural network/statistical physics framework, enforces two-way (assignment) constraints without the use of penalty terms in the energy functions. The soft assign can also be generalized from two-way winner-take-all constraints to multiple membership constraints which are required for graph par titioning. The soft assign technique is compared to the softmax (Potts glass). Within the statistical physics framework, softmax and a penalty term has been a widely used method for enforcing the two-way constraints common within many combinatorial optimiza tion problems. The benchmarks present evidence that soft assign has clear advantages in accuracy, speed, parallelizabilityand algo rithmic simplicity over softmax and a penalty term in optimization problems with two-way c",Optimization & Theoretical ML,b1563a78ec59337587f6ab6397699afc-Paper.pdf,1995
From Isolation to Cooperation:,"We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predic tions. Each expert is trained by minimizing a penalized local cross vali dation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the recep tive field in which its predictions are valid, and also to detect relevant in put features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning.",Optimization & Theoretical ML,b4d168b48157c623fbd095b4a565b5bb-Paper.pdf,1995
Fast Learning by Bounding Likelihoods,"Sigmoid type belief networks, a class of probabilistic neural net works, provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and super vised learning problems. Often the parameters used in these net works need to be learned from examples. Unfortunately, estimat ing the parameters via exact probabilistic calculations (i.e, the EM-algorithm) is intractable even for networks with fairly small numbers of hidden units. We propose to avoid the infeasibility of the E step by bounding likelihoods instead of computing them ex actly. We introduce extended and complementary representations for these networks and show that the estimation of the network parameters can be made fast (reduced to quadratic optimization) by performing the estimation in either of the alternative domains. The complementary networks can be used for continuous density estimation as well. 1 Introduction The appeal of probabilistic networks for knowledge representation",Optimization & Theoretical ML,b59c67bf196a4758191e42f76670ceba-Paper.pdf,1995
Neural Control for Nonlinear Dynamic Systems,"A neural network based approach is presented for controlling two distinct types of nonlinear systems. The first corresponds to nonlinear systems with parametric uncertainties where the parameters occur nonlinearly. The second corresponds to systems for which stabilizing control struc tures cannot be determined. The proposed neural controllers are shown to result in closed-loop system stability under certain conditions. 1 INTRODUCTION The problem that we address here is the control of general nonlinear dynamic systems in the presence of uncertainties. Suppose the nonlinear dynamic system is described as x= f(x, u, 0) , y = h(x, u, 0) where u denotes an external input, y is the output, x is the state, and 0 is the parameter which represents constant quantities in the system. The control objectives are to stabilize the system in the presence of disturbances and to ensure that reference trajectories can be tracked accurately, with minimum delay. While uncertainties can be classified in man",Reinforcement Learning,b9141aff1412dc76340b3822d9ea6c72-Paper.pdf,1995
Sample Complexity for Learning,"Recurrent perceptron classifiers generalize the classical perceptron model. They take into account those correlations and dependences among input coordinates which arise from linear digital filtering. This paper provides tight bounds on sample complexity associated to the fitting of such models to experimental data. 1 Introduction One of the most popular approaches to binary pattern classification, underlying many statistical techniques, is based on perceptrons or linear discriminants; see for instance the classical reference (Duda and Hart, 1973). In this context, one is interested in classifying k-dimensional input patterns V=(Vl, . . . ,Vk) into two disjoint classes A + and A -. A perceptron P which classifies vectors into A + and A - is characterized by a vector (of ""weights"") C E lR k, and operates as follows. One forms the inner product = + ... C.V CIVI CkVk. If this inner product is positive, v is classified into A +, otherwise into A - . In signal processing and control applica",Optimization & Theoretical ML,b9d487a30398d42ecff55c228ed5652b-Paper.pdf,1995
Is Learning The n-th Thing Any Easier Than,"This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learn ing tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks. 1 Introduction Supervised learning is concerned with approximating an unknown function based on exam ples. Virtually all current approaches to supervised learning assume that one is given a set of input-output examples, denoted by X, which characterize an unknown function, denoted by f. The target function f is drawn from a class of functions, F, and the learner is give",Reinforcement Learning,bdb106a0560c4e46ccc488ef010af787-Paper.pdf,1995
Parallel analog VLSI architectures for,"We describe two parallel analog VLSI architectures that integrate optical flow data obtained from arrays of elementary velocity sen sors to estimate heading direction and time-to-contact. For heading direction computation, we performed simulations to evaluate the most important qualitative properties of the optical flow field and determine the best functional operators for the implementation of the architecture. For time-to-contact we exploited the divergence theorem to integrate data from all velocity sensors present in the architecture and average out possible errors. 1 Introduction We have designed analog VLSI velocity sensors invariant to absolute illuminance and stimulus contrast over large ranges that are able to achieve satisfactory per formance in a wide variety of cases; yet such sensors, due to the intrinsic nature of analog processing, lack a high degree of precision in their output values. To exploit their properties at a system level, we developed parallel image processing",Computer Vision,c21002f464c5fc5bee3b98ced83963b8-Paper.pdf,1995
A Dynamical Systems Approach for a Learnable Autonomous Robot,"This paper discusses how a robot can learn goal-directed naviga tion tasks using local sensory inputs. The emphasis is that such learning tasks could be formulated as an embedding problem of dynamical systems: desired trajectories in a task space should be embedded into an adequate sensory-based internal state space so that an unique mapping from the internal state space to the motor command could be established. The paper shows that a recurrent neural network suffices in self-organizing such an adequate internal state space from the temporal sensory input. In our experiments, using a real robot with a laser range sensor, the robot navigated robustly by achieving dynamical coherence with the environment. It was also shown that such coherence becomes structurally sta ble as the global attractor is self-organized in the coupling of the internal and the environmental dynamics. 1 Introd uction Conventionally, robot navigation problems have been formulated assuming a global view of the worl",Reinforcement Learning,c3e0c62ee91db8dc7382bde7419bb573-Paper.pdf,1995
Optimization Principles for the Neural,"Recent experiments show that the neural codes at work in a wide range of creatures share some common features. At first sight, these observations seem unrelated. However, we show that these features arise naturally in a linear filtered threshold crossing (LFTC) model when we set the threshold to maximize the transmitted information. This maximization process requires neural adaptation to not only the DC signal level, as in conventional light and dark adaptation, but also to the statistical structure of the signal and noise distribu tions. We also present a new approach for calculating the mutual information between a neuron's output spike train and any aspect of its input signal which does not require reconstruction of the in put signal. This formulation is valid provided the correlations in the spike train are small, and we provide a procedure for checking this assumption. This paper is based on joint work (DeWeese [1], 1995). Preliminary results from the LFTC model appeared in a prev",Optimization & Theoretical ML,c6036a69be21cb660499b75718a3ef24-Paper.pdf,1995
A Framework for Non-rigid Matching,"Matching feature point sets lies at the core of many approaches to object recognition. We present a framework for non-rigid match ing that begins with a skeleton module, affine point matching, and then integrates multiple features to improve correspondence and develops an object representation based on spatial regions to model local transformations. The algorithm for feature matching iteratively updates the transformation parameters and the corre spondence solution, each in turn. The affine mapping is solved in closed form, which permits its use for data of any dimension. The correspondence is set via a method for two-way constraint satisfac tion, called softassign, which has recently emerged from the neural network/statistical physics realm. The complexity of the non-rigid matching algorithm with multiple features is the same as that of the affine point matching algorithm. Results for synthetic and real world data are provided for point sets in 2D and 3D, and for 2D data with multiple",Computer Vision,c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf,1995
Hierarchical Recurrent Neural Networks for,"We have already shown that extracting long-term dependencies from se quential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables rep resenting past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencIes are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Ex periments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs. 1 Introduction Learning from examples basically amounts to identifying the relations between random variables of ",NLP,c667d53acd899a97a85de0c201ba99be-Paper.pdf,1995
Finite State Automata that Recurrent,"This paper relates the computational power of Fahlman' s Recurrent Cascade Correlation (RCC) architecture to that of fInite state automata (FSA). While some recurrent networks are FSA equivalent, RCC is not. The paper presents a theoretical analysis of the RCC architecture in the form of a proof describing a large class of FSA which cannot be realized by RCC. 1 INTRODUCTION Recurrent networks can be considered to be defmed by two components: a network architecture, and a learning rule. The former describes how a network with a given set of weights and topology computes its output values, while the latter describes how the weights (and possibly topology) of the network are updated to fIt a specifIc problem. It is possible to evaluate the computational power of a network architecture by analyzing the types of computations a network could perform assuming appropriate connection weights (and topology). This type of analysis provides an upper bound on what a network can be expected to learn",Optimization & Theoretical ML,c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf,1995
Memory-based Stochastic Optimization,"In this paper we introduce new algorithms for optimizing noisy plants in which each experiment is very expensive. The algorithms build a global non-linear model of the expected output at the same time as using Bayesian linear regression analysis of locally weighted polynomial models. The local model answers queries about confi dence, noise, gradient and Hessians, and use them to make auto mated decisions similar to those made by a practitioner of Response Surface Methodology. The global and local models are combined naturally as a locally weighted regression. We examine the ques tion of whether the global model can really help optimization, and we extend it to the case of time-varying functions. We compare the new algorithms with a highly tuned higher-order stochastic op timization algorithm on randomly-generated functions and a sim ulated manufacturing task. We note significant improvements in total regret, time to converge, and final solution quality. 1 INTRODUCTION In a stochastic o",Optimization & Theoretical ML,c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf,1995
Handwritten Word Recognition using Contextual,"A hybrid and contextual radial basis function networklhidden Markov model off-line handwritten word recognition system is presented. The task assigned to the radial basis function networks is the estimation of emission probabilities associated to Markov states. The model is contex tual because the estimation of emission probabilities takes into account the left context of the current image segment as represented by its pred ecessor in the sequence. The new system does not outperform the previ ous system without context but acts differently. 1 INTRODUCTION Hidden Markov models (HMMs) are now commonly used in off-line recognition of handwritten words (Chen et aI., 1994) (Gilloux et aI., 1993) (Gilloux et al. 1995a). In some of these approaches (Gilloux et al. 1993), word images are transformed into sequences of image segments through some explicit segmentation procedure. These segments are passed on to a module which is in charge of estimating the probability for each segment to appear w",Computer Vision,c9f95a0a5af052bffce5c89917335f67-Paper.pdf,1995
Universal Approximation and Learning,"Natural and artificial neural circuits must be capable of travers ing specific state space trajectories. A natural approach to this problem is to learn the relevant trajectories from examples. Un fortunately, gradient descent learning of complex trajectories in amorphous networks is unsuccessful. We suggest a possible ap proach where trajectories are realized by combining simple oscil lators, in various modular ways. We contrast two regimes of fast and slow oscillations. In all cases, we show that banks of oscillators with bounded frequencies have universal approximation properties. Open questions are also discussed briefly. 1 INTRODUCTION: TRAJECTORY LEARNING The design of artificial neural systems, in robotics applications and others, often leads to the problem of constructing a recurrent neural network capable of producing a particular trajectory, in the state space of its visible units. Throughout evolution, biological neural systems, such as central pattern generators, have also b",Optimization & Theoretical ML,cd89fef7ffdd490db800357f47722b20-Paper.pdf,1995
Correlated Neuronal Response:,"We have analyzed the relationship between correlated spike count and the peak in the cross-correlation of spike trains for pairs of si multaneously recorded neurons from a previous study of area MT in the macaque monkey (Zohary et al., 1994). We conclude that common input, responsible for creating peaks on the order of ten milliseconds wide in the spike train cross-correlograms (CCGs), is also responsible for creating the correlation in spike count ob served at the two second time scale of the trial. We argue that both common excitation and inhibition may play significant roles in establishing this correlation. 1 INTRODUCTION In a previous study of pairs of MT neurons recorded using a single extracellular electrode, it was found that the spike count during two seconds of visual motion stimulation had an average correlation coefficient of r = 0.12 and that this cor relation could significantly limit the usefulness of pooling across increasingly large populations of neurons (Zohary et aI",NLP,ce5140df15d046a66883807d18d0264b-Paper.pdf,1995
Worst-case Loss Bounds,"We analyze and compare the well-known Gradient Descent algo rithm and a new algorithm, called the Exponentiated Gradient algorithm, for training a single neuron with an arbitrary transfer function. Both algorithms are easily generalized to larger neural networks, and the generalization of Gradient Descent is the stan dard back-propagation algorithm. In this paper we prove worst case loss bounds for both algorithms in the single neuron case. Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima. We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bound for any such transfer function and its corresponding matching loss. For exam ple, the matching loss for the identity function is the square loss and the matching loss for the logistic sigmoid is the entropic loss. The different structure of the ",Optimization & Theoretical ML,d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf,1995
Stock Selection via Nonlinear,"This paper discusses the use of multilayer feed forward neural net works for predicting a stock's excess return based on its exposure to various technical and fundamental factors. To demonstrate the effectiveness of the approach a hedged portfolio which consists of equally capitalized long and short positions is constructed and its historical returns are benchmarked against T-bill returns and the S&P500 index. 1 Introduction Traditional investment approaches (Elton and Gruber, 1991) assume that the return of a security can be described by a multifactor linear model: (1) where Hi denotes the return on security i, Fl are a set of factor values and Uil are security i exposure to factor I, ai is an intercept term (which under the CAPM framework is assumed to be equal to the risk free rate of return (Sharpe, 1984)) and ei is a random term with mean zero which is assumed to be uncorrelated across securities. The factors may consist of any set of variables deemed to have explanatory power for",Deep Learning,d6ef5f7fa914c19931a55bb262ec879c-Paper.pdf,1995
The Geometry of Eye Rotations,"We analyse the geometry of eye rotations, and in particular saccades, using basic Lie group theory and differential geome try. Various parameterizations of rotations are related through a unifying mathematical treatment, and transformations between co-ordinate systems are computed using the Campbell-Baker Hausdorff formula. Next, we describe Listing's law by means of the Lie algebra so(3). This enables us to demonstrate a direct connection to Donders' law, by showing that eye orientations are restricted to the quotient space 80(3)/80(2). The latter is equiv alent to the sphere S2, which is exactly the space of gaze directions. Our analysis provides a mathematical framework for studying the oculomotor system and could also be extended to investigate the geometry of mUlti-joint arm movements. 1 INTRODUCTION 1.1 SACCADES AND LISTING'S LAW Saccades are fast eye movements, bringing objects of interest into the center of the visual field. It is known that eye positions are restricted to a su",Optimization & Theoretical ML,d736bb10d83a904aefc1d6ce93dc54b8-Paper.pdf,1995
When is an Integrate-and-fire Neuron,"In the Poisson neuron model, the output is a rate-modulated Pois son process (Snyder and Miller, 1991); the time varying rate pa rameter ret) is an instantaneous function G[.] of the stimulus, ret) = G[s(t)]. In a Poisson neuron, then, ret) gives the instan taneous firing rate-the instantaneous probability of firing at any instant t-and the output is a stochastic function of the input. In part because of its great simplicity, this model is widely used (usu ally with the addition of a refractory period), especially in in vivo single unit electrophysiological studies, where set) is usually taken to be the value of some sensory stimulus. In the integrate-and-fire neuron model, by contrast, the output is a filtered and thresholded function of the input: the input is passed through a low-pass filter (determined by the membrane time constant T) and integrated un til the membrane potential vet) reaches threshold 8, at which point vet) is reset to its initial value. By contrast with the Poisso",Optimization & Theoretical ML,d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf,1995
Examples of learning curves from a modified,"We examine the issue of evaluation of model specific parameters in a modified VC-formalism. Two examples are analyzed: the 2-dimensional homogeneous perceptron and the I-dimensional higher order neuron. Both models are solved theoretically, and their learning curves are com pared against true learning curves. It is shown that the formalism has the potential to generate a variety of learning curves, including ones displaying ''phase transitions."" 1 Introduction One of the main criticisms of the Vapnik-Chervonenkis theory of learning [15] is that the results of the theory appear very loose when compared with empirical data. In contrast, theory based on statistical physics ideas [1] provides tighter numerical results as well as qualitatively distinct predictions (such as ""phase transitions"" to perfect generalization). (See [5, 14] for a fuller discussion.) A question arises as to whether the VC-theory can be modified to give these improvements. The general direction of such a modification",Optimization & Theoretical ML,d91d1b4d82419de8a614abce9cc0e6d4-Paper.pdf,1995
Using Unlabeled Data for Supervised,"Many classification problems have the property that the only costly part of obtaining examples is the class label. This paper suggests a simple method for using distribution information contained in unlabeled examples to augment labeled examples in a supervised training framework. Empirical tests show that the technique de scribed in this paper can significantly improve the accuracy of a supervised learner when the learner is well below its asymptotic accuracy level. 1 INTRODUCTION Supervised learning problems often have the following property: unlabeled examples have little or no cost while class labels have a high cost. For example, it is trivial to record hours of heartbeats from hundreds of patients. However, it is expensive to hire cardiologists to label each of the recorded beats. One response to the expense of class labels is to squeeze the most information possible out of each labeled example. Regularization and cross-validation both have this goal. A second response is to star",Optimization & Theoretical ML,db2b4182156b2f1f817860ac9f409ad7-Paper.pdf,1995
Implementation Issues in the Fourier,"The Fourier transform of boolean functions has come to play an important role in proving many important learnability results. We aim to demonstrate that the Fourier transform techniques are also a useful and practical algorithm in addition to being a powerful theoretical tool. We describe the more prominent changes we have introduced to the algorithm, ones that were crucial and without which the performance of the algorithm would severely deterio rate. One of the benefits we present is the confidence level for each prediction which measures the likelihood the prediction is correct. 1 INTRODUCTION Over the last few years the Fourier Transform (FT) representation of boolean func tions has been an instrumental tool in the computational learning theory commu nity. It has been used mainly to demonstrate the learnability of various classes of functions with respect to the uniform distribution. The first connection between the Fourier representation and learnability of boolean functions was e",Optimization & Theoretical ML,db576a7d2453575f29eab4bac787b919-Paper.pdf,1995
A Bound on the Error of Cross Validation Using,No abstract found,Optimization & Theoretical ML,dc58e3a306451c9d670adcd37004f48f-Paper.pdf,1995
Classifying Facial Action,"The Facial Action Coding System, (FACS), devised by Ekman and Friesen (1978), provides an objective meanS for measuring the facial muscle contractions involved in a facial expression. In this paper, we approach automated facial expression analysis by detecting and classifying facial actions. We generated a database of over 1100 image sequences of 24 subjects performing over 150 distinct facial actions or action combinations. We compare three different ap proaches to classifying the facial actions in these images: Holistic spatial analysis based on principal components of graylevel images; explicit measurement of local image features such as wrinkles; and template matching with motion flow fields. On a dataset contain ing six individual actions and 20 subjects, these methods had 89%, 57%, and 85% performances respectively for generalization to novel subjects. When combined, performance improved to 92%. 1 INTRODUCTION Measurement of facial expressions is important for research and assess",Computer Vision,dd77279f7d325eec933f05b1672f6a1f-Paper.pdf,1995
Unknown Title,No abstract found,The category cannot be determined from the provided information.,df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf,1995
A New Learning Algorithm for Blind,"A new on-line learning algorithm which minimizes a statistical de pendency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual in formation (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations. 1 INTRODUCTION The problem of blind signal separation arises in many areas such as speech recog nition, data communication, sensor signal processing, and medical science. Several neural network algorithms [3, 5, 7] have been proposed for solving this problem. The performance of these alg",Deep Learning,e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf,1995
Adaptive Back-Propagation in On-Line,"An adaptive back-propagation algorithm is studied and compared with gradient descent (standard back-propagation) for on-line learning in two-layer neural networks with an arbitrary number of hidden units. Within a statistical mechanics framework, both numerical studies and a rigorous analysis show that the adaptive back-propagation method results in faster training by breaking the symmetry between hidden units more efficiently and by providing faster convergence to optimal generalization than gradient descent. 1 INTRODUCTION Multilayer feedforward perceptrons (MLPs) are widely used in classification and regression applications due to their ability to learn a range of complicated maps [1] e from examples. When learning a map fo from N-dimensional inputs to scalars ( the parameters {W} of the student network are adjusted according to some training algorithm so that the map defined by these parameters fw approximates the teacher fo as close as possible. The resulting performance is measur",Optimization & Theoretical ML,e22312179bf43e61576081a2f250f845-Paper.pdf,1995
Neuron-MOS Temporal Winner Search,"A unique architecture of winner search hardware has been de veloped using a novel neuron-like high functionality device called Neuron MOS transistor (or vMOS in short) [1,2] as a key circuit element. The circuits developed in this work can find the location of the maximum (or minimum) signal among a number of input data on the continuous-time basis, thus enabling real-time winner tracking as well as fully-parallel sorting of multiple input data. We have developed two circuit schemes. One is an ensemble of self loop-selecting v M OS ring oscillators finding the winner as an oscil lating node. The other is an ensemble of vMOS variable threshold inverters receiving a common ramp-voltage for competitive excita tion where data sorting is conducted through consecutive winner search actions. Test circuits were fabricated by a double-polysilicon CMOS process and their operation has been experimentally veri fied. 1 INTRODUCTION Search for the largest (or the smallest) among a number of input da",Computer Vision,e515df0d202ae52fcebb14295743063b-Paper.pdf,1995
Gradient and Hamiltonian Dynamics,"The process of machine learning can be considered in two stages: model selection and parameter estimation. In this paper a technique is presented for constructing dynamical systems with desired qualitative properties. The approach is based on the fact that an n-dimensional nonlinear dynamical system can be decomposed into one gradient and (n - 1) Hamiltonian sys tems. Thus, the model selection stage consists of choosing the gradient and Hamiltonian portions appropriately so that a certain behavior is obtainable. To estimate the parameters, a stably convergent learning rule is presented. This algorithm has been proven to converge to the desired system trajectory for all initial conditions and system inputs. This technique can be used to design neural network models which are guaranteed to solve the trajectory learning problem. 1 Introduction A fundamental problem in mathematical systems theory is the identification of dy namical systems. System identification is a dynamic analogue of th",Optimization & Theoretical ML,e17184bcb70dcf3942c54e0b537ffc6d-Paper.pdf,1995
Human Reading and the Curse of,"Whereas optical character recognition (OCR) systems learn to clas sify single characters; people learn to classify long character strings in parallel, within a single fixation. This difference is surprising because high dimensionality is associated with poor classification learning. This paper suggests that the human reading system avoids these problems because the number of to-be-classified im ages is reduced by consistent and optimal eye fixation positions, and by character sequence regularities. An interesting difference exists between human reading and optical character recog nition (OCR) systems. The input/output dimensionality of character classification in human reading is much greater than that for OCR systems (see Figure 1). OCR systems classify one character at time; while the human reading system classifies as many as 8-13 characters per eye fixation (Rayner, 1979) and within a fixation, character category and sequence information is extracted in parallel (Blanchard, McConki",NLP,e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf,1995
Unsupervised Pixel-prediction,"When a sensory system constructs a model of the environment from its input, it might need to verify the model's accuracy. One method of verification is multivariate time-series prediction: a good model could predict the near-future activity of its inputs, much as a good scientific theory predicts future data. Such a predict ing model would require copious top-down connections to compare the predictions with the input. That feedback could improve the model's performance in two ways: by biasing internal activity to ward expected patterns, and by generating specific error signals if the predictions fail. A proof-of-concept model-an event-driven, computationally efficient layered network, incorporating ""cortical"" features like all-excitatory synapses and local inhibition- was con structed to make near-future predictions of a simple, moving stim ulus. After unsupervised learning, the network contained units not only tuned to obvious features of the stimulus like contour orienta tion and mot",Computer Vision,e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf,1995
Control of Selective Visual Attention:,"Intermediate and higher vision processes require selection of a sub set of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called ""focus of at tention"" which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functional ity of biological vision but also to be essential for the understanding of complex scenes in machine vision. 1 Introduction: ""What"" and ""Where"" In Vision It is a generally accepted fact that the computations of early vision are massively parallel operations, i.e., applied in parallel to all parts of the visual field. This high degree of parallelism cannot be sustained in in~ermediate and higher vision because of the astronomical number of different possib",Computer Vision,e8b1cbd05f6e6a358a81dee52493dd06-Paper.pdf,1995
Quadratic-Type Lyapunov Functions for,"The dynamics of complex neural networks modelling the self organization process in cortical maps must include the aspects of long and short-term memory. The behaviour of the network is such characterized by an equation of neural activity as a fast phenom enon and an equation of synaptic modification as a slow part of the neural system. We present a quadratic-type Lyapunov function for the flow of a competitive neural system with fast and slow dynamic variables. We also show the consequences of the stability analysis on the neural net parameters. 1 INTRODUCTION This paper investigates a special class of laterally inhibited neural networks. In particular, we have examined the dynamics of a restricted class of laterally inhibited neural networks from a rigorous analytic standpoint. The network models for retinotopic and somatotopic cortical maps are usually com posed of several layers of neurons from sensory receptors to cortical units, with feedforward excitations between the layers and ",Optimization & Theoretical ML,eddb904a6db773755d2857aacadb1cb0-Paper.pdf,1995
Learning long-term dependencies,"It has recently been shown that gradient descent learning algo rithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies. In this paper we explore this problem for a class of architectures called NARX networks, which have powerful representational capabilities. Previous work reported that gradient descent learning is more effective in NARX networks than in recurrent networks with ""hidden states"". We show that although NARX networks do not circumvent the prob lem of long-term dependencies, they can greatly improve perfor mance on such problems. We present some experimental 'results that show that NARX networks can often retain information for two to three times as long as conventional recurrent networks. 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlin ear dynamical systems [19, 20]. However, learning simple behavior can be quite ""Also with NEC Research Institute. tAlso with UMIACS, University of Mary",Deep Learning,f197002b9a0853eca5e046d9ca4663d5-Paper.pdf,1995
Learning the structure of similarity,"The additive clustering (ADCL US) model (Shepard & Arabie, 1979) treats the similarity of two stimuli as a weighted additive measure of their common features. Inspired by recent work in unsupervised learning with multiple cause models, we propose anew, statistically well-motivated algorithm for discovering the structure of natural stimulus classes using the ADCLUS model, which promises substan tial gains in conceptual simplicity, practical efficiency, and solution quality over earlier efforts. We also present preliminary results with artificial data and two classic similarity data sets. 1 INTRODUCTION The capacity to judge one stimulus, object, or concept as similado another is thought to play a pivotal role in many cognitive processes, including generalization, recog nition, categorization, and inference. Consequently, modeling subjective similarity judgments in order to discover the underlying structure of stimulus representations in the brain/mind holds a central place in contempora",NLP,f4dd765c12f2ef67f98f3558c282a9cd-Paper.pdf,1995
Investment Learning,"We propose a hierarchical scheme for rapid learning of context dependent ""skills"" that is based on the recently introduced ""Parameterized Self Organizing Map"" (""PSOM""). The underlying idea is to first invest some learning effort to specialize the system into a rapid learner for a more restricted range of contexts. The specialization is carried out by a prior ""investment learning stage"", during which the system acquires a set of basis mappings or ""skills"" for a set of prototypical contexts. Adaptation of a ""skill"" to a new context can then be achieved by interpolating in the space of the basis mappings and thus can be extremely rapid. We demonstrate the potential of this approach for the task of a 3D visuo motor map for a Puma robot and two cameras. This includes the for ward and backward robot kinematics in 3D end effector coordinates, the 2D+2D retina coordinates and also the 6D joint angles. After the invest ment phase the transformation can be learned for a new camera set-up with a ",Reinforcement Learning,f7f580e11d00a75814d2ded41fe8e8fe-Paper.pdf,1995
How Perception Guides Production •,"A c.:omputational model of song learning in the song sparrow (M elospiza melodia) learns to categorize the different syllables of a song sparrow song and uses this categorization to train itself to reproduce song. The model fills a crucial gap in the computational explanation of birdsong learning by exploring the organization of perception in songbirds. It shows how competitive learning may lead to the organization of a specific nucleus in the bird brain, replicates the song production results of a previous model (Doya and Sejnowski, 1995), and demonstrates how perceptual learning can guide production through reinforcement learning. 1 INTRODUCTION The passeriformes or songbirds make up more than half of all bird species and are divided into two groups: the os cines which learn their songs and sub-oscines which do not. Oscines raised in isolation sing degraded species typical songs similar to wild song. Deafened oscines sing completely degraded songs (Konishi, 1965) , while deafened sub",Computer Vision,fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf,1995
Stable Fitted Reinforcement Learning,"We describe the reinforcement learning problem, motivate algo rithms which seek an approximation to the Q function, and present new convergence results for two such algorithms. 1 INTRODUCTION AND BACKGROUND Imagine an agent acting in some environment. At time t, the environment is in some state Xt chosen from a finite set of states. The agent perceives Xt, and is allowed to choose an action at from some finite set of actions. The environment then changes state, so that at time (t + 1) it is in a new state Xt+1 chosen from a probability distribution which depends only on Xt and at. Meanwhile, the agent experiences a real-valued cost Ct, chosen from a distribution which also depends only on Xt and at and which has finite mean and variance. Such an environment is called a Markov decision process, or MDP. The reinforce ment learning problem is to control an MDP to minimize the expected discounted Lt cost ,tCt for some discount factor, E [0,1]. Define the function Q so that Q(x, a) is the c",Reinforcement Learning,fd06b8ea02fe5b1c2496fe1700e9d16c-Paper.pdf,1995
Information through a Spiking Neuron,"While it is generally agreed that neurons transmit information about their synaptic inputs through spike trains, the code by which this information is transmitted is not well understood. An upper bound on the information encoded is obtained by hypothesizing that the precise timing of each spike conveys information. Here we develop a general approach to quantifying the information carried by spike trains under this hypothesis, and apply it to the leaky integrate-and-fire (IF) model of neuronal dynamics. We formu late the problem in terms of the probability distribution peT) of interspike intervals (ISIs), assuming that spikes are detected with arbitrary but finite temporal resolution. In the absence of added noise, all the variability in the ISIs could encode information, and the information rate is simply the entropy of the lSI distribution, H (T) = (-p(T) log2 p(T)}, times the spike rate. H (T) thus pro vides an exact expression for the information rate. The methods developed here can",Neuroscience,fd2c5e4680d9a01dba3aada5ece22270-Paper.pdf,1995
Discriminant Adaptive Nearest Neighbor,"Nearest neighbor classification expects the class conditional prob abilities to be locally constant, and suffers from bias in high di mensions We propose a locally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective met ric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighbor hoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. We indicate how these techniques can be extended to the regression problem. 1 Introduction We consider a discrimination problem with J classes and N training observations. The training observations consist of predictor measureme",Optimization & Theoretical ML,fe709c654eac84d5239d1a12a4f71877-Paper.pdf,1995
A Predictive Switching Model of,"We present a hypothesis about how the cerebellum could partici pate in regulating movement in the presence of significant feedback delays without resorting to a forward model of the motor plant. We show how a simplified cerebellar model can learn to control end point positioning of a nonlinear spring-mass system with realistic delays in both afferent and efferent pathways. The model's opera tion involves prediction, but instead of predicting sensory input, it directly regulates movement by reacting in an anticipatory fashion to input patterns that include delayed sensory feedback. 1 INTRODUCTION The existence of significant delays in sensorimotor feedback pathways has led several researchers to suggest that the cerebellum might function as a forward model of the motor plant in order to predict the sensory consequences of motor commands before actual feedback is available; e.g., (Ito, 1984; Keeler, 1990; Miall et ai., 1993). While we agree that there are many potential roles for forward",Reinforcement Learning,feab05aa91085b7a8012516bc3533958-Paper.pdf,1995
Recurrent Neural Networks for Missing or,"In this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems. On the one hand, this scheme can be used for static data when some of the input variables are missing. On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies. Unlike in the case of probabilistic models (e.g. Gaussian) of the missing variables, the network does not attempt to model the distribution of the missmg variables given the observed variables. Instead it is a more ""discriminant"" approach that fills in the missing variables for the sole purpose of minimizing a learning criterion (e.g., to minimize an output error). 1 Introduction Learning from examples implies discovering certain relations between variables of interest. The most general form of learning requires to essentially capture the joint distribution between these variables. However, for many specif",NLP,ffeed84c7cb1ae7bf4ec4bd78275bb98-Paper.pdf,1995
Hebb Learning of Features,"This paper investigates the stationary points of a Hebb learning rule with a sigmoid nonlinearity in it. We show mathematically that when the input has a low information content, as measured by the input's variance, this learning rule suppresses learning, that is, forces the weight vector to converge to the zero vector. When the information content exceeds a certain value, the rule will automatically begin to learn a feature in the input. Our analysis suggests that under certain conditions it is the first principal component that is learned. The weight vector length remains bounded, provided the variance of the input is finite. Simulations confirm the theoretical results derived. 1 Introduction Hebb learning, one of the main mechanisms of synaptic strengthening, is induced by cooccurrent activity of pre- and post-synaptic neurons. It is used in artificial neural networks like perceptrons, associative memories, and unsupervised learning neural networks. Unsupervised Hebb learning typica",Reinforcement Learning,0188e8b8b014829e2fa0f430f0a95961-Paper.pdf,1996
A variational principle for,"Given a multidimensional data set and a model of its density, we consider how to define the optimal interpolation between two points. This is done by assigning a cost to each path through space, based on two competing goals-one to interpolate through regions of high density, the other to minimize arc length. From this path functional, we derive the Euler-Lagrange equations for extremal motionj given two points, the desired interpolation is found by solv ing a boundary value problem. We show that this interpolation can be done efficiently, in high dimensions, for Gaussian, Dirichlet, and mixture models. 1 Introduction The problem of non-linear interpolation arises frequently in image, speech, and signal processing. Consider the following two examples: (i) given two profiles of the same face, connect them by a smooth animation of intermediate poses[l]j (ii) given a telephone signal masked by intermittent noise, fill in the missing speech. Both these examples may be viewed as instances of",Optimization & Theoretical ML,018b59ce1fd616d874afad0f44ba338d-Paper.pdf,1996
ARTEX: A Self-Organizing Architecture,"A self-organizing architecture is developed for image region classi fication. The system consists of a preprocessor that utilizes multi scale filtering, competition, cooperation, and diffusion to compute a vector of image boundary and surface properties, notably texture and brightness properties. This vector inputs to a system that incrementally learns noisy multidimensional mappings and their probabilities. The architecture is applied to difficult real-world image classification problems, including classification of synthet ic aperture radar and natural texture images, and outperforms a recent state-of-the-art system at classifying natural textures. 1 INTRODUCTION Automatic processing of visual scenes often begins by detecting regions of an image with common values of simple local features, such as texture, and mapping the pat tern offeature activation into a predicted region label. We develop a self-organizing neural architecture, called the ARTEX algorithm, for automatically extract",Computer Vision,01e9565cecc4e989123f9620c1d09c09-Paper.pdf,1996
The algorithm -,"The power of sampling methods in Bayesian reconstruction of noisy signals is well known. The extension of sampling to temporal prob lems is discussed. Efficacy of sampling over time is demonstrated with visual tracking. 1 INTRODUCTION The problem of tracking curves in dense visual clutter is a challenging one. Trackers based on Kalman filters are of limited power; because they are based on Gaussian densities which are unimodal they cannot represent simultaneous alternative hy potheses. Extensions to the Kalman filter to handle multiple data associations (Bar-Shalom and Fortmann, 1988) work satisfactorily in the simple case of point targets but do not extend naturally to continuous curves. Tracking is the propagation of shape and motion estimates over time, driven by a temporal stream of observations. The noisy observations that arise in realistic problems demand a robust approach involving propagation of probability distribu tions over time. Modest levels of noise may be treated satisf",Computer Vision,0829424ffa0d3a2547b6c9622c77de03-Paper.pdf,1996
A Silicon Model of,"Detectim of the periodicity of amplitude modulatim is a major step in the determinatim of the pitch of a SOODd. In this article we will present a silicm model that uses synchrroicity of spiking neurms to extract the fundamental frequency of a SOODd. It is based m the observatim that the so called 'Choppers' in the mammalian Cochlear Nucleus synchrmize well for certain rates of amplitude modulatim, depending m the cell's intrinsic chopping frequency. Our silicm model uses three different circuits, i.e., an artificial cochlea, an Inner Hair Cell circuit, and a spiking neuron circuit",Computer Vision,0ce2ffd21fc958d9ef0ee9ba5336e357-Paper.pdf,1996
Adaptive On-line Learning in Changing,"An adaptive on-line algorithm extending the learning of learning idea is proposed and theoretically motivated. Relying only on gra dient flow information it can be applied to learning continuous functions or distributions, even when no explicit loss function is gi ven and the Hessian is not available. Its efficiency is demonstrated for a non-stationary blind separation task of acoustic signals. 1 Introduction Neural networks provide powerful tools to capture the structure in data by learning. Often the batch learning paradigm is assumed, where the learner is given all trai ning examples simultaneously and allowed to use them as often as desired. In large practical applications batch learning is often experienced to be rather infeasible and instead on-line learning is employed. In the on-line learning scenario only one example is given at a time and then discar ded after learning. So it is less memory consuming and at the same time it fits well into more natural learning, where the lear",Optimization & Theoretical ML,0e095e054ee94774d6a496099eb1cf6a-Paper.pdf,1996
Minimizing Statistical Bias with Queries,"I describe a querying criterion that attempts to minimize the error of a learner by minimizing its estimated squared bias. I describe experiments with locally-weighted regression on two simple prob lems, and observe that this ""bias-only"" approach outperforms the more common ""variance-only"" exploration approach, even in the presence of noise. 1 INTRODUCTION In recent years, there has been an explosion of interest in ""active"" machine learning systems. These are learning systems that make queries, or perform experiments to gather data that are expected to maximize performance. When compared with ""passive"" learning systems, which accept given, or randomly drawn data, active learners have demonstrated significant decreases in the amount of data required to achieve equivalent performance. In industrial applications, where each experiment may take days to perform and cost thousands of dollars, a method for optimally selecting these points would offer enormous savings in time and money. An act",Optimization & Theoretical ML,0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf,1996
A Micropower Analog VLSI,"We describe the implementation of a hidden Markov model state decoding system, a component for a wordspotting speech recogni tion system. The key specification for this state decoder design is microwatt power dissipation; this requirement led to a continuous time, analog circuit implementation. We characterize the operation of a 10-word (81 state) state decoder test chip.",Computer Vision,144a3f71a03ab7c4f46f9656608efdb2-Paper.pdf,1996
Dual Kalman Filtering Methods for,"Prediction, estimation, and smoothing are fundamental to signal processing. To perform these interrelated tasks given noisy data, we form a time series model of the process that generates the data. Taking noise in the system explicitly into account, maximum likelihood and Kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the un derlying state of the system. We review several established meth ods in the linear case, and propose severa! extensions utilizing dual Kalman filters (DKF) and forward-backward (FB) filters that are applicable to neural networks. Methods are compared on several simulations of noisy time series. We also include an example of nonlinear noise reduction in speech. 1 INTRODUCTION Consider the general autoregressive model of a noisy time series with both process and additive observation noise: x(k) I(x(k - 1), ... x(k - M), w) + v(k - 1) (1) y(k) x(k) + r(k), (2) where x(k) corresponds to the true underlying tim",Optimization & Theoretical ML,147702db07145348245dc5a2f2fe5683-Paper.pdf,1996
Source Separation and Density,"We couple the tasks of source separation and density estimation by extracting the local geometrical structure of distributions ob tained from mixtures of statistically independent sources. Our modifications of the self-organizing map (SOM) algorithm results in purely digital learning rules which perform non-parametric his togram density estimation. The non-parametric nature of the sep aration allows for source separation of non-linear mixtures. An anisotropic coupling is introduced into our SOM with the role of aligning the network locally with the independent component con tours. This approach provides an exact verification condition for source separation with no prior on the source distributions. 1 INTRODUCTION Much of the current work on visual cortex modeling has focused on the generation of coding which captures statistical independence and sparseness (Bell and Sejnowski 1996, Olshausen and Field 1996). The Bell and Sejnowski model suffers from the parametric and intrinsically non",NLP,160c88652d47d0be60bfbfed25111412-Paper.pdf,1996
Continuous sigmoidal belief networks,"Real-valued random hidden variables can be useful for modelling latent structure that explains correlations among observed vari ables. I propose a simple unit that adds zero-mean Gaussian noise to its input before passing it through a sigmoidal squashing func tion. Such units can produce a variety of useful behaviors, ranging from deterministic to binary stochastic to continuous stochastic. I show how ""slice sampling"" can be used for inference and learning in top-down networks of these units and demonstrate learning on two simple problems. 1 Introduction A variety of unsupervised connectionist models containing discrete-valued hidden units have been developed. These include Boltzmann machines (Hinton and Se jnowski 1986), binary sigmoidal belief networks (Neal 1992) and Helmholtz ma chines (Hinton et al. 1995; Dayan et al. 1995). However, some hidden variables, such as translation or scaling in images of shapes, are best represented using continu ous values. Continuous-valued Boltzmann",Optimization & Theoretical ML,17326d10d511828f6b34fa6d751739e2-Paper.pdf,1996
"Compositionality, MDL Priors, and","Images are ambiguous at each of many levels of a contextual hi erarchy. Nevertheless, the high-level interpretation of most scenes is unambiguous, as evidenced by the superior performance of hu mans. This observation argues for global vision models, such as de formable templates. Unfortunately, such models are computation ally intractable for unconstrained problems. We propose a composi tional model in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings. Ambiguity is propagated up the hierarchy in the form of multiple interpretations, which are later resolved by a Bayesian, equivalently minimum-description-Iength, cost functional. 1 Bayesian decision theory and compositionaiity In his Essay on Probability, Laplace (1812) devotes a short chapter-his ""Sixth Principle"" -to what we call today the Bayesian decision rule. Laplace observes that we interpret a ""regular combination,"" e.g., an arrangement of objects t",Computer Vision,17fafe5f6ce2f1904eb09d2e80a4cbf6-Paper.pdf,1996
A mean field algorithm for Bayes learning,"We present an algorithm which is expected to realise Bayes optimal predictions in large feed-forward networks. It is based on mean field methods developed within statistical mechanics of disordered sys tems. We give a derivation for the single layer perceptron and show that the algorithm also provides a leave-one-out cross-validation test of the predictions. Simulations show excellent agreement with theoretical results of statistical mechanics. 1 INTRODUCTION Bayes methods have become popular as a consistent framework for regularization and model selection in the field of neural networks (see e.g. [MacKay,1992]). In the Bayes approach to statistical inference [Berger, 1985] one assumes that the prior uncertainty about parameters of an unknown data generating mechanism can be encoded in a probability distribution, the so called prior. Using the prior and the likelihood of the data given the parameters, the posterior distribution of the parameters can be derived from Bayes rule. From thi",Optimization & Theoretical ML,193002e668758ea9762904da1a22337c-Paper.pdf,1996
Ordered Classes and Incomplete Examples,"The classes in classification tasks often have a natural ordering, and the training and testing examples are often incomplete. We propose a non linear ordinal model for classification into ordered classes. Predictive, simulation-based approaches are used to learn from past and classify fu ture incomplete examples. These techniques are illustrated by making prognoses for patients who have suffered severe head injuries. 1 Motivation Jennett et al. (1979) reported data on patients with severe head injuries. For each patient some of the information in Table 1 was available shortly after injury. The objective is to predict the degree of recovery attained within six months as measured by outcome. This problem exhibits two characteristics that are common in classification tasks: allocation qf examples into classes which have a natural ordering, and learning from past and classifying future incomplete examples. 2 A Flexible Model for Ordered Classes The Bayes decision rule (see, for example, R",Optimization & Theoretical ML,1c65cef3dfd1e00c0b03923a1c591db4-Paper.pdf,1996
Unification of Information Maximization,"In the present paper, we propose a method to unify information maximization and minimization in hidden units. The information maximization and minimization are performed on two different lev els: collective and individual level. Thus, two kinds of information: collective and individual information are defined. By maximizing collective information and by minimizing individual information, simple networks can be generated in terms of the number of con nections and the number of hidden units. Obtained networks are expected to give better generalization and improved interpretation of internal representations. This method was applied to the infer ence of the maximum onset principle of an artificial language. In this problem, it was shown that the individual information min imization is not contradictory to the collective information max imization. In addition, experimental results confirmed improved generalization performance, because over-training can significantly be suppressed. 1 Introdu",Optimization & Theoretical ML,1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf,1996
Multi-Task Learning for Stock Selection,"Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions. Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared. When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning. In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks. 1 Introd uction Previous applications of ANNs to financial time-series suggest that several of these prediction and decision-taking tasks present sufficient non-linearities to justify the use of ANNs (Refenes, 1994; Moody, Levin and Rehfuss, 1993). These models can incorporate various types of explanatory variables: so-called technical variables (de pending on the past price sequence), micro-e",Deep Learning,1d72310edc006dadf2190caad5802983-Paper.pdf,1996
Cholinergic Modulation Preserves Spike,"Neuromodulation can change not only the mean firing rate of a neuron, but also its pattern of firing. Therefore, a reliable neu ral coding scheme, whether a rate coding or a spike time based coding, must be robust in a dynamic neuromodulatory environ ment. The common observation that cholinergic modulation leads to a reduction in spike frequency adaptation implies a modifica tion of spike timing, which would make a neural code based on precise spike timing difficult to maintain. In this paper, the effects of cholinergic modulation were studied to test the hypothesis that precise spike timing can serve as a reliable neural code. Using the whole cell patch-clamp technique in rat neocortical slice prepara tion and compartmental modeling techniques, we show that cholin ergic modulation, surprisingly, preserved spike timing in response to a fluctuating inputs that resembles in vivo conditions. This re sult suggests that in vivo spike timing may be much more resistant to changes in neuromodu",NLP,1ee3dfcd8a0645a25a35977997223d22-Paper.pdf,1996
Analytical Mean Squared Error Curves,"We have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing Markov chains using lookup table representations. We illustrate classes of learning curve behavior in various chains, and show the manner in which TD is sensitive to the choice of its step size and eligibility trace parameters. 1 INTRODUCTION A reassuring theory of asymptotic convergence is available for many reinforcement learning (RL) algorithms. What is not available, however, is a theory that explains the finite-term learning curve behavior of RL algorithms, e.g., what are the different kinds of learning curves, what are their key determinants, and how do different problem parameters effect rate of convergence. Answering these questions is crucial not only for making useful comparisons between algorithms, but also for developing hybrid and new RL methods. In this paper we pr",Reinforcement Learning,242c100dc94f871b6d7215b868a875f8-Paper.pdf,1996
Dynamics of Training,"A new method to calculate the full training process of a neural net work is introduced. No sophisticated methods like the replica trick are used. The results are directly related to the actual number of training steps. Some results are presented here, like the maximal learning rate, an exact description of early stopping, and the neces sary number of training steps. Further problems can be addressed with this approach. 1 INTRODUCTION Training guided by empirical risk minimization does not always minimize the ex pected risk. This phenomenon is called overfitting and is one of the major problems in neural network learning. In a previous work [Bos 1995] we developed an approx imate description of the training process using statistical mechanics. To solve this problem exactly, we introduce a new description which is directly dependent on the actual training steps. As a first result we get analytical curves for empirical risk and expected risk as functions of the training time, like the one",Optimization & Theoretical ML,25df35de87aa441b88f22a6c2a830a17-Paper.pdf,1996
A Constructive RBF Network,"This paper discusses a fairly general adaptation algorithm which augments a standard neural network to increase its recognition ac curacy for a specific user. The basis for the algorithm is that the output of a neural network is characteristic of the input, even when the output is incorrect. We exploit this characteristic output by using an Output Adaptation Module (OAM) which maps this out put into the correct user-dependent confidence vector. The OAM is a simplified Resource Allocating Network which constructs ra dial basis functions on-line. We applied the OAM to construct a writer-adaptive character recognition system for on-line hand printed characters. The OAM decreases the word error rate on a test set by an average of 45%, while creating only 3 to 25 basis functions for each writer in the test set. 1 Introduction One of the major difficulties in creating any statistical pattern recognition system is that the statistics of the training set is often different from the statistics ",Deep Learning,26588e932c7ccfa1df309280702fe1b5-Paper.pdf,1996
A Model of View-Tuned Neurons,"In 1990 Poggio and Edelman proposed a view-based model of ob ject recognition that accounts for several psychophysical properties of certain recognition tasks. The model predicted the existence of view-tuned and view-invariant units, that were later found by Lo gothetis et al. (Logothetis et al., 1995) in IT cortex of monkeys trained with views of specific paperclip objects. The model, how ever, does not specify the inputs to the view-tuned units and their internal organization. In this paper we propose a model of these view-tuned units that is consistent with physiological data from single cell responses. 1 INTRODUCTION Recognition of specific objects, such as recognition of a particular face, can be based on representations that are object centered, such as 3D structural models. Alternatively, a 3D object may be represented for the purpose of recognition in terms of a set of views. This latter class of models is biologically attractive because model acquisition - the learning phase -",Computer Vision,2812e5cf6d8f21d69c91dddeefb792a7-Paper.pdf,1996
MLP can provably generalise much better,"Results of a study of the worst case learning curves for a partic ular class of probability distribution on input space to MLP with hard threshold hidden units are presented. It is shown in partic ular, that in the thermodynamic limit for scaling by the number of connections to the first hidden layer, although the true learning curve behaves as ~ a-I for a ~ 1, its VC-dimension based bound is trivial (= 1) and its VC-entropy bound is trivial for a ::; 6.2. It is also shown that bounds following the true learning curve can be derived from a formalism based on the density of error patterns. 1 Introduction The VC-formalism and its extensions link the generalisation capabilities of a binary valued neural network with its counting functionl , e.g. via upper bounds implied by VC-dimension or VC-entropy on this function [17, 18]. For linear perceptrons the counting function is constant for almost every selection of a fixed number of input samples [2], and essentially equal to its upper bound ",Optimization & Theoretical ML,2715518c875999308842e3455eda2fe3-Paper.pdf,1996
Learning Bayesian belief networks with,"In this paper we propose a method for learning Bayesian belief networks from data. The method uses artificial neural networks as probability estimators, thus avoiding the need for making prior assumptions on the nature of the probability distributions govern ing the relationships among the participating variables. This new method has the potential for being applied to domains containing both discrete and continuous variables arbitrarily distributed. We compare the learning performance of this new method with the performance of the method proposed by Cooper and Herskovits in [7]. The experimental results show that, although the learning scheme based on the use of ANN estimators is slower, the learning accuracy of the two methods is comparable. Category: Algorithms and Architectures. 1 Introduction Bayesian belief networks (BBN) are a powerful formalism for representing and rea soning under uncertainty. This representation has a solid theoretical foundation [13], and its practical value ",Deep Learning,285ab9448d2751ee57ece7f762c39095-Paper.pdf,1996
Approximate Solutions to,"We propose and analyze an algorithm that approximates solutions to the problem of optimal stopping in a discounted irreducible ape riodic Markov chain. The scheme involves the use of linear com binations of fixed basis functions to approximate a Q-function. The weights of the linear combination are incrementally updated through an iterative process similar to Q-Iearning, involving sim ulation of the underlying Markov chain. Due to space limitations, we only provide an overview of a proof of convergence (with prob ability 1) and bounds on the approximation error. This is the first theoretical result that establishes the soundness of a Q-Iearning like algorithm when combined with arbitrary linear function ap proximators to solve a sequential decision problem. Though this paper focuses on the case of finite state spaces, the results extend naturally to continuous and unbounded state spaces, which are ad dressed in a forthcoming full-length paper. 1 INTRODUCTION Problems of sequential deci",Reinforcement Learning,286674e3082feb7e5afb92777e48821f-Paper.pdf,1996
An Orientation Selective Neural Network,"We present an algorithm for identifying linear patterns on a two dimensional lattice based on the concept of an orientation selective cell, a concept borrowed from neurobiology of vision. Construct ing a multi-layered neural network with fixed architecture which implements orientation selectivity, we define output elements cor responding to different orientations, which allow us to make a se lection decision. The algorithm takes into account the granularity of the lattice as well as the presence of noise and inefficiencies. The method is applied to a sample of data collected with the ZEUS detector at HERA in order to identify cosmic muons that leave a linear pattern of signals in the segmented calorimeter. A two dimensional representation of the relevant part of the detector is used. The algorithm performs very well. Given its architecture, this system becomes a good candidate for fast pattern recognition in parallel processing devices. I Introduction A typical problem in experiments p",Computer Vision,2a50e9c2d6b89b95bcb416d6857f8b45-Paper.pdf,1996
Early Brain Damage,"Optimal Brain Damage (OBD) is a method for reducing the num ber of weights in a neural network. OBD estimates the increase in cost function if weights are pruned and is a valid approximation if the learning algorithm has converged into a local minimum. On the other hand it is often desirable to terminate the learning pro cess before a local minimum is reached (early stopping). In this paper we show that OBD estimates the increase in cost function incorrectly if the network is not in a local minimum. We also show how OBD can be extended such that it can be used in connec tion with early stopping. We call this new approach Early Brain Damage, EBD. EBD also allows to revive already pruned weights. We demonstrate the improvements achieved by EBD using three publicly available data sets. 1 Introduction Optimal Brain Damage (OBD) was introduced by Le Cun et al. (1990) as a method to significantly reduce the number of weights in a neural network. By reducing the number of free parameters, the",Optimization & Theoretical ML,2ac2406e835bd49c70469acae337d292-Paper.pdf,1996
Text-Based Information Retrieval Using,"The following investigates the use of single-neuron learning algo rithms to improve the performance of text-retrieval systems that accept natural-language queries. A retrieval process is explained that transforms the natural-language query into the query syntax of a real retrieval system: the initial query is expanded using statis tical and learning techniques and is then used for document ranking and binary classification. The results of experiments suggest that Kivinen and Warmuth's Exponentiated Gradient Descent learning algorithm works significantly better than previous approaches. 1 Introduction The following work explores two learning algorithms - Least Mean Squared (LMS) [1] and Exponentiated Gradient Descent (EG) [2] - in the context of text-based Information Retrieval (IR) systems. The experiments presented in [3] use connec tionist learning models to improve the retrieval of relevant documents from a large collection of text. Here, we present further analysis of those experim",NLP,2b6d65b9a9445c4271ab9076ead5605a-Paper.pdf,1996
An Analog Implementation of the,"We use the constant statistics constraint to calibrate an array of sensors that contains gain and offset variations. This algorithm has been mapped to analog hardware and designed and fabricated with a 2um CMOS technology. Measured results from the chip show that the system achieves invariance to gain and offset variations of the input signal. 1 Introduction Transistor mismatches and parameter variations cause unavoidable nonuniformities from sensor to sensor. A one-time calibration procedure is normally used to coun teract the effect of these fixed variations between components. Unfortunately, many of these variations fluctuate with time--either with operating point (such as data dependent variations) or with external conditions (such as temperature). Calibrat ing these sensors one-time only at the ""factory"" is not suitable-much more frequent calibration is required. The sensor calibration problem becomes more challenging as an increasing number of different types of sensors are integ",Optimization & Theoretical ML,2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf,1996
A neural model of visual contour,"We introduce a neurobiologically plausible model of contour inte gration from visual inputs of individual oriented edges. The model is composed of interacting excitatory neurons and inhibitory in terneurons, receives visual inputs via oriented receptive fields (RFs) like those in VI. The RF centers are distributed in space. At each location, a finite number of cells tuned to orientations spanning 1800 compose a model hypercolumn. Cortical interactions modify neural activities produced by visual inputs, selectively amplifying activities for edge elements belonging to smooth input contours. El ements within one contour produce synchronized neural activities. We show analytically and empirically that contour enhancement and neural synchrony increase with contour length, smoothness and closure, as observed experimentally. This model gives testable predictions, and in addition, introduces a feedback mechanism al lowing higher visual centers to enhance, suppress, and segment contours.",Computer Vision,2c89109d42178de8a367c0228f169bf8-Paper.pdf,1996
Unsupervised Learning by,"Unsupervised learning algorithms based on convex and conic en coders are proposed. The encoders find the closest convex or conic combination of basis vectors to the input. The learning algorithms produce basis vectors that minimize the reconstruction error of the encoders. The convex algorithm develops locally linear models of the input, while the conic algorithm discovers features. Both al gorithms are used to model handwritten digits and compared with vector quantization and principal component analysis. The neural network implementations involve feedback connections that project a reconstruction back to the input layer. 1 Introduction Vector quantization (VQ) and principal component analysis (peA) are two widely used unsupervised learning algorithms, based on two fundamentally different ways of encoding data. In VQ, the input is encoded as the index of the closest prototype stored in memory. In peA, the input is encoded as the coefficients of a linear superposition of a set of basis",Optimization & Theoretical ML,2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf,1996
Reconstructing Stimulus Velocity from,"We employed a white-noise velocity signal to study the dynamics of the response of single neurons in the cortical area MT to visual motion. Responses were quantified using reverse correlation, opti mal linear reconstruction filters, and reconstruction signal-to-noise ratio (SNR). The SNR and lower bound estimates of information rate were lower than we expected. Ninety percent of the informa tion was transmitted below 18 Hz, and the highest lower bound on bit rate was 12 bits/so A simulated opponent motion energy sub unit with Poisson spike statistics was able to out-perform the MT neurons. The temporal integration window, measured from the re verse correlation half-width, ranged from 30-90 ms. The window was narrower when a stimulus moved faster, but did not change when temporal frequency was held constant. 1 INTRODUCTION Area MT neurons can show precise and rapid modulation in response to dynamic noise stimuli (Bair and Koch, 1996); however, computational models of these neu rons and ",NLP,2df45244f09369e16ea3f9117ca45157-Paper.pdf,1996
Multidimensional Triangulation and,"Dynamic Programming, Q-Iearning and other discrete Markov Decision Process solvers can be -applied to continuous d-dimensional state-spaces by quantizing the state space into an array of boxes. This is often problematic above two dimensions: a coarse quantization can lead to poor policies, and fine quantization is too expensive. Possible solutions are variable-resolution discretization, or function approximation by neural nets. A third option, which has been little studied in the reinforcement learning literature, is interpolation on a coarse grid. In this paper we study interpolation tech niques that can result in vast improvements in the online behavior of the resulting control systems: multilinear interpolation, and an interpolation algorithm based on an interesting regular triangulation of d-dimensional space. We adapt these interpolators under three reinforcement learning paradigms: (i) offline value iteration with a known model, (ii) Q-Iearning, and (iii) online value iteration w",Reinforcement Learning,310ce61c90f3a46e340ee8257bc70e93-Paper.pdf,1996
Viewpoint invariant face recognition using,"We have explored two approaches to recogmzmg faces across changes in pose. First, we developed a representation of face images based on independent component analysis (ICA) and compared it to a principal component analysis (PCA) representation for face recognition. The ICA basis vectors for this data set were more spatially local than the PCA basis vectors and the ICA representa tion had greater invariance to changes in pose. Second, we present a model for the development of viewpoint invariant responses to faces from visual experience in a biological system. The temporal continuity of natural visual experience was incorporated into an attractor network model by Hebbian learning following a lowpass temporal filter on unit activities. When combined with the tem poral filter, a basic Hebbian update rule became a generalization of Griniasty et al. (1993), which associates temporally proximal input patterns into basins of attraction. The system acquired rep resentations of faces that were ",Computer Vision,3210ddbeaa16948a702b6049b8d9a202-Paper.pdf,1996
On the Effect of Analog Noise in,"We introduce a model for noise-robust analog computations with discrete time that is flexible enough to cover the most important concrete cases, such as computations in noisy analog neural nets and networks of noisy spiking neurons. We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the VC-dimension of computational models with analog noise. 1 Introduction Analog noise is a serious issue in practical analog computation. However there exists no formal model for reliable computations by noisy analog systems which allows us to address this issue in an adequate manner. The investigation of noise-tolerant digital computations in the presence of stochastic failures of gates or wires had been initiated by [von Neumann, 1956]. We refer to [Cowan, 1966] and [Pippenger, 1989] for a small sample of the nllmerous results that have been achieved in this di",Optimization & Theoretical ML,33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf,1996
Combined Weak Classifiers,"To obtain classification systems with both good generalization per formance and efficiency in space and time, we propose a learning method based on combinations of weak classifiers, where weak clas sifiers are linear classifiers (perceptrons) which can do a little better than making random guesses. A randomized algorithm is proposed to find the weak classifiers. They· are then combined through a ma jority vote. As demonstrated through systematic experiments, the method developed is able to obtain combinations of weak classifiers with good generalization performance and a fast training time on a variety of test problems and real applications. 1 Introduction The problem we will investigate in this work is how to develop a classifier with both good generalization performance and efficiency in space and time in a supervised learning environment. The generalization performance is measured by the proba bility of classification error of a classifier. A classifier is said to be efficient if it",Machine Learning,3546ab441e56fa333f8b44b610d95691-Paper.pdf,1996
Reinforcement Learning for Dynamic,"In cellular telephone systems, an important problem is to dynami cally allocate the communication resource (channels) so as to max imize service in a stochastic caller environment. This problem is naturally formulated as a dynamic programming problem and we use a reinforcement learning (RL) method to find dynamic channel allocation policies that are better than previous heuristic solutions. The policies obtained perform well for a broad variety of call traf fic patterns. We present results on a large cellular system with approximately 4949 states. In cellular communication systems, an important problem is to allocate the com munication resource (bandwidth) so as to maximize the service provided to a set of mobile callers whose demand for service changes stochastically. A given geograph ical area is divided into mutually disjoint cells, and each cell serves the calls that are within its boundaries (see Figure 1a). The total system bandwidth is divided into channels, with each channel ce",Reinforcement Learning,3948ead63a9f2944218de038d8934305-Paper.pdf,1996
Neural Learning in Structured,"The parameter space of neural networks has a Riemannian met ric structure. The natural Riemannian gradient should be used instead of the conventional gradient, since the former denotes the true steepest descent direction of a loss function in the Riemannian space. The behavior of the stochastic gradient learning algorithm is much more effective if the natural gradient is used. The present paper studies the information-geometrical structure of perceptrons and other networks, and prove that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm. Adaptive modification of the learning constant is proposed and analyzed in terms of the Riemannian mea sure and is shown to be efficient. The natural gradient is finally applied to blind separation of mixtured independent signal sources. 1 Introd uction Neural learning takes place in the parameter space of modifiable synaptic weights of a neural network. The role of each parameter i",Optimization & Theoretical ML,39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf,1996
Extraction of temporal features in the,"The encoding of random time-varying stimuli in single spike trains of electrosensory neurons in the weakly electric fish Eigenmannia was investigated using methods of statistical signal processing. At the first stage of the electrosensory system, spike trains were found to encode faithfully the detailed time-course of random stimuli, while at the second stage neurons responded specifically to features in the temporal waveform of the stimulus. Therefore stimulus infor mation is processed at the second stage of the electrosensory system by extracting temporal features from the faithfully preserved image of the environment sampled at the first stage. 1 INTRODUCTION The weakly electric fish, Eigenmannia, generates a quasi sinusoidal, dipole-like elec tric field at individually fixed frequencies (250 - 600 Hz) by discharging an electric organ located in its tail (see Bullock and Heilgenberg, 1986 for reviews). The fish sense local changes in the electric field by means of two types of tuber",Optimization & Theoretical ML,3a029f04d76d32e79367c4b3255dda4d-Paper.pdf,1996
A Model of Recurrent Interactions in,"A general feature of the cerebral cortex is its massive intercon nectivity - it has been estimated anatomically [19] that cortical neurons receive upwards of 5,000 synapses, the majority of which originate from other nearby cortical neurons. Numerous experi ments in primary visual cortex (VI) have revealed strongly nonlin ear interactions between stimulus elements which activate classical and non-classical receptive field regions. Recurrent cortical con nections likely contribute substantially to these effects. However, most theories of visual processing have either assumed a feedfor ward processing scheme [7], or have used recurrent interactions to account for isolated effects only [1, 16, 18]. Since nonlinear sys tems cannot in general be taken apart and analyzed in pieces, it is not clear what one learns by building a recurrent model that only accounts for one, or very few phenomena. Here we develop a relatively simple model of recurrent interactions in VI, that re flects major anat",Deep Learning,3bf55bbad370a8fcad1d09b005e278c2-Paper.pdf,1996
On a Modification to the Mean Field EM,"A modification is described to the use of mean field approxima tions in the E step of EM algorithms for analysing data from latent structure models, as described by Ghahramani (1995), among oth ers. The modification involves second-order Taylor approximations to expectations computed in the E step. The potential benefits of the method are illustrated using very simple latent profile models. 1 Introduction Ghahramani (1995) advocated the use of mean field methods as a means to avoid the heavy computation involved in the E step of the EM algorithm used for estimating parameters within a certain latent structure model, and Ghahramani & Jordan (1995) used the same ideas in a more complex situation. Dunmur & Titterington (1996a) identified Ghahramani's model as a so-called latent profile model, they observed that Zhang (1992,1993) had used mean field methods for a similar purpose, and they showed, in a simulation study based on very simple examples, that the mean field version of the EM alg",Optimization & Theoretical ML,3f67fd97162d20e6fe27748b5b372509-Paper.pdf,1996
VLSI Implementation of Cortical Visual Motion,"Two dimensional image motion detection neural networks have been implemented using a general purpose analog neural computer. The neural circuits perform spatiotemporal feature extraction based on the cortical motion detection model of Adelson and Bergen. The neural computer provides the neurons, synapses and synaptic time-constants required to realize the model in VLSI hardware. Results show that visual motion estimation can be implemented with simple sum-and threshold neural hardware with temporal computational capabilities. The neural circuits compute general 20 visual motion in real-time. 1 INTRODUCTION Visual motion estimation is an area where spatiotemporal computation is of fundamental importance. Each distinct motion vector traces a unique locus in the space-time domain. Hence, the problem of visual motion estimation reduces to a feature extraction task, with each feature extractor tuned to a particular motion vector. Since neural networks are particularly efficient feature extr",Computer Vision,4079016d940210b4ae9ae7d41c4a2065-Paper.pdf,1996
Local Bandit Approximation,"In general, procedures for determining Bayes-optimal adaptive controls for Markov decision processes (MDP's) require a pro hibitive amount of computation-the optimal learning problem is intractable. This paper proposes an approximate approach in which bandit processes are used to model, in a certain ""local"" sense, a given MDP. Bandit processes constitute an important subclass of MDP's, and have optimal learning strategies (defined in terms of Gittins indices) that can be computed relatively efficiently. Thus, one scheme for achieving approximately-optimal learning for gen eral MDP's proceeds by taking actions suggested by strategies that are optimal with respect to local bandit models. 1 INTRODUCTION Watkins [1989] has defined optimal learning as:"" the process of collecting and using information during learning in an optimal manner, so that the learner makes the best possible decisions at all stages of learning: learning itself is regarded as a mUltistage decision process, and learning",Optimization & Theoretical ML,4122cb13c7a474c1976c9706ae36521d-Paper.pdf,1996
Learning Appearance Based Models:,"This paper describes a new technique for object recognition based on learning appearance models. The image is decomposed into local regions which are described by a new texture representation called ""Generalized Second Mo ments"" that are derived from the output of multiscale, multiorientation filter banks. Class-characteristic local texture features and their global composition is learned by a hierarchical mixture of experts architecture (Jordan & Jacobs). The technique is applied to a vehicle database consisting of 5 general car categories (Sedan, Van with back-doors, Van without back-doors, old Sedan, and Volkswagen Bug). This is a difficult problem with considerable in-class variation. The new technique has a 6.5% misclassification rate, compared to eigen-images which give 17.4% misclassification rate, and nearest neighbors which give 15.7% misclassification rate. 1 Introduction Until a few years ago neural network and other statistical learning techniques were not very popular in c",Computer Vision,43cca4b3de2097b9558efefd0ecc3588-Paper.pdf,1996
Interpreting images by propagating,"A central theme of computational vision research has been the re alization that reliable estimation of local scene properties requires propagating measurements across the image. Many authors have therefore suggested solving vision problems using architectures of locally connected units updating their activity in parallel. Unfor tunately, the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global mini mum. In this paper we show that an architecture in which Bayesian Be liefs about image properties are propagated between neighboring units yields convergence times which are several orders of magni tude faster than traditional methods and avoids local minima. In particular our architecture is non-iterative in the sense of Marr [5]: at every time step, the local estimates at a given location are op timal given the information which has already been propagated to that",Computer Vision,4476b929e30dd0c4e8bdbcc82c6ba23a-Paper.pdf,1996
Why did TD-Gammon Work?,"Although TD-Gammon is one of the major successes in machine learn ing, it has not led to similar impressive breakthroughs in temporal dif ference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neu ral network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro's program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself. 1 INTRODUCTION It took great chutzpah for Gerald Tesauro to start wasting computer cycles on temporal difference learning in the game of Backgammon (Tesauro, 1992). Letting a machine learn ing program play itself in the hopes of becoming an expert, indeed! After all, the dre",Reinforcement Learning,459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf,1996
NeuroScale: Novel Topographic Feature,"Dimension-reducing feature extraction neural network techniques which also preserve neighbourhood relationships in data have tra ditionally been the exclusive domain of Kohonen self organising maps. Recently, we introduced a novel dimension-reducing feature extraction process, which is also topographic, based upon a Radial Basis Function architecture. It has been observed that the gener alisation performance of the system is broadly insensitive to model order complexity and other smoothing factors such as the kernel widths, contrary to intuition derived from supervised neural net work models. In this paper we provide an effective demonstration of this property and give a theoretical justification for the apparent 'self-regularising' behaviour of the 'NEUROSCALE' architecture. 1 'NeuroScale': A Feed-forward Neural Network Topographic Transformation Recently an important class of topographic neural network based feature extraction approaches, which can be related to the traditional stati",Computer Vision,4671aeaf49c792689533b00664a5c3ef-Paper.pdf,1996
Time Series Prediction Using Mixtures of,"We consider the problem of prediction of stationary time series, using the architecture known as mixtures of experts (MEM). Here we suggest a mixture which blends several autoregressive models. This study focuses on some theoretical foundations of the predic tion problem in this context. More precisely, it is demonstrated that this model is a universal approximator, with respect to learn ing the unknown prediction function. This statement is strength ened as upper bounds on the mean squared error are established. Based on these results it is possible to compare the MEM to other families of models (e.g., neural networks and state dependent mod els). It is shown that a degenerate version of the MEM is in fact equivalent to a neural network, and the number of experts in the architecture plays a similar role to the number of hidden units in the latter model. 310 A. 1. Zeevi, R. Meir and R. 1. Adler 1 Introduction In this work we pursue a new family of models for time series, substantially ",Optimization & Theoretical ML,491442df5f88c6aa018e86dac21d3606-Paper.pdf,1996
MIMIC: Finding Optima by Estimating,"In many optimization problems, the structure of solutions reflects complex relationships between the different input parameters. For example, experience may tell us that certain parameters are closely related and should not be explored independently. Similarly, ex perience may establish that a subset of parameters must take on particular values. Any search of the cost landscape should take advantage of these relationships. We present MIMIC, a framework in which we analyze the global structure of the optimization land scape. A novel and efficient algorithm for the estimation of this structure is derived. We use knowledge of this structure to guide a randomized search through the solution space and, in turn, to re fine our estimate ofthe structure. Our technique obtains significant speed gains over other randomized optimization procedures. 1 Introduction Given some cost function C(x) with local minima, we may search for the optimal x in many ways. Variations of gradient descent are perha",Optimization & Theoretical ML,4c22bd444899d3b6047a10b20a2f26db-Paper.pdf,1996
Neural network models of chemotaxis In,"We train recurrent networks to control chemotaxis in a computer model of the nematode C. elegans. The model presented is based closely on the body mechanics, behavioral analyses, neuroanatomy and neurophysiology of C. elegans, each imposing constraints rel evant for information processing. Simulated worms moving au tonomously in simulated chemical environments display a variety of chemotaxis strategies similar to those of biological worms. 1 INTRODUCTION The nematode C. elegans provides a unique opportunity to study the neuronal ba sis of neural computation in an animal capable of complex goal-oriented behaviors. The adult hermaphrodite is only 1 mm long, and has exactly 302 neurons and 95 muscle cells. The morphology of every cell and the location of most electrical and chemical synapses are known precisely (White et al., 1986), making C. elegans espe cially attractive for study. Whole-cell recordings are now being made on identified neurons in the nerve ring of C. elegans to determin",NLP,4d2e7bd33c475784381a64e43e50922f-Paper.pdf,1996
GTM: A Principled Alternative,"The Self-Organizing Map (SOM) algorithm has been extensively studied and has been applied with considerable success to a wide variety of problems. However, the algorithm is derived from heuris tic ideas and this leads to a number of significant limitations. In this paper, we consider the problem of modelling the probabil ity density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. We introduce a novel form of latent variable model, which we call the GTM algo rithm (for Generative Topographic Mapping), which allows general non-linear transformations from latent space to data space, and which is trained using the EM (expectation-maximization) algo rithm. Our approach overcomes the limitations of the SOM, while introducing no significant disadvantages. We demonstrate the per formance of the GTM algorithm on simulated data from flow diag nostics for a multi-phase oil pipeline. 1 Introduction The Self-Organizing Map (SOM) algorithm of Ko",NLP,4e4e53aa080247bc31d0eb4e7aeb07a0-Paper.pdf,1996
Support Vector Method for Function,"The Support Vector (SV) method was recently proposed for es timating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presenta tion we report results of applying the SV method to these problems. 1 Introduction The Support Vector method is a universal tool for solving multidimensional function estimation problems. Initially it was designed to solve pattern recognition problems, where in order to find a decision rule with good generalization ability one selects some (small) subset of the training data, called the Support Vectors (SVs). Optimal separation of the SV s is equivalent to optimal separation the entire data. This led to a new method of representing decision functions where the decision functions are a linear expansion on a basis whose elements are nonlinear functions parameterized by the SVs (we need one SV for each element of the basis). This type of function representation is especially useful for high dimension",Optimization & Theoretical ML,4f284803bd0966cc24fa8683a34afc6e-Paper.pdf,1996
Smoothing Regularizers for,"Smoothing regularizers for radial basis functions have been studied extensively, but no general smoothing regularizers for projective basis junctions (PBFs), such as the widely-used sigmoidal PBFs, have heretofore been proposed. We de rive new classes of algebraically-simple mH'-order smoothing regularizers for networks of the form f(W, x) = L7=1 Ujg [xT Vj + Vjol + uo, with general projective basis functions g[.]. These regularizers are: N Ra(W,m) = LU;lIvjIl2m-1 GlobalForm j=1 N RdW,m) = LU;lIvjll2m Local Form j=1 t"" These regularizers bound the corresponding m -order smoothing integral where W denotes all the network weights {Uj, uo, vj, vo}, and O(x) is a weight ing function on the D-dimensional input space. The global and local cases are distinguished by different choices of O( x). The simple algebraic forms R(W, m) enable the direct enforcement of smooth ness without the need for costly Monte-Carlo integrations of S (W, m). The new regularizers are shown to yield better generaliz",Optimization & Theoretical ML,50905d7b2216bfeccb5b41016357176b-Paper.pdf,1996
"Bangs. Clicks, Snaps, Thuds and Whacks:","We propose a neuromorphic architecture for real-time processing of acoustic transients in analog VLSI. We show how judicious normalization of a time-frequency signal allows an elegant and robust implementation of a correlation algorithm. The algorithm uses binary multiplexing instead of analog-analog multiplication. This removes the need for analog storage and analog-multiplication. Simulations show that the resulting algorithm has the same out-of-sample classification performance (-93% correct) as a baseline template-matching algorithm. 1 INTRODUCTION We report progress towards our long-term goal of developing low-cost, low-power, low complexity analog-VLSI processors for real-time applications. We propose a neuromorphic architecture for acoustic processing in analog VLSI. The characteristics of the architecture are explored by using simulations and real-world acoustic transients. We use acoustic transients in our experiments because information in the form of acoustic transients perv",Optimization & Theoretical ML,52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf,1996
Salient Contour Extraction by Temporal Binding,"It has been suggested that long-range intrinsic connections in striate cortex may play a role in contour extraction (Gilbert et aI., 1996). A number of recent physiological and psychophysical studies have examined the possible role of long range connections in the modulation of contrast detection thresholds (Polat and Sagi, 1993,1994; Kapadia et aI., 1995; Kovacs and Julesz, 1994) and various pre-attentive detection tasks (Kovacs and Julesz, 1993; Field et aI., 1993). We have developed a network architecture based on the anatomical connectivity of striate cortex, as well as the temporal dynamics of neuronal processing, that is able to reproduce the observed experimental results. The network has been tested on real images and has applications in terms of identifying salient contours in automatic image processing systems. 1 INTRODUCTION Vision is an active process, and one of the earliest, preattentive actions in visual processing is the identification of the salient contours in a scene.",Computer Vision,535ab76633d94208236a2e829ea6d888-Paper.pdf,1996
Learning Decision Theoretic Utilities Through,"Probability models can be used to predict outcomes and compensate for missing data, but even a perfect model cannot be used to make decisions unless the utility of the outcomes, or preferences between them, are also provided. This arises in many real-world problems, such as medical di agnosis, where the cost of the test as well as the expected improvement in the outcome must be considered. Relatively little work has been done on learning the utilities of outcomes for optimal decision making. In this paper, we show how temporal-difference reinforcement learning (TO(A» can be used to determine decision theoretic utilities within the context of a mixture model and apply this new approach to a problem in medical di agnosis. TO( A) learning of utilities reduces the number of tests that have to be done to achieve the same level of performance compared with the probability model alone, which results in significant cost savings and in creased efficiency. 1 INTRODUCTION Decision theory is norma",Reinforcement Learning,5680522b8e2bb01943234bce7bf84534-Paper.pdf,1996
Spatial Decorrelation in Orientation,"In this paper we propose a model for the lateral connectivity of orientation-selective cells in the visual cortex based on information theoretic considerations. We study the properties of the input sig nal to the visual cortex and find new statistical structures which have not been processed in the retino-geniculate pathway. Applying the idea that the system optimizes the representation of incoming signals, we derive the lateral connectivity that will achieve this for a set of local orientation-selective patches, as well as the complete spatial structure of a layer of such patches. We compare the results with various physiological measurements. 1 Introduction In recent years much work has been done on how the structure of the visual sys tem reflects properties of the visual environment (Atick and Redlich 1992; Attneave 1954; Barlow 1989). Based on the statistics of natural scenes compiled and studied by Field (1987) and Ruderman and Bialek (1993), work was done by Atick and Redlich (19",Computer Vision,5caf41d62364d5b41a893adc1a9dd5d4-Paper.pdf,1996
Sequential Tracking in Pricing Financial,"This paper shows how the prices of option contracts traded in finan cial markets can be tracked sequentially by means of the Extended Kalman Filter algorithm. I consider call and put option pairs with identical strike price and time of maturity as a two output nonlin ear system. The Black-Scholes approach popular in Finance liter ature and the Radial Basis Functions neural network are used in modelling the nonlinear system generating these observations. I show how both these systems may be identified recursively using the EKF algorithm. I present results of simulations on some FTSE 100 Index options data and discuss the implications of viewing the pricing problem in this sequential manner. 1 INTRODUCTION Data from the financial markets has recently been of much interest to the neural computing community. The complexity of the underlying macro-economic system and how traders react to the flow of information leads to highly nonlinear rela tionships between observations. Further, the unde",Deep Learning,5eac43aceba42c8757b54003a58277b5-Paper.pdf,1996
Are Hopfield Networks Faster Than,"It is shown that conventional computers can be exponentiallx faster than planar Hopfield networks: although there are planar Hopfield networks that take exponential time to converge, a stable state of an arbitrary planar Hopfield network can be found by a conventional computer in polynomial time. The theory of 'P.cS-completeness gives strong evidence that such a separation is unlikely for nonpla nar Hopfield networks, and it is demonstrated that this is also the case for several restricted classes of nonplanar Hopfield networks, including those who interconnection graphs are the class of bipar tite graphs, graphs of degree 3, the dual of the knight's graph, the 8-neighbor mesh, the hypercube, the butterfly, the cube-connected cycles, and the shuffle-exchange graph. 1 Introduction Are Hopfield networks faster than conventional computers? This apparently straightforward question is complicated by the fact that conventional computers are universal computational devices, that is, they are ",Optimization & Theoretical ML,68a83eeb494a308fe5295da69428a507-Paper.pdf,1996
Learning From Demonstration,"By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely at tempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based rein forcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities o",Reinforcement Learning,68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf,1996
Clustering Sequences with Hidden,"This paper discusses a probabilistic model-based approach to clus tering sequences, using hidden Markov models (HMMs). The prob lem can be framed as a generalization of the standard mixture model approach to clustering in feature space. Two primary issues are addressed. First, a novel parameter initialization procedure is proposed, and second, the more difficult problem of determining the number of clusters K, from the data, is investigated. Experi mental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences. 1 Introduction = Consider a data set D consisting of N sequences, D = {SI,"""" SN}' Si (.f.L ... .f.~J is a sequence of length Li composed of potentially multivariate fea ture vectors.f.. The problem addressed in this paper is the discovery from data of a natural grouping of the sequences into K clusters. This is analagous to clustering in multivariate feature space which is normally handled by methods such as k-mea",Optimization & Theoretical ML,6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf,1996
Promoting Poor Features to Supervisors:,"In supervised learning there is usually a clear distinction between inputs and outputs - inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can. learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing. 1 Introduction The goal in supervised learning is to learn functions that map inputs to outputs with high predictive accuracy. The standard practice in neural nets is to use all features that",Optimization & Theoretical ML,6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf,1996
Representing Face Images for Emotion,"We compare the generalization performance of three distinct rep resentation schemes for facial emotions using a single classification strategy (neural network). The face images presented to the clas sifiers are represented as: full face projections of the dataset onto their eigenvectors (eigenfaces); a similar projection constrained to eye and mouth areas (eigenfeatures); and finally a projection of the eye and mouth areas onto the eigenvectors obtained from 32x32 random image patches from the dataset. The latter system achieves 86% generalization on novel face images (individuals the networks were not trained on) drawn from a database in which human sub jects consistently identify a single emotion for the face. 1 Introduction Some of the most successful research in machine perception of complex natural image objects (like faces), has relied heavily on reduction strategies that encode an object as a set of values that span the principal component sub-space of the object's images [Cottr",Computer Vision,6eb6e75fddec0218351dc5c0c8464104-Paper.pdf,1996
Separating Style and Content,"We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor struc ture. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMsj, which generalizes earlier work on factorial mixture models [7, 3]. Significant per formance improvement on a benchmark speech dataset shows the benefits of our approach. 1 Introduction In many pattern analysis or synthesis tasks, the observed data are generated from the interaction of two underlying factors which we will generically call ""style"" and ""content."" For example, in a character recognition task, w",Computer Vision,70222949cc0db89ab32c9969754d4758-Paper.pdf,1996
Unknown Title,No abstract found,"Based on the information provided, it is difficult to",8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf,1994
Unknown Title,No abstract found,"Based on the information provided, it is challenging to",8b4066554730ddfaa0266346bdc1b202-Paper.pdf,1995
Contour Organisation with the EM,"This paper describes how the early visual process of contour organ isation can be realised using the EM algorithm. The underlying computational representation is based on fine spline coverings. Ac cording to our EM approach the adjustment of spline parameters draws on an iterative weighted least-squares fitting process. The expectation step of our EM procedure computes the likelihood of the data using a mixture model defined over the set of spline cover ings. These splines are limited in their spatial extent using Gaus sian windowing functions. The maximisation of the likelihood leads to a set of linear equations in the spline parameters which solve the weighted least squares problem. We evaluate the technique on the localisation of road structures in aerial infra-red images. 1 Introduction Dempster, Laird and Rubin's EM (expectation and maximisation) [1] algorithm was originally introduced as a means of finding maximum likelihood solutions to prob lems posed in terms of incomplete dat",Computer Vision,71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf,1996
Hidden Markov decision trees,"We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales. 1 Introduction Decision trees are regression or classification models that are based on a nested decomposition of the input space. An input vector x is classified recursively by a set of ""decisions"" at the nonterminal nodes of a tree, resulting in the choice of a terminal node at which an output y is generated. A statistical approach to d",Optimization & Theoretical ML,6c8dba7d0df1c4a79dd07646be9a26c8-Paper.pdf,1996
Combining Neural Network Regression,"When combining a set of learned models to form an improved es timator, the issue of redundancy or multicollinearity in the set of models must be addressed. A progression of existing approaches and their limitations with respect to the redundancy is discussed. A new approach, PCR* , based on principal components regres sion is proposed to address these limitations. An evaluation of the new approach on a collection of domains reveals that: 1) PCR* was the most robust combination method as the redundancy of the learned models increased, 2) redundancy could be handled without eliminating any of the learned models, and 3) the principal compo nents of the learned models provided a continuum of ""regularized"" weights from which PCR* could choose. 1 INTRODUCTION Combining a set of learned modelsl to improve classification and regres sion estimates has been an area of much research in machine learning and neural networks [Wolpert, 1992, Merz, 1995, Perrone and Cooper, 1992, Leblanc and Tibshiran",Optimization & Theoretical ML,7501e5d4da87ac39d782741cd794002d-Paper.pdf,1996
Triangulation by Continuous Embedding,"When triangulating a belief network we aim to obtain a junction tree of minimum state space. According to (Rose, 1970), searching for the optimal triangulation can be cast as a search over all the permutations of the graph's vertices. Our approach is to embed the discrete set of permutations in a convex continuous domain D. By suitably extending the cost function over D and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost. This paper presents two ways of embedding the triangulation problem into continuous domain and shows that they perform well compared to the best known heuristic. 1 INTRODUCTION. WHAT IS TRIANGULATION? Belief networks are graphical representations of probability distributions over a set of variables. In what follows it will be always assumed that the variables take values in a finite set and that they correspond to the vertices of a graph. The graph's arcs will represent the dependencies am",Optimization & Theoretical ML,76cf99d3614e23eabab16fb27e944bf9-Paper.pdf,1996
Bayesian Model Comparison,"The techniques of Bayesian inference have been applied with great success to many problems in neural computing including evaluation of regression functions, determination of error bars on predictions, and the treatment of hyper-parameters. However, the problem of model comparison is a much more challenging one for which current techniques have significant limitations. In this paper we show how an extended form of Markov chain Monte Carlo, called chaining, is able to provide effective estimates of the relative probabilities of different models. We present results from the robot arm problem and compare them with the corresponding results obtained using the standard Gaussian approximation framework. 1 Bayesian Model Comparison In a Bayesian treatment of statistical inference, our state of knowledge of the values of the parameters w in a model M is described in terms of a probability distribution function. Initially this is chosen to be some prior distribution p(wIM), which can be combined",Optimization & Theoretical ML,7810ccd41bf26faaa2c4e1f20db70a71-Paper.pdf,1996
Practical confidence and prediction,"We propose a new method to compute prediction intervals. Espe cially for small data sets the width of a prediction interval does not only depend on the variance of the target distribution, but also on the accuracy of our estimator of the mean of the target, i.e., on the width of the confidence interval. The confidence interval follows from the variation in an ensemble of neural networks, each of them trained and stopped on bootstrap replicates of the original data set. A second improvement is the use of the residuals on validation pat terns instead of on training patterns for estimation of the variance of the target distribution. As illustrated on a synthetic example, our method is better than existing methods with regard to extrap olation and interpolation in data regimes with a limited amount of data, and yields prediction intervals which actual confidence levels are closer to the desired confidence levels. 1 STATISTICAL INTERVALS In this paper we will consider feedforward neural net",Optimization & Theoretical ML,7940ab47468396569a906f75ff3f20ef-Paper.pdf,1996
Neural Models for Part-Whole Hierarchies,"We present a connectionist method for representing images that ex plicitly addresses their hierarchical nature. It blends data from neu roscience about whole-object viewpoint sensitive cells in inferotem poral cortex8 and attentional basis-field modulation in V43 with ideas about hierarchical descriptions based on microfeatures.5,11 The resulting model makes critical use of bottom-up and top-down pathways for analysis and synthesis.6 We illustrate the model with a simple example of representing information about faces. 1 Hierarchical Models Images of objects constitute an important paradigm case of a representational hi erarchy, in which 'wholes', such as faces, consist of 'parts', such as eyes, noses and mouths. The representation and manipulation of part-whole hierarchical informa tion in fixed hardware is a heavy millstone around connectionist necks, and has consequently been the inspiration for many interesting proposals, such as Pollack's RAAM.l1 We turned to the primate visual sy",Computer Vision,7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf,1996
"Consistent Classification, Firm and Soft","A classifier is called consistent with respect to a given set of class labeled points if it correctly classifies the set. We consider classi fiers defined by unions of local separators and propose algorithms for consistent classifier reduction. The expected complexities of the proposed algorithms are derived along with the expected classifier sizes. In particular, the proposed approach yields a consistent re duction of the nearest neighbor classifier, which performs ""firm"" classification, assigning each new object to a class, regardless of the data structure. The proposed reduction method suggests a notion of ""soft"" classification, allowing for indecision with respect to objects which are insufficiently or ambiguously supported by the data. The performances of the proposed classifiers in predict ing stock behavior are compared to that achieved by the nearest neighbor method. 1 Introduction Certain classification problems, such as recognizing the digits of a hand written zip code, requi",Optimization & Theoretical ML,7bb060764a818184ebb1cc0d43d382aa-Paper.pdf,1996
Bayesian Unsupervised Learning of,"Multilayer architectures such as those used in Bayesian belief net works and Helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs. Because exact probability calculations with these mod els are often intractable, there is much interest in finding approxi mate algorithms. We present an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling. The model can be interpreted as a stochastic recurrent network in which ambi guity in lower-level states is resolved through feedback from higher levels. We demonstrate the performance of the algorithm on bench mark problems. 1 Introduction Discovering high order structure in patterns is one of the keys to performing complex recognition and discrimination tasks. Many real world patterns have a hierarchical underlying structure in which simple features have a higher order structure among themselves. Because these relationships are often statistical",Computer Vision,7c82fab8c8f89124e2ce92984e04fb40-Paper.pdf,1996
An Architectural Mechanism for,"A linear architectural model of cortical simple cells is presented. The model evidences how mutual inhibition, occurring through synaptic coupling functions asymmetrically distributed in space, can be a possible basis for a wide variety of spatio-temporal simple cell response properties, including direction selectivity and velocity tuning. While spatial asymmetries are included explicitly in the structure of the inhibitory interconnections, temporal asymmetries originate from the specific mutual inhibition scheme considered. Extensive simulations supporting the model are reported. 1 INTRODUCTION One of the most distinctive features of striate cortex neurons is their combined selectivity for stimulus orientation and the direction of motion. The majority of simple cells, indeed, responds better to sinusoidal gratings that are moving in one direction than to the opposite one, exhibiting also a narrower velocity tuning with respect to that of geniculate cells. Recent theoretical and neurop",Computer Vision,7d771e0e8f3633ab54856925ecdefc5d-Paper.pdf,1996
Complex-Cell Responses Derived from,"Biophysical modeling studies have previously shown that cortical pyramidal cells driven by strong NMDA-type synaptic currents and/or containing dendritic voltage-dependent Ca++ or Na+ chan nels, respond more strongly when synapses are activated in several spatially clustered groups of optimal size-in comparison to the same number of synapses activated diffusely about the dendritic arbor [8]- The nonlinear intradendritic interactions giving rise to this ""cluster sensitivity"" property are akin to a layer of virtual non linear ""hidden units"" in the dendrites, with implications for the cel lular basis of learning and memory [7, 6], and for certain classes of nonlinear sensory processing [8]- In the present study, we show that a single neuron, with access only to excitatory inputs from unori ented ON- and OFF-center cells in the LGN, exhibits the principal nonlinear response properties of a ""complex"" cell in primary visual cortex, namely orientation tuning coupled with translation invari an",Computer Vision,7e7e69ea3384874304911625ac34321c-Paper.pdf,1996
Training Algorithms for Hidden Markov Models,"We present new algorithms for parameter estimation of HMMs. By adapting a framework used for supervised learning, we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay ""close"" to the current estimated parameters. We use a bound on the relative entropy between the two HMMs as a distance mea sure between them. The result is new iterative training algorithms which are similar to the EM (Baum-Welch) algorithm for training HMMs. The proposed algorithms are composed of a step similar to the expectation step of Baum-Welch and a new update of the parameters which replaces the maximization (re-estimation) step. The algorithm takes only negligi bly more time per iteration and an approximated version uses the same expectation step as Baum-Welch. We evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data. For sparse models, i.e. models with relatively small number of non-zero parameters, the proposed a",Optimization & Theoretical ML,7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf,1996
A Constructive Learning Algorithm for,"To reduce the computational complexity of classification systems using tangent distance, Hastie et al. (HSS) developed an algo rithm to devise rich models for representing large subsets of the data which computes automatically the ""best"" associated tan gent subspace. Schwenk & Milgram proposed a discriminant mod ular classification system (Diabolo) based on several autoassociative multilayer perceptrons which use tangent distance as error recon struction measure. We propose a gradient based constructive learning algorithm for building a tangent subspace model with discriminant capabilities which combines several of the the advantages of both HSS and Diabolo: devised tangent models hold discriminant capabilities, space requirements are improved with respect to HSS since our algorithm is discriminant and thus it needs fewer prototype models, dimension of the tangent subspace is determined automatically by the constructive algorithm, and our algorithm is able to learn new transformations.",Computer Vision,81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf,1996
Effective Training of a Neural Network,"We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton MessagePad. We present some innovations in the training and use of ANNs al; character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and stroke warping. A recurring theme of reducing a priori biases emerges and is discussed. 1 INTRODUCTION We have been conducting research on bottom-up classification techniques ba<;ed on trainable artificial neural networks (ANNs), in combination with comprehensive but weakly-applied language models. To focus our work on a subproblem that is tractable enough to le.:'ld to usable products in a reasonable time, we have restricted the domain to hand-printing, so that strokes are clearly delineated by pen ",Computer Vision,81e5f81db77c596492e6f1a5a792ed53-Paper.pdf,1996
Second-order Learning Algorithm with,"This paper compares three penalty terms with respect to the effi ciency of supervised learning, by using first- and second-order learn ing algorithms. Our experiments showed that for a reasonably ade quate penalty factor, the combination of the squared penalty term and the second-order learning algorithm drastically improves the convergence performance more than 20 times over the other com binations, at the same time bringing about a better generalization performance. 1 INTRODUCTION It has been found empirically that adding some penalty term to an objective func tion in the learning of neural networks can lead to significant improvements in network generalization. Such terms have been proposed on the basis of several viewpoints such as weight-decay (Hinton, 1987), regularization (Poggio & Girosi, 1990), function-smoothing (Bishop, 1995), weight-pruning (Hanson & Pratt, 1989; Ishikawa, 1990), and Bayesian priors (MacKay, 1992; Williams, 1995). Some are calculated by using simple arithme",Optimization & Theoretical ML,82c2559140b95ccda9c6ca4a8b981f1e-Paper.pdf,1996
Multi-effect Decompositions,"High frequency foreign exchange data can be decomposed into three components: the inventory effect component, the surprise infonnation (news) component and the regular infonnation component. The presence of the inventory effect and news can make analysis of trends due to the diffusion of infonnation (regular information component) difficult. We propose a neural-net-based, independent component analysis to sep arate high frequency foreign exchange data into these three components. Our empirical results show that our proposed multi-effect decomposition can reveal the intrinsic price behavior. 1 Introduction Tick-by-tick, high frequency foreign exchange rates are extremely noisy and volatile, but they are not simply pure random walks (Moody & Wu 1996). The price movements are characterized by a number of ""stylized facts"" ,including the following two properties: (1) short tenn, weak oscillations on a time scale of several ticks and (2) erratic occurrence of turbulence lasting from minutes ",Optimization & Theoretical ML,838e8afb1ca34354ac209f53d90c3a43-Paper.pdf,1996
Dynamic features for visual speech,"Humans use visual as well as auditory speech signals to recognize spoken words. A variety of systems have been investigated for per forming this task. The main purpose of this research was to sys tematically compare the performance of a range of dynamic visual features on a speechreading task. We have found that normal ization of images to eliminate variation due to translation, scale, and planar rotation yielded substantial improvements in general ization performance regardless of the visual representation used. In addition, the dynamic information in the difference between suc cessive frames yielded better performance than optical-flow based approaches, and compression by local low-pass filtering worked sur prisingly better than global principal components analysis (PCA). These results are examined and possible explanations are explored. 1 INTRODUCTION Visual speech recognition is a challenging task in sensory integration. Psychophys ical work by McGurk and MacDonald [5] first showed",Computer Vision,84438b7aae55a0638073ef798e50b4ef-Paper.pdf,1996
Microscopic Equations in Rough Energy,"We consider the microscopic equations for learning problems in neural networks. The aligning fields of an example are obtained from the cavity fields, which are the fields if that example were absent in the learning process. In a rough energy landscape, we assume that the density of the local minima obey an exponential distribution, yielding macroscopic properties agreeing with the first step replica symmetry breaking solution. Iterating the microscopic equations provide a learning algorithm, which results in a higher stability than conventional algorithms. 1 INTRODUCTION Most neural networks learn iteratively by gradient descent. As a result, closed ex pressions for the final network state after learning are rarely known. This precludes further analysis of their properties, and insights into the design of learning algo rithms. To complicate the situation, metastable states (i.e. local minima) are often present in the energy landscape of the learning space so that, depending on the ini",Optimization & Theoretical ML,83f97f4825290be4cb794ec6a234595f-Paper.pdf,1996
The effect of correlated input data on the,"The convergence properties of the gradient descent algorithm in the case of the linear perceptron may be obtained from the response function. We derive a general expression for the response function and apply it to the case of data with simple input correlations. It is found that correlations severely may slow down learning. This explains the success of PCA as a method for reducing training time. Motivated by this finding we furthermore propose to transform the input data by removing the mean across input variables as well as examples to decrease correlations. Numerical findings for a medical classification problem are in fine agreement with the theoretical results. 1 INTRODUCTION Learning and generalization are important areas of research within the field of neu ral networks. Although good generalization is the ultimate goal in feed-forward networks (perceptrons), it is of practical importance to understand the mechanism which control the amount of time required for learning, i. e. th",Optimization & Theoretical ML,884ce4bb65d328ecb03c598409e2b168-Paper.pdf,1996
Learning temporally persistent,"A biologically motivated model of cortical self-organization is pro posed. Context is combined with bottom-up information via a maximum likelihood cost function. Clusters of one or more units are modulated by a common contextual gating Signal; they thereby organize themselves into mutually supportive predictors of abstract contextual features. The model was tested in its ability to discover viewpoint-invariant classes on a set of real image sequences of cen tered, gradually rotating faces. It performed considerably better than supervised back-propagation at generalizing to novel views from a small number of training examples. 1 THE ROLE OF CONTEXT The importance of context effectsl in perception has been demonstrated in many domains. For example, letters are recognized more quickly and accurately in the context of words (see e.g. McClelland & Rumelhart, 1981), words are recognized more efficiently when preceded by related words (see e.g. Neely, 1991), individual speech utterances are m",Computer Vision,8e2cfdc275761edc592f73a076197c33-Paper.pdf,1996
Exploiting Model Uncertainty Estimates,"Model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems. The simplest method assumes the learned model is correct and applies dynamic programming to it, but many approximators provide uncertainty estimates on the fit. How can they be exploited? This paper addresses the case where the system must be prevented from having catastrophic failures dur ing learning. We propose a new algorithm adapted from the dual control literature and use Bayesian locally weighted regression models with dy namic programming. A common reinforcement learning assumption is that aggressive exploration should be encouraged. This paper addresses the con verse case in which the system has to reign in exploration. The algorithm is illustrated on a 4 dimensional simulated control problem. 1 Introduction Reinforcement learning and related grid-based dynamic programming techniques are increasingly being applied to dynamic systems with con",Reinforcement Learning,93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf,1996
Spectroscopic Detection of Cervical,"The mortality related to cervical cancer can be substantially re duced through early detection and treatment. However, cur rent detection techniques, such as Pap smear and colposcopy, fail to achieve a concurrently high sensitivity and specificity. In vivo fluorescence spectroscopy is a technique which quickly, non invasively and quantitatively probes the biochemical and morpho logical changes that occur in pre-cancerous tissue. RBF ensemble algorithms based on such spectra provide automated, and near real time implementation of pre-cancer detection in the hands of non experts. The results are more reliable, direct and accurate than those achieved by either human experts or multivariate statistical algorithms. 1 Introduction Cervical carcinoma is the second most common cancer in women worldwide, ex ceeded only by breast cancer (Ramanujam et al., 1996). The mortality related to cervical cancer can be reduced if this disease is detected at the pre-cancerous state, known as squamous intra",Computer Vision,944bdd9636749a0801c39b6e449dbedc-Paper.pdf,1996
Basis Function Networks and Complexity,"In this paper we apply the method of complexity regularization to de rive estimation bounds for nonlinearfunction estimation using a single hidden layer radial basis function network. Our approach differs from thepreviouscomplexityregularizationneural networkfunction learning schemes inthatweoperatewithrandomcoveringnumbers and1 metric 1 entropy, makingitpo~sibleto considermuchbroaderfamilies ofactiva tionfunctions, namely functionsofboundedvariation. Someconstraints previously imposed on the network parameters are also eliminated this way. The networkis trained by means ofcomplexity regularization in volving empirical risk minimization. Bounds on the expected risk in tenns ofthe samplesizeare obtainedfor alargeclass ofloss functions. Rates ofconvergencetotheoptimallossarealsoderived. 1 INTRODUCTION Artificialneuralnetworkshavebeenfoundeffectiveinlearninginput-outputmappingsfrom noisyexamples. Inthislearningproblemanunknowntargetfunctionistobeinferredfroma setofindependentobservationsd",Optimization & Theoretical ML,97af4fb322bb5c8973ade16764156bed-Paper.pdf,1996
Adaptively Growing Hierarchical,"We propose a novel approach to automatically growing and pruning Hierarchical Mixtures of Experts. The constructive algorithm pro posed here enables large hierarchies consisting of several hundred experts to be trained effectively. We show that HME's trained by our automatic growing procedure yield better generalization per formance than traditional static and balanced hierarchies. Eval uation of the algorithm is performed (1) on vowel classification and (2) within a hybrid version of the JANUS r9] speech recog nition system using a subset of the Switchboard large-vocabulary speaker-independent continuous speech recognition database. INTRODUCTION The Hierarchical Mixtures of Experts (HME) architecture [2,3,4] has proven use ful for classification and regression tasks in small to medium sized applications with convergence times several orders of magnitude lower than comparable neu ral networks such as the multi-layer perceptron. The HME is best understood as a probabilistic decision tre",Optimization & Theoretical ML,995665640dc319973d3173a74a03860c-Paper.pdf,1996
On-line Policy Improvement using,"We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo sim ulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP! and SP2 parallel-RISC supercomputers. We have obtained promising initial results in applying this algo rithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in whi",Optimization & Theoretical ML,996009f2374006606f4c0b0fda878af1-Paper.pdf,1996
Blind separation of delayed and convolved,"We address the difficult problem of separating multiple speakers with multiple microphones in a real room. We combine the work of Torkkola and Amari, Cichocki and Yang, to give Natural Gra dient information maximisation rules for recurrent (IIR) networks, blindly adjusting delays, separating and deconvolving mixed sig nals. While they work well on simulated data, these rules fail in real rooms which usually involve non-minimum phase transfer functions, not-invertible using stable IIR filters. An approach that sidesteps this problem is to perform infomax on a feedforward archi tecture in the frequency domain (Lambert 1996). We demonstrate real-room separation of two natural signals using this approach. 1 The problem. In the linear blind signal processing problem ([3, 2] and references therein), N signals, s(t) = [S1(t) ... SN(t)V, are transmitted through a medium so that an array of N sensors picks up a set of signals x(t) = [Xl (t) ... XN(t)V, each of which Blind Separation of Delayed ",Computer Vision,9996535e07258a7bbfd8b132435c5962-Paper.pdf,1996
A New Approach to Hybrid HMMJANN Speech,"This paper presents a new approach to speech recognition with hybrid HMM/ANN technology. While the standard approach to hybrid HMMIA NN systems is based on the use of neural networks as posterior probability estimators, the new approach is based on the use of mutual information neural networks trained with a special learning algorithm in order to maximize the mutual information between the input classes of the network and its resulting sequence of firing output neurons during training. It is shown in this paper that such a neural network is an optimal neural vector quantizer for a discrete hidden Markov model system trained on Maximum Likelihood principles. One of the main advantages of this approach is the fact, that such neural networks can be easily combined with HMM's of any complexity with context-dependent capabilities. It is shown that the resulting hybrid system achieves very high recognition rates, which are now already on the same level as the best conventional HMM systems wi",NLP,9a3d458322d70046f63dfd8b0153ece4-Paper.pdf,1996
Competition Among Networks,"The separation of generalization error into two types, bias and variance (Geman, Bienenstock, Doursat, 1992), leads to the notion of error reduction by averaging over a ""committee"" of classifiers (Perrone, 1993). Committee perfonnance decreases with both the average error of the constituent classifiers and increases with the degree to which the misclassifications are correlated across the committee. Here, a method for reducing correlations is introduced, that uses a winner-take-all procedure similar to competitive learning to drive the individual networks to different minima in weight space with respect to the training set, such that correlations in generalization perfonnance will be reduced, thereby reducing committee error. 1 INTRODUCTION The problem of constructing a predictor can generally be viewed as finding the right combination of bias and variance (Geman, Bienenstock, Doursat, 1992) to reduce the expected error. Since a neural network predictor inherently has an excessive numb",Optimization & Theoretical ML,9adeb82fffb5444e81fa0ce8ad8afe7a-Paper.pdf,1996
Selective Integration: A Model for,"Local disparity information is often sparse and noisy, which creates two conflicting demands when estimating disparity in an image re gion: the need to spatially average to get an accurate estimate, and the problem of not averaging over discontinuities. We have devel oped a network model of disparity estimation based on disparity selective neurons, such as those found in the early stages of process ing in visual cortex. The model can accurately estimate multiple disparities in a region, which may be caused by transparency or oc clusion, in real images and random-dot stereograms. The use of a selection mechanism to selectively integrate reliable local disparity estimates results in superior performance compared to standard back-propagation and cross-correlation approaches. In addition, the representations learned with this selection mechanism are con sistent with recent neurophysiological results of von der Heydt, Zhou, Friedman, and Poggio [8] for cells in cortical visual area V2. Comb",Computer Vision,a01610228fe998f515a72dd730294d87-Paper.pdf,1996
The Neurothermostat:,"The Neurothermostat is an adaptive controller that regulates in door air temperature in a residence by switching a furnace on or off. The task is framed as an optimal control problem in which both comfort and energy costs are considered as part of the con trol objective. Because the consequences of control decisions are delayed in time, the Ne urothermostat must anticipate heating de mands with predictive models of occupancy patterns and the ther mal response of the house and furnace. Occupancy pattern predic tion is achieved by a hybrid neural net / look-up table. The Neu rothermostat searches, at each discrete time step, for a decision sequence that minimizes the expected cost over a fixed planning horizon. The first decision in this sequence is taken, and this pro cess repeats. Simulations of the Neurothermostat were conducted using artificial occupancy data in which regularity was systemat ically varied, as well as occupancy data from an actual residence. The Neurothermostat is com",Optimization & Theoretical ML,a0833c8a1817526ac555f8d67727caf6-Paper.pdf,1996
Softening Discrete Relaxation,"This paper describes a new framework for relational graph match ing. The starting point is a recently reported Bayesian consistency measure which gauges structural differences using Hamming dis tance. The main contributions of the work are threefold. Firstly, we demonstrate how the discrete components of the cost func tion can be softened. The second contribution is to show how the softened cost function can be used to locate matches using continuous non-linear optimisation. Finally, we show how the res ulting graph matching algorithm relates to the standard quadratic assignment problem. 1 Introduction Graph matching [6, 5, 7, 2, 3, 12, 11J is a topic of central importance in pattern perception. The main computational issues are how to compare inexact relational descriptions (7J and how to search efficiently for the best match [8J. These two issues have recently stimulated interest in the connectionist literature (9, 6, 5, lOJ. For instance, Simic [9], Suganathan et al. (101 and Gold e",Computer Vision,a0872cc5b5ca4cc25076f3d868e1bdf8-Paper.pdf,1996
A comparison between neural networks,"Epidemiological data is traditionally analyzed with very simple techniques. Flexible models, such as neural networks, have the potential to discover unanticipated features in the data. However, to be useful, flexible models must have effective control on overfit ting. This paper reports on a comparative study of the predictive quality of neural networks and other flexible models applied to real and artificial epidemiological data. The results suggest that there are no major unanticipated complex features in the real data, and also demonstrate that MacKay's [1995] Bayesian neural network methodology provides effective control on overfitting while retain ing the ability to discover complex features in the artificial data. 1 Introduction Traditionally, very simple statistical techniques are used in the analysis of epidemi ological studies. The predominant technique is logistic regression, in which the effects of predictors are linear (or categorical) and additive on the log-odds scale. An",NLP,a284df1155ec3e67286080500df36a9a-Paper.pdf,1996
An Apobayesian Relative of Winnow,"We study a mistake-driven variant of an on-line Bayesian learn ing algorithm (similar to one studied by Cesa-Bianchi, Helmbold, and Panizza [CHP96]). This variant only updates its state (learns) on trials in which it makes a mistake. The algorithm makes binary classifications using a linear-threshold classifier and runs in time lin ear in the number of attributes seen by the learner. We have been able to show, theoretically and in simulations, that this algorithm performs well under assumptions quite different from those embod ied in the prior of the original Bayesian algorithm. It can handle situations that we do not know how to handle in linear time with Bayesian algorithms. We expect our techniques to be useful in deriving and analyzing other apobayesian algorithms. 1 Introduction We consider two styles of on-line learning. In both cases, learning proceeds in a sequence of trials. In each trial, a learner observes an instance to be classified, makes a prediction of its classificatio",Optimization & Theoretical ML,a42a596fc71e17828440030074d15e74-Paper.pdf,1996
LSTM CAN SOLVE HARD,"Standard recurrent nets cannot deal with long minimal time lags between relevantsignals. Severalrecent NIPS papers propose alter native methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm,to solve a hard problem that can neither be quicklysolved by random search nor by any other recurrent net algorithm we are aware of. 1 TRIVIAL PREVIOUS LONG TIME LAG PROBLEMS Traditional recurrent nets fail in case 'oflong minimaltime lags between input sig nals and corresponding error signals [7, 3]. Many recent papers propose alternative methods, e.g., [16, 12, 1,5,9]. For instance, Bengioet ale investigate methodssuch as simulated annealing, multi-grid random search, time-weighted pseudo-Newton optimization, and discrete error propagation [3]. They also propose.an EM ap proach [1]. Quite a few papers use variantsofthe ""2-sequenceproblem"" (and ('la",NLP,a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf,1996
Frans M. Coetzee * Virginia L. Stonick,"A globally convergent homotopy method is defined that is capable of sequentially producing large numbers of stationary points of the multi-layer perceptron mean-squared error surface. Using this al gorithm large subsets of the stationary points of two test problems are found. It is shown empirically that the MLP neural network appears to have an extreme ratio of saddle points compared to local minima, and that even small neural network problems have extremely large numbers of solutions. 1 Introduction The number and type of stationary points of the error surface provide insight into the difficulties of finding the optimal parameters ofthe network, since the stationary points determine the degree of the system[l]. Unfortunately, even for the small canonical test problems commonly used in neural network studies, it is still unknown how many stationary points there are, where they are, and how these are divided into minima, maxima and saddle points. Since solving the neural equations expl",Optimization & Theoretical ML,a51fb975227d6640e4fe47854476d133-Paper.pdf,1996
Statistical Mechanics of the Mixture of,"We study generalization capability of the mixture of experts learn ing from examples generated by another network with the same architecture. When the number of examples is smaller than a crit ical value, the network shows a symmetric phase where the role of the experts is not specialized. Upon crossing the critical point, the system undergoes a continuous phase transition to a symme try breaking phase where the gating network partitions the input space effectively and each expert is assigned to an appropriate sub space. We also find that the mixture of experts with multiple level of hierarchy shows multiple phase transitions. 1 Introduction Recently there has been considerable interest among neural network community in techniques that integrate the collective predictions of a set of networks[l, 2, 3, 4]. The mixture of experts [1, 2] is a well known example which implements the phi losophy of divide-and-conquer elegantly. Whereas this model are gaining more popularity in various appli",Optimization & Theoretical ML,a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf,1996
A Mixture of Experts Classifier with,"We address statistical classifier design given a mixed training set con sisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is re quired to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to im prove performance. The learning is based on maximization of the total data likelihood, i.e. over both the labelled and unlabelled data sub sets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a ""mixture of experts"" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the obs",Optimization & Theoretical ML,a58149d355f02887dfbe55ebb2b64ba3-Paper.pdf,1996
Predicting Lifetimes in Dynamically,"Predictions oflifetimes of dynamically allocated objects can be used to improve time and space efficiency of dynamic memory manage ment in computer programs. Barrett and Zorn [1993] used a simple lifetime predictor and demonstrated this improvement on a variety of computer programs. In this paper, we use decision trees to do lifetime prediction on the same programs and show significantly better prediction. Our method also has the advantage that during training we can use a large number of features and let the decision tree automatically choose the relevant subset. 1 INTELLIGENT MEMORY ALLOCATION Dynamic memory allocation is used in many computer applications. The appli cation requests blocks of memory from the operating system or from a memory manager when needed and explicitly frees them up after use. Typically, all of these requests are handled in the same way, without any regard for how or for how long the requested block will be used. Sometimes programmers use runtime profiles to a",Computer Vision,a9078e8653368c9c291ae2f8b74012e7-Paper.pdf,1996
Reinforcement Learning for Mixed,"Closed-loop control relies on sensory feedback that is usually as sumed to be free. But if sensing incurs a cost, it may be cost effective to take sequences of actions in open-loop mode. We de scribe a reinforcement learning algorithm that learns to combine open-loop and closed-loop control when sensing incurs a cost. Al though we assume reliable sensors, use of open-loop control means that actions must sometimes be taken when the current state of the controlled system is uncertain. This is a special case of the hidden-state problem in reinforcement learning, and to cope, our algorithm relies on short-term memory. The main result of the pa per is a rule that significantly limits exploration of possible memory states by pruning memory states for which the estimated value of information is greater than its cost. We prove that this rule allows convergence to an optimal policy. 1 Introduction Reinforcement learning (RL) is widely-used for learning closed-loop control poli cies. Closed-loop",Reinforcement Learning,ab1a4d0dd4d48a2ba1077c4494791306-Paper.pdf,1996
Computing with infinite networks,"For neural networks with a wide class of weight-priors, it can be shown that in the limit of an infinite number of hidden units the prior over functions tends to a Gaussian process. In this paper an alytic forms are derived for the covariance function of the Gaussian processes corresponding to networks with sigmoidal and Gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units, and shows that, somewhat paradoxically, it may be easier to compute with infinite networks than finite ones. 1 Introduction To someone training a neural network by maximizing the likelihood of a finite amount of data it makes no sense to use a network with an infinite number of hidden units; the network will ""overfit"" the data and so will be expected to generalize poorly. However, the idea of selecting the network size depending on the amount of training data makes little sense to a Bayesian; a model should be chosen that reflects the understand",Optimization & Theoretical ML,ae5e3ce40e0404a45ecacaaf05e5f735-Paper.pdf,1996
Regression with Input-Dependent Noise:,"In most treatments of the regression problem it is assumed that the distribution of target data can be described by a deterministic function of the inputs, together with additive Gaussian noise hav ing constant variance. The use of maximum likelihood to train such models then corresponds to the minimization of a sum-of-squares error function. In many applications a more realistic model would allow the noise variance itself to depend on the input variables. However, the use of maximum likelihood to train such models would give highly biased results. In this paper we show how a Bayesian treatment can allow for an input-dependent variance while over coming the bias of maximum likelihood. 1 Introduction In regression problems it is important not only to predict the output variables but also to have some estimate of the error bars associated with those predictions. An important contribution to the error bars arises from the intrinsic noise on the data. In most conventional treatments of reg",Optimization & Theoretical ML,b20bb95ab626d93fd976af958fbc61ba-Paper.pdf,1996
The Generalisation Cost of RAMnets,"Given unlimited computational resources, it is best to use a crite rion of minimal expected generalisation error to select a model and determine its parameters. However, it may be worthwhile to sac rifice some generalisation performance for higher learning speed. A method for quantifying sub-optimality is set out here, so that this choice can be made intelligently. Furthermore, the method is applicable to a broad class of models, including the ultra-fast memory-based methods such as RAMnets. This brings the added benefit of providing, for the first time, the means to analyse the generalisation properties of such models in a Bayesian framework. 1 Introduction In order to quantitatively predict the performance of methods such as the ultra-fast RAMnet, which are not trained by minimising a cost function, we develop a Bayesian formalism for estimating the generalisation cost of a wide class of algorithms. if We consider the noisy interpolation problem, in which each output data point = res",Optimization & Theoretical ML,b24d516bb65a5a58079f0f3526c87c57-Paper.pdf,1996
Multilayer neural networks:,"We study the number of hidden layers required by a multilayer neu ral network with threshold units to compute a function f from n d to {O, I}. In dimension d = 2, Gibson characterized the functions computable with just one hidden layer, under the assumption that there is no ""multiple intersection point"" and that f is only defined on a compact set. We consider the restriction of f to the neighbor hood of a multiple intersection point or of infinity, and give neces sary and sufficient conditions for it to be locally computable with one hidden layer. We show that adding these conditions to Gib son's assumptions is not sufficient to ensure global computability with one hidden layer, by exhibiting a new non-local configuration, the ""critical cycle"", which implies that f is not computable with one hidden layer. 1 INTRODUCTION The number of hidden layers is a crucial parameter for the architecture of multilayer neural networks. Early research, in the 60's, addressed the problem of exactly rea",Optimization & Theoretical ML,b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf,1996
Improving the Accuracy and Speed of,"Support Vector Learning Machines (SVM) are finding application in pattern recognition, regression estimation, and operator inver sion for ill-posed problems. Against this very general backdrop, any methods for improving the generalization performance, or for improving the speed in test phase, of SVMs are of increasing in terest. In this paper we combine two such techniques on a pattern recognition problem. The method for improving generalization per formance (the ""virtual support vector"" method) does so by incor porating known invariances of the problem. This method achieves a drop in the error rate on 10,000 NIST test digit images of 1.4% to 1.0%. The method for improving the speed (the ""reduced set"" method) does so by approximating the support vector decision sur face. We apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine. The combined approach yields a machine which is both 22 times faster than the original machine, and which",Optimization & Theoretical ML,b495ce63ede0f4efc9eec62cb947c162-Paper.pdf,1996
Adaptive Access Control Applied to Ethernet Data,"This paper presents a method that decides which combinations of traffic can be accepted on a packet data link, so that quality of service (QoS) constraints can be met. The method uses samples of QoS results at dif ferent load conditions to build a neural network decision function. Pre vious similar approaches to the problem have a significant bias. This bias is likely to occur in any real system and results in accepting loads that miss QoS targets by orders of magnitude. Preprocessing the data to either remove the bias or provide a confidence level, the method was applied to sources based on difficult-to-analyze ethernet data traces. With this data, the method produces an accurate access control function that dramatically outperforms analytic alternatives. Interestingly, the results depend on throwing away more than 99% of the data. 1 INTRODUCTION In a communication network in which traffic sources can be dynamically added or removed, an access controller must decide when to accept or ",Computer Vision,b51a15f382ac914391a58850ab343b00-Paper.pdf,1996
An Adaptive WTA using Floating Gate,"We have designed, fabricated, and tested an adaptive Winner Take-All (WTA) circuit based upon the classic WTA of Lazzaro, et al [IJ. We have added a time dimension (adaptation) to this circuit to make the input derivative an important factor in winner selection. To accomplish this, we have modified the classic WTA circuit by adding floating gate transistors which slowly null their inputs over time. We present a simplified analysis and experimen tal data of this adaptive WTA fabricated in a standard CMOS 2f.tm process. 1 Winner-Take-All Circuits In a WTA network, each cell has one input and one output. For any set of inputs, the outputs will all be at zero except for the one which is from the cell with the maximum input. One way to accomplish this is by a global nonlinear inhibition coupled with a self-excitation term [2J. Each cell inhibits all others while exciting itself; thus a cell with even a slightly greater input than the others will excite itself up to its maximal state and inh",Computer Vision,b571ecea16a9824023ee1af16897a582-Paper.pdf,1996
Representation and Induction of Finite,"This work investigates the representational and inductive capabili ties of time-delay neural networks (TDNNs) in general, and of two subclasses of TDNN, those with delays only on the inputs (IDNN), and those which include delays on hidden units (HDNN). Both ar chitectures are capable of representing the same class of languages, the definite memory machine (DMM) languages, but the delays on the hidden units in the HDNN helps it outperform the IDNN on problems composed of repeated features over short time windows. 1 Introduction In this paper we consider the representational and inductive capabilities of time delay neural networks (TDNN) [Waibel et al., 1989] [Lang et al., 1990], also known as NNFIR [Wan, 1993]. A TDNN is a feed-forward network in which the set of inputs to any node i may include the output from previous layers not only in the current time step t, but from d earlier time steps as well. The activation function 404 D. S. Clouse, C. L Giles, B. G. Home and G. W. Cottrell fo",Computer Vision,bb04af0f7ecaee4aae62035497da1387-Paper.pdf,1996
Probabilistic Interpretation of Population,"We present a theoretical framework for population codes which generalizes naturally to the important case where the population provides information about a whole probability distribution over an underlying quantity rather than just a single value. We use the framework to analyze two existing models, and to suggest and evaluate a third model for encoding such probability distributions. 1 Introduction Population codes, where information is represented in the activities of whole pop ulations of units, are ubiquitous in the brain. There has been substantial work on how animals should and/or actually do extract information about the underlying encoded quantity. 5,3,11,9,12 With the exception of Anderson,l this work has con centrated on the case of extracting a single value for this quantity. We study ways of characterizing the joint activity of a population as coding a whole probability distribution over the underlying quantity. Two examples motivate this paper: place cells in the hippocamp",Optimization & Theoretical ML,bb7946e7d85c81a9e69fee1cea4a087c-Paper.pdf,1996
Analog VLSI Circuits for,"A one-dimensional visual tracking chip has been implemented us ing neuromorphic, analog VLSI techniques to model selective visual attention in the control of saccadic and smooth pursuit eye move ments. The chip incorporates focal-plane processing to compute image saliency and a winner-take-all circuit to select a feature for tracking. The target position and direction of motion are reported as the target moves across the array. We demonstrate its function ality in a closed-loop system which performs saccadic and smooth pursuit tracking movements using a one-dimensional mechanical eye. 1 Introduction Tracking a moving object on a cluttered background is a difficult task. When more than one target is in the field of view, a decision must be made to determine which target to track and what its movement characteristics are. If motion information is being computed in parallel across the visual field, as is believed to occur in the mid dle temporal area (MT) of primates, some mechanism must ",Computer Vision,c0a271bc0ecb776a094786474322cb82-Paper.pdf,1996
Online learning from finite training sets:,"We analyse online learning from finite training sets at non infinitesimal learning rates TJ. By an extension of statistical me chanics methods, we obtain exact results for the time-dependent generalization error of a linear network with a large number of weights N. We find, for example, that for small training sets of size p ~ N, larger learning rates can be used without compromis ing asymptotic generalization performance or convergence speed. Encouragingly, for optimal settings of TJ (and, less importantly, weight decay ,\) at given final learning time, the generalization per formance of online learning is essentially as good as that of offline learning. 1 INTRODUCTION The analysis of online (gradient descent) learning, which is one of the most common approaches to supervised learning found in the neural networks community, has recently been the focus of much attention [1]. The characteristic feature of online learning is that the weights of a network ('student') are updated each time",Optimization & Theoretical ML,c1e39d912d21c91dce811d6da9929ae8-Paper.pdf,1996
Spatiotemporal Coupling and Scaling of,"We study the spatiotemporal correlation in natural time-varying images and explore the hypothesis that the visual system is con cerned with the optimal coding of visual representation through spatiotemporal decorrelation of the input signal. Based on the measured spatiotemporal power spectrum, the transform needed to decorrelate input signal is derived analytically and then compared with the actual processing observed in psychophysical experiments. 1 Introduction The visual system is concerned with the perception of objects in a dynamic world. A significant fact about natural time-varying images is that they do not change ran domly over space-time; instead image intensities at different times and/or spatial positions are highly correlated. We measured the spatiotemporal correlation func tion - equivalently the power spectrum - of natural images and we find that it is non-separable, i.e., coupled in space and time, and exhibits a very interesting scaling behaviour. When expressed as a f",Computer Vision,c44e503833b64e9f27197a484f4257c0-Paper.pdf,1996
U sing Curvature Information for,"We present an algorithm for fast stochastic gradient descent that uses a nonlinear adaptive momentum scheme to optimize the late time convergence rate. The algorithm makes effective use of cur vature information, requires only O(n) storage and computation, and delivers convergence rates close to the theoretical optimum. We demonstrate the technique on linear and large nonlinear back prop networks. Improving Stochastic Search Learning algorithms that perform gradient descent on a cost function can be for mulated in either stochastic (on-line) or batch form. The stochastic version takes the form Wt+l = Wt + J1.t G( Wt, Xt ) (1) where Wt is the current weight estimate, J1.t is the learning rate, G is minus the instantaneous gradient estimate, and Xt is the input at time ti . One obtains the corresponding batch mode learning rule by taking J1. constant and averaging Gover all x. Stochastic learning provides several advantages over batch learning. For large datasets the batch average is exp",Optimization & Theoretical ML,c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf,1996
ARC-LH: A New Adaptive Resampling,"We introduce arc-Ih, a new algorithm for improvement of ANN clas sifier performance, which measures the importance of patterns by aggregated network output errors. On several artificial benchmark problems, this algorithm compares favorably with other resample and combine techniques. 1 Introduction The training of artificial neural networks (ANNs) is usually a stochastic and unsta ble process. As the weights of the network are initialized at random and training patterns are presented in random order, ANNs trained on the same data will typ ically be different in value and performance. In addition, small changes in the training set can lead to two completely different trained networks with different performance even if the nets had the same initial weights. Roughly speaking, ANNs have a low bias because of their approximation capabili ties, but a rather high variance because of the instability. Recently, several resample and combine techniques for improving ANN performance have been propo",Optimization & Theoretical ML,c54e7837e0cd0ced286cb5995327d1ab-Paper.pdf,1996
Estimating Equivalent Kernels For Neural,"We describe the notion of ""equivalent kernels"" and suggest that this provides a framework for comparing different classes of regression models, including neural networks and both parametric and non-parametric statistical techniques. Unfortunately, standard techniques break down when faced with models, such as neural networks, in which there is more than one ""layer"" of adjustable parameters. We propose an algorithm which overcomes this limitation, estimating the equivalent kernels for neural network models using a data perturbation approach. Experimental results indicate that the networks do not use the maximum possible number of degrees of freedom, that these can be controlled using regularisation techniques and that the equivalent kernels learnt by the network vary both in ""size"" and in ""shape"" in different regions of the input space. 1 INTRODUCTION The dominant approaches within the statistical community, such as multiple linear regression but even extending to advanced techniques su",Optimization & Theoretical ML,c70daf247944fe3add32218f914c75a6-Paper.pdf,1996
Monotonicity Hints,"A hint is any piece of side information about the target function to be learned. We consider the monotonicity hint, which states that the function to be learned is monotonic in some or all of the input variables. The application of mono tonicity hints is demonstrated on two real-world problems- a credit card application task, and a problem in medical diagnosis. A measure of the monotonicity error of a candidate function is defined and an objective function for the enforcement of monotonicity is derived from Bayesian principles. We report experimental results which show that using monotonicity hints leads to a statistically significant improvement in performance on both problems. 1 Introduction Researchers in pattern recognition, statistics, and machine learning often draw a contrast between linear models and nonlinear models such as neural networks. Linear models make very strong assumptions about the function to be modelled, whereas neural networks are said to make no such assumptions",Optimization & Theoretical ML,c850371fda6892fbfd1c5a5b457e5777-Paper.pdf,1996
A Convergence Proof for the Softassign,"The softassign quadratic assignment algorithm has recently emerged as an effective strategy for a variety of optimization prob lems in pattern recognition and combinatorial optimization. While the effectiveness of the algorithm was demonstrated in thousands of simulations, there was no known proof of convergence. Here, we provide a proof of convergence for the most general form of the algorithm. 1 Introduction Recently, a new neural optimization algorithm has emerged for solving quadratic assignment like problems [4, 2]. Quadratic assignment problems (QAP) are char acterized by quadratic objectives with the variables obeying permutation matrix constraints. Problems that roughly fall into this class are TSP, graph partitioning (GP) and graph matching. The new algorithm is based on the softassign procedure which guarantees the satisfaction of the doubly stochastic matrix constraints (result ing from a ""neural"" style relaxation of the permutation matrix constraints). While the effectivene",Optimization & Theoretical ML,c8ba76c279269b1c6bc8a07e38e78fa4-Paper.pdf,1996
Size of multilayer networks for exact,"This article presents a new result about the size of a multilayer neural network computing real outputs for exact learning of a finite set of real samples. The architecture of the network is feedforward, with one hidden layer and several outputs. Starting from a fixed training set, we consider the network as a function of its weights. We derive, for a wide family of transfer functions, a lower and an upper bound on the number of hidden units for exact learning, given the size of the dataset and the dimensions of the input and output spaces. 1 RELATED WORKS The context of our work is rather similar to the well-known results of Baum et al. [1, 2,3,5, 10], but we consider both real inputs and outputs, instead ofthe dichotomies usually addressed. We are interested in learning exactly all the examples of a fixed database, hence our work is different from stating that multilayer networks are universal approximators [6, 8, 9]. Since we consider real outputs and not only dichotomies, it is not",Optimization & Theoretical ML,d282ef263719ab842e05382dc235f69e-Paper.pdf,1996
Removing Noise in On-Line Search using,"Stochastic (on-line) learning can be faster than batch learning. However, at late times, the learning rate must be annealed to re move the noise present in the stochastic weight updates. In this annealing phase, the convergence rate (in mean square) is at best proportional to l/T where T is the number of input presentations. An alternative is to increase the batch size to remove the noise. In this paper we explore convergence for LMS using 1) small but fixed batch sizes and 2) an adaptive batch size. We show that the best adaptive batch schedule is exponential and has a rate of conver gence which is the same as for annealing, Le., at best proportional to l/T. 1 Introduction Stochastic (on-line) learning can speed learning over its batch training particularly ,,,,hen data sets are large and contain redundant information [M0l93J. However, at late times in learning, noise present in the weight updates prevents complete conver gence from taking place. To reduce the noise, the learning rate",Optimization & Theoretical ML,cd758e8f59dfdf06a852adad277986ca-Paper.pdf,1996
Support Vector Regression Machines,"A new regression technique based on Vapnik's concept of support vectors is introduced. We compare support vector regression (SVR) with a committee regression technique (bagging) based on regression trees and ridge regression done in feature space. On the basis of these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space.",Optimization & Theoretical ML,d38901788c533e8286cb6400b40b386d-Paper.pdf,1996
An Hierarchical Model of Visual Rivalry,"Binocular rivalry is the alternating percept that can result when the two eyes see different scenes. Recent psychophysical evidence supports an account for one component of binocular rivalry similar to that for other bistable percepts. We test the hypothesisl9, 16, 18 that alternation can be generated by competition between top down cortical explanations for the inputs, rather than by direct competition between the inputs. Recent neurophysiological ev idence shows that some binocular neurons are modulated with the changing percept; others are not, even if they are selective be tween the stimuli presented to the eyes. We extend our model to a hierarchy to address these effects. 1 Introduction Although binocular rivalry leads to distinct perceptual distress, it is revealing about the mechanisms of visual information processing. The first accounts for rivalry argued on the basis of phenomena such as increases in thresholds for test stimuli presented in the suppressed eye24, 8, 3 that ther",Computer Vision,d759175de8ea5b1d9a2660e45554894f-Paper.pdf,1996
Dynamically Adaptable CMOS,"The major problem that has prevented practical application of analog neuro-LSIs has been poor accuracy due to fluctuating analog device characteristics inherent in each device as a result of manufacturing. This paper proposes a dynamic control architecture that allows analog silicon neural networks to compensate for the fluctuating device characteristics and adapt to a change in input DC level. We have applied this architecture to compensate for input offset voltages of an analog CMOS WTA (Winner-Take-AlI) chip that we have fabricated. Experimental data show the effectiveness of the architecture. 1 INTRODUCTION Analog VLSI implementation of neural networks, such as silicon retinas and adaptive filters, has been the focus of much active research. Since it utilizes physical laws that electric devices obey for neural operation, circuit scale can be much smaller than that of a digital counterpart and massively parallel implementation is possible. The major problem that has prevented practi",Computer Vision,d94e18a8adb4cc0f623f7a83b1ac75b4-Paper.pdf,1996
Ensemble Methods for Phoneme,"This paper investigates a number of ensemble methods for improv ing the performance of phoneme classification for use in a speech recognition system. Two ensemble methods are described; boosting and mixtures of experts, both in isolation and in combination. Re sults are presented on two speech recognition databases: an isolated word database and a large vocabulary continuous speech database. These results show that principled ensemble methods such as boost ing and mixtures provide superior performance to more naive en semble methods such as averaging. INTRODUCTION There is now considerable interest in using ensembles or committees of learning machines to improve the performance of the system over that of a single learning machine. In most neural network ensembles, the ensemble members are trained on either the same data (Hansen & Salamon 1990) or different subsets of the data (Per rone & Cooper 1993). The ensemble members typically have different initial con ditions and/or different ar",NLP,da11e8cd1811acb79ccf0fd62cd58f86-Paper.pdf,1996
Maximum Likelihood Blind Source,"In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result Xi(t) of mixing n unknown independent sources 8i(t) through an unknown E * n x n mixing matrix A( t) of causal linear filters: Xi = j aij 8 j . We cast the problem as one of maximum likelihood density estima tion, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm ""Contextual ICA,"" after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kur tosis, colored Gaussian sources, and sources which have Gaussian histograms. 1 The Blind Source Separation Problem Consider a set of n indepent sources 81 (t), ... ,8n(t). We are given n linearly dis E torted senso",Computer Vision,dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf,1996
Temporal Low-Order Statistics of Natural,"In order to process incoming sounds efficiently, it is advantageous for the auditory system to be adapted to the statistical structure of natural auditory scenes. As a first step in investigating the relation between the system and its inputs, we study low-order statistical properties in several sound ensembles using a filter bank analysis. Focusing on the amplitude and phase in different frequency bands, we find simple parametric descriptions for their distribution and power spectrum that are valid for very different types of sounds. In particular, the amplitude distribution has an exponential tail and its power spectrum exhibits a modified power-law behavior, which is manifested by self-similarity and long-range temporal cor relations. Furthermore, the statistics for different bands within a given ensemble are virtually identical, suggesting translation in variance along the cochlear axis. These results show that natural sounds are highly redundant, and have possible implications to ",Computer Vision,dc4c44f624d600aa568390f1f1104aa0-Paper.pdf,1996
Limitations of self-organizing maps for,"The limitations of using self-organizing maps (SaM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SaM's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined tech nique of online K-means clustering plus Sammon mapping of the cluster centroids. SaM are shown to perform significantly worse in terms of quantization error, in recovering the structure of the clus ters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems. 1 Introduction Self-organizing maps (SaM) introduced by [Kohonen 84] are a very popular tool used for visualization of high dimensional data spaces. SaM can be said to do clustering/vector quantization (VQ) and at the same time to preserve the spatial ordering of the input data reflected by an ordering of the code book vectors (cluster centroids) in ",Computer Vision,dc87c13749315c7217cdc4ac692e704c-Paper.pdf,1996
A spike based learning neuron in analog,"Many popular learning rules are formulated in terms of continu ous, analog inputs and outputs. Biological systems, however, use action potentials, which are digital-amplitude events that encode analog information in the inter-event interval. Action-potential representations are now being used to advantage in neuromorphic VLSI systems as well. We report on a simple learning rule, based on the Riccati equation described by Kohonen [1], modified for action-potential neuronal outputs. We demonstrate this learning rule in an analog VLSI chip that uses volatile capacitive storage for synaptic weights. We show that our time-dependent learning rule is sufficient to achieve approximate weight normalization and can detect temporal correlations in spike trains. A Spike Based Learning Neuron in Analog VLSI 693 1 INTRODUCTION It is an ongoing debate how information in the nervous system is encoded and carried between neurons. In many subsystems of the brain it is now believed that it is done by the",Computer Vision,dc960c46c38bd16e953d97cdeefdbc68-Paper.pdf,1996
One-unit Learning Rules for,"Neural one-unit learning rules for the problem of Independent Com ponent Analysis (ICA) and blind source separation are introduced. In these new algorithms, every ICA neuron develops into a sepa rator that finds one of the independent components. The learning rules use very simple constrained Hebbianjanti-Hebbian learning in which decorrelating feedback may be added. To speed up the convergence of these stochastic gradient descent rules, a novel com putationally efficient fixed-point algorithm is introduced. 1 Introduction Independent Component Analysis (ICA) (Comon, 1994; Jutten and Herault, 1991) is a signal processing technique whose goal is to express a set of random vari ables as linear combinations of statistically independent component variables. The main applications of ICA are in blind source separation, feature extraction, and blind deconvolution. In the simplest form of ICA (Comon, 1994), we observe m scalar random variables Xl, ... , Xm which are assumed to be linear combin",Optimization & Theoretical ML,dfd7468ac613286cdbb40872c8ef3b06-Paper.pdf,1996
Analysis of Temporal-Difference Learning,"We present new results about the temporal-difference learning al gorithm, as applied to approximating the cost-to-go function of a Markov chain using linear function approximators. The algo rithm we analyze performs on-line updating of a parameter vector during a single endless trajectory of an aperiodic irreducible finite state Markov chain. Results include convergence (with probability 1), a characterization of the limit of convergence, and a bound on the resulting approximation error. In addition to establishing new and stronger results than those previously available, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning. Furthermore, we discuss the implications of two counter-examples with regards to the Significance of on-line updating and linearly parameterized function approximators. 1 INTRODUCTION The problem of predicting the expected long-term future cost (or reward) of a stochastic dynamic system mani",Reinforcement Learning,e00406144c1e7e35240afed70f34166a-Paper.pdf,1996
Fast Network Pruning and Feature,"The algorithm described in this article is based on the OBS algo rithm by Hassibi, Stork and Wolff ([1] and [2]). The main disad vantage of OBS is its high complexity. OBS needs to calculate the inverse Hessian to delete only one weight (thus needing much time to prune a big net). A better algorithm should use this matrix to remove more than only one weight, because calculating the inverse Hessian takes the most time in the OBS algorithm. The algorithm, called Unit-OBS, described in this article is a method to overcome this disadvantage. This algorithm only needs to calculate the inverse Hessian once to remove one whole unit thus drastically reducing the time to prune big nets. A further advantage of Unit-OBS is that it can be used to do a feature extraction on the input data. This can be helpful on the understanding of unknown problems. 1 Introduction This article is based on the technical report [3] about speeding up the OBS algo rithm. The main target of this work was to reduce the ",Optimization & Theoretical ML,e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf,1996
Learning with Noise and Regularizers,"We study the effect of noise and regularization in an on-line gradient-descent learning scenario for a general two-layer student network with an arbitrary number of hidden units. Training ex amples are randomly drawn input vectors labeled by a two-layer teacher network with an arbitrary number of hidden units; the ex amples are corrupted by Gaussian noise affecting either the output or the model itself. We examine the effect of both types of noise and that of weight-decay regularization on the dynamical evolu tion of the order parameters and the generalization error in various phases of the learning process. 1 Introduction One of the most powerful and commonly used methods for training large layered neural networks is that of on-line learning, whereby the internal network parameters {J} are modified after the presentation of each training example so as to minimize the corresponding error. The goal is to bring the map fJ implemented by the network as close as possible to a desired map j",Optimization & Theoretical ML,e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf,1996
The Learning Dynamics of,"The learning properties of a universal approximator, a normalized committee machine with adjustable biases, are studied for on-line back-propagation learning. Within a statistical mechanics frame work, numerical studies show that this model has features which do not exist in previously studied two-layer network models with out adjustable biases, e.g., attractive suboptimal symmetric phases even for realizable cases and noiseless data. 1 INTRODUCTION Recently there has been much interest in the theoretical breakthrough in the un derstanding of the on-line learning dynamics of multi-layer feedforward perceptrons (MLPs) using a statistical mechanics framework. In the seminal paper (Saad & Solla, 1995), a two-layer network with an arbitrary number of hidden units was studied, allowing insight into the learning behaviour of neural network models whose complexity is of the same order as those used in real world applications. The model studied, a soft committee machine (Biehl & Schwarze, 1995",Optimization & Theoretical ML,e3251075554389fe91d17a794861d47b-Paper.pdf,1996
Gaussian Processes for Bayesian,"The full Bayesian method for applying neural networks to a pre diction problem is to set up the prior/hyperprior structure for the net and then perform the necessary integrals. However, these inte grals are not tractable analytically, and Markov Chain Monte Carlo (MCMC) methods are slow, especially if the parameter space is high-dimensional. Using Gaussian processes we can approximate the weight space integral analytically, so that only a small number of hyperparameters need be integrated over by MCMC methods. We have applied this idea to classification problems, obtaining ex cellent results on the real-world problems investigated so far. 1 INTRODUCTION To make predictions based on a set of training data, fundamentally we need to combine our prior beliefs about possible predictive functions with the data at hand. In the Bayesian approach to neural networks a prior on the weights in the net induces a prior distribution over functions. This leads naturally to the idea of specifying our b",Optimization & Theoretical ML,e53a0a2978c28872a4505bdb51db06dc-Paper.pdf,1996
Genetic Algorithms and Explicit Search Statistics,"The genetic algorithm (GA) is a heuristic search procedure based on mechanisms we showed that much simpler algorithms, such as hillcIimbing and Population Based Incremental Learning (PBIL), perform comparably to GAs on an optimiza tion problem custom designed to benefit from the GA's operators. This paper extends these results in two directions. First, in a large-scale empirical comparison of problems that have been reported in GA literature, we show that on many prob lems, simpler algorithms can perform significantly better than GAs. Second, we describe when crossover is useful, and show how it can be incorporated into PBIL. 1 IMPLICIT VS. EXPLICIT SEARCH STATISTICS Although there has recently been controversy in the genetic algorithm (GA) community as to whether GAs should be used for static function optimization, a large amount of research has been, and continues to be, conducted in this direction [De Jong, 1992]. Since much of GA research focuses on optimization (most often in stat",Optimization & Theoretical ML,e6d8545daa42d5ced125a4bf747b3688-Paper.pdf,1996
Efficient Nonlinear Control with,"A new reinforcement learning architecture for nonlinear control is proposed. A direct feedback controller, or the actor, is trained by a value-gradient based controller, or the tutor. This architecture enables both efficient use of the value function and simple computa tion for real-time implementation. Good performance was verified in multi-dimensional nonlinear control tasks using Gaussian soft max networks. 1 INTRODUCTION In the study of temporal difference (TD) learning in continuous time and space (Doya, 1996b), an optimal nonlinear feedback control law was derived using the gradient of the value function and the local linear model of the system dynam ics. It was demonstrated in the simulation of a pendulum swing-up task that the value-gradient based control scheme requires much less learning trials than the con ventional ""actor-critic"" control scheme (Barto et al., 1983). In the actor-critic scheme, the actor, a direct feedback controller, improves its con trol policy stochastica",Reinforcement Learning,eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf,1996
Learning Exact Patterns of Quasi-synchronization,This paper develops arguments for a family of temporal log-linear models to represent spatio-temporal correlations among the spiking events in a group of neurons. The models can represent not just pairwise correlations but also correlations of higher order. Methods are discussed for inferring the existence or absence of correlations and estimating their strength. A frequentist and a Bayesian approach to correlation detection are compared. The frequentist method is based on G 2 statistic with estimates obtained via the Max-Ent principle. In the Bayesian approach a Markov Chain Monte Carlo Model Composition (MC3) algorithm is applied to search over connectivity structures and Laplace's method is used to approximate their posterior probability. Performance of the methods was tested on synthetic data. The methods were applied to experimental data obtained by the fourth author by means of measurements carried out on behaving Rhesus monkeys at the Hadassah Medical School of the Hebrew Univer,Optimization & Theoretical ML,e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf,1996
Interpolating Earth-science Data using RBF,"We present a mixture of experts (ME) approach to interpolate sparse, spatially correlated earth-science data. Kriging is an interpolation method which uses a global covariation model estimated from the data to take account of the spatial dependence in the data. Based on the close relationship between kriging and the radial basis function (RBF) network (Wan & Bone, 1996), we use a mixture of generalized RBF networks to partition the input space into statistically correlated regions and learn the local covariation model of the data in each region. Applying the ME approach to simulated and real-world data, we show that it is able to achieve good partitioning of the input space, learn the local covariation models and improve generalization.",Optimization & Theoretical ML,f09696910bdd874a99cd74c8f05b5c44-Paper.pdf,1996
Statistically Efficient Estimation Using,"Coarse codes are widely used throughout the brain to encode sen sory and motor variables. Methods designed to interpret these codes, such as population vector analysis, are either inefficient, i.e., the variance of the estimate is much larger than the smallest possi ble variance, or biologically implausible, like maximum likelihood. Moreover, these methods attempt to compute a scalar or vector estimate of the encoded variable. Neurons are faced with a simi lar estimation problem. They must read out the responses of the presynaptic neurons, but, by contrast, they typically encode the variable with a further population code rather than as a scalar. We show how a non-linear recurrent network can be used to per form these estimation in an optimal way while keeping the estimate in a coarse code format. This work suggests that lateral connec tions in the cortex may be involved in cleaning up uncorrelated noise among neurons representing similar variables. 1 Introduction Most sensory and moto",Computer Vision,f29b38f160f87ae86df31cee1982066f-Paper.pdf,1996
Recursive algorithms for approximating,"We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightfor wardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they main tain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks, or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is veri fied experimentally. 1 Introduction Graphical models (see, e.g., Lauritzen 1996) provide a medium for rigorously em bedding domain knowledge into network models. The structure in these graphical models embodies the qualitative assumptions about the independence relationships in the domain while the probability model attached to the graph permits a consis tent computation of belief (or uncertainty) about the values of",Optimization & Theoretical ML,f3f1b7fc5a8779a9e618e1f23a7b7860-Paper.pdf,1996
Clustering via Concave Minimization,"The problem of assigning m points in the n-dimensional real space Rn to k clusters is formulated as that of determining k centers in Rn such that the sum of distances of each point to the nearest center is minimized. If a polyhedral distance is used, the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program: minimizing a bilinear function on a polyhe dral set. A fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program. Computational testing on a number of real world databases was carried out. On the Wisconsin Diagnostic Breast Cancer (WDBC) database, k-Median training set correct ness was comparable to that of the k-Mean Algorithm, however its testing set correctness was better. Additionally, on the Wisconsin Prognostic Breast Cancer (WPBC) database, distinct and clini cally important survival curves w",Optimization & Theoretical ML,f4573fc71c731d5c362f0d7860945b88-Paper.pdf,1996
Balancing between bagging and bumping,"We compare different methods to combine predictions from neu ral networks trained on different bootstrap samples of a regression problem. One of these methods, introduced in [6] and which we here call balancing, is based on the analysis of the ensemble gen eralization error into an ambiguity term and a term incorporating generalization performances of individual networks. We show how to estimate these individual errors from the residuals on valida tion patterns. Weighting factors for the different networks follow from a quadratic programming problem. On a real-world problem concerning the prediction of sales figures and on the well-known Boston housing data set, balancing clearly outperforms other re cently proposed alternatives as bagging [1] and bumping [8]. 1 EARLY STOPPING AND BOOTSTRAPPING Stopped training is a popular strategy to prevent overfitting in neural networks. The complete data set is split up into a training and a validation set. Through learning the weights are adapted",Optimization & Theoretical ML,f47330643ae134ca204bf6b2481fec47-Paper.pdf,1996
Self-Organizing and Adaptive Algorithms for,"The paper is developed in two parts where we discuss a new approach to self-organization in a single-layer linear feed-forward network. First, two novel algorithms for self-organization are derived from a two-layer linear hetero-associative network performing a one-of-m classification, and trained with the constrained least-mean-squared classification error criterion. Second, two adaptive algorithms are derived from these self organizing procedures to compute the principal generalized eigenvectors of two correlation matrices from two sequences of random vectors. These novel adaptive algorithms can be implemented in a single-layer linear feed-forward network. We give a rigorous convergence analysis of the adaptive algorithms by using stochastic approximation theory. As an example, we consider a problem of online signal detection in digital mobile communications.",Optimization & Theoretical ML,f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf,1996
Multi-Grid Methods for Reinforcement,"Reinforcement learning methods for discrete and semi-Markov de cision problems such as Real-Time Dynamic Programming can be generalized for Controlled Diffusion Processes. The optimal control problem reduces to a boundary value problem for a fully nonlinear second-order elliptic differential equation of Hamilton Jacobi-Bellman (HJB-) type. Numerical analysis provides multi grid methods for this kind of equation. In the case of Learning Con trol, however, the systems of equations on the various grid-levels are obtained using observed information (transitions and local cost). To ensure consistency, special attention needs to be directed to ward the type of time and space discretization during the obser vation. An algorithm for multi-grid observation is proposed. The multi-grid algorithm is demonstrated on a simple queuing problem. 1 Introduction Controlled Diffusion Processes (CDP) are the analogy to Markov Decision Problems in continuous state space and continuous time. A CDP can always",Reinforcement Learning,f91e24dfe80012e2a7984afa4480a6d6-Paper.pdf,1996
Noisy Spiking Neurons with Temporal,"We exhibit a novel way of simulating sigmoidal neural nets by net works of noisy spiking neurons in temporal coding. Furthermore it is shown that networks of noisy spiking neurons with temporal coding have a strictly larger computational power than sigmoidal neural nets with the same number of units. 1 Introduction and Definitions We consider a formal model SNN for a §piking neuron network that is basically a reformulation of the spike response model (and of the leaky integrate and fire model) without using 6-functions (see [Maass, 1996a] or [Maass, 1996b] for further backgrou nd). An SNN consists of a finite set V of spiking neurons, a set E ~ V x V of synapses, a weight wu,v 2: 0 and a response function cu,v : R+ --+ R for each synapse {u, v} E E (where R+ := {x E R: x 2: O}) , and a threshold function 8 v : R+ --+ R+ for each neuron v E V . If Fu ~ R+ is the set of firing times of a neuron u , then the potential at the trigger zone of neuron v at time t is given by L L Pv(t) := Wu,v",Computer Vision,f93882cbd8fc7fb794c1011d63be6fb6-Paper.pdf,1996
"For valid generalization, the size of the","This paper shows that if a large neural network is used for a pattern classification problem, and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. More specifi cally, consider an i-layer feed-forward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A. The misclassification probability con verges to an error estimate (that is closely related to squared error on the training set) at rate O((cA)l(l+1)/2J(log n)jm) ignoring log factors, where m is the number of training patterns, n is the input dimension, and c is a constant. This may explain the gen eralization performance of neural networks, particularly when the number of training examples is considerably smaller than the num ber of weights. It also supports heuristics (such as weight decay and early sto",Optimization & Theoretical ML,fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf,1996
Edges are the 'Independent Components' of,"Field (1994) has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, dis tributed representation of natural scenes, and Barlow (1989) has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that non-linear 'infomax', when applied to an ensemble of nat ural scenes, produces sets of visual filters that are localised and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximisation network of Olshausen & Field (1996). In addition, the outputs of these filters are as independent as possible, since the info max network is able to perform Independent Components Analysis (ICA). We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA",Computer Vision,f9be311e65d81a9ad8150a60844bb94c-Paper.pdf,1996
Neural Network Modeling of Speech and Music,Time series prediction is one of the major applications of neural net works. After a short introduction into the basic theoretical foundations we argue that the iterated prediction of a dynamical system may be in terpreted as a model of the system dynamics. By means of RBF neural networks we describe a modeling approach and extend it to be able to model instationary systems. As a practical test for the capabilities of the method we investigate the modeling of musical and speech signals and demonstrate that the model may be used for synthesis of musical and speech signals. 1 Introduction Since the formulation of the reconstruction theorem by Takens [10] it has been clear that a nonlinear predictor of a dynamical system may be directly derived from a systems time series. The method has been investigated extensively and with good success for the pre diction of time series of nonlinear systems. Especially the combination of reconstruction techniques and neural networks has shown good resul,Computer Vision,fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf,1996
Rapid Visual Processing using Spike Asynchrony,"We have investigated the possibility that rapid processing in the visual system could be achieved by using the order of firing in different neurones as a code, rather than more conventional firing rate schemes. Using SPIKENET, a neural net simulator based on integrate-and-fire neurones and in which neurones in the input layer function as analog to-delay converters, we have modeled the initial stages of visual processing. Initial results are extremely promising. Even with activity in retinal output cells limited to one spike per neuron per image (effectively ruling out any form of rate coding), sophisticated processing based on asynchronous activation was nonetheless possible.",Computer Vision,fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf,1996
Visual Cortex Circuitry and Orientation,"A simple mathematical model for the large-scale circuitry of pri mary visual cortex is introduced. It is shown that a basic cor tical architecture of recurrent local excitation and lateral inhi bition can account quantitatively for such properties as orien tation tuning. The model can also account for such local ef fects as cross-orientation suppression. It is also shown that non local state-dependent coupling between similar orientation patches, when added to the model, can satisfactorily reproduce such ef fects as non-local iso--orientation suppression, and non-local cross orientation enhancement. Following this an account is given of per ceptual phenomena involving object segmentation, such as ""pop out"", and the direct and indirect tilt illusions. 1 INTRODUCTION The edge detection mechanism in the primate visual cortex (VI) involves at least two fairly well characterized circuits. There is a local circuit operating at sub hypercolumn dimensions comprising strong orientation specific",Computer Vision,fe2d010308a6b3799a3d9c728ee74244-Paper.pdf,1996
Orientation contrast sensitivity from,"Recently Sill ito and coworkers (Nature 378, pp. 492,1995) demon strated that stimulation beyond the classical receptive field (cRF) can not only modulate, but radically change a neuron's response to oriented stimuli. They revealed that patch-suppressed cells when stimulated with contrasting orientations inside and outside their cRF can strongly respond to stimuli oriented orthogonal to their nominal preferred orientation. Here we analyze the emergence of such complex response patterns in a simple model of primary vi sual cortex. We show that the observed sensitivity for orientation contrast can be explained by a delicate interplay between local isotropic interactions and patchy long-range connectivity between distant iso-orientation domains. In particular we demonstrate that the observed properties might arise without specific connections be tween sites with cross-oriented cRFs. 1 Introduction Long range horizontal connections form a ubiquitous structural element of intra cortical cir",Computer Vision,fe51510c80bfd6e5d78a164cd5b1f688-Paper.pdf,1996
Prior Knowledge in Support Vector Kernels,"We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invari ances under group transfonnations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions. 1 INTRODUCTION When we are trying to extract regularities from data, we often have additional knowledge about functions that we estimate. For instance, in image classification tasks, there exist transfonnations which leave class membership invariant (e.g. local translations); moreover, it is usually the case that images have a local structure in that not all correlations between image regions carry equal amounts of infonnation. The present study investigates the question how to make use of these two sources of know ledge by designing appropriate Support Vector (SV) kernel functions. We start by giving a brief introduction to SV machines (Vapnik & Chervonenkis, 1979; Vapnik, 1995) (Sec. 2). Regarding prior k",Optimization & Theoretical ML,01d8bae291b1e4724443375634ccfa0e-Paper.pdf,1997
A Revolution: Belief Propagation •,"Until recently, artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief net works that have cycles. The probability propagation algorithm is only exact in networks that are cycle-free. However, it has recently been discovered that the two best error-correcting decoding algo rithms are actually performing probability propagation in belief networks with cycles. 1 Communicating over a noisy channel Our increasingly wired world demands efficient methods for communicating bits of information over physical channels that introduce errors. Examples of real-world channels include twisted-pair telephone wires, shielded cable-TV wire, fiber-optic cable, deep-space radio, terrestrial radio, and indoor radio. Engineers attempt to correct the errors introduced by the noise in these channels through the use of channel coding which adds protection to the information source, so that some channel errors can be corrected. A popular model of a ph",Computer Vision,0245952ecff55018e2a459517fdb40e3-Paper.pdf,1997
Dynamic Stochastic Synapses as,"In most neural network models, synapses are treated as static weights that change only on the slow time scales of learning. In fact, however, synapses are highly dynamic, and show use-dependent plasticity over a wide range of time scales. Moreover, synaptic transmission is an inherently stochastic process: a spike arriving at a presynaptic terminal triggers release of a vesicle of neurotransmitter from a release site with a probability that can be much less than one. Changes in release probability represent one of the main mechanisms by which synaptic efficacy is modulated in neural circuits. We propose and investigate a simple model for dynamic stochastic synapses that can easily be integrated into common models for neural computation. We show through computer simulations and rigorous theoretical analysis that this model for a dynamic stochastic synapse increases computational power in a nontrivial way. Our results may have implications for the process ing of time-varying signals by b",Computer Vision,05311655a15b75fab86956663e1819cd-Paper.pdf,1997
Self-similarity properties of natural images,"Scale invariance is a fundamental property of ensembles of nat ural images [1]. Their non Gaussian properties [15, 16] are less well understood, but they indicate the existence of a rich statis tical structure. In this work we present a detailed study of the marginal statistics of a variable related to the edges in the images. A numerical analysis shows that it exhibits extended self-similarity [3, 4, 5]. This is a scaling property stronger than self-similarity: all its moments can be expressed as a power of any given moment. More interesting, all the exponents can be predicted in terms of a multiplicative log-Poisson process. This is the very same model that was used very recently to predict the correct exponents of the structure functions of turbulent flows [6]. These results allow us to study the underlying multifractal singularities. In particular we find that the most singular structures are one-dimensional: the most singular manifold consists of sharp edges. Category: Visual Proc",Computer Vision,07042ac7d03d3b9911a00da43ce0079a-Paper.pdf,1997
Multiple Threshold Neural Logic,"We introduce a new Boolean computing element related to the Lin ear Threshold element, which is the Boolean version of the neuron. Instead of the sign function, it computes an arbitrary (with poly nornialy many transitions) Boolean function of the weighted sum of its inputs. We call the new computing element an LTM element, which stands for Linear Threshold with Multiple transitions. The paper consists of the following main contributions related to our study of LTM circuits: (i) the creation of efficient designs of LTM circuits for the addition of a multiple number of integers and the product of two integers. In particular, we show how to compute the addition of m integers with a single layer of LTM elements. (ii) a proof that the area of the VLSI layout is reduced from O(n2) in LT circuits to O(n) in LTM circuits, for n inputs symmetric Boolean functions, and (iii) the characterization of the computing power of LTM relative to LT circuits. 1 Introduction Human brains are by far superi",Computer Vision,0966289037ad9846c5e994be2a91bafa-Paper.pdf,1997
Asymptotic Theory for Regularization:,"The generalization ability of a neural network can sometimes be improved dramatically by regularization. To analyze the improve ment one needs more refined results than the asymptotic distri bution of the weight vector. Here we study the simple case of one-dimensional linear regression under quadratic regularization, i.e., ridge regression. We study the random design, misspecified case, where we derive expansions for the optimal regularization pa rameter and the ensuing improvement. It is possible to construct examples where it is best to use no regularization. 1 INTRODUCTION Suppose that we have available training data (Xl, Yd, .. 0' (Xn' Yn) consisting of pairs of vectors, and we try to predict Yi on the basis of Xi with a neural network with weight vector w. One popular way of selecting w is by the criterion -1 Ln (1) £(Xi' Yi, w) + >..Q(w) = min!, n I Ily - where the loss £(x,y,w) is, e.g., the squared error g(x,w)112, the function g(., w) is the input/output function of the neural",Optimization & Theoretical ML,0a1bf96b7165e962e90cb14648c9462d-Paper.pdf,1997
On Efficient Heuristic Ranking of,"This paper considers the problem of learning the ranking of a set of alternatives based upon incomplete information (e.g., a limited number of observations). We describe two algorithms for hypoth esis ranking and their application for probably approximately cor rect (PAC) and expected loss (EL) learning criteria. Empirical results are provided to demonstrate the effectiveness of these rank ing procedures on both synthetic datasets and real-world data from a spacecraft design optimization problem. 1 INTRODUCTION In many learning applications, the cost of information can be quite high, imposing a requirement that the learning algorithms glean as much usable information as possible with a minimum of data. For example: • In speedup learning, the expense of processing each training example can be significant [Tadepalli921. • In decision tree learning, ihe cost of using all available training examples when evaluating potential attributes for partitioning can be computation ally ex.pensive [M",Optimization & Theoretical ML,0c0a7566915f4f24853fc4192689aa7e-Paper.pdf,1997
Learning Continuous Attractors in,"One approach to invariant object recognition employs a recurrent neu ral network as an associative memory. In the standard depiction of the network's state space, memories of objects are stored as attractive fixed points of the dynamics. I argue for a modification of this picture: if an object has a continuous family of instantiations, it should be represented by a continuous attractor. This idea is illustrated with a network that learns to complete patterns. To perform the task of filling in missing in formation, the network develops a continuous attractor that models the manifold from which the patterns are drawn. From a statistical view point, the pattern completion task allows a formulation of unsupervised learning in terms of regression rather than density estimation. A classic approach to invariant object recognition is to use a recurrent neural net work as an associative memory[l]. In spite of the intuitive appeal and biological plausibility of this approach, it has largely been",Computer Vision,0e3a37aa85a14e359df74fa77eded3f6-Paper.pdf,1997
Boltzmann Machine learning using mean,"We present a new approximate learning algorithm for Boltzmann Machines, using a systematic expansion of the Gibbs free energy to second order in the weights. The linear response correction to the correlations is given by the Hessian of the Gibbs free energy. The computational complexity of the algorithm is cubic in the number of neurons. We compare the performance of the exact BM learning algorithm with first order (Weiss) mean field theory and second order (TAP) mean field theory. The learning task consists of a fully connected Ising spin glass model on 10 neurons. We conclude that 1) the method works well for paramagnetic problems 2) the TAP correction gives a significant improvement over the Weiss mean field theory, both for paramagnetic and spin glass problems and 3) that the inclusion of diagonal weights improves the Weiss approximation for paramagnetic problems, but not for spin glass problems. 1 Introduction Boltzmann Machines (BMs) [1], are networks of binary neurons with a sto",Optimization & Theoretical ML,0e4e946668cf2afc4299b462b812caca-Paper.pdf,1997
Incorporating Test Inputs into Learning,"In many applications, such as credit default prediction and medical im age recognition, test inputs are available in addition to the labeled train ing examples. We propose a method to incorporate the test inputs into learning. Our method results in solutions having smaller test errors than that of simple training solution, especially for noisy problems or small training sets. 1 Introduction We introduce an estimator of test error that takes into consideration the test inputs. The new estimator, augmented error, is composed of the training error and an additional term computed using the test inputs. In some applications, such as credit default prediction and medical image recognition, we do have access to the test inputs. In our experiments, we found that the augmented error (which is computed without looking at the test outputs but only test inputs and training examples) can result in a smaller test error. In particular, it tends to increase when the test error increases (overtraining)",Optimization & Theoretical ML,0e55666a4ad822e0e34299df3591d979-Paper.pdf,1997
An application of Reversible-J ump,Applications of Gaussian mixture models occur frequently in the fields of statistics and artificial neural networks. One of the key issues arising from any mixture model application is how to es timate the optimum number of mixture components. This paper extends the Reversible-Jump Markov Chain Monte Carlo (MCMC) algorithm to the case of multivariate spherical Gaussian mixtures using a hierarchical prior model. Using this method the number of mixture components is no longer fixed but becomes a param eter of the model which we shall estimate. The Reversible-Jump MCMC algorithm is capable of moving between parameter sub spaces which correspond to models with different numbers of mix ture components. As a result a sample from the full joint distribu tion of all unknown model parameters is generated. The technique is then demonstrated on a simulated example and a well known vowel dataset. 1 Introduction Applications of Gaussian mixture models regularly appear in the neural networks literat,Optimization & Theoretical ML,0ed9422357395a0d4879191c66f4faa2-Paper.pdf,1997
Yuzo Hirai,"An asynchronous PDM (Pulse-Density-Modulating) digital neural network system has been developed in our laboratory. It consists of one thousand neurons that are physically interconnected via one million 7-bit synapses. It can solve one thousand simultaneous nonlinear first-order differential equations in a fully parallel and continuous fashion. The performance of this system was measured by a winner-take-all network with one thousand neurons. Although the magnitude of the input and network parameters were identi cal for each competing neuron, one of them won in 6 milliseconds. This processing speed amounts to 360 billion connections per sec ond. A broad range of neural networks including spatiotemporal filtering, feedforward, and feedback networks can be run by loading appropriate network parameters from a host system. 1 INTRODUCTION The hardware implementation of neural networks is crucial in order to realize the real-time operation of neural functions such as spatiotemporal filtering,",Computer Vision,14d9e8007c9b41f57891c48e07c23f57-Paper.pdf,1997
Reinforcement Learning for Continuous,"This paper is concerned with the problem of Reinforcement Learn ing (RL) for continuous state space and time stocha.stic control problems. We state the Harnilton-Jacobi-Bellman equation satis fied by the value function and use a Finite-Difference method for designing a convergent approximation scheme. Then we propose a RL algorithm based on this scheme and prove its convergence to the optimal solution. 1 Introduction to RL in the continuous, stochastic case The objective of RL is to find -thanks to a reinforcement signal- an optimal strategy for solving a dynamical control problem. Here we sudy the continuous time, con tinuous state-space stochastic case, which covers a wide variety of control problems including target, viability, optimization problems (see [FS93], [KP95])}or which a formalism is the following. The evolution of the current state x(t) E 0 (the state space, with 0 open subset of IRd), depends on the control u(t) E U (compact subset) by a stochastic differential equation,",Reinforcement Learning,186a157b2992e7daed3677ce8e9fe40f-Paper.pdf,1997
Intrusion Detection with Neural Networks,"With the rapid expansion of computer networks during the past few years, security has become a crucial issue for modern computer systems. A good way to detect illegitimate use is through monitoring unusual user activity. Methods of intrusion detection based on hand-coded rule sets or predicting commands on-line are laborous to build or not very reliable. This paper proposes a new way of applying neural networks to detect intrusions. We believe that a user leaves a 'print' when using the system; a neural network can be used to learn this print and identify each user much like detectives use thumbprints to place people at crime scenes. If a user's behavior does not match hislher print, the system administrator can be alerted of a possible security breech. A backpropagation neural network called NNID (Neural Network Intrusion Detector) was trained in the identification task and tested experimentally on a system of 10 users. The system was 96% accurate in detecting unusual activity, with 7",Computer Vision,1abb1e1ea5f481b589da52303b091cbb-Paper.pdf,1997
Incorporating Contextual Information in White,"In this paper we propose a technique to incorporate contextual informa tion into object classification. In the real world there are cases where the identity of an object is ambiguous due to the noise in the measurements based on which the classification should be made. It is helpful to re duce the ambiguity by utilizing extra information referred to as context, which in our case is the identities of the accompanying objects. This technique is applied to white blood cell classification. Comparisons are made against ""no context"" approach, which demonstrates the superior classification performance achieved by using context. In our particular application, it significantly reduces false alarm rate and thus greatly re duces the cost due to expensive clinical tests. • Author for correspondence. Incorporating Contextual Information in White Blood Cell Identification 951 1 INTRODUCTION One of the most common assumptions made in the study of machine learning is that the examples are drawn indepe",Computer Vision,1f3202d820180a39f736f20fce790de8-Paper.pdf,1997
Function Approximat.ion with the,"We present a computationally efficient algorithm for function ap proximation with piecewise linear sigmoidal nodes. A one hidden layer network is constructed one node at a time using the method of fitting the residual. The task of fitting individual nodes is accom plished using a new algorithm that searchs for the best fit by solving a sequence of Quadratic Programming problems. This approach of fers significant advantages over derivative-based search algorithms (e.g. backpropagation and its extensions). Unique characteristics of this algorithm include: finite step convergence, a simple stop ping criterion, a deterministic methodology for seeking ""good"" local minima, good scaling properties and a robust numerical implemen tation. 1 Introduction The learning algorithm developed in this paper is quite different from the tradi tional family of derivative-based descent methods used to train Multilayer Percep trons (MLPs) for function approximation. First, a constructive approach is used, w",Optimization & Theoretical ML,215a71a12769b056c3c32e7299f1c5ed-Paper.pdf,1997
Mapping a manifold of perceptual observations,"Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-space embedding of a set of observations that preserves as closely as possible their intrinsic metric structure - the distances between points on the observation manifold as measured along geodesic paths. Our isometric feature mapping procedure, or isomap, is able to reliably recover low-dimensional nonlinear structure in realistic perceptual data sets, such as a manifold of face images, where conventional global mapping methods find only local minima. The recovered map provides a canonical set of globally meaningful features, which allows perceptual transformations such as interpolation, extrapolation, and analogy - highly nonlinear transformations in the original observation space - to be computed with simple linear operations in feature space. 1 Introduction In psychological or computational research on perceptual categorization, it is generally taken for granted that the perce",Computer Vision,28e209b61a52482a0ae1cb9f5959c792-Paper.pdf,1997
Correlates of Attention in a Model of,"Given a set of objects in the visual field, how does the the visual system learn to attend to a particular object of interest while ignoring the rest? How are occlusions and background clutter so effortlessly discounted for when rec ognizing a familiar object? In this paper, we attempt to answer these ques tions in the context of a Kalman filter-based model of visual recognition that has previously proved useful in explaining certain neurophysiological phe nomena such as endstopping and related extra-classical receptive field ef fects in the visual cortex. By using results from the field of robust statistics, we describe an extension of the Kalman filter model that can handle multiple objects in the visual field. The resulting robust Kalman filter model demon strates how certain forms of attention can be viewed as an emergent prop erty of the interaction between top-down expectations and bottom-up sig nals. The model also suggests functional interpretations of certain attention related",Computer Vision,23ad3e314e2a2b43b4c720507cec0723-Paper.pdf,1997
The Rectified Gaussian Distribution,"A simple but powerful modification of the standard Gaussian dis tribution is studied. The variables of the rectified Gaussian are constrained to be nonnegative, enabling the use of nonconvex en ergy functions. Two multimodal examples, the competitive and cooperative distributions, illustrate the representational power of the rectified Gaussian. Since the cooperative distribution can rep resent the translations of a pattern, it demonstrates the potential of the rectified Gaussian for modeling pattern manifolds. 1 INTRODUCTION The rectified Gaussian distribution is a modification of the standard Gaussian in which the variables are constrained to be nonnegative. This simple modification brings increased representational power, as illustrated by two multimodal examples of the rectified Gaussian, the competitive and the cooperative distributions. The modes of the competitive distribution are well-separated by regions of low probabil ity. The modes of the cooperative distribution are closely",Optimization & Theoretical ML,28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf,1997
"RCC Cannot Compute Certain FSA,","Existing proofs demonstrating the computational limitations of Re current Cascade Correlation and similar networks (Fahlman, 1991; Bachrach, 1988; Mozer, 1988) explicitly limit their results to units having sigmoidal or hard-threshold transfer functions (Giles et aI., 1995; and Kremer, 1996). The proof given here shows that for any finite, discrete transfer function used by the units of an RCC network, there are finite-state automata (FSA) that the network cannot model, no matter how many units are used. The proof also applies to continuous transfer functions with a finite number of fixed-points, such as sigmoid and radial-basis functions. 1 Introduction The Recurrent Cascade Correlation (RCC) network was proposed by Fahlman (1991) to offer a fast and efficient alternative to fully connected recurrent networks. The network is arranged such that each unit has only a single recurrent connection: the connection that goes from itself to itself. Networks with the same structure have been pr",Computer Vision,299fb2142d7de959380f91c01c3a293c-Paper.pdf,1997
L e a r n i n g G e n e r a t i v e M o d e l s w i t h t h e,No abstract found,Optimization & Theoretical ML,29c4a0e4ef7d1969a94a5f4aadd20690-Paper.pdf,1997
Local Dimensionality Reduction,"If globally high dimensional data has locally only low dimensional distribu tions, it is advantageous to perform a local dimensionality reduction before further processing the data. In this paper we examine several techniques for local dimensionality reduction in the context of locally weighted linear re gression. As possible candidates, we derive local versions of factor analysis regression, principle component regression, principle component regression on joint distributions, and partial least squares regression. After outlining the statistical bases of these methods, we perform Monte Carlo simulations to evaluate their robustness with respect to violations of their statistical as sumptions. One surprising outcome is that locally weighted partial least squares regression offers the best average results, thus outperforming even factor analysis, the theoretically most appealing of our candidate techniques. 1 INTRODUCTION Regression tasks involve mapping a n-dimensional continuous input",Optimization & Theoretical ML,2bd7f907b7f5b6bbd91822c0c7b835f6-Paper.pdf,1997
Adaptation in Speech Motor Control,"Human subjects are known to adapt their motor behavior to a shift of the visual field brought about by wearing prism glasses over their eyes. We have studied the analog of this effect in speech. Us ing a device that can feed back transformed speech signals in real time, we exposed subjects to alterations of their own speech feedback. We found that speakers learn to adjust their production of a vowel to compensate for feedback alterations that change the vowel's perceived phonetic identity; moreover, the effect generalizes across consonant contexts and to different vowels. 1 INTRODUCTION For more than a century, it has been know that humans will adapt their reaches to altered visual feedback [8]. One of the most studied examples of this adaptation is prism adaptation, which is seen when a subject reaches to targets while wearing image-shifting prism glasses [2]. Initially, the subject misses the targets, but he soon learns to compensate and reach accurately. This compensation is retaine",NLP,2dffbc474aa176b6dc957938c15d0c8b-Paper.pdf,1997
An Incremental Nearest Neighbor,"We consider the general problem of learning multi-category classifi cation from labeled examples. We present experimental results for a nearest neighbor algorithm which actively selects samples from different pattern classes according to a querying rule instead of the a priori class probabilities. The amount of improvement of this query-based approach over the passive batch approach depends on the complexity of the Bayes rule. The principle on which this al gorithm is based is general enough to be used in any learning algo rithm which permits a model-selection criterion and for which the error rate of the classifier is calculable in terms of the complexity of the model. 1 INTRODUCTION We consider the general problem of learning multi-category classification from la beled examples. In many practical learning settings the time or sample size available for training are limited. This may have adverse effects on the accuracy of the result ing classifier. For instance, in learning to recogni",Optimization & Theoretical ML,309928d4b100a5d75adff48a9bfc1ddb-Paper.pdf,1997
Competitive On-Line Linear Regression,"We apply a general algorithm for merging prediction strategies (the Aggregating Algorithm) to the problem of linear regression with the square loss; our main assumption is that the response variable is bounded. It turns out that for this particular problem the Aggre gating Algorithm resembles, but is slightly different from, the well known ridge estimation procedure. From general results about the Aggregating Algorithm we deduce a guaranteed bound on the dif ference between our algorithm's performance and the best, in some sense, linear regression function's performance. We show that the AA attains the optimal constant in our bound, whereas the con stant attained by the ridge regression procedure in general can be 4 times worse. 1 INTRODUCTION The usual approach to regression problems is to assume that the data are gen erated by some stochastic mechanism and make some, typically very restrictive, assumptions about that stochastic mechanism. In recent years, however, a different approac",Optimization & Theoretical ML,30c8e1ca872524fbf7ea5c519ca397ee-Paper.pdf,1997
Nonlinear Markov Networks for Continuous,We address the problem oflearning structure in nonlinear Markov networks with continuous variables. This can be viewed as non-Gaussian multidi mensional density estimation exploiting certain conditional independencies in the variables. Markov networks are a graphical way of describing con ditional independencies well suited to model relationships which do not ex hibit a natural causal ordering. We use neural network structures to model the quantitative relationships between variables. The main focus in this pa per will be on learning the structure for the purpose of gaining insight into the underlying process. Using two data sets we show that interesting struc tures can be found using our approach. Inference will be briefly addressed. 1 Introduction Knowledge about independence or conditional independence between variables is most help ful in ''understanding'' a domain. An intuitive representation of independencies is achieved by graphical models in which independency statements can be,Computer Vision,33ebd5b07dc7e407752fe773eed20635-Paper.pdf,1997
Online learning from finite training sets,"Online learning is one of the most common forms of neural net work training. We present an analysis of online learning from finite training sets for non-linear networks (namely, soft-committee ma chines), advancing the theory to more realistic learning scenarios. Dynamical equations are derived for an appropriate set of order parameters; these are exact in the limiting case of either linear networks or infinite training sets. Preliminary comparisons with simulations suggest that the theory captures some effects of finite training sets, but may not yet account correctly for the presence of local minima. 1 INTRODUCTION The analysis of online gradient descent learning, as one of the most common forms of supervised learning, has recently stimulated a great deal of interest [1, 5, 7, 3]. In online learning, the weights of a network ('student') are updated immediately after presentation of each training example (input-output pair) in order to reduce the error that the network makes on that e",Optimization & Theoretical ML,359f38463d487e9e29bd20e24f0c050a-Paper.pdf,1997
Automated Aircraft Recovery,"Initial experiments described here were directed toward using reinforce ment learning (RL) to develop an automated recovery system (ARS) for high-agility aircraft. An ARS is an outer-loop flight-control system de signed to bring an aircraft from a range of out-of-control states to straight and-level flight in minimum time while satisfying physical and phys iological constraints. Here we report on results for a simple version of the problem involving only single-axis (pitch) simulated recoveries. Through simulated control experience using a medium-fidelity aircraft simulation, the RL system approximates an optimal policy for pitch-stick inputs to produce minimum-time transitions to straight-and-Ievel flight in unconstrained cases while avoiding ground-strike. The RL system was also able to adhere to a pilot-station acceleration constraint while execut ing simulated recoveries. Automated Aircraft Recovery via Reinforcement Learning 1023 1 INTRODUCTION An emerging use of reinforcement lea",Reinforcement Learning,363763e5c3dc3a68b399058c34aecf2c-Paper.pdf,1997
Adaptive choice of grid and time •,"We propose local error estimates together with algorithms for adap tive a-posteriori grid and time refinement in reinforcement learn ing. We consider a deterministic system with continuous state and time with infinite horizon discounted cost functional. For grid re finement we follow the procedure of numerical methods for the Bellman-equation. For time refinement we propose a new criterion, based on consistency estimates of discrete solutions of the Bellman equation. We demonstrate, that an optimal ratio of time to space discretization is crucial for optimal learning rates and accuracy of the approximate optimal value function. 1 Introduction Reinforcement learning can be performed for fully continuous problems by discretiz ing state space and time, and then performing a discrete algorithm like Q-Iearning or RTDP (e.g. [5]). Consistency problems arise if the discretization needs to be refined, e.g. for more accuracy, application of multi-grid iteration or better starting values for the",Reinforcement Learning,372d3f309fef061977fb2f7ba36d74d2-Paper.pdf,1997
Graph Matching with Hierarchical,"Our aim in this paper is to develop a Bayesian framework for match ing hierarchical relational models. The goal is to make discrete la bel assignments so as to optimise a global cost function that draws information concerning the consistency of match from different lev els of the hierarchy. Our Bayesian development naturally distin guishes between intra-level and inter-level constraints. This allows the impact of reassigning a match to be assessed not only at its own (or peer) level ofrepresentation, but also upon its parents and children in the hierarchy. 1 Introd uction Hierarchical graphical structures are of critical importance in the interpretation of sensory or perceptual data. For instance, following the influential work of Marr [6] there has been sustained efforts at effectively organising and processing hierarchical information in vision systems. There are a plethora of concrete examples which in clude pyramidal hierarchies [3] that are concerned with multi-resolution informat",Optimization & Theoretical ML,3bbfdde8842a5c44a0323518eec97cbe-Paper.pdf,1997
Minimax and Hamiltonian Dynamics of,"A Lyapunov function for excitatory-inhibitory networks is constructed. The construction assumes symmetric interactions within excitatory and inhibitory populations of neurons, and antisymmetric interactions be tween populations. The Lyapunov function yields sufficient conditions for the global asymptotic stability of fixed points. If these conditions are violated, limit cycles may be stable. The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by minimax and dissipative Hamiltonian forms of the network dynamics. The dynamics of a neural network with symmetric interactions provably converges to fixed points under very general assumptions[l, 2]. This mathematical result helped to establish the paradigm of neural computation with fixed point attractors[3]. But in reality, interactions between neurons in the brain are asymmetric. Furthermore, the dynamical behaviors seen in the brain are not confined to fixed point attractors, but also include ",Optimization & Theoretical ML,3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf,1997
Hybrid NNIHMM-Based Speech Recognition,"In this paper, we present a novel hybrid architecture for continuous speech recognition systems. It consists of a continuous HMM system extended by an arbitrary neural network that is used as a preprocessor that takes several frames of the feature vector as input to produce more discrimin ative feature vectors with respect to the underlying HMM system. This hybrid system is an extension of a state-of-the-art continuous HMM sys tem, and in fact, it is the first hybrid system that really is capable of outper forming these standard systems with respect to the recognition accuracy. Experimental results show an relative error reduction of about 10% that we achieved on a remarkably good recognition system based on continu ous HMMs for the Resource Management 1O OO-word continuous speech recognition task. 1 INTRODUCTION Standard state-of-the-art speech recognition systems utilize Hidden Markov Models (HMMs) to model the acoustic behavior of basic speech units like phones or words. Most common",NLP,3de2334a314a7a72721f1f74a6cb4cee-Paper.pdf,1997
Zili Liu Daniel Kersten,No abstract found,Optimization & Theoretical ML,3e313b9badf12632cdae5452d20e1af6-Paper.pdf,1997
A N e u r a l N e t w o r k B a s e d,No abstract found,Optimization & Theoretical ML,3ff31b21755de79edf5668a07bd37f81-Paper.pdf,1997
Perturbative M-Sequences for Auditory,"In this paper we present a new method for studying auditory sys tems based on m-sequences. The method allows us to perturba tively study the linear response of the system in the presence of various other stimuli, such as speech or sinusoidal modulations. This allows one to construct linear kernels (receptive fields) at the same time that other stimuli are being presented. Using the method we calculate the modulation transfer function of single units in the inferior colli cui us of the cat at different operating points and discuss nonlinearities in the response. 1 Introduction A popular approach to systems identification, i.e., identifying an accurate analyt ical model for the system behavior, is to use Volterra or Wiener expansions to model behavior via functional Taylor or orthogonal polynomial series, respectively [Marmarelis and Marmarelis1978]. Both approaches model the response r(t) as a linear combination of small powers of the stimulus s(t). Although effective for mild nonlinear",Optimization & Theoretical ML,411ae1bf081d1674ca6091f8c59a266f-Paper.pdf,1997
Bach in a Box - Real-Time Harmony,"We describe a system for learning J. S. Bach's rules of musical har mony. These rules are learned from examples and are expressed as rule-based neural networks. The rules are then applied in real time to generate new accompanying harmony for a live performer. Real-time functionality imposes constraints on the learning and harmonizing processes, including limitations on the types of infor mation the system can use as input and the amount of processing the system can perform. We demonstrate algorithms for gener ating and refining musical rules from examples which meet these constraints. We describe a method for including a priori knowl edge into the rules which yields significant performance gains. We then describe techniques for applying these rules to generate new music in real-time. We conclude the paper with an analysis of experimental results. 1 Introduction The goal of this research is the development of a system to learn musical rules from examples of J.S. Bach's music, and then t",Computer Vision,42ffcf057e133f94c1b7b5cf543ef3bd-Paper.pdf,1997
Reinforcement Learning for Continuous,"This paper is concerned with the problem of Reinforcement Learn ing (RL) for continuous state space and time stocha.stic control problems. We state the Harnilton-Jacobi-Bellman equation satis fied by the value function and use a Finite-Difference method for designing a convergent approximation scheme. Then we propose a RL algorithm based on this scheme and prove its convergence to the optimal solution. 1 Introduction to RL in the continuous, stochastic case The objective of RL is to find -thanks to a reinforcement signal- an optimal strategy for solving a dynamical control problem. Here we sudy the continuous time, con tinuous state-space stochastic case, which covers a wide variety of control problems including target, viability, optimization problems (see [FS93], [KP95])}or which a formalism is the following. The evolution of the current state x(t) E 0 (the state space, with 0 open subset of IRd), depends on the control u(t) E U (compact subset) by a stochastic differential equation,",Reinforcement Learning,46771d1f432b42343f56f791422a4991-Paper.pdf,1997
Learning nonlinear overcomplete,"We derive a learning algorithm for inferring an overcomplete basis by viewing it as probabilistic model of the observed data. Over complete bases allow for better approximation of the underlying statistical density. Using a Laplacian prior on the basis coefficients removes redundancy and leads to representations that are sparse and are a nonlinear function of the data. This can be viewed as a generalization of the technique of independent component anal ysis and provides a method for blind source separation of fewer mixtures than sources. We demonstrate the utility of overcom plete representations on natural speech and show that compared to the traditional Fourier basis the inferred representations poten tially have much greater coding efficiency. A traditional way to represent real-values signals is with Fourier or wavelet bases. A disadvantage of these bases, however, is that they are not specialized for any particular dataset. Principal component analysis (PCA) provides one means fo",Optimization & Theoretical ML,489d0396e6826eb0c1e611d82ca8b215-Paper.pdf,1997
Receptive field formation in natural scene,"We study several statistically and biologically motivated learning rules using the same visual environment, one made up of natural scenes, and the same single cell neuronal architecture. This allows us to concentrate on the feature extraction and neuronal coding properties of these rules. Included in these rules are kurtosis and skewness maximization, the quadratic form of the BCM learning rule, and single cell ICA. Using a structure removal method, we demonstrate that receptive fields developed using these rules de pend on a small portion of the distribution. We find that the quadratic form of the BCM rule behaves in a manner similar to a kurtosis maximization rule when the distribution contains kurtotic directions, although the BCM modification equations are compu tationally simpler. 424 B. S. Blais, N. Intrator, H. Shouval and L N. Cooper 1 INTRODUCTION Recently several learning rules that develop simple cell-like receptive fields in a natural image environment have been proposed (L",Computer Vision,4dcae38ee11d3a6606cc6cd636a3628b-Paper.pdf,1997
Hippocampal Model of Rat Spatial Abilities,"We provide a model of the standard watermaze task, and of a more challenging task involving novel platform locations, in which rats exhibit one-trial learning after a few days of training. The model uses hippocampal place cells to support reinforcement learning, and also, in an integrated manner, to build and use allocentric coordinates. 1 INTRODUCTION Whilst it has long been known both that the hippocampus of the rat is needed for normal performance on spatial tasksl3, 11 and that certain cells in the hippocampus exhibit place-related firing,12 it has not been clear how place cells are actually used for navigation. One of the principal conceptual problems has been understanding how the hippocampus could specify or learn paths to goals when spatially tuned cells in the hippocampus respond only on the basis of the rat's current location. This work uses recent ideas from reinforcement learning to solve this problem in the context of two rodent spatial learning results. Reference memory i",NLP,4e0d67e54ad6626e957d15b08ae128a6-Paper.pdf,1997
The Error Coding and Substitution PaCTs,"A new class of plug in classification techniques have recently been de veloped in the statistics and machine learning literature. A plug in clas sification technique (PaCT) is a method that takes a standard classifier (such as LDA or TREES) and plugs it into an algorithm to produce a new classifier. The standard classifier is known as the Plug in Classi fier (PiC). These methods often produce large improvements over using a single classifier. In this paper we investigate one of these methods and give some motivation for its success. 1 Introduction Dietterich and Bakiri (1995) suggested the following method, motivated by Error Correct ing Coding Theory, for solving k class classification problems using binary classifiers. • Produce a k by B (B large) binary coding matrix, ie a matrix of zeros and ones. We will denote this matrix by Z, its i, jth component by Zij, its ith row by Zi and zj. its j th column by • Use the first column of the coding matrix (Zl) to create two super groups by a",Optimization & Theoretical ML,4e8412ad48562e3c9934f45c3e144d48-Paper.pdf,1997
Modeling Complex Cells in an A wake,"We model the responses of cells in visual area VI during natural vision. Our model consists of a classical energy mechanism whose output is divided by nonclassical gain control and texture contrast mechanisms. We apply this model to review movies, a stimulus sequence that replicates the stimulation a cell receives during free viewing of natural images. Data were collected from three cells using five different review movies, and the model was fit separately to the data from each movie. For the energy mechanism alone we find modest but significant correlations (rE = 0.41, 0.43, 0.59, 0.35) between model and data. These correlations are improved somewhat when we allow for suppressive surround effects (rE+G = 0.42, 0.56, 0.60, 0.37). In one case the inclusion of a delayed suppressive surround dramatically improves the fit to the data by modifying the time course of the model's response. 1 INTRODUCTION Complex cells in the primary visual cortex (area VI in primates) are tuned to localized v",Computer Vision,4edaa105d5f53590338791951e38c3ad-Paper.pdf,1997
Generalization in decision trees and DNF:,"Recent theoretical results for pattern classification with thresh olded real-valued functions (such as support vector machines, sig moid networks, and boosting) give bounds on misclassification probability that do not depend on the size of the classifier, and hence can be considerably smaller than the bounds that follow from the VC theory. In this paper, we show that these techniques can be more widely applied, by representing other boolean functions as two-layer neural networks (thresholded convex combinations of boolean functions). For example, we show that with high probabil ity any decision tree of depth no more than d that is consistent with m training examples has misclassification probability no more than o ( (~ (Neff VCdim(U) log2 m log d)) 1/2), where U is the class of node decision functions, and Neff ::; N can be thought of as the effective number of leaves (it becomes small as the distribution on the leaves induced by the training data gets far from uniform). This bound is ",Optimization & Theoretical ML,4f87658ef0de194413056248a00ce009-Paper.pdf,1997
Local Dimensionality Reduction,"If globally high dimensional data has locally only low dimensional distribu tions, it is advantageous to perform a local dimensionality reduction before further processing the data. In this paper we examine several techniques for local dimensionality reduction in the context of locally weighted linear re gression. As possible candidates, we derive local versions of factor analysis regression, principle component regression, principle component regression on joint distributions, and partial least squares regression. After outlining the statistical bases of these methods, we perform Monte Carlo simulations to evaluate their robustness with respect to violations of their statistical as sumptions. One surprising outcome is that locally weighted partial least squares regression offers the best average results, thus outperforming even factor analysis, the theoretically most appealing of our candidate techniques. 1 INTRODUCTION Regression tasks involve mapping a n-dimensional continuous input",Optimization & Theoretical ML,4fa7c62536118cc404dec4a0ca88d4f6-Paper.pdf,1997
A mathematical model of axon guidance by,"In the developing nervous system, gradients of target-derived dif fusible factors play an important role in guiding axons to appro priate targets. In this paper, the shape that such a gradient might have is calculated as a function of distance from the target and the time since the start of factor production. Using estimates of the relevant parameter values from the experimental literature, the spatiotemporal domain in which a growth cone could detect such a gradient is derived. For large times, a value for the maximum guidance range of about 1 mm is obtained. This value fits well with experimental data. For smaller times, the analysis predicts that guidance over longer ranges may be possible. This prediction remains to be tested. 1 Introduction In the developing nervous system, growing axons are guided to targets that may be some distance away. Several mechanisms contribute to this (reviewed in Tessier Lavigne & Goodman (1996». One such mechanism is the diffusion of a factor from the ",Optimization & Theoretical ML,512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf,1997
Asymptotic Theory for Regularization:,"The generalization ability of a neural network can sometimes be improved dramatically by regularization. To analyze the improve ment one needs more refined results than the asymptotic distri bution of the weight vector. Here we study the simple case of one-dimensional linear regression under quadratic regularization, i.e., ridge regression. We study the random design, misspecified case, where we derive expansions for the optimal regularization pa rameter and the ensuing improvement. It is possible to construct examples where it is best to use no regularization. 1 INTRODUCTION Suppose that we have available training data (Xl, Yd, .. 0' (Xn' Yn) consisting of pairs of vectors, and we try to predict Yi on the basis of Xi with a neural network with weight vector w. One popular way of selecting w is by the criterion -1 Ln (1) £(Xi' Yi, w) + >..Q(w) = min!, n I Ily - where the loss £(x,y,w) is, e.g., the squared error g(x,w)112, the function g(., w) is the input/output function of the neural",Optimization & Theoretical ML,536a76f94cf7535158f66cfbd4b113b6-Paper.pdf,1997
Neural Basis of Object-Centered,"We present a neural model that can perform eye movements to a particular side of an object regardless of the position and orienta tion of the object in space, a generalization of a task which has been recently used by Olson and Gettner [4] to investigate the neu ral structure of object-centered representations. Our model uses an intermediate representation in which units have oculocentric recep tive fields-just like collicular neurons-whose gain is modulated by the side of the object to which the movement is directed, as well as the orientation of the object. We show that these gain modulations are consistent with Olson and Gettner's single cell recordings in the supplementary eye field. This demonstrates that it is possible to perform an object-centered task without a representation involv ing an object-centered map, viz., without neurons whose receptive fields are defined in object-centered coordinates. We also show that the same approach can account for object-centered neglect, a si",Computer Vision,540ae6b0f6ac6e155062f3dd4f0b2b01-Paper.pdf,1997
Modelling Seasonality and Trends in Daily,"This paper presents a new approach to the problem of modelling daily rainfall using neural networks. We first model the conditional distribu tions of rainfall amounts, in such a way that the model itself determines the order of the process, and the time-dependent shape and scale of the conditional distributions. After integrating over particular weather pat terns, we are able to extract seasonal variations and long-term trends. 1 Introduction Analysis of rainfall data is important for many agricultural, ecological and engineering activities. Design of irrigation and drainage systems, for instance, needs to take account not only of mean expected rainfall, but also of rainfall volatility. In agricultural planning, changes in the annual cycle, e.g. advances in the onset of winter rain, are significant in determining the optimum time for planting crops. Estimates of crop yields also depend on the distribution of rainfall during the growing season, as well as on the overall amount. Such pro",Computer Vision,56352739f59643540a3a6e16985f62c7-Paper.pdf,1997
The Storage Capacity,"We study the storage capacity of a fully-connected committee ma chine with a large number K of hidden nodes. The storage capac ity is obtained by analyzing the geometrical structure of the weight space related to the internal representation. By examining the as ymptotic behavior of order parameters in the limit of large K, the storage capacity is found to be proportional to ]{ Jln ]{ up to the Q c leading order. This result satisfies the mathematical bound given by Mitchison and Durbin, whereas the replica-symmetric solution in a conventional Gardner's approach violates this bound. 1 INTRODUCTION Since Gardner's pioneering work on the storage capacity of a single layer perceptron[1], there have been numerous efforts to use the statistical mechanics formulation to study feed-forward neural networks. The storage capacity of multi layer neural networks has been of particular interest, together with the general ization problem. Barkai, Hansel and Kanter[2] studied a parity machine with a T",Optimization & Theoretical ML,56468d5607a5aaf1604ff5e15593b003-Paper.pdf,1997
Structural Risk Minimization for,"The problem of time series prediction is studied within the uniform con vergence framework of Vapnik and Chervonenkis. The dependence in herent in the temporal structure is incorporated into the analysis, thereby generalizing the available theory for memoryless processes. Finite sam ple bounds are calculated in terms of covering numbers of the approxi mating class, and the tradeoff between approximation and estimation is discussed. A complexity regularization approach is outlined, based on Vapnik's method of Structural Risk Minimization, and shown to be ap plicable in the context of mixing stochastic processes. 1 Time Series Prediction and Mixing Processes A great deal of effort has been expended in recent years on the problem of deriving robust distribution-free error bounds for learning, mainly in the context of memory less processes (e.g. [9]). On the other hand, an extensive amount of work has been devoted by statisticians and econometricians to the study of parametric (often linea",Optimization & Theoretical ML,571d3a9420bfd9219f65b643d0003bf4-Paper.pdf,1997
Selecting weighting factors in logarithmic,"A simple linear averaging of the outputs of several networks as e.g. in bagging [3], seems to follow naturally from a bias/variance decomposition of the sum-squared error. The sum-squared error of the average model is a quadratic function of the weighting factors assigned to the networks in the ensemble [7], suggesting a quadratic programming algorithm for finding the ""optimal"" weighting factors. If we interpret the output of a network as a probability statement, the sum-squared error corresponds to minus the loglikelihood or the Kullback-Leibler divergence, and linear averaging of the out puts to logarithmic averaging of the probability statements: the logarithmic opinion pool. The crux of this paper is that this whole story about model aver aging, bias/variance decompositions, and quadratic programming to find the optimal weighting factors, is not specific for the sum squared error, but applies to the combination of probability state ments of any kind in a logarithmic opinion pool, a",Optimization & Theoretical ML,59f51fd6937412b7e56ded1ea2470c25-Paper.pdf,1997
Reinforcement Learning with,"We present a new approach to reinforcement learning in which the poli cies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learn ing and ""behavior-based"" or ""teleo-reactive"" approaches to control. We present provably convergent algorithms for problem-solving and learn ing with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states. 1 Introduction Optimal decision making in virtually all spheres of human activity is rendered intractable by the complexity of the task environment. Generally speaking, the only way around in tractability has been to provide a hierarchical organization for",Reinforcement Learning,5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf,1997
Multiplicative Updating Rule,"For blind source separation, when the Fisher information matrix is used as the Riemannian metric tensor for the parameter space, the steepest descent algorithm to maximize the likelihood function in this Riemannian parameter space becomes the serial updating rule with equivariant property. This algorithm can be further simplified by using the asymptotic form of the Fisher information matrix around the equilibrium. 1 Introduction The relative gradient was introduced by (Cardoso and Laheld, 1996) to design multiplicative updating algorithms with equivariant property for blind separation problems. The idea is to calculate differentials by using a relative increment instead of an absolute increment in the parameter space. This idea has been extended to compute the relative Hessian by (Pham, 1996). For a matrix function f = f (W), the relative gradient is defined by ::V Vf= WT. (1) From the differential of f (W) based on the relative gradient, the following learning rule is given by (Cardos",Optimization & Theoretical ML,5e1b18c4c6a6d31695acbae3fd70ecc6-Paper.pdf,1997
On the Separation of Signals from,"We discuss a solution to the problem of separating waveforms pro duced by multiple cells in an extracellular neural recording. We take an explicitly probabilistic approach, using latent-variable mod els of varying sophistication to describe the distribution of wave forms produced by a single cell. The models range from a single Gaussian distribution of waveforms for each cell to a mixture of hidden Markov models. We stress the overall statistical structure of the approach, allowing the details of the generative model chosen to depend on the specific neural preparation. 1 INTRODUCTION Much of our empirical understanding of the systems-level functioning of the brain has come from a procedure called extracellular recording. The electrophysiologist inserts an insulated electrode with exposed tip into the extracellular space near one or more neuron cell bodies. Transient currents due to action potentials across nearby cell membranes are then recorded as deflections in potential, spikes, at ",Computer Vision,5e76bef6e019b2541ff53db39f407a98-Paper.pdf,1997
Task and Spatial Frequency Effects on Face,"There is strong evidence that face processing is localized in the brain. The double dissociation between prosopagnosia, a face recognition deficit occurring after brain damage, and visual object agnosia, difficulty recognizing otber kinds of complex objects, indicates tbat face and non face object recognition may be served by partially independent mecha nisms in the brain. Is neural specialization innate or learned? We sug gest that this specialization could be tbe result of a competitive learn ing mechanism that, during development, devotes neural resources to the tasks they are best at performing. Furtber, we suggest that the specializa tion arises as an interaction between task requirements and developmen tal constraints. In this paper, we present a feed-forward computational model of visual processing, in which two modules compete to classify input stimuli. When one module receives low spatial frequency infor mation and the other receives high spatial frequency information, and the",Computer Vision,602d1305678a8d5fdb372271e980da6a-Paper.pdf,1997
Extended leA Removes Artifacts from,"Severe contamination of electroencephalographic (EEG) activity by eye movements, blinks, muscle, heart and line noise is a serious problem for EEG interpretation and analysis. Rejecting contami nated EEG segments results in a considerable loss of information and may be impractical for clinical data. Many methods have been proposed to remove eye movement and blink artifacts from EEG recordings. Often regression in the time or frequency domain is performed on simultaneous EEG and electrooculographic (EOG) recordings to derive parameters characterizing the appearance and spread of EOG artifacts in the EEG channels. However, EOG records also contain brain signals [1, 2], so regressing out EOG ac tivity inevitably involves subtracting a portion of the relevant EEG signal from each recording as well. Regression cannot be used to remove muscle noise or line noise, since these have no reference channels. Here, we propose a new and generally applicable method for removing a wide variety of arti",Computer Vision,674bfc5f6b72706fb769f5e93667bd23-Paper.pdf,1997
Instabilities in Eye Movement Control: A Model,"Nystagmus is a pattern of eye movement characterized by smooth rota tions of the eye in one direction and rapid rotations in the opposite di rection that reset eye position. Periodic alternating nystagmus (PAN) is a form of uncontrollable nystagmus that has been described as an un stable but amplitude-limited oscillation. PAN has been observed previ ously only in subjects with vestibulo-cerebellar damage. We describe results in which PAN can be produced in normal subjects by prolonged rotation in darkness. We propose a new model in which the neural cir cuits that control eye movement are inherently unstable, but this insta bility is kept in check under normal circumstances by the cerebellum. Circumstances which alter this cerebellar restraint, such as vestibulo cerebellar damage or plasticity due to rotation in darkness, can lead to PAN. 1 INTRODUCTION Visual perception involves not only an operating visual sensory system, but also the abil ity to control eye movements. The oculomotor ",Optimization & Theoretical ML,54072f485cdb7897ebbcaf7525139561-Paper.pdf,1997
Blind Separation of Radio Signals •,"We apply information maximization / maximum likelihood blind source separation [2, 6) to complex valued signals mixed with com plex valued nonstationary matrices. This case arises in radio com munications with baseband signals. We incorporate known source signal distributions in the adaptation, thus making the algorithms less ""blind"". This results in drastic reduction of the amount of data needed for successful convergence. Adaptation to rapidly changing signal mixing conditions, such as to fading in mobile communica tions, becomes now feasible as demonstrated by simulations. 1 Introduction In SDMA (spatial division multiple access) the purpose is to separate radio signals of interfering users (either intentional or accidental) from each others on the basis of the spatial characteristics of the signals using smart antennas, array processing, and beamforming [5, 8). Supervised methods typically use a variant of LMS (least mean squares), either gradient based, or algebraic, to adapt the ",Optimization & Theoretical ML,6a5889bb0190d0211a991f47bb19a777-Paper.pdf,1997
Radial Basis Functions:,"Bayesian methods have been successfully applied to regression and classification problems in multi-layer perceptrons. We present a novel application of Bayesian techniques to Radial Basis Function networks by developing a Gaussian approximation to the posterior distribution which, for fixed basis function widths, is analytic in the parameters. The setting of regularization constants by cross validation is wasteful as only a single optimal parameter estimate is retained. We treat this issue by assigning prior distributions to these constants, which are then adapted in light of the data under a simple re-estimation formula. 1 Introduction Radial Basis Function networks are popular regression and classification tools[lO]. For fixed basis function centers, RBFs are linear in their parameters and can there fore be trained with simple one shot linear algebra techniques[lO]. The use of unsupervised techniques to fix the basis function centers is, however, not generally optimal since setting t",Optimization & Theoretical ML,6786f3c62fbf9021694f6e51cc07fe3c-Paper.pdf,1997
Computing with Action Potentials,"Most computational engineering based loosely on biology uses contin uous variables to represent neural activity. Yet most neurons communi cate with action potentials. The engineering view is equivalent to using a rate-code for representing information and for computing. An increas ing number of examples are being discovered in which biology may not be using rate codes. Information can be represented using the timing of action potentials, and efficiently computed with in this representation. The ""analog match"" problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an un derlying rhythm. By using adapting units to effect a fundamental change of representation of a problem, we map the recognition of words (hav ing uniform time-warp) in connected speech into the same analog match problem. We describe the architecture and preliminary results of such a recognition system. Using the fast events of biology in conjunction with an un",Computer Vision,6c1da886822c67822bcf3679d04369fa-Paper.pdf,1997
New Approximations of Differential,"We derive a first-order approximation of the density of maximum entropy for a continuous 1-D random variable, given a number of simple constraints. This results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edgeworth. Using this approximation of density, an approximation of 1-D differential entropy is derived. The approximation of entropy is both more exact and more ro bust against outliers than the classical approximation based on the polynomial density expansions, without being computationally more expensive. The approximation has applications, for example, in independent component analysis and projection pursuit. 1 Introduction The basic information-theoretic quantity for continuous one-dimensional random variables is differential entropy. The differential entropy H of a scalar random variable X with density f(x) is defined as H(X) = - / f(x) log f(x)dx. (1) The 1-D differential entropy, henceforth called simply ",Optimization & Theoretical ML,6d9c547cf146054a5a720606a7694467-Paper.pdf,1997
Independent Component Analysis for,"We have studied the application of an independent component analysis (ICA) approach to the identification and possible removal of artifacts from a magnetoencephalographic (MEG) recording. This statistical tech nique separates components according to the kurtosis of their amplitude distributions over time, thus distinguishing between strictly periodical signals, and regularly and irregularly occurring signals. Many artifacts belong to the last category. In order to assess the effectiveness of the method, controlled artifacts were produced, which included saccadic eye movements and blinks, increased muscular tension due to biting and the presence of a digital watch inside the magnetically shielded room. The results demonstrate the capability of the method to identify and clearly isolate the produced artifacts. 1 Introduction When using a magnetoencephalographic (MEG) record, as a research or clinical tool, the investigator may face a problem of extracting the essential features of the ne",Optimization & Theoretical ML,6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf,1997
Classification by Pairwise Coupling,"We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then cou pling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the na ture of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: applica tion to support vector machines is also briefly described. 1 Introduction We consider the discrimination problem with J{ classes and N training observations. = The training observations consist of predictor measurements x (Xl, X2, ... Xp) on p predictors and the known class memberships. Our goal is to predict the class membership of an observation with predictor vector Xo Typically J{ -class classification rules tend to be easier to learn for J{ = 2 than for f{ > 2 - only one decision boundary requires attention. Friedman (199",NLP,70feb62b69f16e0238f741fab228fec2-Paper.pdf,1997
Synaptic Transmission: An,"Here we analyze synaptic transmission from an infonnation-theoretic perspective. We derive c1osed-fonn expressions for the lower-bounds on the capacity of a simple model of a cortical synapse under two explicit coding paradigms. Under the ""signal estimation"" paradigm, we assume the signal to be encoded in the mean firing rate of a Poisson neuron. The perfonnance of an optimal linear estimator of the signal then provides a lower bound on the capacity for signal estimation. Under the ""signal detection"" paradigm, the presence or absence of the signal has to be de tected. Perfonnance of the optimal spike detector allows us to compute a lower bound on the capacity for signal detection. We find that single synapses (for empirically measured parameter values) transmit infonna tion poorly but significant improvement can be achieved with a small amount of redundancy. 1 Introduction Tools from estimation and infonnation theory have recently been applied by researchers (Bialek et. ai, 1991) to qu",Optimization & Theoretical ML,78b9cab19959e4af8ff46156ee460c74-Paper.pdf,1997
Hybrid reinforcement learning and its,"A learning system composed of linear control modules, reinforce ment learning modules and selection modules (a hybrid reinforce ment learning system) is proposed for the fast learning of real-world control problems. The selection modules choose one appropriate control module dependent on the state. This hybrid learning sys tem was applied to the control of a stilt-type biped robot. It learned the control on a sloped floor more quickly than the usual reinforce ment learning because it did not need to learn the control on a flat floor, where the linear control module can control the robot. When it was trained by a 2-step learning (during the first learning step, the selection module was trained by a training procedure con trolled only by the linear controller), it learned the control more quickly. The average number of trials (about 50) is so small that the learning system is applicable to real robot control. 1 Introduction Reinforcement learning has the ability to solve general control ",Reinforcement Learning,7895fc13088ee37f511913bac71fa66f-Paper.pdf,1997
Agnostic Classification of Markovian,"Classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications. We propose a new information theoretic approach to this problem which is based on the following ingredients: (i) se quences are similar when they are likely to be generated by the same source; (ii) cross entropies can be estimated via ""universal compres sion""; (iii) Markovian sequences can be asymptotically-optimally merged. With these ingredients we design a method for the classification of discrete sequences whenever they can be compressed. We introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences. 1 Introd uction While the relationship between compression (minimal description) and supervised learning is by now well established, no such connection is generally accepted for the unsupervised case. Unsupervised classification is still largely based on",NLP,79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf,1997
Just One View:,"In macaque inferotemporal cortex (IT), neurons have been found to re spond selectively to complex shapes while showing broad tuning (""in variance"") with respect to stimulus transformations such as translation and scale changes and a limited tuning to rotation in depth. Training monkeys with novel, paperclip-like objects, Logothetis et al. 9 could in vestigate whether these invariance properties are due to experience with exhaustively many transformed instances of an object or if there are mech anisms that allow the cells to show response invariance also to previously unseen instances of that object. They found object-selective cells in an terior IT which exhibited limited invariance to various transformations after training with single object views. While previous models accounted for the tuning of the cells for rotations in depth and for their selectiv ity to a specific object relative to a population of distractor objects,14,1 the model described here attempts to explain in a biologi",Computer Vision,792c7b5aae4a79e78aaeda80516ae2ac-Paper.pdf,1997
Visual Navigation in a Robot using,"We implement a model of obstacle avoidance in flying insects on a small, monocular robot. The result is a system that is capable of rapid navigation through a dense obstacle field. The key to the system is the use of zigzag behavior to articulate the body during movement. It is shown that this behavior compensates for a parallax blind spot surrounding the focus of expansion nor mally found in systems without parallax behavior. The system models the coop eration of several behaviors: halteres-ocular response (similar to VOR), optomotor response, and the parallax field computation and mapping to motor system. The resulting system is neurally plausible, very simple, and should be easily hosted on a VLSI hardware. 1 INTRODUCTION Srinivasan and Zhang (1993) describe behavioral evidence for two distinct movement detecting systems in bee: (1) A direction selective pathway with low frequency response characteristics serving the optomotor response and (2) A non-direction selective move ment sys",Computer Vision,7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf,1997
Generalized Prioritized Sweeping,"Prioritized sweeping is a model-based reinforcement learning method that attempts to focus an agent's limited computational resources to achieve a good estimate of the value of environment states. To choose ef fectively where to spend a costly planning step, classic prioritized sweep ing uses a simple heuristic to focus computation on the states that are likely to have the largest errors. In this paper, we introduce generalized prioritized sweeping, a principled method for generating such estimates in a representation-specific manner. This allows us to extend prioritized sweeping beyond an explicit, state-based representation to deal with com pact representations that are necessary for dealing with large state spaces. We apply this method for generalized model approximators (such as Bayesian networks), and describe preliminary experiments that compare our approach with classical prioritized sweeping. 1 Introduction In reinforcement learning, there is a tradeoff between spending time ac",Reinforcement Learning,7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf,1997
Multiresolution Tangent Distance for,"The ability to rely on similarity metrics invariant to image transforma tions is an important issue for image classification tasks such as face or character recognition. We analyze an invariant metric that has performed well for the latter - the tangent distance - and study its limitations when applied to regular images, showing that the most significant among these (convergence to local minima) can be drastically reduced by computing the distance in a multiresolution setting. This leads to the multi resolution tangent distance, which exhibits significantly higher invariance to im age transformations, and can be easily combined with robust estimation procedures. 1 Introduction Image classification algorithms often rely on distance metrics which are too sensitive to variations in the imaging environment or set up (e.g. the Euclidean and Hamming distances), or on metrics which, even though less sensitive to these variations, are application specific or too expensive from a computational ",Computer Vision,7d6044e95a16761171b130dcb476a43e-Paper.pdf,1997
Effects of Spike Timing Underlying,"In normal vision, the inputs from the two eyes are inte grated into a single percept. When dissimilar images are presented to the two eyes, however, perceptual integra tion gives way to alternation between monocular inputs, a phenomenon called binocular rivalry. Although recent evidence indicates that binocular rivalry involves a mod ulation of neuronal responses in extrastriate cortex, the basic mechanisms responsible for differential processing of con:6.icting and congruent stimuli remain unclear. Using a neural network that models the mammalian early visual system, I demonstrate here that the desynchronized fir ing of cortical-like neurons that first receive inputs from the two eyes results in rivalrous activity patterns at later stages in the visual pathway. By contrast, synchronization of firing among these cells prevents such competition. The temporal coordination of cortical activity and its effects on neural competition emerge naturally from the network connectivity and from it",Computer Vision,7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf,1997
Detection of first and second order motion,"A model of motion detection is presented. The model contains three stages. The first stage is unoriented and is selective for con trast polarities. The next two stages work in parallel. A phase insensitive stage pools across different contrast polarities through a spatiotemporal filter and thus can detect first and second order motion. A phase sensitive stage keeps contrast polarities separate, each of which is filtered through a spatiotemporal filter, and thus only first order motion can be detected. Differential phase sensitiv ity can therefore account for the detection of first and second order motion. Phase insensitive detectors correspond to cortical complex cells, and phase sensitive detectors to simple cells. 1 INTRODUCTION In our environment objects are constantly in motion, and the visual system faces the task of identifying the motion of objects. This task can be subdivided into two components: motion detection and motion integration. In this study we will look at motion dete",Computer Vision,81ca0262c82e712e50c580c032d99b60-Paper.pdf,1997
A Framework for Multiple-Instance Learning,"Multiple-instance learning is a variation on supervised learning, where the task is to learn a concept given positive and negative bags of instances. Each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. A bag is labeled negative only if all the instances in it are negative. We describe a new general framework, called Diverse Density, for solving multiple-instance learning problems. We apply this framework to learn a simple description of a person from a series of images (bags) containing that person, to a stock selection problem, and to the drug activity prediction problem. 1 Introduction One ofthe drawbacks of applying the supervised learning model is that it is not always possible for a teacher to provide labeled examples for training. Multiple-instance learning provides a new way of modeling the teacher's weakness. Instead of receiving a set of instances which are labeled positive or negative, the learne",Optimization & Theoretical ML,82965d4ed8150294d4330ace00821d77-Paper.pdf,1997
Relative Loss Bounds for Multidimensional,"We study on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes. We allow at the final layer transfer functions such as the soft max function that need to consider the linear activations to all the output neurons. We use distance functions of a certain kind in two completely independent roles in deriving and analyzing on-line learning algorithms for such tasks. We use one distance function to define a matching loss function for the (possibly multidimensional) transfer function, which al lows us to generalize earlier results from one-dimensional to multidimen sional outputs. We use another distance function as a tool for measuring progress made by the on-line updates. This shows how previously stud ied algorithms such as gradient descent and exponentiated gradient fit into a common framework. We evaluate the performance of the algo rithms using relative loss bounds that compare the loss of the on-line alg",Optimization & Theoretical ML,83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf,1997
Monotonic Networks,"Monotonicity is a constraint which arises in many application do mains. We present a machine learning model, the monotonic net work, for which monotonicity can be enforced exactly, i.e., by virtue offunctional form. A straightforward method for implementing and training a monotonic network is described. Monotonic networks are proven to be universal approximators of continuous, differen tiable monotonic functions. We apply monotonic networks to a real-world task in corporate bond rating prediction and compare them to other approaches. 1 Introduction Several recent papers in machine learning have emphasized the importance of pri ors and domain-specific knowledge. In their well-known presentation of the bias variance tradeoff (Geman and Bienenstock, 1992)' Geman and Bienenstock conclude by arguing that the crucial issue in learning is the determination of the ""right bi ases"" which constrain the model in the appropriate way given the task at hand. The No-Free-Lunch theorem of Wolpert (Wolp",Optimization & Theoretical ML,83adc9225e4deb67d7ce42d58fe5157c-Paper.pdf,1997
Gradients for retinotectal mapping,"The initial activity-independent formation of a topographic map in the retinotectal system has long been thought to rely on the matching of molecular cues expressed in gradients in the retina and the tectum. However, direct experimental evidence for the existence of such gradients has only emerged since 1995. The new data has provoked the discussion of a new set of models in the ex perimentalliterature. Here, the capabilities of these models are an alyzed, and the gradient shapes they predict in vivo are derived. 1 Introduction During the early development of the visual system in for instance rats, fish and chickens, retinal axons grow across the surface of the optic tectum and establish connections so as to form an ordered map. Although later neural activity refines the map, it is not required to set up the initial topography (for reviews see Udin & Fawcett (1988); Goodhill (1992». A long-standing idea is that the initial topog raphy is formed by matching gradients of receptor express",Optimization & Theoretical ML,856fc81623da2150ba2210ba1b51d241-Paper.pdf,1997
Multi-modular Associative Memory,"Motivated by the findings of modular structure in the association cortex, we study a multi-modular model of associative memory that can successfully store memory patterns with different levels of ac tivity. We show that the segregation of synaptic conductances into intra-modular linear and inter-modular nonlinear ones considerably enhances the network's memory retrieval performance. Compared with the conventional, single-module associative memory network, the multi-modular network has two main advantages: It is less sus ceptible to damage to columnar input, and its response is consistent with the cognitive data pertaining to category specific impairment. 1 Introduction Cortical modules were observed in the somatosensory and visual cortices a few decades ago. These modules differ in their structure and functioning but are likely to be an elementary unit of processing in the mammalian cortex. Within each module the neurons are interconnected. Input and output fibers from and to other cor",Computer Vision,86109d400f0ed29e840b47ed72777c84-Paper.pdf,1997
Analysis of Drifting Dynamics with,"We present a method for the analysis of nonstationary time se ries with multiple operating modes. In particular, it is possible to detect and to model both a switching of the dynamics and a less abrupt, time consuming drift from one mode to another. This is achieved in two steps. First, an unsupervised training method pro vides prediction experts for the inherent dynamical modes. Then, the trained experts are used in a hidden Markov model that allows to model drifts. An application to physiological wake/sleep data demonstrates that analysis and modeling of real-world time series can be improved when the drift paradigm is taken into account. 1 Introduction Modeling dynamical systems through a measured time series is commonly done by reconstructing the state space with time-delay coordinates [10]. The prediction of the time series can then be accomplished by training neural networks [11]. H, how ever, a system operates in multiple modes and the dynamics is drifting or switching, standard",Optimization & Theoretical ML,861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf,1997
The Observer-Observation Dilemma,"We explain how the training data can be separated into clean informa tion and unexplainable noise. Analogous to the data, the neural network is separated into a time invariant structure used for forecasting, and a noisy part. We propose a unified theory connecting the optimization al gorithms for cleaning and learning together with algorithms that control the data noise and the parameter noise. The combined algorithm allows a data-driven local control of the liability of the network parameters and therefore an improvement in generalization. The approach is proven to be very useful at the task of forecasting the German bond market. 1 Introduction: The Observer-Observation Dilemma Human beings believe that they are able to solve a psychological version of the Observer Observation Dilemma. On the one hand, they use their observations to constitute an under standing of the laws of the world, on the other hand, they use this understanding to evaluate the correctness of the incoming pieces o",Optimization & Theoretical ML,86e8f7ab32cfd12577bc2619bc635690-Paper.pdf,1997
A Model of Early Visual Processing,"We propose a model for early visual processing in primates. The model consists of a population of linear spatial filters which inter act through non-linear excitatory and inhibitory pooling. Statisti cal estimation theory is then used to derive human psychophysical thresholds from the responses of the entire population of units. The model is able to reproduce human thresholds for contrast and ori entation discrimination tasks, and to predict contrast thresholds in the presence of masks of varying orientation and spatial frequency. 1 INTRODUCTION A remarkably wide range of human visual thresholds for spatial patterns appears to be determined by the earliest stages of visual processing, namely, orientation- and spatial frequency-tuned visual filters and their interactions [18, 19, 3, 22, 9]. Here we consider the possibility of quantitatively relating arbitrary spatial vision thresholds to a single computational model. The success of such a unified account should reveal the extent to whic",Computer Vision,8b0d268963dd0cfb808aac48a549829f-Paper.pdf,1997
A Simple and Fast Neural Network,"A neural network approach to stereovision is presented based on aliasing effects of simple disparity estimators and a fast coherence detection scheme. Within a single network structure, a dense dis parity map with an associated validation map and, additionally, the fused cyclopean view of the scene are available. The network operations are based on simple, biological plausible circuitry; the algorithm is fully parallel and non-iterative. 1 Introduction Humans experience the three-dimensional world not as it is seen by either their left or right eye, but from a position of a virtual cyclopean eye, located in the middle between the two real eye positions. The different perspectives between the left and right eyes cause slight relative displacements of objects in the two retinal images (disparities), which make a simple superposition of both images without diplopia impossible. Proper fusion of the retinal images into the cyclopean view requires the registration of both images to a common ",Computer Vision,8b0dc65f996f98fd178a9defd0efa077-Paper.pdf,1997
On Parallel Versus Serial Processing:,"A novel neural network model of pre-attention processing in visual search tasks is presented. Using displays of line orientations taken from Wolfe's experiments [1992], we study the hypothesis that the distinction between parallel versus serial processes arises from the availability of global information in the internal representations of the visual scene. The model operates in two phases. First, the visual displays are compressed via principal-component-analysis. Second, the compressed data is processed by a target detector mod ule in order to identify the existence of a target in the display. Our main finding is that targets in displays which were found exper imentally to be processed in parallel can be detected by the sys tem, while targets in experimentally-serial displays cannot. This fundamental difference is explained via variance analysis of the compressed representations, providing a numerical criterion distin guishing parallel from serial displays. Our model yields a mapping ",Computer Vision,8d3369c4c086f236fabf61d614a32818-Paper.pdf,1997
Using Expectation to Guide Processing:,"In many real world tasks, only a small fraction of the available inputs are important at any particular time. This paper presents a method for ascertaining the relevance of inputs by exploiting temporal coherence and predictability. The method pro posed in this paper dynamically allocates relevance to inputs by using expectations of their future values. As a model of the task is learned, the model is simulta neously extended to create task-specific predictions of the future values of inputs. Inputs which are either not relevant, and therefore not accounted for in the model, or those which contain noise, will not be predicted accurately. These inputs can be de-emphasized, and, in turn, a new, improved, model of the task created. The tech niques presented in this paper have yielded significant improvements for the vision-based autonomous control of a land vehicle, vision-based hand tracking in cluttered scenes, and the detection of faults in the etching of semiconductor wafers. 1 Introdu",Computer Vision,8d9a0adb7c204239c9635426f35c9522-Paper.pdf,1997
Features as Sufficient Statistics,"An image is often represented by a set of detected features. We get an enormous compression by representing images in this way. Fur thermore, we get a representation which is little affected by small amounts of noise in the image. However, features are typically chosen in an ad hoc manner. \Ve show how a good set of fea tures can be obtained using sufficient statistics. The idea of sparse data representation naturally arises. We treat the I-dimensional and 2-dimensional signal reconstruction problem to make our ideas concrete. 1 Introduction Consider an image, I, that is the result of a stochastic image-formation process. The process depends on the precise state, f, of an environment. The image, accordingly, contains information about the environmental state f, possibly corrupted by noise. We wish to choose feature vectors ¢leI) derived from the image that summarize this information concerning the environment. We are not otherwise interested in the contents of the image and wish to dis",Computer Vision,8edd72158ccd2a879f79cb2538568fdc-Paper.pdf,1997
Factorizing Multivariate Function Classes,"The mathematical framework for factorizing equivalence classes of multivariate functions is formulated in this paper. Independent component analysis is shown to be a special case of this decompo sition. Using only the local geometric structure of a class repre sentative, we derive an analytic solution for the factorization. We demonstrate the factorization solution with numerical experiments and present a preliminary tie to decorrelation. 1 FORMALISM In independent component analysis (ICA), the goal is to find an unknown linear coordinate system where the joint distribution function admits a factorization into the product of one dimensional functions. However, this decomposition is only rarely possible. To formalize the notion of multivariate function factorization, we begin by defining an equivalence relation. Definition. We say that two functions f, 9 : IRn -t IR are equivalent if there exists A,b and c such that: f(x) = cg(Ax+b), where A is a non-singular matrix and c f. O. Thus, th",Optimization & Theoretical ML,8fb21ee7a2207526da55a679f0332de2-Paper.pdf,1997
An Annealed Self-Organizing Map for Source,"We derive and analyse robust optimization schemes for noisy vector quantization on the basis of deterministic annealing. Starting from a cost function for central clustering that incorporates distortions from channel noise we develop a soft topographic vector quantization al gorithm (STVQ) which is based on the maximum entropy principle and which performs a maximum-likelihood estimate in an expectation maximization (EM) fashion. Annealing in the temperature parameter f3 leads to phase transitions in the existing code vector representation dur ing the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise. A whole family of vector quantization algorithms is derived from STVQ, among them a deterministic annealing scheme for Kohonen's self-organizing map (SOM). This algorithm, which we call SSOM, is then applied to vector quantization of image d",Optimization & Theoretical ML,8fb5f8be2aa9d6c64a04e3ab9f63feee-Paper.pdf,1997
How to Dynamically Merge Markov,"We are frequently called upon to perform multiple tasks that com pete for our attention and resource. Often we know the optimal solution to each task in isolation; in this paper, we describe how this knowledge can be exploited to efficiently find good solutions for doing the tasks in parallel. We formulate this problem as that of dynamically merging multiple Markov decision processes (MDPs) into a composite MDP, and present a new theoretically-sound dy namic programming algorithm for finding an optimal policy for the composite MDP. We analyze various aspects of our algorithm and illustrate its use on a simple merging problem. Every day, we are faced with the problem of doing mUltiple tasks in parallel, each of which competes for our attention and resource. If we are running a job shop, we must decide which machines to allocate to which jobs, and in what order, so that no jobs miss their deadlines. If we are a mail delivery robot, we must find the intended recipients of the mail while s",Optimization & Theoretical ML,90db9da4fc5414ab55a9fe495d555c06-Paper.pdf,1997
Refractoriness and Neural Precision,"The relationship between a neuron's refractory period and the precision of its response to identical stimuli was investigated. We constructed a model of a spiking neuron that combines probabilistic firing with a refractory period. For realistic refractoriness, the model closely reproduced both the average firing rate and the response precision of a retinal ganglion cell. The model is based on a ""free"" firing rate, which exists in the absence of refractoriness. This function may be a better description of a spiking neuron's response than the peri-stimulus time histogram. 1 INTRODUCTION The response of neurons to repeated stimuli is intrinsically noisy. In order to take this trial-to-trial variability into account, the response of a spiking neuron is often described by an instantaneous probability for generating an action potential. The response variability of such a model is determined by Poisson counting statistics; in particular, the variance in the spike count is equal to the mean sp",Optimization & Theoretical ML,95151403b0db4f75bfd8da0b393af853-Paper.pdf,1997
Active Data Clustering,"Active data clustering is a novel technique for clustering of proxim ity data which utilizes principles from sequential experiment design in order to interleave data generation and data analysis. The pro posed active data sampling strategy is based on the expected value of information, a concept rooting in statistical decision theory. This is considered to be an important step towards the analysis of large scale data sets, because it offers a way to overcome the inherent data sparseness of proximity data. '''Ie present applications to unsu pervised texture segmentation in computer vision and information retrieval in document databases. 1 Introduction Data clustering is one of the core methods for numerous tasks in pattern recognition, exploratory data analysis, computer vision, machine learning, data mining, and in many other related fields. Concerning the data representation it is important to distinguish between vectorial data and proximity data, cf. [Jain, Dubes, 1988]. In vectorial",Optimization & Theoretical ML,9683cc5f89562ea48e72bb321d9f03fb-Paper.pdf,1997
Coding of Naturalistic Stimuli by,"It is known that humans can make finer discriminations between familiar sounds (e.g. syllables) than between unfamiliar ones (e.g. different noise segments). Here we show that a corresponding en hancement is present in early auditory processing stages. Based on previous work which demonstrated that natural sounds had robust statistical properties that could be quantified, we hypothesize that the auditory system exploits those properties to construct efficient neural codes. To test this hypothesis, we measure the informa tion rate carried by auditory spike trains on narrow-band stimuli whose amplitude modulation has naturalistic characteristics, and compare it to the information rate on stimuli with non-naturalistic modulation. We find that naturalistic inputs significantly enhance the rate of transmitted information, indicating that auditiory neu ral responses are matched to characteristics of natural auditory scenes. 1 Natural Scene Statistics and the Neural Code A primary goal of hea",NLP,9701a1c165dd9420816bfec5edd6c2b1-Paper.pdf,1997
Enhancing Q-Learning for,"This paper enhances the Q-Iearning algorithm for optimal asset alloca tion proposed in (Neuneier, 1996 [6]). The new formulation simplifies the approach by using only one value-function for many assets and al lows model-free policy-iteration. After testing the new algorithm on real data, the possibility of risk management within the framework of Markov decision problems is analyzed. The proposed methods allows the construction of a multi-period portfolio management system which takes into account transaction costs, the risk preferences of the investor, and several constraints on the allocation. 1 Introduction Asset allocation and portfolio management deal with the distribution of capital to various investment opportunities like stocks, bonds, foreign exchanges and others. The aim is to construct a portfolio with a maximal expected return for a given risk level and time horizon while simultaneously obeying institutional or legally required constraints. To find such an optimal portfolio ",Optimization & Theoretical ML,970af30e481057c48f87e101b61e6994-Paper.pdf,1997
Nonparametric Model-Based,"This paper describes some of the interactions of model learning algorithms and planning algorithms we have found in exploring model-based reinforcement learning. The paper focuses on how lo cal trajectory optimizers can be used effectively with learned non parametric models. We find that trajectory planners that are fully consistent with the learned model often have difficulty finding rea sonable plans in the early stages of learning. Trajectory planners that balance obeying the learned model with minimizing cost (or maximizing reward) often do better, even if the plan is not fully consistent with the learned model. 1 INTRODUCTION We are exploring the use of nonparametric models in robot learning (Atkeson et al., 1997b; Atkeson and Schaal, 1997). This paper describes the interaction of model learning algorithms and planning algorithms, focusing on how local trajectory opti mization can be used effectively with nonparametric models in reinforcement learn ing. We find that trajectory opt",Reinforcement Learning,97275a23ca44226c9964043c8462be96-Paper.pdf,1997
On Parallel Versus Serial Processing:,"A novel neural network model of pre-attention processing in visual search tasks is presented. Using displays of line orientations taken from Wolfe's experiments [1992], we study the hypothesis that the distinction between parallel versus serial processes arises from the availability of global information in the internal representations of the visual scene. The model operates in two phases. First, the visual displays are compressed via principal-component-analysis. Second, the compressed data is processed by a target detector mod ule in order to identify the existence of a target in the display. Our main finding is that targets in displays which were found exper imentally to be processed in parallel can be detected by the sys tem, while targets in experimentally-serial displays cannot. This fundamental difference is explained via variance analysis of the compressed representations, providing a numerical criterion distin guishing parallel from serial displays. Our model yields a mapping ",Computer Vision,980ecd059122ce2e50136bda65c25e07-Paper.pdf,1997
A Superadditive-Impairment Theory,"Accounts of neurological disorders often posit damage to a specific functional pathway of the brain. Farah (1990) has proposed an alterna tive class of explanations involving partial damage to multiple path ways. We explore this explanation for optic aphasia, a disorder in which severe perfonnance deficits are observed when patients are asked to name visually presented objects, but surprisingly, performance is rela tively nonnal on naming objects from auditory cues and on gesturing the appropriate use of visually presented objects. We model this highly specific deficit through partial damage to two pathways-one that maps visual input to semantics, and the other that maps semantics to naming responses. The effect of this damage is superadditive, meaning that tasks which require one pathway or the other show little or no perfor mance deficit, but the damage is manifested when a task requires both pathways (i.e., naming visually presented objects). Our model explains other phenomena assoc",Optimization & Theoretical ML,991de292e76f74f3c285b3f6d57958d5-Paper.pdf,1997
S-Map: A network with a simple,"The S-Map is a network with a simple learning algorithm that com bines the self-organization capability of the Self-Organizing Map (SOM) and the probabilistic interpretability of the Generative To pographic Mapping (GTM). The simulations suggest that the S Map algorithm has a stronger tendency to self-organize from ran dom initial configuration than the GTM. The S-Map algorithm can be further simplified to employ pure Hebbian learning, with out changing the qualitative behaviour of the network. 1 Introduction The self-organizing map (SOM; for a review, see [1]) forms a topographic mapping from the data space onto a (usually two-dimensional) output space. The SOM has been succesfully used in a large number of applications [2]; nevertheless, there are some open theoretical questions, as discussed in [1, 3]. Most of these questions arise because of the following two facts: the SOM is not a generative model, i.e. it does not generate a density in the data space, and it does not have a well",Computer Vision,9a1756fd0c741126d7bbd4b692ccbd91-Paper.pdf,1997
Bayesian model of surface perception,"Image intensity variations can result from several different object surface effects, including shading from 3-dimensional relief of the object, or paint on the surface itself. An essential problem in vision, which people solve naturally, is to attribute the proper physical cause, e.g. surface relief or paint, to an observed image. We ad dressed this problem with an approach combining psychophysical and Bayesian computational methods. We assessed human performance on a set of test images, and found that people made fairly consistent judgements of surface properties. Our computational model assigned simple prior probabilities to different relief or paint explanations for an image, and solved for the most probable interpretation in a Bayesian framework. The ratings of the test images by our algorithm compared surprisingly well with the mean ratings of our subjects. 1 Introduction When people study a picture, they can judge whether it depicts a shaded, 3- dimensional surface, or simply a f",Computer Vision,9aa42b31882ec039965f3c4923ce901b-Paper.pdf,1997
Training Methods for Adaptive Boosting,"""Boosting"" is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been ap plied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], and decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 1.5% error on the UCI Letters and 8.1 % error on the UCI satellite data set. 1 Introduction AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emph",Optimization & Theoretical ML,9cb67ffb59554ab1dabb65bcb370ddd9-Paper.pdf,1997
An Analog VLSI Model of the Fly,"Flies are capable of rapidly detecting and integrating visual motion in formation in behaviorly-relevant ways. The first stage of visual motion processing in flies is a retinotopic array of functional units known as el ementary motion detectors (EMDs). Several decades ago, Reichardt and colleagues developed a correlation-based model of motion detection that described the behavior of these neural circuits. We have implemented a variant of this model in a 2.0-JLm analog CMOS VLSI process. The re sult is a low-power, continuous-time analog circuit with integrated pho toreceptors that responds to motion in real time. The responses of the circuit to drifting sinusoidal gratings qualitatively resemble the temporal frequency response, spatial frequency response, and direction selectivity of motion-sensitive neurons observed in insects. In addition to its pos sible engineering applications, the circuit could potentially be used as a building block for constructing hardware models of higher-lev",Computer Vision,a223c6b3710f85df22e9377d6c4f7553-Paper.pdf,1997
Experiences with Bayesian Learning In a,"This paper reports about an application of Bayes' inferred neu ral network classifiers in the field of automatic sleep staging. The reason for using Bayesian learning for this task is two-fold. First, Bayesian inference is known to embody regularization automati cally. Second, a side effect of Bayesian learning leads to larger variance of network outputs in regions without training data. This results in well known moderation effects, which can be used to detect outliers. In a 5 fold cross-validation experiment the full Bayesian solution found with R. Neals hybrid Monte Carlo algo rithm, was not better than a single maximum a-posteriori (MAP) solution found with D.J. MacKay's evidence approximation. In a second experiment we studied the properties of both solutions in rejecting classification of movement artefacts. Experiences with Bayesian Learning in a Real World Application 965 1 Introduction Sleep staging is usually based on rules defined by Rechtschaffen and Kales (see [8]). Rechts",NLP,a424ed4bd3a7d6aea720b86d4a360f75-Paper.pdf,1997
Analytical study of the interplay between,"We study model feed forward networks as time series predictors in the stationary limit. The focus is on complex, yet non-chaotic, behavior. The main question we address is whether the asymptotic behavior is governed by the architecture, regardless the details of the weights. We find hierarchies among classes of architectures with respect to the attract or dimension of the long term sequence they are capable of generating; larger number of hidden units can generate higher dimensional attractors. In the case of a perceptron, we develop the stationary solution for general weights, and show that the flow is typically one dimensional. The relaxation time from an arbitrary initial condition to the stationary solution is found to scale linearly with the size of the network. In multilayer networks, the number of hidden units gives bounds on the number and dimension of the possible attractors. We conclude that long term prediction (in the non-chaotic regime) with such models is governed by attr",Optimization & Theoretical ML,a50abba8132a77191791390c3eb19fe7-Paper.pdf,1997
A Hippocampal Model of Recognition Memory,"A rich body of data exists showing that recollection of specific infor mation makes an important contribution to recognition memory, which is distinct from the contribution of familiarity, and is not adequately cap tured by existing unitary memory models. Furthennore, neuropsycholog ical evidence indicates that recollection is sub served by the hippocampus. We present a model, based largely on known features of hippocampal anatomy and physiology, that accounts for the following key character istics of recollection: 1) false recollection is rare (i.e., participants rarely claim to recollect having studied nonstudied items), and 2) increasing in terference leads to less recollection but apparently does not compromise the quality of recollection (i.e., the extent to which recollected infonna tion veridically reflects events that occurred at study). 1 Introduction For nearly 50 years, memory researchers have known that our ability to remember specific past episodes depends critically on th",Optimization & Theoretical ML,a60937eba57758ed45b6d3e91e8659f3-Paper.pdf,1997
Wavelet Models for Video Time-Series,"In this work, we tackle the problem of time-series modeling of video traffic. Different from the existing methods which model the time series in the time domain, we model the wavelet coefficients in the wavelet domain. The strength of the wavelet model includes (1) a unified approach to model both the long-range and the short-range dependence in the video traffic simultaneously, (2) a computation ally efficient method on developing the model and generating high quality video traffic, and (3) feasibility of performance analysis us ing the model. 1 Introduction As multi-media (compressed Variable Bit Rate (VBR) video, data and voice) traffic is expected to be the main loading component in future communication networks, accurate modeling of the multi-media traffic is crucial to many important appli cations such as video-conferencing and video-on-demand. From modeling stand point, multi-media traffic can be regarded as a time-series, which can in principle be modeled by techniques in time-",Computer Vision,a8f8f60264024dca151f164729b76c0b-Paper.pdf,1997
Multi-time Models for Temporally Abstract,"Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an ap proach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based re inforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on tem porally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and es tablish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical frame work of multi-time models and illustrates their potential advantages in a grid world planning task. The need for hierarchical and abstract",Reinforcement Learning,a9be4c2a4041cadbf9d61ae16dd1389e-Paper.pdf,1997
Comparison of Human and Machine Word,"We present a study which is concerned with word recognition rates for heavily degraded documents. We compare human with machine read ing capabilities in a series of experiments, which explores the interaction of word/non-word recognition, word frequency and legality of non-words with degradation level. We also study the influence of character segmen tation, and compare human performance with that of our artificial neural network model for reading. We found that the proposed computer model uses word context as efficiently as humans, but performs slightly worse on the pure character recognition task. 1 Introduction Optical Character Recognition (OCR) of machine-print document images ·has matured considerably during the last decade. Recognition rates as high as 99.5% have been re ported on good quality documents. However, for lower image resolutions (200 Dpl and below), noisy images, images with blur or skew, the recognition rate declines consider ably. In bad quality documents, character",Computer Vision,ad3019b856147c17e82a5bead782d2a8-Paper.pdf,1997
Linear concepts and hidden variables:,"Some learning techniques for classification tasks work indirectly, by first trying to fit a full probabilistic model to the observed data. Whether this is a good idea or not depends on the robustness with respect to deviations from the postulated model. We study this question experimentally in a restricted, yet non-trivial and interesting case: we consider a conditionally independent attribute (CIA) model which postulates a single binary-valued hidden variable z on which all other attributes (i.e., the target and the observables) depend. In this model, finding the most likely value of anyone variable (given known values for the others) reduces to testing a linear function of the observed values. We learn CIA with two techniques: the standard EM algorithm, and a new algorithm we develop based on covariances. We compare these, in a controlled fashion, against an algorithm (a version of Winnow) that attempts to find a good linear classifier directly. Our conclusions help delimit the fragi",Optimization & Theoretical ML,ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf,1997
Characterizing Neurons in the Primary,"While the understanding of the functional role of different classes of neurons in the awake primary visual cortex has been extensively studied since the time of Hubel and Wiesel (Hubel and Wiesel, 1962), our understanding of the feature selectivity and functional role of neurons in the primary auditory cortex is much farther from com plete. Moving bars have long been recognized as an optimal stimulus for many visual cortical neurons, and this finding has recently been confirmed and extended in detail using reverse correlation methods (Jones and Palmer, 1987; Reid and Alonso, 1995; Reid et al., 1991; llingach et al., 1997). In this study, we recorded from neurons in the primary auditory cortex of the awake primate, and used a novel re verse correlation technique to compute receptive fields (or preferred stimuli), encompassing both multiple frequency components and on going time. These spectrotemporal receptive fields make clear that neurons in the primary auditory cortex, as in the prim",NLP,ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf,1997
Phase transitions and the perceptual,"Estimating motion in scenes containing multiple moving objects remains a difficult problem in computer vision. A promising ap proach to this problem involves using mixture models, where the motion of each object is a component in the mixture. However, ex isting methods typically require specifying in advance the number of components in the mixture, i.e. the number of objects in the scene. • Here we show that the number of objects can be estimated auto matically in a maximum likelihood framework, given an assumption about the level of noise in the video sequence. We derive analytical results showing the number of models which maximize the likeli hood for a given noise level in a given sequence. We illustrate these results on a real video sequence, showing how the phase transitions correspond to different perceptual organizations of the scene. Figure la depicts a scene where motion estimation is difficult for many computer vision systems. A semi-transparent surface partially occludes a s",Computer Vision,af4732711661056eadbf798ba191272a-Paper.pdf,1997
Regression with Input-dependent Noise:,"Gaussian processes provide natural non-parametric prior distribu tions over regression functions. In this paper we consider regression problems where there is noise on the output, and the variance of the noise depends on the inputs. If we assume that the noise is a smooth function of the inputs, then it is natural to model the noise variance using a second Gaussian process, in addition to the Gaussian process governing the noise-free output value. We show that prior uncertainty about the parameters controlling both pro cesses can be handled and that the posterior distribution of the noise rate can be sampled from using Markov chain Monte Carlo methods. Our results on a synthetic data set give a posterior noise variance that well-approximates the true variance. 1 Background and Motivation A very natural approach to regression problems is to place a prior on the kinds of function that we expect, and then after observing the data to obtain a posterior. The prior can be obtained by placing",Optimization & Theoretical ML,afe434653a898da20044041262b3ac74-Paper.pdf,1997
A Generic Approach for Identification of,"We present a novel generic approach to the problem of Event Related Potential identification and classification, based on a competitive N eu ral Net architecture. The network weights converge to the embedded signal patterns, resulting in the formation of a matched filter bank. The network performance is analyzed via a simulation study, exploring identification robustness under low SNR conditions and compared to the expected performance from an information theoretic perspective. The classifier is applied to real event-related potential data recorded during a classic odd-ball type paradigm; for the first time, within session variable signal patterns are automatically identified, dismiss ing the strong and limiting requirement of a-priori stimulus-related selective grouping of the recorded data. 902 D. H. Lange, H. T. Siegelmann, H. Pratt and G. F. Inbar 1 INTRODUCTION 1.1 EVENT RELATED POTENTIALS Ever since Hans Berger's discovery that the electrical activity of the brain can be measured",NLP,b069b3415151fa7217e870017374de7c-Paper.pdf,1997
On the infeasibility of training neural,"We demonstrate that the problem of training neural networks with small (average) squared error is computationally intractable. Con sider a data set of M points (Xi, Yi), i = 1,2, ... , M, where Xi are input vectors from Rd, Yi are real outputs (Yi E R). For a net- work 10 in some class F of neural networks, (11M) L~l (fO(Xi) Yi)2)1/2 - inlfEF(l/ M) ""2:f!1 (f(Xi) - YJ2)1/2 is the (avarage) rel ative error occurs when one tries to fit the data set by 10. We will prove for several classes F of neural networks that achieving a rela tive error smaller than some fixed positive threshold (independent from the size of the data set) is NP-hard. 1 Introduction Given a data set (Xi, Yi), i = 1,2, ... , M. Xi are input vectors from Rd, Yi are real outputs (Yi E R). We call the points (Xi, Yi) data points. The training problem for neural networks is to find a network from some class (usually with fixed number of nodes and layers), which fits the data set with small error. In the following we descri",Optimization & Theoretical ML,b197ffdef2ddc3308584dce7afa3661b-Paper.pdf,1997
Structure Driven Image Database,"A new algorithm is presented which approximates the perceived visual similarity between images. The images are initially trans formed into a feature space which captures visual structure, tex ture and color using a tree of filters. Similarity is the inverse of the distance in this perceptual feature space. Using this algorithm we have constructed an image database system which can perform example based retrieval on large image databases. Using carefully constructed target sets, which limit variation to only a single visual characteristic, retrieval rates are quantitatively compared to those of standard methods. 1 Introduction Without supplementary information, there exists no way to directly measure the similarity between the content of images. In general, one cannot answer a question of the form: ""is image A more like image B or image C?"" without defining the criteria by which this comparison is to be made. People perform such tasks by inferring some criterion, based on their visual e",Computer Vision,b265ce60fe4c5384e622b09eb829b8df-Paper.pdf,1997
Bayesian Robustification for Audio Visual,"We discuss the problem of catastrophic fusion in multimodal recog nition systems. This problem arises in systems that need to fuse different channels in non-stationary environments. Practice shows that when recognition modules within each modality are tested in contexts inconsistent with their assumptions, their influence on the fused product tends to increase, with catastrophic results. We ex plore a principled solution to this problem based upon Bayesian ideas of competitive models and inference robustification: each sensory channel is provided with simple white-noise context mod els, and the perceptual hypothesis and context are jointly esti mated. Consequently, context deviations are interpreted as changes in white noise contamination strength, automatically adjusting the influence of the module. The approach is tested on a fixed lexicon automatic audiovisual speech recognition problem with very good results. 1 Introduction In this paper we address the problem of catastrophic fusio",Computer Vision,b7087c1f4f89e63af8d46f3b20271153-Paper.pdf,1997
A Neural Network Model of Naive Preference,"Filial imprinting in domestic chicks is of interest in psychology, biology, and computational modeling because it exemplifies simple, rapid, in nately programmed learning which is biased toward learning about some objects. Hom et al. have recently discovered a naive visual preference for heads and necks which develops over the course of the first three days of life. The neurological basis of this predisposition is almost en tirely unknown; that of imprinting-related learning is fairly clear. This project is the first model of the predisposition consistent with what is known about learning in imprinting. The model develops the predisposi tion appropriately, learns to ""approach"" a training object, and replicates one interaction between the two processes. Future work will replicate more interactions between imprinting and the predisposition in chicks, and analyze why the system works. 1 Background Filial imprinting iIi domestic chicks is of interest in psychology, biology, and computation",Computer Vision,b8c27b7a1c450ffdacb31483454e0b54-Paper.pdf,1997
Shared Context Probabilistic Transducers,"Recently, a model for supervised learning of probabilistic transduc ers represented by suffix trees was introduced. However, this algo rithm tends to build very large trees, requiring very large amounts of computer memory. In this paper, we propose anew, more com pact, transducer model in which one shares the parameters of distri butions associated to contexts yielding similar conditional output distributions. We illustrate the advantages of the proposed algo rithm with comparative experiments on inducing a noun phrase recogmzer. 1 Introduction Learning algorithms for sequential data modeling are important in many applica tions such as natural language processing and time-series analysis, in which one has to learn a model from one or more sequences of training data. Many of these algo rithms can be cast as weighted transducers (Pereira, Riley and Sproat, 1994), which associate input sequences to output sequences, with weights for each input/output * Yoshua Bengio is also with AT&T Labo",NLP,bad5f33780c42f2588878a9d07405083-Paper.pdf,1997
Learning to Schedule Straight-Line Code,"Program execution speed on modem computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the proces sor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, ob taining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empirical results show that just a few features are ad equate for quite good performance at this task for a real modem processor, and that any of several supervised learning methods perform nearly opti mally with respect to the features used. 1 Introduction Modem computer architectures provide semantics of execution equivalent to sequential exe cution of instructions one at a ti",Optimization & Theoretical ML,bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf,1997
Approximating Posterior Distributions,"Exact inference in densely connected Bayesian networks is computation ally intractable, and so there is considerable interest in developing effec tive approximation schemes. One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution. While this leads to a tractable algorithm, the mean field distribution is as sumed to be factorial and hence unimodal. In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions. We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learn ing in sigmoid belief networks. Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased. 1 Introduction Bayesian belief networks can be regarded as a fully probabilistic interpretation of feed forward neural networks. Maximum likelihood learning for Bayesian networks ",Optimization & Theoretical ML,c0826819636026dd1f3674774f06c51d-Paper.pdf,1997
The Canonical Distortion Measure in Feature,"We prove that the Canonical Distortion Measure (CDM) [2, 3] is the optimal distance measure to use for I nearest-neighbour (l-NN) classifi cation, and show that it reduces to squared Euclidean distance in feature space for function classes that can be expressed as linear combinations of a fixed set of features. PAC-like bounds are given on the sample complexity required to learn the CDM. An experiment is presented in which a neural network CDM was learnt for a Japanese OCR environ ment and then used to do I-NN classification. 1 INTRODUCTION Let X be an input space, P a distribution on X, F a class of functions mapping X into Y (called the ""environment""), Q a distribution on F and (J' a function (J': Y X Y -t [0, .""1]. The Canonical Distortion Measure (CLDM) between two inputs x, Xl is defined to be: p(x, Xl) = (J'(f(x) , f(xl)) dQ(f). (1) Throughout this paper we will be considering real-valued functions and squared loss, so Y = ~ and (J'(y, yl) := (y - yl)2. The CDM was introduced in ",Optimization & Theoretical ML,c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf,1997
A Non-parametric Multi-Scale Statistical,"The observed distribution of natural images is far from uniform. On the contrary, real images have complex and important struc ture that can be exploited for image processing, recognition and analysis. There have been many proposed approaches to the prin cipled statistical modeling of images, but each has been limited in either the complexity of the models or the complexity of the im ages. We present a non-parametric multi-scale statistical model for images that can be used for recognition, image de-noising, and in a ""generative mode"" to synthesize high quality textures. 1 Introduction In this paper we describe a multi-scale statistical model which can capture the structure of natural images across many scales. Once trained on example images, it can be used to recognize novel images, or to generate new images. Each of these tasks is reasonably efficient, requiring no more than a few seconds or minutes on a workstation. The statistical modeling of images is an endeavor which reaches bac",Computer Vision,c5cc17e395d3049b03e0f1ccebb02b4d-Paper.pdf,1997
A Solution for Missing Data in Recurrent Neural,"We consider neural network models for stochastic nonlinear dynamical systems where measurements of the variable of interest are only avail able at irregular intervals i.e. most realizations are missing. Difficulties arise since the solutions for prediction and maximum likelihood learn ing with missing data lead to complex integrals, which even for simple cases cannot be solved analytically. In this paper we propose a spe cific combination of a nonlinear recurrent neural predictive model and a linear error model which leads to tractable prediction and maximum likelihood adaptation rules. In particular, the recurrent neural network can be trained using the real-time recurrent learning rule and the linear error model can be trained by an EM adaptation rule, implemented us ing forward-backward Kalman filter equations. The model is applied to predict the glucose/insulin metabolism of a diabetic patient where blood glucose measurements are only available a few times a day at irregular interv",Reinforcement Learning,c73dfe6c630edb4c1692db67c510f65c-Paper.pdf,1997
Regularisation in Sequential Learning,"In this paper, we discuss regularisation in online/sequential learn ing algorithms. In environments where data arrives sequentially, techniques such as cross-validation to achieve regularisation or model selection are not possible. Further, bootstrapping to de termine a confidence level is not practical. To surmount these problems, a minimum variance estimation approach that makes use of the extended Kalman algorithm for training multi-layer percep trons is employed. The novel contribution of this paper is to show the theoretical links between extended Kalman filtering, Sutton's variable learning rate algorithms and Mackay's Bayesian estima tion framework. In doing so, we propose algorithms to overcome the need for heuristic choices of the initial conditions and noise covariance matrices in the Kalman approach. 1 INTRODUCTION Model estimation involves building mathematical representations of physical pro cesses using measured data. This problem is often referred to as system identifi c",Optimization & Theoretical ML,c913303f392ffc643f7240b180602652-Paper.pdf,1997
An Improved Policy Iteratioll Algorithm,"A new policy iteration algorithm for partially observable Markov decision processes is presented that is simpler and more efficient than an earlier policy iteration algorithm of Sondik (1971,1978). The key simplification is representation of a policy as a finite-state controller. This representation makes policy evaluation straightforward. The pa per's contribution is to show that the dynamic-programming update used in the policy improvement step can be interpreted as the trans formation of a finite-state controller into an improved finite-state con troller. The new algorithm consistently outperforms value iteration as an approach to solving infinite-horizon problems. 1 Introduction A partially observable Markov decision process (POMDP) is a generalization of the standard completely observable Markov decision process that allows imperfect infor mation about the state of the system. First studied as a model of decision-making in operations research, it has recently been used as a framew",Optimization & Theoretical ML,c930eecd01935feef55942cc445f708f-Paper.pdf,1997
The Asymptotic Convergence-Rate of,"In this paper we show that for discounted MDPs with discount factor, > 1/2 the asymptotic rate of convergence of Q-Iearning is O(1/tR(1-1') if R(1 - ,) < 1/2 and O( Jlog log tit) otherwise provided that the state-action pairs are sampled from a fixed prob ability distribution. Here R = Pmin/Pmax is the ratio of the min imum and maximum state-action occupation frequencies. The re sults extend to convergent on-line learning provided that Pmin > 0, where Pmin and Pmax now become the minimum and maximum state-action occupation frequencies corresponding to the station ary distribution. 1 INTRODUCTION Q-Iearning is a popular reinforcement learning (RL) algorithm whose convergence is well demonstrated in the literature (Jaakkola et al., 1994; Tsitsiklis, 1994; Littman and Szepesvari, 1996; Szepesvari and Littman, 1996). Our aim in this paper is to provide an upper bound for the convergence rate of (lookup-table based) Q-Iearning algorithms. Although, this upper bound is not strict, computer e",Optimization & Theoretical ML,cd0dce8fca267bf1fb86cf43e18d5598-Paper.pdf,1997
Learning Human-like Knowledge by Singular,"Singular value decomposition (SVD) can be viewed as a method for unsupervised training of a network that associates two classes of events reciprocally by linear connections through a single hidden layer. SVD was used to learn and represent relations among very large numbers of words (20k-60k) and very large numbers of natural text passages (lk- 70k) in which they occurred. The result was 100-350 dimensional ""semantic spaces"" in which any trained or newly aibl word or passage could be represented as a vector, and similarities were measured by the cosine of the contained angle between vectors. Good accmacy in simulating human judgments and behaviors has been demonstrated by performance on multiple-choice vocabulary and domain knowledge tests, emulation of expert essay evaluations, and in several other ways. Examples are also given of how the kind of knowledge extracted by this method can be applied. 1 INTRODUCTION Traditionally, imbuing machines with human-like knowledge has relied prima",Computer Vision,cec6f62cfb44b1be110b7bf70c8362d8-Paper.pdf,1997
Zili Liu Daniel Kersten,No abstract found,Optimization & Theoretical ML,cf1f78fe923afe05f7597da2be7a3da8-Paper.pdf,1997
Recurrent Neural Networks Can Learn to,"Recently researchers have derived formal complexity analysis of analog computation in the setting of discrete-time dynamical systems. As an empirical constrast, training recurrent neural networks (RNNs) produces self- organized systems that are realizations of analog mechanisms. Pre vious work showed that a RNN can learn to process a simple context-free language (CFL) by counting. Herein, we extend that work to show that a RNN can learn a harder CFL, a simple palindrome, by organizing its re sources into a symbol-sensitive counting solution, and we provide a dy namical systems analysis which demonstrates how the network: can not only count, but also copy and store counting infonnation. 1 INTRODUCTION Several researchers have recently derived results in analog computation theory in the set ting of discrete-time dynamical systems(Siegelmann, 1994; Maass & Opren, 1997; Moore, 1996; Casey, 1996). For example, a dynamical recognizer (DR) is a discrete-time continu ous dynamical system with ",Computer Vision,cf9a242b70f45317ffd281241fa66502-Paper.pdf,1997
Modeling acoustic correlations by,"Hidden Markov models (HMMs) for automatic speech recognition rely on high dimensional feature vectors to summarize the short time properties of speech. Correlations between features can arise when the speech signal is non-stationary or corrupted by noise. We investigate how to model these correlations using factor analysis, a statistical method for dimensionality reduction. Factor analysis uses a small number of parameters to model the covariance struc ture of high dimensional data. These parameters are estimated by an Expectation-Maximization (EM) algorithm that can be em bedded in the training procedures for HMMs. We evaluate the combined use of mixture densities and factor analysis in HMMs that recognize alphanumeric strings. Holding the total number of parameters fixed, we find that these methods, properly combined, yield better models than either method on its own. 1 Introduction Hidden Markov models (HMMs) for automatic speech recognition[l] rely on high dimensional feature vecto",NLP,d10ec7c16cbe9de8fbb1c42787c3ec26-Paper.pdf,1997
MELONET I: Neural Nets for Inventing,"MELONET I is a multi-scale neural network system producing baroque-style melodic variations. Given a melody, the system in vents a four-part chorale harmonization and a variation of any chorale voice, after being trained on music pieces of composers like J. S. Bach and J. Pachelbel. Unlike earlier approaches to the learn ing of melodic structure, the system is able to learn and reproduce high-order structure like harmonic, motif and phrase structure in melodic sequences. This is achieved by using mutually interacting feedforward networks operating at different time scales, in combi nation with Kohonen networks to classify and recognize musical structure. The results are chorale partitas in the style of J. Pachel bel. Their quality has been judged by experts to be comparable to improvisations invented by an experienced human organist. 1 INTRODUCTION The investigation of neural information structures in music is a rather new, excit ing research area bringing together different discipline",Computer Vision,d1ee59e20ad01cedc15f5118a7626099-Paper.pdf,1997
Hippocampal Model of Rat Spatial Abilities,"We provide a model of the standard watermaze task, and of a more challenging task involving novel platform locations, in which rats exhibit one-trial learning after a few days of training. The model uses hippocampal place cells to support reinforcement learning, and also, in an integrated manner, to build and use allocentric coordinates. 1 INTRODUCTION Whilst it has long been known both that the hippocampus of the rat is needed for normal performance on spatial tasksl3, 11 and that certain cells in the hippocampus exhibit place-related firing,12 it has not been clear how place cells are actually used for navigation. One of the principal conceptual problems has been understanding how the hippocampus could specify or learn paths to goals when spatially tuned cells in the hippocampus respond only on the basis of the rat's current location. This work uses recent ideas from reinforcement learning to solve this problem in the context of two rodent spatial learning results. Reference memory i",NLP,d82118376df344b0010f53909b961db3-Paper.pdf,1997
EM Algorithms for PCA and SPCA,"I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also natu rally accommodates missing infonnation. I also introduce a new variant of PC A called sensible principal component analysis (SPCA) which de fines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the lead ing eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions. 1 Why EM for peA? Principal component analysis (PCA) is a widely used dimensionality reduction technique in data analysis. Its popularity comes from three important properties. First, it is the optimal (in tenns of m",Optimization & Theoretical ML,d9731321ef4e063ebbee79298fa36f56-Paper.pdf,1997
Hierarchical Non-linear Factor Analysis,"We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs per ceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally avail able information. We then show how to incorporate lateral con nections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topo graphically organised local feature detectors. 1 Introduction Factor analysis is a probabilistic model for real-valued data which assumes that the data is a linear combination of real-valued uncorrelated Gaussian sources (the factors",Optimization & Theoretical ML,daa96d9681a21445772454cbddf0cac1-Paper.pdf,1997
Learning Path Distributions using,"We propose diffusion networks, a type of recurrent neural network with probabilistic dynamics, as models for learning natural signals that are continuous in time and space. We give a formula for the gradient of the log-likelihood of a path with respect to the drift parameters for a diffusion network. This gradient can be used to optimize diffusion networks in the nonequilibrium regime for a wide variety of problems paralleling techniques which have succeeded in engineering fields such as system identification, state estimation and signal filtering. An aspect of this work which is of particu lar interest to computational neuroscience and hardware design is that with a suitable choice of activation function, e.g., quasi-linear sigmoidal, the gradient formula is local in space and time. 1 Introduction Many natural signals, like pixel gray-levels, line orientations, object position, veloc ity and shape parameters, are well described as continuous-time continuous-valued stochastic processes",Computer Vision,db6ebd0566994d14a1767f14eb6fba81-Paper.pdf,1997
Unsupervised On-Line Learning of,"An adaptive on-line algorithm is proposed to estimate hierarchical data structures for non-stationary data sources. The approach is based on the principle of minimum cross entropy to derive a decision tree for data clustering and it employs a metalearning idea (learning to learn) to adapt to changes in data characteristics. Its efficiency is demonstrated by grouping non-stationary artifical data and by hierarchical segmentation of LANDSAT images. 1 Introduction Unsupervised learning addresses the problem to detect structure inherent in un labeled and unclassified data. The simplest, but not necessarily the best ap proach for extracting a grouping structure is to represent a set of data samples X ,N} ,K}, = {Xi E Rdli = 1, ... by a set of prototypes y = {Ya E Rdlo = 1, ... « K N. The encoding usually is represented by an assignment matrix M = (Mia), where Mia = 1 if and only if Xi belongs to cluster 0, and Mia = 0 otherwise. Accord- = ing to this encoding scheme, the cost function 1i (M",Optimization & Theoretical ML,dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf,1997
Recovering Perspective Pose with a Dual,This paper describes a new approach to extracting 3D perspective structure from 2D point-sets. The novel feature is to unify the tasks of estimating transformation geometry and identifying point correspondence matches. Unification is realised by constructing a mixture model over the bi-partite graph representing the correspon dence match and by effecting optimisation using the EM algorithm. According to our EM framework the probabilities of structural cor respondence gate contributions to the expected likelihood function used to estimate maximum likelihood perspective pose parameters. This provides a means of rejecting structural outliers. 1 Introduction The estimation of transformational geometry is key to many problems of computer vision and robotics [10]. Broadly speaking the aim is to recover a matrix representa tion of the transformation between image and world co-ordinate systems. In order to estimate the matrix requires a set of correspondence matches between features in the two,Computer Vision,e077e1a544eec4f0307cf5c3c721d944-Paper.pdf,1997
Learning to Order Things,"There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference Junction, of the form PREF( u, v), which indicates whether it is advisable to rank u before v. New instances are then ordered so as to maximize agreements with the learned preference func tion. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the ""Hedge"" algorithm, for finding a good linear combination of ranking ""experts."" We use the ordering algorithm combined",Optimization & Theoretical ML,e11943a6031a0e6114ae69c257617980-Paper.pdf,1997
Analog VLSI Model of Intersegmental,"We have a developed an analog VLSI system that models the coordina tion of neurobiological segmental oscillators. We have implemented and tested a system that consists of a chain of eleven pattern generating cir cuits that are synaptically coupled to their nearest neighbors. Each pat tern generating circuit is implemented with two silicon Morris-Lecar neurons that are connected in a reciprocally inhibitory network. We dis cuss the mechanisms of oscillations in the two-cell network and explore system behavior based on isotropic and anisotropic coupling, and fre quency gradients along the chain of oscillators. 1 INTRODUCTION In recent years, neuroscientists and modelers have made great strides towards illuminat ing structure and computational properties in biological motor systems. For example, much progress has been made toward understanding the neural networks that elicit rhyth mic motor behaviors, including leech heartbeat (Calabrese and De Schutter, 1992), crus tacean stomatogastric ",Computer Vision,e48e13207341b6bffb7fb1622282247b-Paper.pdf,1997
Globally Optimal On-line Learning Rules,"We present a method for determining the globally optimal on-line learning rule for a soft committee machine under a statistical me chanics framework. This work complements previous results on locally optimal rules, where only the rate of change in general ization error was considered. We maximize the total reduction in generalization error over the whole learning process and show how the resulting rule can significantly outperform the locally optimal rule. 1 Introduction We consider a learning scenario in which a feed-forward neural network model (the student) emulates an unknown mapping (the teacher), given a set of training exam ples produced by the teacher. The performance of the student network is typically measured by its generalization error, which is the expected error on an unseen ex ample. The aim of training is to reduce the generalization error by adapting the student network's parameters appropriately. A common form of training is on-line learning, where training patterns a",Optimization & Theoretical ML,e56b06c51e1049195d7b26d043c478a0-Paper.pdf,1997
Ensemble Learning,"Bayesian treatments of learning in neural networks are typically based either on local Gaussian approximations to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called ensemble learning, was in troduced by Hinton and van Camp (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler di vergence between the true posterior and a parametric approximat ing distribution. However, the derivation of a deterministic algo rithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and so was unable to capture the posterior correlations between parameters. In this paper, we show how the ensemble learning approach can be extended to full covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparam eters, leading to a simple re-estimation procedure. Initial results from a standard benchmark problem",Optimization & Theoretical ML,e816c635cad85a60fabd6b97b03cbcc9-Paper.pdf,1997
Estimating Dependency Structure as a Hidden,"This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors. 1 INTRODUCTION A fundamental feature of a good model is the ability to uncover and exploit independencies in the data it is presented with. For many commonly used models, such as neural nets and belief networks, the dependency structure encoded in the model is fixed, in the sense that it is not allowed to vary depending on actual values of the variables or with the current case. However, dependency structures that are conditional on values of variables abound in the world around us. Consider for example bitmaps of handwritten digits. They obviously contain many dependencies between pixels; however, the pattern of these dependencies will vary acros",Optimization & Theoretical ML,ea8fcd92d59581717e06eb187f10666d-Paper.pdf,1997
Ensemble and Modular Approaches for,"A new learning model based on autoassociative neural networks is developped and applied to face detection. To extend the de tection ability in orientation and to decrease the number of false alarms, different combinations of networks are tested: ensemble, conditional ensemble and conditional mixture of networks. The use of a conditional mixture of networks allows to obtain state of the art results on different benchmark face databases. 1 A constrained generative model Our purpose is to classify an extracted window x from an image as a face (x E V) or non-face (x EN). The set of all possible windows is E = V uN, with V n N = 0. Since collecting a representative set of non-face examples is impossible, face detection by a statistical model is a difficult task. An autoassociative network, using five layers of neurons, is able to perform a non-linear dimensionnality reduction [Kramer, 1991]. However, its use as an estimator, to classify an extracted window as face or non-face, raises two pr",Computer Vision,eaa32c96f620053cf442ad32258076b9-Paper.pdf,1997
Stacked Density Estimation,"In this paper, the technique of stacking, previously only used for supervised learning, is applied to unsupervised learning. Specifi cally, it is used for non-parametric multivariate density estimation, to combine finite mixture model and kernel density estimators. Ex perimental results on both simulated data and real world data sets clearly demonstrate that stacked density estimation outperforms other strategies such as choosing the single best model based on cross-validation, combining with uniform weights, and even the sin gle best model chosen by ""cheating"" by looking at the data used for independent testing. 1 Introduction Multivariate probability density estimation is a fundamental problem in exploratory data analysis, statistical pattern recognition and machine learning. One frequently estimates density functions for which there is little prior knowledge on the shape of the density and for which one wants a flexible and robust estimator (allowing multimodality if it exists). In ",Optimization & Theoretical ML,ee8374ec4e4ad797d42350c904d73077-Paper.pdf,1997
An Analog VLSI Neural Network for Phase,"We describe the design, fabrication and test results of an analog CMOS VLSI neural network prototype chip intended for phase-based machine vision algorithms. The chip implements an image filtering operation similar to Gabor-filtering. Because a Gabor filter's output is complex valued, it can be used to define a phase at every pixel in an image. This phase can be used in robust algorithms for disparity estimation and bin ocular stereo vergence control in stereo vision and for image motion analysis. The chip reported here takes an input image and generates two outputs at every pixel corresponding to the real and imaginary parts of the output. 1 INTRODUCTION Gabor filters are used as preprocessing stages for different tasks in machine vision and image processing. Their use has been partially motivated by findings that two dimensional Gabor filters can be used to model receptive fields of orientation selective neurons in the visual cortex (Daugman, 1980) and three dimensional spatio-tempor",Computer Vision,effc299a1addb07e7089f9b269c31f2f-Paper.pdf,1997
"Attentional Streams, and the Impact of","We have developed a neural network architecture that implements a the ory of attention, learning, and trans-cortical communication based on adaptive synchronization of 5-15 Hz and 30-80 Hz oscillations between cortical areas. Here we present a specific higher order cortical model of attentional networks, rhythmic expectancy, and the interaction of hi~her­ order and primar¥, cortical levels of processing. It accounts for the' mis match negativity' of the auditory ERP and the results of psychological experiments of Jones showing that auditory stream segregation depends on the rhythmic structure of inputs. The timing mechanisms of the model allow us to explain how relative timing information such as the relative order of events between streams is lost when streams are formed. The model suggests how the theories of auditory perception and attention of Jones and Bregman may be reconciled. 1 Introduction Amplitude patterns of synchronized ""gamma band"" (30 to 80 Hz) oscillation have been ob s",Computer Vision,f0dd4a99fba6075a9494772b58f95280-Paper.pdf,1997
Combining Classifiers Using,"Several effective methods for improving the performance of a sin gle learning algorithm have been developed recently. The general approach is to create a set of learned models by repeatedly apply ing the algorithm to different versions of the training data, and then combine the learned models' predictions according to a pre scribed voting scheme. Little work has been done in combining the predictions of a collection of models generated by many learning algorithms having different representation and/or search strategies. This paper describes a method which uses the strategies of stack ing and correspondence analysis to model the relationship between the learning examples and the way in which they are classified by a collection of learned models. A nearest neighbor method is then applied within the resulting representation to classify previously unseen examples. The new algorithm consistently performs as well or better than other combining techniques on a suite of data sets. 1 Introducti",NLP,f016e59c7ad8b1d72903bb1aa5720d53-Paper.pdf,1997
From Regularization Operators,"We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Sup port Vector Machines. More specifica1ly, we prove that the Green's Func tions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely condition ally positive definite functions may be used as Support Vector kernels. 1 INTRODUCTION Support Vector (SV) Machines for pattern recognition, regression estimation and operator inversion exploit the idea of transforming into a high dimensional feature space where they perform a linear algorithm. Instead of evaluating this map explicitly, one uses Hilbert Schmidt Kernels k(x, y) which correspond to dot products of the mapped data in high dimensional space, i.e. k(x, y) = (<I>(x) · <I>(y)) (I) with <I> : .!Rn --* :F denoting the map into feature space. Mostly, this map and",Optimization & Theoretical ML,f3bd5ad57c8389a8a1a541a76be463bf-Paper.pdf,1997
Two Approaches to Optimal Annealing,"We employ both master equation and order parameter approaches to analyze the asymptotic dynamics of on-line learning with dif ferent learning rate annealing schedules. We examine the relations between the results obtained by the two approaches and obtain new results on the optimal decay coefficients and their dependence on the number of hidden nodes in a two layer architecture. 1 Introduction The asymptotic dynamics of stochastic on-line learning and it's dependence on the annealing schedule adopted for the learning coefficients have been studied for some time in the stochastic approximation literature [1, 2] and more recently in the neural network literature [3, 4, 5]. The latter studies are based on examining the Kramers Moyal expansion of the master equation for the weight space probability densities. A different approach, based on the deterministic dynamics of macroscopic quantities called order parameters, has been recently presented [6, 7]. This approach enables one to monitor th",Optimization & Theoretical ML,f4a331b7a22d1b237565d8813a34d8ac-Paper.pdf,1997
Bidirectional Retrieval from Associative,"Similarity based fault tolerant retrieval in neural associative mem ories (N AM) has not lead to wiedespread applications. A draw back of the efficient Willshaw model for sparse patterns [Ste61, WBLH69], is that the high asymptotic information capacity is of little practical use because of high cross talk noise arising in the retrieval for finite sizes. Here a new bidirectional iterative retrieval method for the Willshaw model is presented, called crosswise bidi rectional (CB) retrieval, providing enhanced performance. We dis cuss its asymptotic capacity limit, analyze the first step, and com pare it in experiments with the Willshaw model. Applying the very efficient CB memory model either in information retrieval systems or as a functional model for reciprocal cortico-cortical pathways requires more than robustness against random noise in the input: Our experiments show also the segmentation ability of CB-retrieval with addresses containing the superposition of pattens, provided even ",Computer Vision,f52378e14237225a6f6c7d802dc6abbd-Paper.pdf,1997
Using Helmholtz Machines to analyze,"One of the current challenges to understanding neural information processing in biological systems is to decipher the ""code"" carried by large populations of neurons acting in parallel. We present an algorithm for automated discovery of stochastic firing patterns in large ensembles of neurons. The algorithm, from the ""Helmholtz Machine"" family, attempts to predict the observed spike patterns in the data. The model consists of an observable layer which is directly activated by the input spike patterns, and hidden units that are ac tivated through ascending connections from the input layer. The hidden unit activity can be propagated down to the observable layer to create a prediction of the data pattern that produced it. Hidden units are added incrementally and their weights are adjusted to im prove the fit between the predictions and data, that is, to increase a bound on the probability of the data given the model. This greedy strategy is not globally optimal but is computationally tract",Optimization & Theoretical ML,f69e505b08403ad2298b9f262659929a-Paper.pdf,1997
Silicon Retina with Adaptive Filtering,"This paper describes a small, compact circuit that captures the temporal and adaptation properties both of the photoreceptor and of the laminar layers of the fly. This circuit uses only six transis tors and two capacitors. It is operated in the subthreshold domain. The circuit maintains a high transient gain by using adaptation to the background intensity as a form of gain control. The adapta tion time constant of the circuit can be controlled via an external bias. Its temporal filtering properties change with the background intensity or signal-to-noise conditions. The frequency response of the circuit shows that in the frequency range of 1 to 100 Hz, the circuit response goes from highpass filtering under high light levels to lowpass filtering under low light levels (Le., when the signal-to noise ratio is low). A chip with 20x20 pixels has been fabricated in 1.2J.Lm ORBIT CMOS nwell technology. 1 BACKGROUND The first two layers in the fly visual system are the retina layer and the lam",Computer Vision,fb508ef074ee78a0e58c68be06d8a2eb-Paper.pdf,1997
Statistical Models of Conditioning,"Conditioning experiments probe the ways that animals make pre dictions about rewards and punishments and use those predic tions to control their behavior. One standard model of condition ing paradigms which involve many conditioned stimuli suggests that individual predictions should be added together. Various key results show that this model fails in some circumstances, and mo tivate an alternative model, in which there is attentional selection between different available stimuli. The new model is a form of mixture of experts, has a close relationship with some other exist ing psychological suggestions, and is statistically well-founded. 1 Introduction Classical and instrumental conditioning experiments study the way that animals learn about the causal texture of the world (Dickinson, 1980) and use this informa tion to their advantage. Although it reached a high level of behavioral sophistica tion, conditioning has long since gone out of fashion as a paradigm for studying learning in a",Optimization & Theoretical ML,fe70c36866add1572a8e2b96bfede7bf-Paper.pdf,1997
The Efficiency and The Robustness of,"The inverse of the Fisher information matrix is used in the natu ral gradient descent algorithm to train single-layer and multi-layer perceptrons. We have discovered a new scheme to represent the Fisher information matrix of a stochastic multi-layer perceptron. Based on this scheme, we have designed an algorithm to compute the natural gradient. When the input dimension n is much larger than the number of hidden neurons, the complexity of this algo rithm is of order O(n). It is confirmed by simulations that the natural gradient descent learning rule is not only efficient but also robust. 1 INTRODUCTION The inverse of the Fisher information matrix is required to find the Cramer-Rae lower bound to analyze the performance of an unbiased estimator. It is also needed in the natural gradient learning framework (Amari, 1997) to design statistically efficient algorithms for estimating parameters in general and for training neural networks in particular. In this paper, we assume a stochastic mod",Optimization & Theoretical ML,ff49cc40a8890e6a60f40ff3026d2730-Paper.pdf,1997
Computational Differences between,"Symmetrically connected recurrent networks have recently been used as models of a host of neural computations. However, be cause of the separation between excitation and inhibition, biolog ical neural networks are asymmetrical. We study characteristic differences between asymmetrical networks and their symmetri cal counterparts, showing that they have dramatically different dynamical behavior and also how the differences can be exploited for computational ends. We illustrate our results in the case of a network that is a selective amplifier. 1 Introduction A large class of non-linear recurrent networks, including those studied by Grossberg,9 the Hopfield net,lO,l1 and many more recent proposals for the head direction system,27 orientation tuning in primarls visual cortex,25, 1,3, 18 eye position,20 and spatial location in the hippocampus 9 make a key simplifying assumption that the connections between the neurons are symmetric. Analysis is relatively straightforward in this case, since",Computer Vision,020c8bfac8de160d4c5543b96d1fdede-Paper.pdf,1998
A Randomized Algorithm for Pairwise Clustering,"We present a stochastic clustering algorithm based on pairwise sim ilarity of datapoints. Our method extends existing deterministic methods, including agglomerative algorithms, min-cut graph algo rithms, and connected components. Thus it provides a common framework for all these methods. Our graph-based method differs from existing stochastic methods which are based on analogy to physical systems. The stochastic nature of our method makes it more robust against noise, including accidental edges and small spurious clusters. We demonstrate the superiority of our algorithm using an example with 3 spiraling bands and a lot of noise. 1 Introduction Clustering algorithms can be divided into two categories: those that require a vec torial representation of the data, and those which use only pairwise representation. In the former case, every data item must be represented as a vector in a real normed space, while in the second case only pairwise relations of similarity or dissimilar ity are use",Optimization & Theoretical ML,06a81a4fb98d149f2d31c68828fa6eb2-Paper.pdf,1998
Learning a Continuous Hidden Variable,"A directed generative model for binary data using a small number of hidden continuous units is investigated. A clipping nonlinear ity distinguishes the model from conventional principal components analysis. The relationships between the correlations of the underly ing continuous Gaussian variables and the binary output variables are utilized to learn the appropriate weights of the network. The advantages of this approach are illustrated on a translationally in variant binary distribution and on handwritten digit images. Introduction Principal Components Analysis (PCA) is a widely used statistical technique for rep resenting data with a large number of variables [1]. It is based upon the assumption that although the data is embedded in a high dimensional vector space, most of the variability in the data is captured by a much lower climensional manifold. In particular for PCA, this manifold is described by a linear hyperplane whose char acteristic directions are given by the eigenvectors",Optimization & Theoretical ML,076023edc9187cf1ac1f1163470e479a-Paper.pdf,1998
Convergence of The Wake-Sleep Algorithm,"The W-S (Wake-Sleep) algorithm is a simple learning rule for the models with hidden variables. It is shown that this algorithm can be applied to a factor analysis model which is a linear version of the Helmholtz ma chine. But even for a factor analysis model, the general convergence is not proved theoretically. In this article, we describe the geometrical un derstanding of the W-S algorithm in contrast with the EM (Expectation Maximization) algorithm and the em algorithm. As the result, we prove the convergence of the W-S algorithm for the factor analysis model. We also show the condition for the convergence in general models. 1 INTRODUCTION The W-S algorithm[5] is a simple Hebbian learning algorithm. Neal and Dayan applied the W-S algorithm to a factor analysis mode1[7]. This model can be seen as a linear version of the Helmholtz machine[3]. As it is mentioned in[7], the convergence of the W-S algorithm has not been proved theoretically even for this simple model. From the similarity ",Optimization & Theoretical ML,0771fc6f0f4b1d7d1bb73bbbe14e0e31-Paper.pdf,1998
Mean field methods for classification with,"We discuss the application of TAP mean field methods known from the Statistical Mechanics of disordered systems to Bayesian classifi cation models with Gaussian processes. In contrast to previous ap proaches, no knowledge about the distribution of inputs is needed. Simulation results for the Sonar data set are given. 1 Modeling with Gaussian Processes Bayesian models which are based on Gaussian prior distributions on function spaces are promising non-parametric statistical tools. They have been recently introduced into the Neural Computation community (Neal 1996, Williams & Rasmussen 1996, Mackay 1997). To give their basic definition, we assume that the likelihood of the output or target variable T for a given input s E RN can be written in the form p(Tlh(s)) where h : RN --+ R is a priori assumed to be a Gaussian random field. If we assume fields with zero prior mean, the statistics of h is entirely defined by the second order correlations C(s, S') == E[h(s)h(S')], where E denotes exp",Optimization & Theoretical ML,08040837089cdf46631a10aca5258e16-Paper.pdf,1998
Call-based Fraud Detection in Mobile,Fraud causes substantial losses to telecommunication carriers. Detec tion systems which automatically detect illegal use of the network can be used to alleviate the problem. Previous approaches worked on features derived from the call patterns of individual users. In this paper we present a call-based detection system based on a hierarchical regime-switching model. The detection problem is formulated as an inference problem on the regime probabilities. Inference is implemented by applying the junc tion tree algorithm to the underlying graphical model. The dynamics are learned from data using the EM algorithm and subsequent discriminative training. The methods are assessed using fraud data from a real mobile communication network. 1 INTRODUCTION Fraud is costly to a network carrier both in terms of lost income and wasted capacity. It has been estimated that the telecommunication industry looses approximately 2-5% of its total revenue to fraud. The true losses are expected to be even hig,Computer Vision,0c9ebb2ded806d7ffda75cd0b95eb70c-Paper.pdf,1998
Learning from Dyadic Data,"Dyadzc data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This type of data arises naturally in many ap plication ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework of learn ing from dyadic data by statistical mixture models. Our approach covers different models with fiat and hierarchical latent class struc tures. We propose an annealed version of the standard EM algo rithm for model fitting which is empirically evaluated on a variety of data sets from different domains. 1 Introduction Over the past decade learning from data has become a highly active field of re search distributed over many disciplines like pattern recognition, neural compu tation, statistics, machine learning, and data mining. Most domain-independent learning architectures as well as the underlying th",Optimization & Theoretical ML,0c8ce55163055c4da50a81e0a273468c-Paper.pdf,1998
Learning Nonlinear Dynamical Systems,"The Expectation-Maximization (EM) algorithm is an iterative pro cedure for maximum likelihood parameter estimation from data sets with missing or hidden variables [2]. It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simulta neously [9]. We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems. The ""expec tation"" step makes use of Extended Kalman Smoothing to estimate the state, while the ""maximization"" step re-estimates the parame ters using these uncertain state estimates. In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states. However, if Gaussian radial basis func tion (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems ",Optimization & Theoretical ML,0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf,1998
Analyzing and Visualizing Single-Trial,"Event-related potentials (ERPs), are portions of electroencephalo graphic (EEG) recordings that are both time- and phase-locked to experimental events. ERPs are usually averaged to increase their signal/noise ratio relative to non-phase locked EEG activ ity, regardless of the fact that response activity in single epochs may vary widely in time course and scalp distribution. This study applies a linear decomposition tool, Independent Component Anal ysis (ICA) [1], to multichannel single-trial EEG records to derive spatial filters that decompose single-trial EEG epochs into a sum of temporally independent and spatially fixed components arising from distinct or overlapping brain or extra-brain networks. Our results on normal and autistic subjects show that ICA can sep arate artifactual, stimulus-locked, response-locked, and. non-event related background EEG activities into separate components, al lowing ( 1) removal of pervasive artifacts of all types from single-trial EEG records, and (2",Computer Vision,0d4f4805c36dc6853edfa4c7e1638b48-Paper.pdf,1998
"Replicator Equations, Maximal Cliques,","We present a new energy-minimization framework for the graph isomorphism problem which is based on an equivalent maximum clique formulation. The approach is centered around a fundamental result proved by Motzkin and Straus in the mid-1960s, and recently expanded in various ways, which allows us to formulate the maxi mum clique problem in terms of a standard quadratic program. To solve the program we use ""replicator"" equations, a class of simple continuous- and discrete-time dynamical systems developed in var ious branches of theoretical biology. We show how, despite their inability to escape from local solutions, they nevertheless provide experimental results which are competitive with those obtained us ing more elaborate mean-field annealing heuristics. 1 INTRODUCTION The graph isomorphism problem is one of those few combinatorial optimization problems which still resist any computational complexity characterization [6]. De spite decades of active research, no polynomial-time algorith",Optimization & Theoretical ML,0f3d014eead934bbdbacb62a01dc4831-Paper.pdf,1998
"Orientation, Scale, and Discontinuity as","A recent neural model of illusory contour formation is based on a distribution of natural shapes traced by particles moving with constant speed in directions given by Brownian motions. The input to that model consists of pairs of position and direction constraints and the output consists of the distribution of contours joining all such pairs. In general, these contours will not be closed and their distribution will not be scale-invariant. In this paper, we show how to compute a scale-invariant distribution of closed contours given position constraints alone and use this result to explain a well known illusory contour effect. 1 INTRODUCTION It has been proposed by Mumford[3] that the distribution of illusory contour shapes can be modeled by particles travelling with constant speed in directions given by Brownian motions. More recently, Williams and Jacobs[7, 8] introduced the notion of a stochastic completion field, the distribution of particle trajectories joining pairs of position and",Computer Vision,109d2dd3608f669ca17920c511c2a41e-Paper.pdf,1998
Active Noise Canceling using Analog Neuro,"A modular analogue neuro-chip set with on-chip learning capability is developed for active noise canceling. The analogue neuro-chip set incorporates the error backpropagation learning rule for practical applications, and allows pin-to-pin interconnections for multi-chip boards. The developed neuro-board demonstrated active noise canceling without any digital signal processor. Multi-path fading of acoustic channels, random noise, and nonlinear distortion of the loud speaker are compensated by the adaptive learning circuits of the neuro-chips. Experimental results are reported for cancellation of car noise in real time. 1 INTRODUCTION Both analog and digital implementations of neural networks have been reported. Digital neuro-chips can be designed and fabricated with the help of well-established CAD tools and digital VLSI fabrication technology [1]. Although analogue neuro chips have potential advantages on integration density and speed over digital chips[2], they suffer from non-ideal c",Computer Vision,1373b284bc381890049e92d324f56de0-Paper.pdf,1998
Coordinate Transformation Learning of,"In order to grasp an object, we need to solve the inverse kine matics problem, i.e., the coordinate transformation from the visual coordinates to the joint angle vector coordinates of the arm. Al though several models of coordinate transformation learning have been proposed, they suffer from a number of drawbacks. In human motion control, the learning of the hand position error feedback controller in the inverse kinematics solver is important. This paper proposes a novel model of the coordinate transformation learning of the human visual feedback controller that uses the change of the joint angle vector and the corresponding change of the square of the hand position error norm. The feasibility of the proposed model is illustrated using numerical simulations. 1 INTRODUCTION The task of calculating every joint angle that would result in a specific hand position is called the inverse kinematics problem. An important topic in neuroscience is the study of the learning mechanisms involved in",Computer Vision,1415db70fe9ddb119e23e9b2808cde38-Paper.pdf,1998
Multi-electrode spike sorting,"A new paradigm is proposed for sorting spikes in multi-electrode data using ratios of transfer functions between cells and electrodes. It is assumed that for every cell and electrode there is a stable linear relation. These are dictated by the properties of the tissue, the electrodes and their relative geometries. The main advantage of the method is that it is insensitive to variations in the shape and amplitude of a spike. Spike sorting is carried out in two separate steps. First, templates describing the statistics of each spike type are generated by clustering transfer function ratios then spikes are detected in the data using the spike statistics. These techniques were applied to data generated in the escape response system of the cockroach. 1 Introduction Simultaneous recording of activity from many neurons can greatly expand our un derstanding of how information is coded in neural systems[l]. 11ultiple electrodes are often used to measure the activity in neural tissue and have be",Computer Vision,1714726c817af50457d810aae9d27a2e-Paper.pdf,1998
Fisher Scoring and a Mixture of Modes,"We present Monte-Carlo generalized EM equations for learning in non linear state space models. The difficulties lie in the Monte-Carlo E-step which consists of sampling from the posterior distribution of the hidden variables given the observations. The new idea presented in this paper is to generate samples from a Gaussian approximation to the true posterior from which it is easy to obtain independent samples. The parameters of the Gaussian approximation are either derived from the extended Kalman filter or the Fisher scoring algorithm. In case the posterior density is mul timodal we propose to approximate the posterior by a sum of Gaussians (mixture of modes approach). We show that sampling from the approxi mate posterior densities obtained by the above algorithms leads to better models than using point estimates for the hidden states. In our exper iment, the Fisher scoring algorithm obtained a better approximation of the posterior mode than the EKF. For a multimodal distribution, the",Optimization & Theoretical ML,17e23e50bedc63b4095e3d8204ce063b-Paper.pdf,1998
Probabilistic Modeling for Face Orientation,"This paper presents probabilistic modeling methods to solve the problem of dis criminating between five facial orientations with very little labeled data. Three models are explored. The first model maintains no inter-pixel dependencies, the second model is capable of modeling a set of arbitrary pair-wise dependencies, and the last model allows dependencies only between neighboring pixels. We show that for all three of these models, the accuracy of the learned models can be greatly improved by augmenting a small number of labeled training images with a large set of unlabeled images using Expectation-Maximization. This is important because it is often difficult to obtain image labels, while many unla beled images are readily available. Through a large set of empirical tests, we examine the benefits of unlabeled data for each of the models. By using only two randomly selected labeled examples per class, we can discriminate between the five facial orientations with an accuracy of 94%; with",Computer Vision,18d10dc6e666eab6de9215ae5b3d54df-Paper.pdf,1998
Direct Optimization of Margins Improves,"Cumulative training margin dis tributions for AdaBoost versus our ""Direct Optimization Of Margins"" (DOOM) algorithm. The dark curve is AdaBoost, the light curve is DOOM. DOOM sacrifices significant training er ror for improved test error (hori zontal marks on margin= 0 line)_ -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 Margin 1 Introduction Many learning algorithms for pattern classification minimize some cost function of the training data, with the aim of minimizing error (the probability of misclassifying an example). One example of such a cost function is simply the classifier's error on the training data. Recent results have examined alternative cost functions that provide better error estimates in some cases_ For example, results in [Bar98] show that the error of a sigmoid network classifier f(-) is no more than the sample average of the cost function sgn(B-yf(x)) (which takes value 1 when yf(x) is no more than Band 0 otherwise) plus a complexity penalty term that scales as IlwlldB",Optimization & Theoretical ML,18ead4c77c3f40dabf9735432ac9d97a-Paper.pdf,1998
A N euromorphic Monaural Sound,"We describe the first single microphone sound localization system and its inspiration from theories of human monaural sound localiza tion. Reflections and diffractions caused by the external ear (pinna) allow humans to estimate sound source elevations using only one ear. Our single microphone localization model relies on a specially shaped reflecting structure that serves the role of the pinna. Spe cially designed analog VLSI circuitry uses echo-time processing to localize the sound. A CMOS integrated circuit has been designed, fabricated, and successfully demonstrated on actual sounds. 1 Introduction The principal cues for human sound localization arise from time and intensity dif ferences between the signals received at the two ears. For low-frequency components of sounds (below 1500Hz for humans), the phase-derived interaural time difference (lTD) can be used to localize the sound source. For these frequencies, the sound wavelength is at least several times larger than the head and ",Computer Vision,1baff70e2669e8376347efd3a874a341-Paper.pdf,1998
Experimental Results on Learning Stochastic,"Partially Observable Markov Decision Processes (pO ""MOPs) constitute an important class of reinforcement learning problems which present unique theoretical and computational difficulties. In the absence of the Markov property, popular reinforcement learning algorithms such as Q-Iearning may no longer be effective, and memory-based methods which remove partial observability via state-estimation are notoriously expensive. An alternative approach is to seek a stochastic memoryless policy which for each observation of the environment prescribes a probability distribution over available actions that maximizes the average reward per timestep. A reinforcement learning algorithm which learns a locally optimal stochastic memoryless policy has been proposed by Jaakkola, Singh and Jordan, but not empirically verified. We present a variation of this algorithm, discuss its implementation, and demonstrate its viability using four test problems. 1 INTRODUCTION Reinforcement learning techniques have p",Reinforcement Learning,1cd3882394520876dc88d1472aa2a93f-Paper.pdf,1998
Boxlets: a Fast Convolution Algorithm for,"Signal processing and pattern recognition algorithms make exten sive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justifies some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomi als, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite effectively. The true convolution can be recov ered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks. 1 Introduction In pattern recognition, convolution is an important tool because of its translation invari",Computer Vision,1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf,1998
Attentional Modulation of Human Pattern,"We previously proposed a quantitative model of early visual pro cessing in primates, based on non-linearly interacting visual filters and statistically efficient decision. We now use this model to inter pret the observed modulation of a range of human psychophysical thresholds with and without focal visual attention. Our model - calibrated by an automatic fitting procedure - simultaneously re produces thresholds for four classical pattern discrimination tasks, performed while attention was engaged by another concurrent task. Our model then predicts that the seemingly complex improvements of certain thresholds, which we observed when attention was fully available for the discrimination tasks, can best be explained by a strengthening of competition among early visual filters. 1 INTRODUCTION What happens when we voluntarily focus our attention to a restricted part of our visual field? Focal attention is often thought as a gating mechanism, which selec tively allows a certain spatial locat",Computer Vision,204da255aea2cd4a75ace6018fad6b4d-Paper.pdf,1998
Signal Detection in Noisy Weakly-Active,"Here we derive measures quantifying the information loss of a synaptic signal due to the presence of neuronal noise sources, as it electrotonically propagates along a weakly-active dendrite. We model the dendrite as an infinite linear cable, with noise sources distributed along its length. The noise sources we consider are thermal noise, channel noise arising from the stochastic nature of voltage-dependent ionic channels (K+ and Na+) and synaptic noise due to spontaneous background activity. We assess the efficacy of information transfer using a signal detection paradigm where the objective is to detect the presence/absence of a presynaptic spike from the post-synaptic membrane voltage. This allows us to analytically assess the role of each of these noise sources in information transfer. For our choice of parameters, we find that the synaptic noise is the dominant noise source which limits the maximum length over which information be reliably transmitted. 1 Introduction This is a conti",Optimization & Theoretical ML,21fe5b8ba755eeaece7a450849876228-Paper.pdf,1998
Kernel peA and De-Noising in Feature Spaces,"Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms. But it can also be con sidered as a natural generalization of linear principal component anal ysis. This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA. This is a nontrivial task, as the results provided by ker nel PCA live in some high dimensional feature space and need not have pre-images in input space. This work presents ideas for finding approxi mate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data. 1 peA and Feature Spaces Principal Component Analysis (PC A) (e.g. [3]) is an orthogonal basis transformation. The new basis is found by diagonalizing the centered covariance matrix of a data set = = {Xk E RNlk 1, ... ,f}, defined by C ((Xi - (",Optimization & Theoretical ML,226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf,1998
Multiple Paired Forward-Inverse Models,"Humans demonstrate a remarkable ability to generate accurate and appropriate motor behavior under many different and oftpn uncprtain environmental conditions. This paper describes a new modular ap proach to human motor learning and control, baspd on multiple pairs of inverse (controller) and forward (prpdictor) models. This architecture simultaneously learns the multiple inverse models necessary for control as well as how to select the inverse models appropriate for a given em'i ronm0nt. Simulations of object manipulation demonstrates the ability to learn mUltiple objects, appropriate generalization to novel objects and the inappropriate activation of motor programs based on visual cues, followed by on-line correction, seen in the ""size-weight illusion"". 1 Introduction Given the multitude of contexts within which we must act, there are two qualitatively distinct strategies to motor control and learning. The first is to uSP a Single controller which would need to be highly complex to al",Optimization & Theoretical ML,228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf,1998
SMEM Algorithm for Mixture Models,"We present a split and merge EM (SMEM) algorithm to overcome the local maximum problem in parameter estimation of finite mixture models. In the case of mixture models, non-global maxima often involve having too many components of a mixture model in one part of the space and too few in an other, widely separated part of the space. To escape from such configurations we repeatedly perform simultaneous split and merge operations using a new criterion for efficiently selecting the split and merge candidates. We apply the proposed algorithm to the training of Gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data. 1 INTRODUCTION Mixture density models, in particular normal mixtures, have been extensively used in the field of statistical pattern recognition [1]. Recently, more sophisticated mix ture density models such as mi",Optimization & Theoretical ML,253f7b5d921338af34da817c00f42753-Paper.pdf,1998
Learning Lie Groups for Invariant Visual Perception*,"One of the most important problems in visual perception is that of visual in variance: how are objects perceived to be the same despite undergoing transfor mations such as translations, rotations or scaling? In this paper, we describe a Bayesian method for learning invariances based on Lie group theory. We show that previous approaches based on first-order Taylor series expansions of inputs can be regarded as special cases of the Lie group approach, the latter being ca pable of handling in principle arbitrarily large transfonnations. Using a matrix exponential based generative model of images, we derive an unsupervised al gorithm for learning Lie group operators from input data containing infinites imal transfonnations. The on-line unsupervised learning algorithm maximizes the posterior probability of generating the training data. We provide experimen tal results suggesting that the proposed method can learn Lie group operators for handling reasonably large I-D translations and 2-D rot",Computer Vision,277281aada22045c03945dcb2ca6f2ec-Paper.pdf,1998
Tractable Variational Structures for,"Graphical models provide a broad probabilistic framework with ap plications in speech recognition (Hidden Markov Models), medical diagnosis (Belief networks) and artificial intelligence (Boltzmann Machines). However, the computing time is typically exponential in the number of nodes in the graph. Within the variational frame work for approximating these models, we present two classes of dis tributions, decimatable Boltzmann Machines and Tractable Belief Networks that go beyond the standard factorized approach. We give generalised mean-field equations for both these directed and undirected approximations. Simulation results on a small bench mark problem suggest using these richer approximations compares favorably against others previously reported in the literature. 1 Introduction Graphical models provide a powerful framework for probabilistic inference[l] but suffer intractability when applied to large scale problems. Recently, variational ap proximations have been popular [2, 3, 4, 5]",Optimization & Theoretical ML,297fa7777981f402dbba17e9f29e292d-Paper.pdf,1998
A Micropower CMOS Adaptive Amplitude and,"In this paper we describe the architecture, implementation and experi mental results for an Intracardiac Electrogram (ICEG) classification and compression chip. The chip processes and vector-quantises 30 dimen sional analogue vectors while consuming a maximum of 2.5 J-tW power for a heart rate of 60 beats per minute (1 vector per second) from a 3.3 V supply. This represents a significant advance on previous work which achieved ultra low power supervised morphology classification since the template matching scheme used in this chip enables unsupervised blind classification of abnonnal rhythms and the computational support for low bit rate data compression. The adaptive template matching scheme used is tolerant to amplitude variations, and inter-and intra-sample time shifts. 1 INTRODUCTION Implantable cardioverter defibrillators (ICDs) are devices used to monitor the electrical activity of the heart muscle and to apply appropriate levels of electrical stimulation if ab nonnal conditions ",Computer Vision,29921001f2f04bd3baee84a12e98098f-Paper.pdf,1998
Learning to Find Pictures of People,"Finding articulated objects, like people, in pictures present.s a par ticularly difficult object. recognition problem. We show how t.o find people by finding putative body segments, and then construct. ing assemblies of those segments that are consist.ent with the con straints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at. least nine segments, it is not possible to present every group to a classi fier. Instead, the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classi fier, and demonstrate that our approach can be used to determine whether images of real scenes contain people. 1 Introduction Several t.ypical collpctions containing over ten million images are listed in [2]. There is an extensiw literature on obtaining images from large collections using features computed from t.he whole image,",Computer Vision,309fee4e541e51de2e41f21bebb342aa-Paper.pdf,1998
Convergence Rates of Algorithms for,"This paper formulates the problem of visual search as Bayesian inference and defines a Bayesian ensemble of problem instances. In particular, we address the problem of the detection of visual contours in noise/clutter by optimizing a global criterion which combines local intensity and geometry information. We analyze * the convergence rates of A search algorithms using results from information theory to bound the probability of rare events within the Bayesian ensemble. This analysis determines characteristics of the domain, which we call order parameters, that determine the convergence rates. In particular, we present a specific admissible * A algorithm with pruning which converges, with high probability, with expected time O(N) in the size of the problem. In addi tion, we briefly summarize extensions of this work which address fundamental limits of target contour detectability (Le. algorithm independent results) and the use of non-admissible heuristics. 1 Introduction Many problems in",Computer Vision,2b3bf3eee2475e03885a110e9acaab61-Paper.pdf,1998
Visualizing Group Structure*,"Cluster analysis is a fundamental principle in exploratory data analysis, providing the user with a description of the group struc ture of given data. A key problem in this context is the interpreta tion and visualization of clustering solutions in high-dimensional or abstract data spaces. In particular, probabilistic descriptions of the group structure, essential to capture inter-cluster relation ships, are hardly assessable by simple inspection ofthe probabilistic assignment variables. VVe present a novel approach to the visual ization of group structure. It is based on a statistical model of the object assignments which have been observed or estimated by a probabilistic clustering procedure. The objects or data points are embedded in a low dimensional Euclidean space by approximating the observed data statistics with a Gaussian mixture model. The algorithm provides a new approach to the visualization of the inher ent structure for a broad variety of data types, e.g. histogram data, ",Computer Vision,351b33587c5fdd93bd42ef7ac9995a28-Paper.pdf,1998
Modeling Surround Suppression in VI Neurons,"We examine the statistics of natural monochromatic images decomposed using a multi-scale wavelet basis. Although the coefficients of this rep resentation are nearly decorrelated, they exhibit important higher-order statistical dependencies that cannot be eliminated with purely linear pro c~ssing. In particular, rectified coefficients corresponding to basis func tions at neighboring spatial positions, orientations and scales are highly correlated. A method of removing these dependencies is to divide each coefficient by a weighted combination of its rectified neighbors. Sev eral successful models of the steady -state behavior of neurons in primary visual cortex are based on such ""divisive normalization"" computations, and thus our analysis provides a theoretical justification for these models. Perhaps more importantly, the statistical measurements explicitly specify the weights that should be used in computing the normalization signal. We demonstrate that this weighting is qualitatively c",Computer Vision,35309226eb45ec366ca86a4329a2b7c3-Paper.pdf,1998
Familiarity Discrimination of Radar,"The ARTMAP-FD neural network performs both identification (placing test patterns in classes encountered during training) and familiarity discrimination (judging whether a test pattern belongs to any of the classes encountered during training). The perfor mance of ARTMAP-FD is tested on radar pulse data obtained in the field, and compared to that of the nearest-neighbor-based NEN algorithm and to a k > 1 extension of NEN. 1 Introduction The recognition process involves both identification and familiarity discrimination. Consider, for example, a neural network designed to identify aircraft based on their radar reflections and trained on sample reflections from ten types of aircraft A ... J. After training, the network should correctly classify radar reflections belonging to the familiar classes A ... J, but it should also abstain from making a meaningless guess when presented with a radar reflection from an object belonging to a different, unfamiliar class. Familiarity discrimination is ",Computer Vision,35464c848f410e55a13bb9d78e7fddd0-Paper.pdf,1998
Graphical Models for Recognizing,"We describe a real-time computer vision and machine learning sys tem for modeling and recognizing human actions and interactions. Two different domains are explored: recognition of two-handed motions in the martial art 'Tai Chi', and multiple-person interac tions in a visual surveillance task. Our system combines top-down with bottom-up information using a feedback loop, and is formu lated with a Bayesian framework. Two different graphical models (HMMs and Coupled HMMs) are used for modeling both individual actions and multiple-agent interactions, and CHMMs are shown to work more efficiently and accurately for a given amount of train ing. Finally, to overcome the limited amounts of training data, we demonstrate that 'synthetic agents' (Alife-style agents) can be used to develop flexible prior models of the person-to-person inter actions. 1 INTRODUCTION We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in two different scena",Computer Vision,3a20f62a0af1aa152670bab3c602feed-Paper.pdf,1998
An entropic estimator for structure discovery,"We introduce a novel framework for simultaneous structure and parameter learning in hidden-variable conditional probability models, based on an en tropic prior and a solution for its maximum a posteriori (MAP) estimator. The MAP estimate minimizes uncertainty in all respects: cross-entropy between model and data; entropy of the model; entropy of the data's descriptive statistics. Iterative estimation extinguishes weakly supported parameters, compressing and sparsifying the model. Trimming operators accelerate this process by removing excess parameters and, unlike most pruning schemes, guarantee an increase in posterior probability. Entropic estimation takes a overcomplete random model and simplifies it, inducing the structure of relations between hidden and observed variables. Applied to hidden Markov models (HMMs), it finds a concise finite-state machine representing the hidden structure of a signal. We entropically model music, handwriting, and video time-series, and show that the re",Optimization & Theoretical ML,3fb451ca2e89b3a13095b059d8705b15-Paper.pdf,1998
The Effect of Correlations on the Fisher,"We study the effect of correlated noise on the accuracy of popu lation coding using a model of a population of neurons that are broadly tuned to an angle in two-dimension. The fluctuations in the neuronal activity is modeled as a Gaussian noise with pairwise correlations which decays exponentially with the difference between the preferred orientations of the pair. By calculating the Fisher in formation of the system, we show that in the biologically relevant regime of parameters positive correlations decrease the estimation capability of the network relative to the uncorrelated population. Moreover strong positive correlations result in information capac ity which saturates to a finite value as the number of cells in the population grows. In contrast, negative correlations substantially increase the information capacity of the neuronal population. 1 Introduction In many neural systems, information regarding sensory inputs or (intended) motor outputs is found to be distributed throughou",Computer Vision,41a60377ba920919939d83326ebee5a1-Paper.pdf,1998
Spike-Based Compared to Rate-Based,"A correlation-based learning rule at the spike level is formulated, mathematically analyzed, and compared to learning in a firing-rate description. A differential equation for the learning dynamics is derived under the assumption that the time scales of learning and spiking can be separated. For a linear Poissonian neuron model which receives time-dependent stochastic input we show that spike correlations on a millisecond time scale play indeed a role. Corre lations between input and output spikes tend to stabilize structure formation, provided that the form of the learning window is in accordance with Hebb's principle. Conditions for an intrinsic nor malization of the average synaptic weight are discussed. 1 Introduction Most learning rules are formulated in terms of mean firing rates, viz., a continuous variable reflecting the mean activity of a neuron. For example, a 'Hebbian' (Hebb 1949) learning rule which is driven by the correlations between presynaptic and postsynaptic rates ma",Optimization & Theoretical ML,42a3964579017f3cb42b26605b9ae8ef-Paper.pdf,1998
Contrast adaptation in simple cells by changing,"The contrast response function (CRF) of many neurons in the primary vi sual cortex saturates and shifts towards higher contrast values following prolonged presentation of high contrast visual stimuli. Using a recurrent neural network of excitatory spiking neurons with adapting synapses we show that both effects could be explained by a fast and a slow compo nent in the synaptic adaptation. (i) Fast synaptic depression leads to sat uration of the CRF and phase advance in the cortical response to high contrast stimuli. (ii) Slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal. This component-given by infomax learning rule-explains contrast adaptation of the averaged membrane potential (DC component) as well as the surprising experi mental result, that the stimulus modulated component (Fl component) of a cortical cell's membrane potential adapts only weakly. Based on our ",Computer Vision,42d6c7d61481d1c21bd1635f59edae05-Paper.pdf,1998
Optimizing Correlation Algorithms for,"The perfonnance of dedicated VLSI neural processing hardware depends critically on the design of the implemented algorithms. We have pre viously proposed an algorithm for acoustic transient classification [1]. Having implemented and demonstrated this algorithm in a mixed-mode architecture, we now investigate variants on the algorithm, using time and frequency channel differencing, input and output nonnalization, and schemes to binarize and train the template values, with the goal of achiev ing optimal classification perfonnance for the chosen hardware. 1 Introduction At the NIPS conference in 1996 [1], we introduced an algorithm for classifying acoustic transient signals using template correlation. While many pattern classification systems use template correlation [2}, our system differs in directly addressing the issue of efficient im plementation in analog hardware, to overcome the area and power consumption drawbacks of equivalent digital systems. In the intervening two years, we ha",Optimization & Theoretical ML,4462bf0ddbe0d0da40e1e828ebebeb11-Paper.pdf,1998
Information Maximization in Single Neurons,"Information from the senses must be compressed into the limited range of firing rates generated by spiking nerve cells. Optimal compression uses all firing rates equally often, implying that the nerve cell's response matches the statistics of naturally occurring stimuli. Since changing the voltage-dependent ionic conductances in the cell membrane alters the flow of information, an unsupervised, non-Hebbian, developmental learning rule is derived to adapt the conductances in Hodgkin-Huxley model neurons. By maximizing the rate of information transmission, each firing rate within the model neuron's limited dynamic range is used equally often. An efficient neuronal representation of incoming sensory information should take advan tage of the regularity and scale invariance of stimulus features in the natural world. In the case of vision, this regularity is reflected in the typical probabilities of encountering particular visual contrasts, spatial orientations, or colors [1]. Given these pr",Optimization & Theoretical ML,452bf208bf901322968557227b8f6efe-Paper.pdf,1998
Mechanisms of generalization,"The learning of many visual perceptual tasks has been shown to be specific to practiced stimuli, while new stimuli require re-Iearning from scratch. Here we demonstrate generalization using a novel paradigm in motion discrimination where learning has been previ ously shown to be specific. We trained subjects to discriminate the directions of moving dots, and verified the previous results that learning does not transfer from the trained direction to a new one. However, by tracking the subjects' performance across time in the new direction, we found that their rate of learning doubled. Therefore, learning generalized in a task previously considered too difficult for generalization. We also replicated, in the second ex periment, transfer following training with ""easy"" stimuli. The specificity of perceptual learning and the dichotomy between learning of ""easy"" vs. ""difficult"" tasks were hypothesized to involve different learning processes, operating at different visual cortical areas. Here",Optimization & Theoretical ML,46031b3d04dc90994ca317a7c55c4289-Paper.pdf,1998
Tight Bounds for the VC-Dimension of,"O(ws(s log d+log(dqh/ s))) and O(ws((h/ s) log q) +log(dqh/ s)) are upper bounds for the VC-dimension of a set of neural networks of units with piecewise polynomial activation functions, where s is the depth of the network, h is the number of hidden units, w is the number of adjustable parameters, q is the maximum of the number of polynomial segments of the activation function, and d is the maximum degree of the polynomials; also n(wslog(dqh/s)) is a lower bound for the VC-dimension of such a network set, which are tight for the cases s = 8(h) and s is constant. For the special case q = 1, the VC-dimension is 8(ws log d). 1 Introduction In spite of its importance, we had been unable to obtain VC-dimension values for practical types of networks, until fairly tight upper and lower bounds were obtained ([6], [8], [9], and [10]) for linear threshold element networks in which all elements perform a threshold function on weighted sum of inputs. Roughly, the lower bound for the networks is (1",Optimization & Theoretical ML,495dabfd0ca768a3c3abd672079f48b6-Paper.pdf,1998
Probabilistic Image Sensor Fusion,"We present a probabilistic method for fusion of images produced by multiple sensors. The approach is based on an image formation model in which the sensor images are noisy, locally linear functions of an underlying, true scene. A Bayesian framework then provides for maximum likelihood or maximum a posteriori estimates of the true scene from the sensor images. Maximum likelihood estimates of the parameters of the image formation model involve (local) second order image statistics, and thus are related to local principal component analysis. We demonstrate the efficacy of the method on images from visible-band and infrared sensors. 1 Introduction Advances in sensing devices have fueled the deployment of multiple sensors in several computational vision systems [1, for example]. Using multiple sensors can increase reliability with respect to single sensor systems. This work was motivated by a need for an aircraft autonomous landing guidance (ALG) system [2, 3] that uses visible-band, infrar",Computer Vision,490640b43519c77281cb2f8471e61a71-Paper.pdf,1998
Phase Diagram and Storage Capacity of,"We solve the dynamics of Hopfield-type neural networks which store se quences of patterns, close to saturation. The asymmetry of the interaction matrix in such models leads to violation of detailed balance, ruling out an equilibrium statistical mechanical analysis. Using generating functional methods we derive exact closed equations for dynamical order parame ters, viz. the sequence overlap and correlation and response functions. in the limit of an infinite system size. We calculate the time translation invariant solutions of these equations. describing stationary limit-cycles. which leads to a phase diagram. The effective retarded self-interaction usually appearing in symmetric models is here found to vanish, which causes a significantly enlarged storage capacity of eYe ~ 0.269. com pared to eYe ~ 0.139 for Hopfield networks s~oring static patterns. Our results are tested against extensive computer simulations and excellent agreement is found. 212 A. Diiring, A. C. C. Coo/en and D. Sh",Optimization & Theoretical ML,49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf,1998
Example Based Image Synthesis of Articulated,We present a method for learning complex appearance mappings. such as occur with images of articulated objects. Traditional interpolation networks fail on this case since appearance is not necessarily a smooth function nor a linear manifold for articulated objects. We define an ap pearance mapping from examples by constructing a set of independently smooth interpolation networks; these networks can cover overlapping re gions of parameter space. A set growing procedure is used to find ex ample clusters which are well-approximated within their convex hull; interpolation then proceeds only within these sets of examples. With this method physically valid images are produced even in regions of param eter space where nearby examples have different appearances. We show results generating both simulated and real arm images. 1 Introduction Image-based view synthesis is.an important application of learning networks. offering the ability to render realistic images without requiring detailed model,Computer Vision,49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf,1998
Lazy Learning Meets,"Lazy learning is a memory-based technique that, once a query is re ceived, extracts a prediction interpolating locally the neighboring exam ples of the query which are considered relevant according to a distance measure. In this paper we propose a data-driven method to select on a query-by-query basis the optimal number of neighbors to be considered for each prediction. As an efficient way to identify and validate local models, the recursive least squares algorithm is introduced in the con text of local approximation and lazy learning. Furthermore, beside the winner-takes-all strategy for model selection, a local combination of the most promising models is explored. The method proposed is tested on six different datasets and compared with a state-of-the-art approach. 1 Introduction Lazy learning (Aha, 1997) postpones all the computation until an explicit request for a prediction is received. The request is fulfilled interpolating locally the examples consid ered relevant according to a",Optimization & Theoretical ML,4dcf435435894a4d0972046fc566af76-Paper.pdf,1998
Shrinking the Thbe:,"A new algorithm for Support Vector regression is described. For a priori chosen 1/, it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction 1/ of the data points lie outside. More over, it is shown how to use parametric tube shapes with non-constant radius. The algorithm is analysed theoretically and experimentally. 1 INTRODUCTION Support Vector (SV) machines comprise a new class of learning algorithms, motivated by results of statistical learning theory (Vapnik, 1995). Originally developed for pattern recog nition, they represent the decision boundary in terms of a typically small subset (SchOikopf et aI., 1995) of all training examples, called the Support Vectors. In order for this property to carryover to the case of SV Regression, Vapnik devised the so-called E-insensitive loss function Iy - f(x)lc = max{O, Iy - f(x)1 - E}, which does not penalize errors below some E > 0, chosen a priori. His algorithm, which we will henceforth call E-SVR",Optimization & Theoretical ML,4d6e4749289c4ec58c0063a90deb3964-Paper.pdf,1998
Reinforcement Learning for Trading,"We propose to train trading systems by optimizing financial objec tive functions via reinforcement learning. The performance func tions that we consider are profit or wealth, the Sharpe ratio and our recently proposed differential Sharpe ratio for online learn ing. In Moody & Wu (1997), we presented empirical results that demonstrate the advantages of reinforcement learning relative to supervised learning. Here we extend our previous work to com pare Q-Learning to our Recurrent Reinforcement Learning (RRL) algorithm. We provide new simulation results that demonstrate the presence of predictability in the monthly S&P 500 Stock Index for the 25 year period 1970 through 1994, as well as a sensitivity analysis that provides economic insight into the trader's structure. 1 Introduction: Reinforcement Learning for Thading The investor's or trader's ultimate goal is to optimize some relevant measure of trading system performance, such as profit, economic utility or risk-adjusted re turn. In th",Optimization & Theoretical ML,4e6cd95227cb0c280e99a195be5f6615-Paper.pdf,1998
Distributional Population Codes and,"Most theoretical and empirical studies of population codes make the assumption that underlying neuronal activities is a unique and unambiguous value of an encoded quantity. However, population activities can contain additional information about such things as multiple values of or uncertainty about the quantity. We have pre viously suggested a method to recover extra information by treat ing the activities of the population of cells as coding for a com plete distribution over the coded quantity rather than just a single value. We now show how this approach bears on psychophys ical and neurophysiological studies of population codes for mo tion direction in tasks involving transparent motion stimuli. We show that, unlike standard approaches, it is able to recover mul tiple motions from population responses, and also that its output is consistent with both correct and erroneous human performance on psychophysical tasks. A population code can be defined as a set of units whose activities c",Computer Vision,4e9cec1f583056459111d63e24f3b8ef-Paper.pdf,1998
USING COLLECTIVE INTELLIGENCE,"A COllective INtelligence (COIN) is a set of interacting reinforce ment learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments us ing that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms. 1 INTRODUCTION COllective INtelligences (COINs) are large, sparsely connected recurrent neural networks, whose ""neurons"" are reinforcement learning (RL) algorithms. The dis tinguishing feature of COINs is that their dynamics involves no centralized control, but only the collective effects of the individual neurons each modifying their be havior via their individual RL algorithms. This restriction holds even though the goal of the COIN concerns the system's global behavior. One naturally-occurring COIN is a human economy, where the ""neurons"" ",Optimization & Theoretical ML,5129a5ddcd0dcd755232baa04c231698-Paper.pdf,1998
Sparse Code Shrinkage: Denoising by,"Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to re dundancy reduction and independent component analysis, and has some neurophysiological plausibility. In this paper, we show how sparse coding can be used for denoising. Using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise, we show how to apply a shrinkage nonlinearity on the components of sparse coding so as to reduce noise. Furthermore, we show how to choose the optimal sparse coding basis for denoising. Our method is closely related to the method of wavelet shrinkage, but has the important benefit over wavelet methods that both the features and the shrinkage parameters are estimated directly from the data. 1 Introduction A fundamental problem in neural network research is to find a suitable representa tion for the data. One of the simplest methods is ",Optimization & Theoretical ML,52947e0ade57a09e4a1386d08f17b656-Paper.pdf,1998
Finite-dimensional approximation of,"Gaussian process (GP) prediction suffers from O(n3) scaling with the data set size n. By using a finite-dimensional basis to approximate the GP predictor, the computational complexity can be reduced. We de rive optimal finite-dimensional predictors under a number of assump tions, and show the superiority of these predictors over the Projected Bayes Regression method (which is asymptotically optimal). We also show how to calculate the minimal model size for a given n. The calculations are backed up by numerical experiments. 1 Introduction Over the last decade there has been a growing interest in the Bayesian approach to regression problems, using both neural networks and Gaussian process (GP) prediction, that is regression performed in function spaces when using a Gaussian random process as a prior. The computational complexity of the GP predictor scales as O(n3), where n is the size Finite-Dimensional Approximation o/Gaussian Processes 219 of the datasetl. This suggests using a finite-",Optimization & Theoretical ML,55c567fd4395ecef6d936cf77b8d5b2b-Paper.pdf,1998
Controlling the Complexity of HMM Systems by,"This paper introduces a method for regularization ofHMM systems that avoids parameter overfitting caused by insufficient training data. Regu larization is done by augmenting the EM training method by a penalty term that favors simple and smooth HMM systems. The penalty term is constructed as a mixture model of negative exponential distributions that is assumed to generate the state dependent emission probabilities of the HMMs. This new method is the successful transfer of a well known regularization approach in neural networks to the HMM domain and can be interpreted as a generalization of traditional state-tying for HMM sys tems. The effect of regularization is demonstrated for continuous speech recognition tasks by improving overfitted triphone models and by speaker adaptation with limited training data. 1 Introduction One general problem when constructing statistical pattern recognition systems is to ensure the capability to generalize well, i.e. the system must be able to classify ",NLP,5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf,1998
Learning to estimate scenes from images,"We seek the scene interpretation that best explains image data. For example, we may want to infer the projected velocities (scene) which best explain two consecutive image frames (image). From synthetic data, we model the relationship between image and scene patches, and between a scene patch and neighboring scene patches. Given' a new image, we propagate likelihoods in a Markov network (ignoring the effect of loops) to infer the underlying scene. This yields an efficient method to form low-level scene interpretations. We demonstrate the technique for motion analysis and estimating high resolution images from low-resolution ones. 1 Introduction There has been recent interest in studying the statistical properties of the visual world. Olshausen and Field [23J and Bell and Sejnowski [2J have derived VI-like receptive fields from ensembles of images; Simon celli and Schwartz [30J account for contrast normalization effects by redundancy reduction. Li and Atick [1 J explain retinal color co",Computer Vision,5c50b4df4b176845cd235b6a510c6903-Paper.pdf,1998
Computation of Smooth Optical Flow in a,"In 1986, Tanner and Mead [1] implemented an interesting constraint sat isfaction circuit for global motion sensing in a VLSI. We report here a new and improved a VLSI implementation that provides smooth optical flow as well as global motion in a two dimensional visual field. The com putation of optical flow is an ill-posed problem, which expresses itself as the aperture problem. However, the optical flow can be estimated by the use of regularization methods, in which additional constraints are intro duced in terms of a global energy functional that must be minimized. We show how the algorithmic constraints of Hom and Schunck [2] on com puting smooth optical flow can be mapped onto the physical constraints of an equivalent electronic network. 1 Motivation The perception of apparent motion is crucial for navigation. Knowledge of local motion of the environment relative to the observer simplifies the calculation of important tasks such as time-to-contact or focus-of-expansion. There are s",Computer Vision,596f713f9a7376fe90a62abaaedecc2d-Paper.pdf,1998
Learning curves for Gaussian processes,"I consider the problem of calculating learning curves (i.e., average generalization performance) of Gaussian processes used for regres sion. A simple expression for the generalization error in terms of the eigenvalue decomposition of the covariance function is derived, and used as the starting point for several approximation schemes. I identify where these become exact, and compare with existing bounds on learning curves; the new approximations, which can be used for any input space dimension, generally get substantially closer to the truth. 1 INTRODUCTION: GAUSSIAN PROCESSES Within the neural networks community, there has in the last few years been a good deal of excitement about the use of Gaussian processes as an alternative to feedforward networks [lJ. The advantages of Gaussian processes are that prior assumptions about the problem to be learned are encoded in a very transparent way, and that inference-at least in the case of regression that I will consider-is relatively straightf",Optimization & Theoretical ML,5cbdfd0dfa22a3fca7266376887f549b-Paper.pdf,1998
Facial Memory is Kernel Density Estimation,"We compare the ability of three exemplar-based memory models, each using three different face stimulus representations, to account for the probability a human subject responded ""old"" in an old/new facial mem ory experiment. The models are 1) the Generalized Context Model, 2) SimSample, a probabilistic sampling model, and 3) MMOM, a novel model related to kernel density estimation that explicitly encodes stim ulus distinctiveness. The representations are 1) positions of stimuli in MDS ""face space,"" 2) projections of test faces onto the ""eigenfaces"" of the study set, and 3) a representation based on response to a grid of Gabor filter jets. Of the 9 model/representation combinations, only the distinc tiveness model in MDS space predicts the observed ""morph familiarity inversion"" effect, in which the subjects' false alarm rate for morphs be tween similar faces is higher than their hit rate for many of the studied faces. This evidence is consistent with the hypothesis that human mem ory for",Computer Vision,5cce8dede893813f879b873962fb669f-Paper.pdf,1998
VLSI Implementation of Motion Centroid,"A circuit for fast, compact and low-power focal-plane motion centroid localization is presented. This chip, which uses mixed signal CMOS components to implement photodetection, edge detection, ON-set detection and centroid localization, models the retina and superior colliculus. The centroid localization circuit uses time-windowed asynchronously triggered row and column address events and two linear resistive grids to provide the analog coordinates of the motion centroid. This VLSI chip is used to realize fast lightweight autonavigating vehicles. The obstacle avoiding line-following algorithm is discussed. 1 INTRODUCTION Many neuromorphic chips which mimic the analog and parallel characteristics of visual, auditory and cortical neural circuits have been designed [Mead, 1989; Koch, 1995]. Recently researchers have started to combine digital circuits with neuromorphic aVLSI systems [Boahen, 1996]. The persistent doctrine, however, has been that computation should be performed in analog, ",Computer Vision,6490791e7abf6b29a381288cc23a8223-Paper.pdf,1998
Discontinuous Recall Transitions Induced By,"We present exact analytical equilibrium solutions for a class of recur rent neural network models, with both sequential and parallel neuronal dynamics, in which there is a tunable competition between nearest neighbour and long-range synaptic interactions. This competition is found to induce novel coexistence phenomena as well as discontinuous transitions between pattern recall states, 2-cycles and non-recall states. 1 INTRODUCTION Analytically solvable models of large recurrent neural networks are bound to be simplified representations of biological reality. In early analytical studies such as [1,2] neurons were, for instance, only allowed to interact with a strength which was independent of their spatial distance (these are the so-called mean field models). At present both the statics of infinitely large mean-field models of recurrent networks, as well as their dynamics away from satura tion are well understood, and have obtained the status of textbook or review paper material [3,4]. ",Optimization & Theoretical ML,655ea4bd3b5736d88afc30c9212ccddf-Paper.pdf,1998
An Integrated Vision Sensor for the,"A robust, integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation (the singular point) in optical flow fields such as those generated by self-motion. Measurements are shown of a fully parallel CMOS analog VLSI motion sensor array which computes the direction of local motion (sign of optical flow) at each pixel and can directly implement this algorithm. The flow field singular point is computed in real time with a power consumption of less than 2 m W. Computation of the singular point for more general flow fields requires measures of field expansion and rotation, which it is shown can also be computed in real-time hardware, again using only the sign of the optical flow field. These measures, along with the location of the singular point, provide robust real-time self-motion information for the visual guidance of a moving platform such as a robot. 1 INTRODUCTION Visually guided navigation of autonomous vehicles requires robust measures ",Computer Vision,69d658d0b2859e32cd4dc3b970c8496c-Paper.pdf,1998
"Robust. Efficient, Globally-Optimized","Parti-game (Moore 1994a; Moore 1994b; Moore and Atkeson 1995) is a reinforcement learning (RL) algorithm that has a lot of promise in over coming the curse of dimensionality that can plague RL algorithms when applied to high-dimensional problems. In this paper we introduce mod ifications to the algorithm that further improve its performance and ro bustness. In addition, while parti-game solutions can be improved locally by standard local path-improvement techniques, we introduce an add-on algorithm in the same spirit as parti-game that instead tries to improve solutions in a non-local manner. 1 INTRODUCTION Parti-game operates on goal problems by dynamically partitioning the space into hyper rectangular cells of varying sizes, represented using a k-d tree data structure. It assumes the existence of a pre-specified local controller that can be commanded to proceed from the current state to a given state. The algorithm uses a game-theoretic approach to assign costs to cells based on past",Reinforcement Learning,6b8eba43551742214453411664a0dcc8-Paper.pdf,1998
Learning a Hierarchical Belief Network of,"Many belief networks have been proposed that are composed of binary units. However, for tasks such as object and speech recog nition which produce real-valued data, binary network models are usually inadequate. Independent component analysis (ICA) learns a model from real data, but the descriptive power of this model is severly limited. We begin by describing the independent factor analysis (IFA) technique, which overcomes some of the limitations of ICA. We then create a multilayer network by cascading single layer IFA models. At each level, the IFA network extracts real valued latent variables that are non-linear functions of the input data with a highly adaptive functional form, resulting in a hier archical distributed representation of these data. Whereas exact maximum-likelihood learning of the network is intractable, we de rive an algorithm that maximizes a lower bound on the likelihood, based on a variational approach. 1 Introduction An intriguing hypothesis for how the brain rep",Computer Vision,6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf,1998
A Reinforcement Learning Algorithm,"We describe a Reinforcement Learning algorithm for partially observ able environments using short-term memory, which we call BLHT. Since BLHT learns a stochastic model based on Bayesian Learning, the over fitting problem is reasonably solved. Moreover, BLHT has an efficient implementation. This paper shows that the model learned by BLHT con verges to one which provides the most accurate predictions of percepts and rewards, given short-term memory. 1 INTRODUCTION Research on Reinforcement Learning (RL) prob model-free lem for partially observable environments is gain ing more attention recently. This is mainly because the assumption that perfect and complete perception of the state of the environment is available for the learning agent, which many previous RL algorithms Figure I: Three approaches require, is not valid for many realistic environments. One of the approaches to the problem is the model-free approach (Singh et al. 1995; Jaakkola et al. 1995) (arrow a in the Fig.l) which giv",Reinforcement Learning,6d3a1e06d6a06349436bc054313b648c-Paper.pdf,1998
Semiparametric Support Vector and,"Semiparametric models are useful tools in the case where domain knowledge exists about the function to be estimated or emphasis is put onto understandability of the model. We extend two learning algorithms - Support Vector machines and Linear Programming machines to this case and give experimental results for SV ma chines. 1 Introduction One of the strengths of Support Vector (SV) machines is that they are nonparamet ric techniques, where one does not have to e.g. specify the number of basis functions beforehand. In fact, for many of the kernels used (not the polynomial kernels) like Gaussian rbf-kernels it can be shown [6] that SV machines are universal approxi mators. While this is advantageous in general, parametric models are useful techniques in their own right. Especially if one happens to have additional knowledge about the problem, it would be unwise not to take advantage of it. For instance it might be the case that the major properties of the data are described by a combinati",Optimization & Theoretical ML,70efba66d3d8d53194fb1a8446ae07fa-Paper.pdf,1998
Maximum-Likelihood Continuity Mapping,"We describe Maximum-Likelihood Continuity Mapping (MALCOM), an alternative to hidden Markov models (HMMs) for processing sequence data such as speech. While HMMs have a discrete ""hidden"" space con strained by a fixed finite-automaton architecture, MALCOM has a con tinuous hidden space-a continuity map-that is constrained only by a smoothness requirement on paths through the space. MALCOM fits into the same probabilistic framework for speech recognition as HMMs, but it represents a more realistic model of the speech production process. To evaluate the extent to which MALCOM captures speech production information, we generated continuous speech continuity maps for three speakers and used the paths through them to predict measured speech articulator data. The median correlation between the MALCOM paths obtained from only the speech acoustics and articulator measurements was 0.77 on an independent test set not used to train MALCOM or the predictor. This unsupervised model achieved correlat",NLP,6dd4e10e3296fa63738371ec0d5df818-Paper.pdf,1998
Regularizing AdaBoost,"Boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases. Also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers, which then leads to the dilemma of non-smooth fits and overfitting. Therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept: (1) AdaBoost and regularized versions of (2) linear reg and (3) quadratic programming AdaBoost. Experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier: the support vector machine. 1 Introd uction Boosting and other ensemble methods have been used with success in several ap plications, e.g. OCR [13, 8]. For low noise cases several lines of explanation have been proposed as candidates for explaining the well functioning of boosting meth ods. (a) Breiman proposed that ",Optimization & Theoretical ML,71a58e8cb75904f24cde464161c3e766-Paper.pdf,1998
The Bias-Variance Tradeoff and the Randomized,"We propose a new in-sample cross validation based method (randomized GACV) for choosing smoothing or bandwidth parameters that govern the bias-variance or fit-complexity tradeoff in 'soft' classification. Soft clas sification refers to a learning procedure which estimates the probability that an example with a given attribute vector is in class 1 vs class O. The target for optimizing the the tradeoff is the Kullback-Liebler distance between the estimated probability distribution and the 'true' probabil ity distribution, representing knowledge of an infinite population. The method uses a randomized estimate of the trace of a Hessian and mimics cross validation at the cost of a single relearning with perturbed outcome data. 1 INTRODUCTION We propose and test a new in-sample cross-validation based method for optimizing the bias variance tradeoff in 'soft classification' (Wahba et al1994), called ranG ACV (randomized Generalized Approximate Cross Validation). Summarizing from Wahba et al(l",Optimization & Theoretical ML,729c68884bd359ade15d5f163166738a-Paper.pdf,1998
Adding Constrained Discontinuities to Gaussian,"Gaussian Processes provide good prior models for spatial data, but can be too smooth. In many physical situations there are discontinuities along bounding surfaces, for example fronts in near-surface wind fields. We describe a modelling method for such a constrained discontinuity and demonstrate how to infer the model parameters in wind fields with MCMC sampling. 1 INTRODUCTION We introduce a model for wind fields based on Gaussian Processes (GPs) with 'constrained discontinuities'. GPs provide a flexible framework for modelling various systems. They have been adopted in the neural network community and are interpreted as placing priors over functions. Stationary vector-valued GP models (Daley, 1991) can produce realistic wind fields when run as a generative model; however, the resulting wind fields do not contain some features typical of the atmosphere. The most difficult features to include are surface fronts. Fronts are generated by complex atmospheric dynamics and are marked by lar",Optimization & Theoretical ML,77f959f119f4fb2321e9ce801e2f5163-Paper.pdf,1998
DTs: Dynamic Trees,"In this paper we introduce a new class of image models, which we call dynamic trees or DTs. A dynamic tree model specifies a prior over a large number of trees, each one of which is a tree-structured belief net (TSBN). Experiments show that DTs are capable of generating images that are less blocky, and the models have better translation invariance properties than a fixed, ""balanced"" TSBN. We also show that Simulated Annealing is effective at finding trees which have high posterior probability. 1 Introduction In this paper we introduce a new class of image models, which we call dynamic trees or DTs. A dynamic tree model specifies a prior over a large number of trees, each one of which is a tree-structured belief net (TSBN). Our aim is to retain the advantages of tree-structured belief networks, namely the hierarchical structure of the model and (in part) the efficient inference algorithms, while avoiding the ""blocky"" artifacts that derive from a single, fixed TSBN structure. One use for",Computer Vision,748ba69d3e8d1af87f84fee909eef339-Paper.pdf,1998
Synergy and redundancy among brain,"Determining the relationship between the activity of a single nerve cell to that of an entire population is a fundamental question that bears on the basic neural computation paradigms. In this paper we apply an information theoretic approach to quantify the level of cooperative activity among cells in a behavioral context. It is possible to discriminate between synergetic activity of the cells vs. redundant activity, depending on the difference between the infor mation they provide when measured jointly and the information they provide independently. We define a synergy value that is pos itive in the first case and negative in the second and show that the synergy value can be measured by detecting the behavioral mode of the animal from simultaneously recorded activity of the cells. We observe that among cortical cells positive synergy can be found, while cells from the basal ganglia, active during the same task, do not exhibit similar synergetic activity. titay,tishby}@cs.huji.ac.il Pe",NLP,7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf,1998
The Belief in TAP,"We show the similarity between belief propagation and TAP, for decoding corrupted messages encoded by Sourlas's method. The latter is a special case of the Gallager error-correcting code, where the code word comprises products of J{ bits selected randomly from the original message. We examine the efficacy of solutions obtained by the two methods for various values of J{ and show that solutions for J{ 2': 3 may be sensitive to the choice of initial conditions in the case of unbiased patterns. Good approximations are obtained generally for J{ = 2 and for biased patterns in the case of J{ 2': 3, especially when Nishimori's temperature is being used. 1 Introduction Belief networks [1] are diagrammatic representations of joint probability distribu tions over a set of variables. This set is usually represented by the vertices of a graph, while arcs between vertices represent probabilistic dependencies between variables. Belief propagation provides a convenient mathematical tool for calculat ",Optimization & Theoretical ML,7949e456002b28988d38185bd30e77fd-Paper.pdf,1998
Classification on Pairwise Proximity Data,"We investigate the problem of learning a classification task on data represented in terms of their pairwise proximities. This representa tion does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of us ing Euclidean feature vectors, from which pairwise proximities can always be calculated. Our first approach is based on a combined linear embedding and classification procedure resulting in an ex tension of the Optimal Hyperplane algorithm to pseudo-Euclidean data. As an alternative we present another approach based on a linear threshold model in the proximity values themselves, which is optimized using Structural Risk Minimization. We show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics W.r.t. their gener alization. Finally, the algorithms are successfully applied to protein structure data and to data from the cat's cerebral cortex. They show b",Optimization & Theoretical ML,7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf,1998
Using Analytic QP and Sparseness to Speed,"Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) problem. This paper proposes an al gorithm for training SVMs: Sequential Minimal Optimization, or SMO. SMO breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable. Thus, SMO does not require a numerical QP library. SMO's computation time is dominated by eval uation of the kernel, hence kernel optimizations substantially quicken SMO. For the MNIST database, SMO is 1.7 times as fast as PCG chunk ing; while for the UCI Adult database and linear SVMs, SMO can be 1500 times faster than the PCG chunking algorithm. 1 INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) [1]. SVMs have empirically been shown to give good generalization performance on a wide variety of problems. However, the use of SVMs is stilI limited to a small group of researchers. One possible reason is that training al",Optimization & Theoretical ML,7e1d842d0f7ee600116ffc6b2d87d83f-Paper.pdf,1998
Dynamically Adapting Kernels in Support,"The kernel-parameter is one of the few tunable parameters in Sup port Vector machines, controlling the complexity of the resulting hypothesis. Its choice amounts to model selection and its value is usually found by means of a validation set. We present an algo rithm which can automatically perform model selection with little additional computational cost and with no need of a validation set. In this procedure model selection and learning are not separate, but kernels are dynamically adjusted during the learning process to find the kernel parameter which provides the best possible upper bound on the generalisation error. Theoretical results motivating the approach and experimental results confirming its validity are presented. 1 Introduction Support Vector Machines (SVMs) are learning systems designed to automatically trade-off accuracy and complexity by minimizing an upper bound on the general isation error provided by VC theory. In practice, however, SVMs still have a few tunable para",Optimization & Theoretical ML,7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf,1998
"Temporally Asymmetric Hebbian Learning,","Recent experimental data indicate that the strengthening or weakening of synaptic connections between neurons depends on the relative timing of pre- and postsynaptic action potentials. A Hebbian synaptic modification rule based on these data leads to a stable state in which the excitatory and inhibitory inputs to a neuron are balanced, producing an irregular pattern of firing. It has been proposed that neurons in vivo operate in such a mode. 1 Introduction Hebbian modification of network interconnections plays a central role in the study of learn ing in neural networks (Rumelhart and McClelland, 1986; Hertz et al., 1991). Most work on Hebbian learning involves network models in which the activities of the individual units are represented by continuous variables. A Hebbian learning rule, in this context, is spec ified by describing how network weights change as a function of the activities of the units that transmit and receive signals across a given network connection. While analyses o",Optimization & Theoretical ML,806beafe154032a5b818e97b4420ad98-Paper.pdf,1998
On the optimality of incremental neural network,"We study the approximation of functions by two-layer feedforward neu ral networks, focusing on incremental algorithms which greedily add units, estimating single unit parameters at each stage. As opposed to standard algorithms for fixed architectures, the optimization at each stage is performed over a small number of parameters, mitigating many of the difficult numerical problems inherent in high-dimensional non-linear op timization. We establish upper bounds on the error incurred by the al gorithm, when approximating functions from the Sobolev class, thereby extending previous results which only provided rates of convergence for functions in certain convex hulls of functional spaces. By comparing our results to recently derived lower bounds, we show that the greedy algo rithms are nearly optimal. Combined with estimation error results for greedy algorithms, a strong case can be made for this type of approach. 1 Introduction and background A major problem in the application of neural n",Optimization & Theoretical ML,81c8727c62e800be708dbf37c4695dff-Paper.pdf,1998
Learning Mixture Hierarchies,"The hierarchical representation of data has various applications in do mains such as data mining, machine vision, or information retrieval. In this paper we introduce an extension of the Expectation-Maximization (EM) algorithm that learns mixture hierarchies in a computationally ef ficient manner. Efficiency is achieved by progressing in a bottom-up fashion, i.e. by clustering the mixture components of a given level in the hierarchy to obtain those of the level above. This cl ustering requires onl y knowledge of the mixture parameters, there being no need to resort to intermediate samples. In addition to practical applications, the algorithm allows a new interpretation of EM that makes clear the relationship with non-parametric kernel-based estimation methods, provides explicit con trol over the trade-off between the bias and variance of EM estimates, and offers new insights about the behavior of deterministic annealing methods commonly used with EM to escape local minima of the likeli",Optimization & Theoretical ML,819c9fbfb075d62a16393b9fe4fcbaa5-Paper.pdf,1998
Optimizing admission control while ensuring,"This paper examines the application of reinforcement learning to a telecommunications networking problem. The problem requires that rev enue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain states. We present a general solution to this multi-criteria problem that is able to earn significantly higher revenues than alternatives. 1 Introduction A number of researchers have recently explored the application of reinforcement learning (RL) to resource allocation and admission control problems in telecommunications. e.g., channel allocation in wireless systems, network routing, and admission control in telecom munication networks [1, 6, 7, 8]. Telecom problems are attractive applications for RL research because good, simple to implement, simulation models exist for them in the en gineering literature that are both widely used and results on which are trusted, because there are existing solutions to compare with, because small improvemen",Optimization & Theoretical ML,83e8ef518174e1eb6be4a0778d050c9d-Paper.pdf,1998
"Coding time-varying signals using sparse,","A common way to represent a time series is to divide it into short duration blocks, each of which is then represented by a set of basis functions. A limitation of this approach, however, is that the tem poral alignment of the basis functions with the underlying structure in the time series is arbitrary. We present an algorithm for encoding a time series that does not require blocking the data. The algorithm finds an efficient representation by inferring the best temporal po sitions for functions in a kernel basis. These can have arbitrary temporal extent and are not constrained to be orthogonal. This allows the model to capture structure in the signal that may occur at arbitrary temporal positions and preserves the relative temporal structure of underlying events. The model is shown to be equivalent to a very sparse and highly over complete basis. Under this model, the mapping from the data to the representation is nonlinear, but can be computed efficiently. This form also allows the u",Signal Processing,83f2550373f2f19492aa30fbd5b57512-Paper.pdf,1998
Evidence for a Forward Dynamics Model,"Based on computational principles, the concept of an internal model for adaptive control has been divided into a forward and an inverse model. However, there is as yet little evidence that learning control by the eNS is through adaptation of one or the other. Here we examine two adaptive control architectures, one based only on the inverse model and other based on a combination of forward and inverse models. We then show that for reaching movements of the hand in novel force fields, only the learning of the forward model results in key characteristics of performance that match the kine matics of human subjects. In contrast, the adaptive control system that relies only on the inverse model fails to produce the kinematic patterns observed in the subjects, despite the fact that it is more stable. Our results provide evidence that learning control of novel dynamics is via formation of a forward model. 1 Introduction The concept of an internal model, a system for predicting behavior of a co",Optimization & Theoretical ML,86df7dcfd896fcaf2674f757a2463eba-Paper.pdf,1998
Restructuring Sparse High Dimensional Data for,"The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words. Classically, documents and queries are represented as vectors of word counts. In its simplest form, relevance is defined to be the dot product between a document and a query vector-a measure of the number of common terms. A central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query. Linear dimensionality reduction has been proposed as a tech nique for extracting underlying structure from the document collection. In some domains (such as vision) dimensionality reduction reduces computational com plexity. In text retrieval it is more often used to improve retrieval performance. We propose an alternative and novel technique that produces sparse represen tations constructed from sets of highly-related words. Documents and queries are represented by their distance t",NLP,87ec2f451208df97228105657edb717f-Paper.pdf,1998
Classification in Non-Metric Spaces,"A key question in vision is how to represent our knowledge of previously encountered objects to classify new ones. The answer depends on how we determine the similarity of two objects. Similarity tells us how relevant each previously seen object is in determining the category to which a new object belongs. Here a dichotomy emerges. Complex notions of similar ity appear necessary for cognitive models and applications, while simple notions of similarity form a tractable basis for current computational ap proaches to classification. We explore the nature of this dichotomy and why it calls for new approaches to well-studied problems in learning. We begin this process by demonstrating new computational methods for supervised learning that can handle complex notions of similarity. (1) We discuss how to implement parametric met.hods that represent a class by its mean when using non-metric similarity functions; and (2) We review non-parametric methods that we have developed using near est neig",Computer Vision,88a199611ac2b85bd3f76e8ee7e55650-Paper.pdf,1998
Approximate Learning of Dynamic Models,"Inference is a key component in learning probabilistic models from par tially observable data. When learning temporal models, each of the many inference phases requires a traversal over an entire long data se quence; furthermore, the data structures manipulated are exponentially large, making this process computationally expensive. In [2], we describe an approximate inference algorithm for monitoring stochastic processes, and prove bounds on its approximation error. In this paper, we apply this algorithm as an approximate forward propagation step in an EM algorithm for learning temporal Bayesian networks. We provide a related approxi mation for the backward step, and prove error bounds for the combined algorithm. We show empirically that, for a real-life domain, EM using our inference algorithm is much faster than EM using exact inference, with almost no degradation in quality of the learned model. We extend our analysis to the online learning task, showing a bound on the error resulti",Optimization & Theoretical ML,894b77f805bd94d292574c38c5d628d5-Paper.pdf,1998
Independent Component Analysis of,"Calcium (Ca2+)is an ubiquitous intracellular messenger which reg ulates cellular processes, such as secretion, contraction, and cell proliferation. A number of different cell types respond to hormonal stimuli with periodic oscillations of the intracellular free calcium concentration ([Ca2+]i). These Ca2+ signals are often organized in complex temporal and spatial patterns even under conditions of sustained stimulation. Here we study the spatio-temporal as pects of intracellular calcium ([Ca2+]i) oscillations in clonal J3-cells (hamster insulin secreting cells, HIT) under pharmacological stim ulation (Schofi et al., 1996). We use a novel fast fixed-point al gorithm (Hyvarinen and Oja, 1997) for Independent Component Analysis (ICA) to blind source separation of the spatio-temporal dynamics of [Ca2+]i in a HIT-cell. Using this approach we find two significant independent components out of five differently mixed in put signals: one [Ca2+]i signal with a mean oscillatory period of 68s and a",Optimization & Theoretical ML,89ae0fe22c47d374bc9350ef99e01685-Paper.pdf,1998
A Model for Associative Multiplication,"Despite the fact that mental arithmetic is based on only a few hun dred basic facts and some simple algorithms, humans have a diffi cult time mastering the subject, and even experienced individuals make mistakes. Associative multiplication, the process of doing multiplication by memory without the use of rules or algorithms, is especially problematic. Humans exhibit certain characteristic phenomena in performing associative multiplications, both in the type of error and in the error frequency. We propose a model for the process of associative multiplication, and compare its perfor mance in both these phenomena with data from normal humans and from the model proposed by Anderson et al (1994). 1 INTRODUCTION Associative mUltiplication is defined as multiplication done without recourse to computational algorithms, and as such is mainly concerned with recalling the basic times table. Learning up to the ten times table requires learning at most 121 facts; in fact, if we assume that normal h",Optimization & Theoretical ML,8a146f1a3da4700cbf03cdc55e2daae6-Paper.pdf,1998
Batch and On-line Parameter Estimation of,"We describe a new iterative method for parameter estimation of Gaus sian mixtures. The new method is based on a framework developed by Kivinen and Warmuth for supervised on-line learning. In contrast to gra dient descent and EM, which estimate the mixture's covariance matrices, the proposed method estimates the inverses of the covariance matrices. Furthennore, the new parameter estimation procedure can be applied in both on-line and batch settings. We show experimentally that it is typi cally faster than EM, and usually requires about half as many iterations as EM. 1 Introduction Mixture models, in particular mixtures of Gaussians, have been a popular tool for density estimation, clustering, and un-supervised learning with a wide range of applications (see for instance [5, 2] and the references therein). Mixture models are one of the most useful tools for handling incomplete data, in particular hidden variables. For Gaussian mixtures the hidden variables indicate for each data point th",Optimization & Theoretical ML,8c00dee24c9878fea090ed070b44f1ab-Paper.pdf,1998
Learning Macro-Actions in Reinforcement,"We present a method for automatically constructing macro-actions from scratch from primitive actions during the reinforcement learning process. The overall idea is to reinforce the tendency to perform action b after action a if such a pattern of actions has been rewarded. We test the method on a bicycle task, the car-on-the-hill task, the race-track task and some grid-world tasks. For the bicycle and race-track tasks the use of macro-actions approximately halves the learning time, while for one of the grid-world tasks the learning time is reduced by a factor of 5. The method did not work for the car-on-the-hill task for reasons we discuss in the conclusion. 1 INTRODUCTION A macro-action is a sequence of actions chosen from the primitive actions of the prob lem.1 Lumping actions together as macros can be of great help for solving large prob lems (Korf, 1985a,b; Gullapalli, 1992) and can sometimes greatly speed up learning (lba, 1989; McGovern, Sutton & Fagg, 1997; McGovern & Sutton, 199",Reinforcement Learning,8f19793b2671094e63a15ab883d50137-Paper.pdf,1998
Fast Neural Network Emulation of Dynamical,"Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can be computation ally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient ""NeuroAnimator"" that exploits neural networks. Neu roAnimators are automatically trained off-line to emulate physical dy namics through the observation of physics-based models in action. De pending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conven tional numerical simulation. We demonstrate NeuroAnimators for a va riety of physics-based models. 1 Introduction Animation based on physical principles has been an influential trend in computer graphics for over a decade (see, e.g., [1, 2, 3]). This is not only due to the unsurpassed realism that physics-based techniques offer. In conjunction with suitable control and ",Computer Vision,92af93f73faf3cefc129b6bc55a748a9-Paper.pdf,1998
Neural Networks for Density Estimation,"We introduce two new techniques for density estimation. Our ap proach poses the problem as a supervised learning task which can be performed using Neural Networks. We introduce a stochas tic method for learning the cumulative distribution and an analo gous deterministic technique. We demonstrate convergence of our methods both theoretically and experimentally, and provide com parisons with the Parzen estimate. Our theoretical results demon strate better convergence properties than the Parzen estimate. 1 Introduction and Background A majority of problems in science and engineering have to be modeled in a prob abilistic manner. Even if the underlying phenomena are inherently deterministic, the complexity of these phenomena often makes a probabilistic formulation the only feasible approach from the computational point of view. Although quantities such as the mean, the variance, and possibly higher order moments of a random variable have often been sufficient to characterize a particular p",Optimization & Theoretical ML,9327969053c0068dd9e07c529866b94d-Paper.pdf,1998
Improved Switching,"In robotics and other control applications it is commonplace to have a pre existing set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible. In this paper we present a framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be ob tained by switching flexibly between given controllers, and example appli cations of the theorem. In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning. In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pair",Optimization & Theoretical ML,9597353e41e6957b5e7aa79214fcb256-Paper.pdf,1998
Dynamics of Supervised Learning with,"We study the dynamics of supervised learning in layered neural net works, in the regime where the size p of the training set is proportional to the number N of inputs. Here the local fields are no longer described by Gaussian distributions. We use dynamical replica theory to predict the evolution of macroscopic observables, including the relevant error measures, incorporating the old formalism in the limit piN --t 00. 1 INTRODUCTION Much progress has been made in solving the dynamics of supervised learning in layered neural networks, using the strategy of statistical mechanics: by deriving closed laws for the evolution of suitably chosen macroscopic observables (order parameters) in the limit of an infinite system size [1, 2, 3, 4]. For a recent review and guide to references see e.g. [5]. The main successful procedure developed so far is built on the following cornerstones: • The task to be learned is defined by a 'teacher', which is itself a neural network. This in duces a natural se",Optimization & Theoretical ML,95d309f0b035d97f69902e7972c2b2e6-Paper.pdf,1998
o p q rs n,No abstract found,"Based on the title of the research paper, I",962e56a8a0b0420d87272a682bfd1e53-Paper.pdf,1998
A Polygonal Line Algorithm for Constructing,"Principal curves have been defined as ""self consistent"" smooth curves which pass through the ""middle"" of a d-dimensional probability distri bution or data cloud. Recently, we [1] have offered a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution. The new definition made it possible to carry out a theoretical analysis of learning principal curves from training data. In this paper we propose a practical construction based on the new definition. Simulation results demonstrate that the new algorithm compares favorably with previous methods both in terms of performance and computational complexity. 1 Introduction Hastie [2] and Hastie and Stuetzle [3] (hereafter HS) generalized the self consistency prop erty of principal components and introduced the notion of principal curves. Consider a d-dimensional random vector X = (X(I), ",Computer Vision,97d0145823aeb8ed80617be62e08bdcc-Paper.pdf,1998
Neuronal Regulation Implements,"Human and animal studies show that mammalian brain undergoes massive synaptic pruning during childhood, removing about half of the synapses until puberty. We have previously shown that main taining network memory performance while synapses are deleted, requires that synapses are properly modified and pruned, remov ing the weaker synapses. We now show that neuronal regulation, a mechanism recently observed to maintain the average neuronal in put field, results in weight-dependent synaptic modification. Under the correct range of the degradation dimension and synaptic up per bound, neuronal regulation removes the weaker synapses and judiciously modifies the remaining synapses. It implements near optimal synaptic modification, and maintains the memory perfor mance of a network undergoing massive synaptic pruning. Thus, this paper shows that in addition to the known effects of Hebbian changes, neuronal regulation may play an important role in the self-organization of brain networks during ",Optimization & Theoretical ML,98986c005e5def2da341b4e0627d4712-Paper.pdf,1998
Finite-Sample Convergence Rates for,"In this paper, we address two issues of long-standing interest in the re inforcement learning literature. First, what kinds of performance guar antees can be made for Q-learning after only a finite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based (indirect) approaches, which use experience to estimate next-state distributions for off-line value iteration? We first show that both Q-learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the num ber of state transitions observed. In particular, on the order of only + (Nlog(1/c)/c2)(log(N) loglog(l/c)) transitions are sufficient for both algorithms to come within c of the optimal policy, in an idealized model that assumes the observed transitions are ""well-mixed"" throughout an N-state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for ",Reinforcement Learning,99adff456950dd9629a5260c4de21858-Paper.pdf,1998
Global Optimisation of Neural Network,"We propose a novel strategy for training neural networks using se quential sampling-importance resampling algorithms. This global optimisation strategy allows us to learn the probability distribu tion of the network weights in a sequential framework. It is well suited to applications involving on-line, nonlinear, non-Gaussian or non-stationary signal processing. 1 INTRODUCTION This paper addresses sequential training of neural networks using powerful sampling techniques. Sequential techniques are important in many applications of neural net works involving real-time signal processing, where data arrival is inherently sequen tial. Furthermore, one might wish to adopt a sequential training strategy to deal with non-stationarity in signals, so that information from the recent past is lent more credence than information from the distant past. One way to sequentially estimate neural network models is to use a state space formulation and the extended Kalman filter (Singhal and Wu 1988, de Fr",Optimization & Theoretical ML,9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf,1998
Linear Hinge Loss and Average Margin,"We describe a unifying method for proving relative loss bounds for on line linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms. For classification problems the discrete loss is used, i.e., the total number of prediction mistakes. We introduce a con tinuous loss function, called the ""linear hinge loss"", that can be employed to derive the updates of the algorithms. We first prove bounds w.r.t. the linear hinge loss and then convert them to the discrete loss. We intro duce a notion of ""average margin"" of a set of examples . We show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds i.t.o. the discrete loss using the average margin. 1 Introduction Consider the classical Perceptron algorithm. The hypothesis of this algorithm at trial t is a linear threshold function determined by a weight vector Wt E Rn. For an instance Xt ERn the linear activation at = Wt . Xt is passed through a threshold function (7 r ",Optimization & Theoretical ML,a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf,1998
Non-linear PI Control Inspired by,"A non-linear modification to PI control is motivated by a model of a signal transduction pathway active in mammalian blood pres sure regulation. This control algorithm, labeled PII (proportional with intermittent integral), is appropriate for plants requiring ex act set-point matching and disturbance attenuation in the presence of infrequent step changes in load disturbances or set-point. The proportional aspect of the controller is independently designed to be a disturbance attenuator and set-point matching is achieved by intermittently invoking an integral controller. The mechanisms observed in the Angiotensin 11/ AT1 signaling pathway are used to control the switching of the integral control. Improved performance over PI control is shown on a model of cyc1opentenol production. A sign change in plant gain at the desirable operating point causes traditional PI control to result in an unstable system. Applica tion of this new approach to this problem results in stable exact set-point m",Control Theory,9e984c108157cea74c894b5cf34efc44-Paper.pdf,1998
Learning Instance-Independent Value Functions,"Reinforcement learning methods can be used to improve the performance of local search algorithms for combinatorial optimization by learning an evaluation function that predicts the outcome of search. The eval uation function is therefore able to guide search to low-cost solutions better than can the original cost function. We describe a reinforcement learning method for enhancing local search that combines aspects of pre vious work by Zhang and Dietterich (1995) and Boyan and Moore (1997, Boyan 1998). In an off-line learning phase, a value function is learned that is useful for guiding search for multiple problem sizes and instances. We illustrate our technique by developing several such functions for the Dial-A-Ride Problem. Our learning-enhanced local search algorithm ex hibits an improvement of more then 30% over a standard local search algorithm. 1 INTRODUCTION Combinatorial optimization is of great importance in computer science, engineering, and operations research. We investigat",Optimization & Theoretical ML,a1afc58c6ca9540d057299ec3016d726-Paper.pdf,1998
Support Vector Machines Applied to Face,Face recognition is a K class problem. where K is the number of known individuals; and support vector machines (SVMs) are a binary classi fication method. By reformulating the face recognition problem and re interpreting the output of the SVM classifier. we developed a SVM -based face recognition algorithm. The face recognition problem is formulated as a problem in difference space. which models dissimilarities between two facial images. In difference space we formulate face recognition as a two class problem. The classes are: dissimilarities between faces of the same person. and dissimilarities between faces of different people. By modifying the interpretation of the decision surface generated by SVM. we generated a similarity metric between faces that is learned from ex amples of differences between faces. The SVM-based algorithm is com pared with a principal component analysis (PeA) based algorithm on a difficult set of images from the FEREf database. Performance was mea sured for b,Computer Vision,a2cc63e065705fe938a4dda49092966f-Paper.pdf,1998
A Theory of Mean Field Approximation,"I present a theory of mean field approximation based on information ge ometry. This theory includes in a consistent way the naive mean field approximation, as well as the TAP approach and the linear response the orem in statistical physics, giving clear information-theoretic interpreta tions to them. 1 INTRODUCTION Many problems of neural networks, such as learning and pattern recognition, can be cast into a framework of statistical estimation problem. How difficult it is to solve a particular problem depends on a statistical model one employs in solving the problem. For Boltzmann machines[ 1] for example, it is computationally very hard to evaluate expectations of state variables from the model parameters. Mean field approximation[2], which is originated in statistical physics, has been frequently used in practical situations in order to circumvent this difficulty. In the context of statistical physics several advanced theories have been known, such as the TAP approach[3], linear resp",Optimization & Theoretical ML,a368b0de8b91cfb3f91892fbf1ebd4b2-Paper.pdf,1998
Unsupervised and supervised clustering:,"Recent works in parameter estimation and neural coding have demonstrated that optimal performance are related to the mutual information between parameters and data. We consider the mutual information in the case where the dependency in the parameter (a vector 8) of the conditional p.d.f. of each observation (a vector 0, is through the scalar product 8.~ only. We derive bounds and asymptotic behaviour for the mutual information and compare with results obtained on the same model with the"" replica technique"" . 1 INTRODUCTION In this contribution we consider an unsupervised clustering task. Recent results on neural coding and parameter estimation (supervised and unsupervised learning tasks) show that the mutual information between data and parameters (equiva lently between neural activities and stimulus) is a relevant tool for deriving optimal performances (Clarke and Barron, 1990; Nadal and Parga, 1994; Opper and Kinzel, 1995; Haussler and Opper, 1995; Opper and Haussler, 1995; Rissanen,",NLP,a981f2b708044d6fb4a71a1463242520-Paper.pdf,1998
Stationarity and Stability of,"We analyze the asymptotic behavior of autoregressive neural net work (AR-NN) processes using techniques from Markov chains and non-linear time series analysis. It is shown that standard AR-NNs without shortcut connections are asymptotically stationary. If lin ear shortcut connections are allowed, only the shortcut weights determine whether the overall system is stationary, hence standard conditions for linear AR processes can be used. 1 Introduction In this paper we consider the popular class of nonlinear autoregressive processes driven by additive noise, which are defined by stochastic difference equations of form (1) where ft is an iid. noise process. If g( ... , (J) is a feedforward neural network with parameter (""weight"") vector (J, we call Equation 1 an autoregressive neural network process of order p, short AR-NN(p) in the following. AR-NNs are a natural generalization of the classic linear autoregressive AR(p) pro cess (2) See, e.g., Brockwell & Davis (1987) for a comprehensive ",Optimization & Theoretical ML,aa486f25175cbdc3854151288a645c19-Paper.pdf,1998
A Principle for Unsupervised,"Structure in a visual scene can be described at many levels of granular ity. At a coarse level, the scene is composed of objects; at a finer level, each object is made up of parts, and the parts of subparts. In this work, I propose a simple principle by which such hierarchical structure can be extracted from visual scenes: Regularity in the relations among different parts of an object is weaker than in the internal structure of a part. This principle can be applied recursively to define part-whole relationships among elements in a scene. The principle does not make use of object models, categories, or other sorts of higher-level knowledge; rather, part-whole relationships can be established based on the statistics of a set of sample visual scenes. I illustrate with a model that performs unsu pervised decomposition of simple scenes. The model can account for the results from a human learning experiment on the ontogeny of part whole relationships. 1 INTRODUCTION The structure in a visual",Computer Vision,ab541d874c7bc19ab77642849e02b89f-Paper.pdf,1998
Gradient Descent for General,"A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcement learning algorithms. These algorithms solve a number of open problems, define several new approaches to reinforcement learning, and unify different approaches to reinforcement learning under a single theory. These algorithms all have guaranteed convergence, and include modifications of several existing algorithms that were known to fail to converge on simple MOPs. These include Q learning, SARSA, and advantage learning. In addition to these value-based algorithms it also generates pure policy-search reinforcement-learning algorithms, which learn optimal policies without learning a value function. In addition, it allows policy search and value-based algorithms to be combined, thus unifying two very different approaches to reinforcement learning into a single Value and Policy Search (V APS) algorithm. And these algorithms converge for POMDPs without requiring a p",Reinforcement Learning,af5afd7f7c807171981d443ad4f4f648-Paper.pdf,1998
Vertex Identification in High Energy,"In High Energy Physics experiments one has to sort through a high flux of events, at a rate of tens of MHz, and select the few that are of interest. One of the key factors in making this decision is the location of the vertex where the interaction, that led to the event, took place. Here we present a novel solution to the problem of finding the location of the vertex, based on two feedforward neu ral networks with fixed architectures, whose parameters are chosen so as to obtain a high accuracy. The system is tested on simu lated data sets, and is shown to perform better than conventional algorithms. 1 Introduction An event in High Energy Physics (HEP) is the experimental result of an interaction during the collision of particles in an accelerator. The result of this interaction is the production of tens of particles, each of which is ejected in a different direction and energy. Due to the quantum mechanical effects involved, the events differ from one another in the number of particles",Computer Vision,aff0a6a4521232970b2c1cf539ad0a19-Paper.pdf,1998
Probabilistic Visualisation of,"We present a probabilistic latent-variable framework for data visu alisation, a key feature of which is its applicability to binary and categorical data types for which few established methods exist. A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters. Illustrations of application to real and synthetic binary data sets are given. 1 Introduction Visualisation is a powerful tool in the exploratory analysis of multivariate data. The rendering of high-dimensional data in two dimensions, while generally implying loss of information, often reveals interesting structure to the human eye. Standard dimensionality-reduction methods from multivariate analysis, notably the principal component projection, are often utilised for this purpose, while techniques such as 'projection pursuit' have been tailored specifically to this end. With the cur rent trend for larger databases and the need for effective 'data mining' methods, visu",Computer Vision,b132ecc1609bfcf302615847c1caa69a-Paper.pdf,1998
Computation of Smooth Optical Flow in a,"In 1986, Tanner and Mead [1] implemented an interesting constraint sat isfaction circuit for global motion sensing in a VLSI. We report here a new and improved a VLSI implementation that provides smooth optical flow as well as global motion in a two dimensional visual field. The com putation of optical flow is an ill-posed problem, which expresses itself as the aperture problem. However, the optical flow can be estimated by the use of regularization methods, in which additional constraints are intro duced in terms of a global energy functional that must be minimized. We show how the algorithmic constraints of Hom and Schunck [2] on com puting smooth optical flow can be mapped onto the physical constraints of an equivalent electronic network. 1 Motivation The perception of apparent motion is crucial for navigation. Knowledge of local motion of the environment relative to the observer simplifies the calculation of important tasks such as time-to-contact or focus-of-expansion. There are s",Computer Vision,b2dd140336c9df867c087a29b2e66034-Paper.pdf,1998
Applications of multi-resolution neural,"We have previously presented a coarse-to-fine hierarchical pyra mid/neural network (HPNN) architecture which combines multi scale image processing techniques with neural networks. In this paper we present applications of this general architecture to two problems in mammographic Computer-Aided Diagnosis (CAD). The first application is the detection of microcalcifications. The <:oarse-to-fine HPNN was designed to learn large-scale context in formation for detecting small objects like microcalcifications. Re ceiver operating characteristic (ROC) analysis suggests that the hierarchical architecture improves detection performance of a well established CAD system by roughly 50 %. The second application is to detect mammographic masses directly. Since masses are large, extended objects, the coarse-to-fine HPNN architecture is not suit able for this problem. Instead we construct a fine-to-coarse HPNN architecture which is designed to learn small-scale detail structure associated with the exten",Computer Vision,b5488aeff42889188d03c9895255cecc-Paper.pdf,1998
"Divisive Normalization, Line Attractor","Gain control by divisive inhibition, a.k.a. divisive normalization, has been proposed to be a general mechanism throughout the vi sual cortex. We explore in this study the statistical properties of this normalization in the presence of noise. Using simulations, we show that divisive normalization is a close approximation to a maximum likelihood estimator, which, in the context of population coding, is the same as an ideal observer. We also demonstrate ana lytically that this is a general property of a large class of nonlinear recurrent networks with line attractors. Our work suggests that divisive normalization plays a critical role in noise filtering, and that every cortical layer may be an ideal observer of the activity in the preceding layer. Information processing in the cortex is often formalized as a sequence of a linear stages followed by a nonlinearity. In the visual cortex, the nonlinearity is best de scribed by squaring combined with a divisive pooling of local activities. Th",Computer Vision,b5a1fc2085986034e448d2ccc5bb9703-Paper.pdf,1998
Robot Docking using Mixtures of Gaussians,"This paper applies the Mixture of Gaussians probabilistic model, com bined with Expectation Maximization optimization to the task of sum marizing three dimensional range data for a mobile robot. This provides a flexible way of dealing with uncertainties in sensor information, and al lows the introduction of prior knowledge into low-level perception mod ules. Problems with the basic approach were solved in several ways: the mixture of Gaussians was reparameterized to reflect the types of objects expected in the scene, and priors on model parameters were included in the optimization process. Both approaches force the optimization to find 'interesting' objects, given the sensor and object characteristics. A higher level classifier was used to interpret the results provided by the model, and to reject spurious solutions. 1 Introduction This paper concerns an application of the Mixture of Gaussians (MoG) probabilistic model (Titterington et aI., 1985) for a robot docking application. We use",Computer Vision,b60c5ab647a27045b462934977ccad9a-Paper.pdf,1998
Semi-Supervised Support Vector,"We introduce a semi-supervised support vector machine (S3yM) method. Given a training set of labeled data and a working set of unlabeled data, S3YM constructs a support vector machine us ing both the training and working sets. We use S3YM to solve the transduction problem using overall risk minimization (ORM) posed by Yapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3YM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3YM model for I-norm lin ear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Re sults of S3YM and the standard I-norm su",Optimization & Theoretical ML,b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf,1998
A Phase Space Approach to Minimax,"There has been much recent work on measuring image statistics and on learning probability distributions on images. We observe that the mapping from images to statistics is many-to-one and show it can be quantified by a phase space factor. This phase space approach throws light on the Minimax Entropy technique for learning Gibbs distributions on images with potentials derived from image statistics and elucidates the ambiguities that are inherent to determining the potentials. In addition, it shows that if the phase factor can be approximated by an analytic distribution then this approximation yields a swift ""Minutemax"" algorithm that vastly reduces the computation time for Minimax entropy learning. An illustration of this concept, using a Gaussian to approximate the phase factor, gives a good approximation to the results of Zhu and Mumford (1997) in just seconds of CPU time. The phase space approach also gives insight into the multi-scale potentials found by Zhu and Mumford (1997) and s",Computer Vision,bc573864331a9e42e4511de6f678aa83-Paper.pdf,1998
Almost Linear VC Dimension Bounds for,"We compute upper and lower bounds on the VC dimension of feedforward networks of units with piecewise polynomial activa tion functions. We show that if the number of layers is fixed, then the VC dimension grows as W log W, where W is the number of parameters in the network. This result stands in opposition to the case where the number of layers is unbounded, in which case the VC dimension grows as W2• 1 MOTIVATION The VC dimension is an important measure of the complexity of a class of binary valued functions, since it characterizes the amount of data required for learning in the PAC setting (see [BEHW89, Vap82]). In this paper, we establish upper and lower bounds on the VC dimension of a specific class of multi-layered feedforward neural networks. Let F be the class of binary-valued functions computed by a feed forward neural network with W weights and k computational (non-input) units, each with a piecewise polynomial activation function. Goldberg and Jerrum [GJ95] have shown that VC",Optimization & Theoretical ML,bc7316929fe1545bf0b98d114ee3ecb8-Paper.pdf,1998
Bayesian Modeling of Facial Similarity,"In previous work [6, 9, 10], we advanced a new technique for direct visual matching of images for the purposes of face recognition and image retrieval, using a probabilistic measure of similarity based primarily on a Bayesian (MAP) analysis of image differ ences, leading to a ""dual"" basis similar to eigenfaces [13]. The performance advantage of this probabilistic matching technique over standard Euclidean nearest-neighbor eigenface matching was recently demonstrated using results from DARPA's 1996 ""FERET"" face recognition competition, in which this probabilistic matching algorithm was found to be the top performer. We have further developed a simple method of replacing the costly com put ion of nonlinear (online) Bayesian similarity measures by the relatively inexpensive computation of linear (offline) subspace projections and simple (online) Euclidean norms, thus resulting in a significant computational speed-up for implementation with very large image databases as typically encounter",Computer Vision,bcb41ccdc4363c6848a1d760f26c28a0-Paper.pdf,1998
The Bias-Variance Tradeoff and the Randomized,"We propose a new in-sample cross validation based method (randomized GACV) for choosing smoothing or bandwidth parameters that govern the bias-variance or fit-complexity tradeoff in 'soft' classification. Soft clas sification refers to a learning procedure which estimates the probability that an example with a given attribute vector is in class 1 vs class O. The target for optimizing the the tradeoff is the Kullback-Liebler distance between the estimated probability distribution and the 'true' probabil ity distribution, representing knowledge of an infinite population. The method uses a randomized estimate of the trace of a Hessian and mimics cross validation at the cost of a single relearning with perturbed outcome data. 1 INTRODUCTION We propose and test a new in-sample cross-validation based method for optimizing the bias variance tradeoff in 'soft classification' (Wahba et al1994), called ranG ACV (randomized Generalized Approximate Cross Validation). Summarizing from Wahba et al(l",Optimization & Theoretical ML,bffc98347ee35b3ead06728d6f073c68-Paper.pdf,1998
General Bounds on Bayes Errors for,"Based on a simple convexity lemma, we develop bounds for differ ent types of Bayesian prediction errors for regression with Gaussian processes. The basic bounds are formulated for a fixed training set. Simpler expressions are obtained for sampling from an input distri bution which equals the weight function of the covariance kernel, yielding asymptotically tight results. The results are compared with numerical experiments. 1 Introduction Nonparametric Bayesian models which are based on Gaussian priors on function spaces are becoming increasingly popular in the Neural Computation Community (see e.g.[2, 3, 4, 7, 1]). Since the model classes considered in this approach are infinite dimensional, the application of Vapnik - Chervonenkis type of methods to determine bounds for the learning curves is nontrivial and has not been performed so far (to our knowledge). In these methods, the target function to be learnt is fixed and input data are drawn independently at random from a fixed (unknown",Optimization & Theoretical ML,c7af0926b294e47e52e46cfebe173f20-Paper.pdf,1998
Bayesian peA,"The technique of principal component analysis (PCA) has recently been expressed as the maximum likelihood solution for a generative latent variable model. In this paper we use this probabilistic reformulation as the basis for a Bayesian treatment of PCA. Our key result is that ef fective dimensionality of the latent space (equivalent to the number of retained principal components) can be determined automatically as part of the Bayesian inference procedure. An important application of this framework is to mixtures of probabilistic PCA models, in which each component can determine its own effective complexity. 1 Introduction Principal component analysis (PCA) is a widely used technique for data analysis. Recently Tipping and Bishop (1997b) showed that a specific form of generative latent variable model has the property that its maximum likelihood solution extracts the principal sub-space of the observed data set. This probabilistic reformulation of PCA permits many extensions including a",Optimization & Theoretical ML,c88d8d0a6097754525e02c2246d8d27f-Paper.pdf,1998
A Precise Characterization of the Class of,"We consider recurrent analog neural nets where each gate is subject to Gaussian noise, or any other common noise distribution whose probabil ity density function is nonzero on a large set. We show that many regular languages cannot be recognized by networks of this type, for example the language {w E {O, I} * I w begins with O}, and we give a precise characterization of those languages which can be recognized. This result implies severe constraints on possibilities for constructing recurrent ana log neural nets that are robust against realistic types of analog noise. On the other hand we present a method for constructing feedf orward analog neural nets that are robust with regard to analog noise of this type. 1 Introduction A fairly large literature (see [Omlin, Giles, 1996] and the references therein) is devoted to the construction of analog neural nets that recognize regular languages. Any physical realization of the analog computational units of an analog neural net in technological",Computer Vision,cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf,1998
General-purpose localization of textured,"We suggest a working definition of texture: Texture is stuff that is more compactly represented by its statistics than by specifying the configuration of its parts. This definition suggests that to fmd texture we look for outliers to the local statistics, and label as texture the regions with no outliers. We present a method, based upon this idea, for labeling points in natural scenes as belonging to texture regions, while simultaneously allowing us to label low level, bottom-up cues for visual attention. This method is based upon recent psychophysics results on processing of texture and popout. 1 WHAT IS TEXTURE, AND WHY DO WE WANT TO FIND IT? In a number of problems in computer VlSlon and image processing, one must distinguish between image regions that correspond to objects and those which correspond to texture, and perform different processing depending upon the type of region. Current computer vision algorithms assume one magically knows this region labeling. But what is texture? ",Computer Vision,cda72177eba360ff16b7f836e2754370-Paper.pdf,1998
Making Templates Rotationally Invariant:,"This paper describes a simple and efficient method to make template-based object classification invariant to in-plane rotations. The task is divided into two parts: orientation discrimination and classification. The key idea is to perform the orientation discrimination before the classification. This can be accom plished by hypothesizing, in turn, that the input image belongs to each class of interest. The image can then be rotated to maximize its similarity to the train ing images in each class (these contain the prototype object in an upright orien tation). This process yields a set of images, at least one of which will have the object in an upright position. The resulting images can then be classified by models which have been trained with only upright examples. This approach has been successfully applied to two real-world vision-based tasks: rotated handwritten digit recognition and rotated face detection in cluttered scenes. 1 Introduction Rotated text is commonly used in a variet",Computer Vision,ced556cd9f9c0c8315cfbe0744a3baf0-Paper.pdf,1998
Outcomes of the Equivalence of Adaptive Ridge,"Adaptive Ridge is a special form of Ridge regression, balancing the quadratic penalization on each parameter of the model. It was shown to be equivalent to Lasso (least absolute shrinkage and selection operator), in the sense that both procedures produce the same estimate. Lasso can thus be viewed as a particular quadratic penalizer. From this observation, we derive a fixed point algorithm to compute the Lasso solution. The analogy provides also a new hyper-parameter for tun ing effectively the model complexity. We finally present a series ofpossi ble extensions oflasso performing sparse regression in kernel smoothing, additive modeling and neural net training. 1 INTRODUCTION In supervised learning, we have a set of explicative variables x from which we wish to pre dict a response variable y. To solve this problem, a learning algorithm is used to produce a predictor J( x) from a learning set Sf. = {(Xi, yd H=l of examples. The goal of prediction may be: 1) to provide an accurate predic",Optimization & Theoretical ML,cfa5301358b9fcbe7aa45b1ceea088c6-Paper.pdf,1998
Bayesian modeling of human concept learning,"I consider the problem of learning concepts from small numbers of pos itive examples, a feat which humans perform routinely but which com puters are rarely capable of. Bridging machine learning and cognitive science perspectives, I present both theoretical analysis and an empirical study with human subjects for the simple task oflearning concepts corre sponding to axis-aligned rectangles in a multidimensional feature space. Existing learning models, when applied to this task, cannot explain how subjects generalize from only a few examples of the concept. I propose a principled Bayesian model based on the assumption that the examples are a random sample from the concept to be learned. The model gives precise fits to human behavior on this simple task and provides qualitati ve insights into more complex, realistic cases of concept learning. 1 Introduction The ability to learn concepts from examples is one of the core capacities of human cognition. From a computational point of view, huma",NLP,d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf,1998
The Role of Lateral Cortical Competition,"Lateral competition within a layer of neurons sharpens and localizes the response to an input stimulus. Here, we investigate a model for the ac tivity dependent development of ocular dominance maps which allows to vary the degree of lateral competition. For weak competition, it re sembles a correlation-based learning model and for strong competition, it becomes a self-organizing map. Thus, in the regime of weak compe tition the receptive fields are shaped by the second order statistics of the input patterns, whereas in the regime of strong competition, the higher moments and ""features"" of the individual patterns become important. When correlated localized stimuli from two eyes drive the cortical de velopment we find (i) that a topographic map and binocular, localized receptive fields emerge when the degree of competition exceeds a critical value and (ii) that receptive fields exhibit eye dominance beyond a sec ond critical value. For anti-correlated activity between the eyes, the sec o",Optimization & Theoretical ML,c559da2ba967eb820766939a658022c8-Paper.pdf,1998
Maximum Conditional Likelihood via,"We present the CEM (Conditional Expectation Maximi::ation) al gorithm as an extension of the EM (Expectation M aximi::ation) algorithm to conditional density estimation under missing data. A bounding and maximization process is given to specifically optimize conditional likelihood instead of the usual joint likelihood. We ap ply the method to conditioned mixture models and use bounding techniques to derive the model's update rules. Monotonic conver gence, computational efficiency and regression results superior to EM are demonstrated. 1 Introduction Conditional densities have played an important role in statistics and their merits over joint density models have been debated. Advantages in feature selection, ro bustness and limited resource allocation have been studied. Ultimately, tasks such as regression and classification reduce to the evaluation of a conditional density. However, popularity of maximumjoint likelihood and EM techniques remains strong in part due to their elegance and",Optimization & Theoretical ML,d1a69640d53a32a9fb13e93d1c8f3104-Paper.pdf,1998
Source Separation as a,"This paper reveals a previously ignored connection between two important fields: regularization and independent component anal ysis (ICA). We show that at least one representative of a broad class of algorithms (regularizers that reduce network complexity) extracts independent features as a by-product. This algorithm is Flat Minimum Search (FMS), a recent general method for finding low-complexity networks with high generalization capability. FMS works by minimizing both training error and required weight pre cision. According to our theoretical analysis the hidden layer of an FMS-trained autoassociator attempts at coding each input by a sparse code with as few simple features as possible. In experi ments the method extracts optimal codes for difficult versions of the ""noisy bars"" benchmark problem by separating the underlying sources, whereas ICA and PCA fail. Real world images are coded with fewer bits per pixel than by ICA or PCA. 1 INTRODUCTION In the field of unsupervised learning ",Computer Vision,d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf,1998
Analog VLSI Cellular Implementation of the,"We present an analog VLSI cellular architecture implementing a simpli .f ied version of the Boundary Contour System (BCS) for real-time image processing. Inspired by neuromorphic models across several layers of visual cortex, the design integrates in each pixel the functions of sim ple cells, complex cells, hyper-complex cells, and bipole cells, in three orientations interconnected on a hexagonal grid. Analog current-mode CMOS circuits are used throughout to perform edge detection, local inhi bition, directionally selective long-range diffusive kernels, and renormal izing global gain control. Experimental results from a fabricated 12 x 10 pixel prototype in 1.2 J-tm CMOS technology demonstrate the robustness of the architecture in selecting image contours in a cluttered and noisy background. 1 Introduction The Boundary Contour System (BCS) and Feature Contour System (FCS) combine models for processes of image segmentation, feature filling, and surface reconstruction in biolog ical visi",Computer Vision,d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf,1998
Unsupervised Classification with,"We present an unsupervised classification algorithm based on an ICA mixture model. The ICA mixture model assumes that the observed data can be categorized into several mutually exclusive data classes in which the components in each class are generated by a linear mixture of independent sources. The algorithm finds the independent sources, the mixing matrix for each class and also computes the class membership probability for each data point. This approach extends the Gaussian mixture model so that the classes can have non-Gaussian structure. We demonstrate that this method can learn efficient codes to represent images of natural scenes and text. The learned classes of basis functions yield a better approximation of the underlying distributions of the data, and thus can provide greater coding efficiency. We believe that this method is well suited to modeling structure in high-dimensional data and has many potential applications. 1 Introd uction Recently, Blind Source Separation (BSS) by",NLP,d72fbbccd9fe64c3a14f85d225a046f4-Paper.pdf,1998
Barycentric Interpolators for Continuous,"In order to find the optimal control of continuous state-space and time reinforcement learning (RL) problems, we approximate the value function (VF) with a particular class of functions called the barycentric interpolators. We establish sufficient conditions under which a RL algorithm converges to the optimal VF, even when we use approximate models of the state dynamics and the reinforce ment functions. 1 INTRODUCTION In order to approximate the value function (VF) of a continuous state-space and time reinforcement learning (RL) problem, we define a particular class of functions called the barycentric interpolator, that use some interpolation process based on finite sets of points. This class of functions, including continuous or discontinuous piecewise linear and multi-linear functions, provides us with a general method for designing RL algorithms that converge to the optimal value function. Indeed these functions permit us to discretize the HJB equation of the continuous control prob",Reinforcement Learning,d961e9f236177d65d21100592edb0769-Paper.pdf,1998
Learning a Continuous Hidden Variable,"A directed generative model for binary data using a small number of hidden continuous units is investigated. A clipping nonlinear ity distinguishes the model from conventional principal components analysis. The relationships between the correlations of the underly ing continuous Gaussian variables and the binary output variables are utilized to learn the appropriate weights of the network. The advantages of this approach are illustrated on a translationally in variant binary distribution and on handwritten digit images. Introduction Principal Components Analysis (PCA) is a widely used statistical technique for rep resenting data with a large number of variables [1]. It is based upon the assumption that although the data is embedded in a high dimensional vector space, most of the variability in the data is captured by a much lower climensional manifold. In particular for PCA, this manifold is described by a linear hyperplane whose char acteristic directions are given by the eigenvectors",Optimization & Theoretical ML,dc5c768b5dc76a084531934b34601977-Paper.pdf,1998
Exploiting generative models •,"Generative probability models such as hidden ~larkov models pro vide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification per formance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combina tion by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability mod els. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of D~A and protein sequence analysis. 1 Introduction Speech, vision, text and biosequence data can be difficult to deal with in the context of simple statistical classification problem",NLP,db1915052d15f7815c8b88e879465a1e-Paper.pdf,1998
Perceiving without Learning: from Spirals to,"As a benchmark task, the spiral problem is well known in neural net works. Unlike previous work that emphasizes learning, we approach the problem from a generic perspective that does not involve learning. We point out that the spiral problem is intrinsically connected to the in side/outside problem. A generic solution to both problems is proposed based on oscillatory correlation using a time delay network. Our simu lation results are qualitatively consistent with human performance, and we interpret human limitations in terms of synchrony and time delays, both biologically plausible. As a special case, our network without time delays can always distinguish these figures regardless of shape, position, size, and orientation. 1 INTRODUCTION The spiral problem refers to distinguishing between a connected single spiral and discon nected double spirals, as illustrated in Fig. 1. Since Minsky and Papert (1969) first intro duced the problem in their influential book on perceptrons, irhas receiv",Computer Vision,dca5672ff3444c7e997aa9a2c4eb2094-Paper.pdf,1998
Optimizing Classifiers for Imbalanced,"Following recent results [9, 8] showing the importance of the fat shattering dimension in explaining the beneficial effect of a large margin on generalization performance, the current paper investi gates the implications of these results for the case of imbalanced datasets and develops two approaches to setting the threshold. The approaches are incorporated into ThetaBoost, a boosting al gorithm for dealing with unequal loss functions. The performance of ThetaBoost and the two approaches are tested experimentally. Keywords: Computational Learning Theory, Generalization, fat-shattering, large margin, pac estimates, unequal loss, imbalanced datasets 1 Introduction Shawe-Taylor [8] demonstrated that the output margin can also be used as an estimate of the confidence with which a particular classification is made. In other words if a new example has an output value we]) clear of the threshold we can be more confident of the associated classification than when the output value is closer to ",Optimization & Theoretical ML,df12ecd077efc8c23881028604dbb8cc-Paper.pdf,1998
Blind Separation of Filtered Sources,"In this paper we present a novel approach to multichannel blind separation/generalized deconvolution, assuming that both mixing and demixing models are described by stable linear state-space sys tems. We decompose the blind separation problem into two pro cess: separation and state estimation. Based on the minimization of Kullback-Leibler Divergence, we develop a novel learning algo rithm to train the matrices in the output equation. To estimate the state of the demixing model, we introduce a new concept, called hidden innovation, to numerically implement the Kalman filter. Computer simulations are given to show the validity and high ef fectiveness of the state-space approach. 1 Introd uction The field of blind separation and deconvolution has grown dramatically during re cent years due to its similarity to the separation feature in human brain, as well as its rapidly growing applications in various fields, such as telecommunication systems, image enhancement and biomedical signal proc",Optimization & Theoretical ML,dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf,1998
Recurrent Cortical Amplification Produces,"Cortical amplification has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex. Less appreciated is the fact that the same form of amplification can also be used to de-tune or broaden selectivity. Using a network model with recurrent cortical circuitry, we propose that the spatial phase invariance of complex cell responses arises through recurrent amplification of feedforward input. Neurons in the network respond like simple cells at low gain and com plex ceUs at high gain. Similar recurrent mechanisms may playa role in generating invariant representations of feedforward input elsewhere in the visual processing pathway. 1 INTRODUCTION Synaptic input to neurons in the primary visual cortex is primarily recurrent, arising from other cortical cells. The dominance of this type of connection suggests that it may play an important role in cortical information processing. Previous studies proposed that recurrent connections amplify weak feedforwa",Computer Vision,e60e81c4cbe5171cd654662d9887aec2-Paper.pdf,1998
Viewing Classifier Systems,"Classifier systems are now viewed disappointing because of their prob lems such as the rule strength vs rule set performance problem and the credit assignment problem. In order to solve the problems, we have de veloped a hybrid classifier system: GLS (Generalization Learning Sys tem). In designing GLS, we view CSs as model free learning in POMDPs and take a hybrid approach to finding the best generalization, given the total number of rules. GLS uses the policy improvement procedure by Jaakkola et al. for an locally optimal stochastic policy when a set of rule conditions is given. GLS uses GA to search for the best set of rule conditions. 1 INTRODUCTION Classifier systems (CSs) (Holland 1986) have been among the most used in reinforcement learning. Some of the advantages of CSs are (1) they have a built-in feature (the use of don't care symbols ""#"") for input generalization, and (2) the complexity of pOlicies can be controlled by restricting the number of rules. In spite of these attrac",Optimization & Theoretical ML,e655c7716a4b3ea67f48c6322fc42ed6-Paper.pdf,1998
Reinforcement Learning based on,"In this article, we propose a new reinforcement learning (RL) method based on an actor-critic architecture. The actor and the critic are approximated by Normalized Gaussian Networks (NGnet), which are networks of local linear regression units. The NGnet is trained by the on-line EM algorithm proposed in our pre vious paper. We apply our RL method to the task of swinging-up and stabilizing a single pendulum and the task of balancing a dou ble pendulum near the upright position. The experimental results show that our RL method can be applied to optimal control prob lems having continuous state/action spaces and that the method achieves good control with a small number of trial-and-errors. 1 INTRODUCTION Reinforcement learning (RL) methods (Barto et al., 1990) have been successfully applied to various Markov decision problems having finite state/action spaces, such as the backgammon game (Tesauro, 1992) and a complex task in a dynamic envi ronment (Lin, 1992). On the other hand, applicati",Reinforcement Learning,e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf,1998
Learning multi-class dynamics,"Standard techniques (eg. Yule-Walker) are available for learning Auto-Regressive process models of simple, directly observable, dy namical processes. When sensor noise means that dynamics are observed only approximately, learning can still been achieved via Expectation-Maximisation (EM) together with Kalman Filtering. However, this does not handle more complex dynamics, involving multiple classes of motion. For that problem, we show here how EM can be combined with the CONDENSATION algorithm, which is based on propagation of random sample-sets. Experiments have been performed with visually observed juggling, and plausible dy namical models are found to emerge from the learning process. 1 Introduction The paper presents a probabilistic framework for estimation (perception) and classi fication of complex time-varying signals, represented as temporal streams of states. Automated learning of dynamics is of crucial importance as practical models may be too complex for parameters to be set b",Optimization & Theoretical ML,ebb71045453f38676c40deb9864f811d-Paper.pdf,1998
Markov processes on curves for,"We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables-one continuous (~), one discrete (s )-evolve jointly in time. We sup pose that the vector ~ traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the arc length traversed along this curve. Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[sl~]' are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where ~ are acoustic feature trajec tories and s are phonetic transcriptions. On two tasks-recognizing New Jersey town names and connected alpha-digits- we find that MPCs yield lower word error rates than comparably trained hidden Markov models. 1 Introduct",Computer Vision,ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf,1998
Discovering hidden features with,"In Gaussianprocessregressionthe covariancebetweenthe outputs at input locations x and x(cid:1) is usually assumed to depend on the distance (x−x(cid:1))T W(x−x(cid:1)), where W is a positive definite ma- trix. W is often taken to be diagonal, but if we allow W to be a generalpositive definite matrix whichcanbe tuned on the basisof training data, then an eigen-analysis of W shows that we are ef- fectively creating hidden features, where the dimensionality of the hidden-feature space is determined by the data. We demonstrate the superiority of predictions using the general matrix over those based on a diagonal matrix on two test problems. 1 Introduction OverthelastfewyearsBayesianapproachestopredictionwithneuralnetworkshave come to the fore. FollowinganargumentinNeal (1996)concerningthe equivalence between infinite neural networks and certain Gaussian processes, Gaussian process (GP)predictionhasalsobecomepopular, andRasmussen(1996)hasdemonstrated good performance of GP predictors on a n",Optimization & Theoretical ML,ed4227734ed75d343320b6a5fd16ce57-Paper.pdf,1998
Utilizing Time: Asynchronous Binding,"Historically, connectionist systems have not excelled at represent ing and manipulating complex structures. How can a system com posed of simple neuron-like computing elements encode complex relations? Recently, researchers have begun to appreciate that rep resentations can extend in both time and space. Many researchers have proposed that the synchronous firing of units can encode com plex representations. I identify the limitations of this approach and present an asynchronous model of binding that effectively rep resents complex structures. The asynchronous model extends the synchronous approach. I argue that our cognitive architecture uti lizes a similar mechanism. 1 Introduction Simple connectionist models can fall prey to the ""binding problem"". A binding problem occurs when two different events (or objects) are represented identically. For example, representing ""John hit Ted"" by activating the units JOHN, HIT, and TED would lead to a binding problem because the same pattern of act",NLP,efb76cff97aaf057654ef2f38cd77d73-Paper.pdf,1998
Very Fast EM-based Mixture Model,"Clustering is important in many fields including manufactlll'ing, biolog~', finance, and astronomy. l\Iixturp models arp a popular ap proach due to their st.atist.ical foundat.ions, and EM is a very pop ular l1wthocl for fillding mixture models. EM, however, requires lllany accesses of the data, and thus has been dismissed as imprac tical (e.g. [9]) for data mining of enormous dataset.s. We present a nt'\\· algorit.hm, baspd on thp l1lultiresolution ~.'Cl-trees of [5], which dramatically reelucps the cost of EtlI-baspd clusteriug, wit.h savings rising linearl:; wit.h the number of datapoints. Although prespnt.pd lwre for maximum likplihoocl estimation of Gaussian mixt.ure mod f'ls, it. is also applicable to non-(~aussian models (provided class densit.ies are monotonic in Mahalanobis dist.ance), mixed categori cal/ nUllwric clusters. anel Bayesian nwthocls such as Antoclass [1]. 1 Learning Mixture Models In a Gaussian mixture lllodf'l (e.g. [3]), we aSSUI1W t.hat datapoints {Xl .. . XR}",Optimization & Theoretical ML,f187a23c3ee681ef6913f31fd6d6446b-Paper.pdf,1998
Tight Bounds for the VC-Dimension of,"O(ws(s log d+log(dqh/ s))) and O(ws((h/ s) log q) +log(dqh/ s)) are upper bounds for the VC-dimension of a set of neural networks of units with piecewise polynomial activation functions, where s is the depth of the network, h is the number of hidden units, w is the number of adjustable parameters, q is the maximum of the number of polynomial segments of the activation function, and d is the maximum degree of the polynomials; also n(wslog(dqh/s)) is a lower bound for the VC-dimension of such a network set, which are tight for the cases s = 8(h) and s is constant. For the special case q = 1, the VC-dimension is 8(ws log d). 1 Introduction In spite of its importance, we had been unable to obtain VC-dimension values for practical types of networks, until fairly tight upper and lower bounds were obtained ([6], [8], [9], and [10]) for linear threshold element networks in which all elements perform a threshold function on weighted sum of inputs. Roughly, the lower bound for the networks is (1",Optimization & Theoretical ML,f18a6d1cde4b205199de8729a6637b42-Paper.pdf,1998
The effect of eligibility traces on finding optimal memoryless,"Agents acting in the real world are confronted with the problem of making good decisions with limited knowledge of the environment. Partially observable Markov decision processes (POMDPs) model decision problems in which an agent tries to maximize its reward in the face of limited sensor feedback. Recent work has shown empirically that a reinforcement learning (RL) algorithm called Sarsa(A) can efficiently find optimal memoryless policies, which map current observations to actions, for POMDP problems (Loch and Singh 1998). The Sarsa(A) algorithm uses a form of short-term memory called an eligibility trace, which distributes temporally delayed rewards to observation-action pairs which lead up to the reward. This paper explores the effect of eligibility traces on the ability of the Sarsa(A) algorithm to find optimal memoryless policies. A variant of Sarsa(A) called k-step truncated Sarsa(A) is applied to four test problems taken from the recent work of Littman, Littman, Cassandra and Kae",Reinforcement Learning,f3173935ed8ac4bf073c1bcd63171f8a-Paper.pdf,1998
Exploratory Data Analysis Using Radial Basis,"Two developments of nonlinear latent variable models based on radial basis functions are discussed: in the first, the use of priors or constraints on allowable models is considered as a means of preserving data structure in low-dimensional representations for visualisation purposes. Also, a resampling approach is introduced which makes more effective use of the latent samples in evaluating the likelihood. 1 INTRODUCTION Radial basis functions (RBF) have been extensively used for problems in discrimination and regression. Here we consider their application for obtaining low-dimensional repre sentations of high-dimensional data as part of the exploratory data analysis process. There has been a great deal of research over the years into linear and nonlinear techniques for dimensionality reduction. The technique most commonly used is principal components analysis (PCA) and there have been several nonlinear generalisations, each taking a partic ular definition of PCA and generalising it to ",Optimization & Theoretical ML,f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf,1998
A VI model of pop out and asymmetry •,"Visual search is the task of finding a target in an image against a background of distractors. Unique features of targets enable them to pop out against the background, while targets defined by lacks of features or conjunctions of features are more difficult to spot. It is known that the ease of target detection can change when the roles of figure and ground are switched. The mechanisms underlying the ease of pop out and asymmetry in visual search have been elusive. This paper shows that a model of segmentation in VI based on intracortical interactions can explain many of the qualitative aspects of visual search. 1 Introduction Visual search is closely related to visual segmentation, and therefore can be used to diagnose the mechanisms of visual segmentation. For instance, a red dot can pop out against a background of green distractor dots instantaneously, suggesting that only pre-attentive mechanisms are necessary (Treisman et aI, 1990). On the other hand, it is much more difficult to",Computer Vision,f60bb6bb4c96d4df93c51bd69dcc15a0-Paper.pdf,1998
A High Performance k-NN Classifier Using a,"This paper presents a novel and fast k-NN classifier that is based on a binary CMM (Correlation Matrix Memory) neural network. A robust encoding method is developed to meet CMM input requirements. A hardware implementation of the CMM is described, which gives over 200 times the speed of a current mid-range workstation, and is scaleable to very large problems. When tested on several benchmarks and compared with a simple k-NN method, the CMM classifier gave less than I % lower accuracy and over 4 and 12 times speed-up in software and hardware respectively. 1 INTRODUCTION Pattern classification is one of most fundamental and important tasks, and a k-NN rule is applicable to a wide range of classification problems. As this method is too slow for many applications with large amounts of data, a great deal of effort has been put into speeding it up via complex pre-processing of training data, such as reducing training data (Dasarathy 1994) and improving computational efficiency (Grother & Can",Optimization & Theoretical ML,fa1e9c965314ccd7810fb5ea838303e5-Paper.pdf,1998
Exploring Unknown Environments with,"Learning Real-Time A* (LRTA*) is a popular control method that interleaves plan ning and plan execution and has been shown to solve search problems in known environments efficiently. In this paper, we apply LRTA * to the problem of getting to a given goal location in an initially unknown environment. Uninformed LRTA * with maximal lookahead always moves on a shortest path to the closest unvisited state, that is, to the closest potential goal state. This was believed to be a good exploration heuristic, but we show that it does not minimize the worst-case plan-execution time compared to other uninformed exploration methods. This result is also of interest to reinforcement-learning researchers since many reinforcement learning methods use asynchronous dynamic programming, interleave planning and plan execution, and exhibit optimism in the face of uncertainty, just like LRTA *. 1 Introduction Real-time (heuristic) search methods are domain-independent control methods that inter leave plann",Computer Vision,faafda66202d234463057972460c04f5-Paper.pdf,1998
Inference in Multilayer Networks V• I a,"We study probabilistic inference in large, layered Bayesian net works represented as directed acyclic graphs. We show that the intractability of exact inference in such networks does not preclude their effective use. We give algorithms for approximate probabilis tic inference that exploit averaging phenomena occurring at nodes with large numbers of parents. We show that these algorithms compute rigorous lower and upper bounds on marginal probabili ties of interest, prove that these bounds become exact in the limit of large networks, and provide rates of convergence. 1 Introduction The promise of neural computation lies in exploiting the information processing abilities of simple computing elements organized into large networks. Arguably one of the most important types of information processing is the capacity for proba bilistic reasoning. The properties of undirectedproDabilistic models represented as symmetric networks have been studied extensively using methods from statistical mecha",Optimization & Theoretical ML,facf9f743b083008a894eee7baa16469-Paper.pdf,1998
Basis Selection For Wavelet Regression,"A wavelet basis selection procedure is presented for wavelet re gression. Both the basis and threshold are selected using cross validation. The method includes the capability of incorporating prior knowledge on the smoothness (or shape of the basis functions) into the basis selection procedure. The results of the method are demonstrated using widely published sampled functions. The re sults of the method are contrasted with other basis function based methods. 1 INTRODUCTION Wavelet regression is a technique which attempts to reduce noise in a sampled function corrupted with noise. This is done by thresholding the small wavelet de composition coefficients which represent mostly noise. Most of the papers published on wavelet regression have concentrated on the threshold selection process. This paper focuses on the effect that different wavelet bases have on cross-validation based threshold selection, and the error in the final result. This paper also suggests how prior information may be",Optimization & Theoretical ML,fc528592c3858f90196fbfacc814f235-Paper.pdf,1998
Where does the population vector of motor,"Visually-guided arm reaching movements are produced by distributed neural networks within parietal and frontal regions of the cerebral cortex. Experimental data indicate that (I) single neurons in these regions are broadly tuned to parameters of movement; (2) appropriate commands are elaborated by populations of neurons; (3) the coordinated action of neu rons can be visualized using a neuronal population vector (NPV). How ever, the NPV provides only a rough estimate of movement parameters (direction, velocity) and may even fail to reflect the parameters of move ment when arm posture is changed. We designed a model of the cortical motor command to investigate the relation between the desired direction of the movement, the actual direction of movement and the direction of the NPV in motor cortex. The model is a two-layer self-organizing neural network which combines broadly-tuned (muscular) proprioceptive and (cartesian) visual information to calculate (angular) motor commands for the in",Computer Vision,fcdf25d6e191893e705819b177cddea0-Paper.pdf,1998
Regular and Irregular Gallager-type,"The performance of regular and irregular Gallager-type error correcting code is investigated via methods of statistical physics. The transmitted codeword comprises products of the original mes sage bits selected by two randomly-constructed sparse matrices; the number of non-zero row/column elements in these matrices constitutes a family of codes. We show that Shannon's channel capacity may be saturated in equilibrium for many of the regular codes while slightly lower performance is obtained for others which may be of higher practical relevance. Decoding aspects are con sidered by employing the TAP approach which is identical to the commonly used belief-propagation-based decoding. We show that irregular codes may saturate Shannon's capacity but with improved dynamical properties. 1 Introduction The ever increasing information transmission in the modern world is based on re liably communicating messages through noisy transmission channels; these can be telephone lines, deep space, magnet",Computer Vision,01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf,1999
An MEG Study of Response Latency and,"Human reaction times during sensory-motor tasks vary consider ably. To begin to understand how this variability arises, we exam ined neuronal populational response time variability at early versus late visual processing stages. The conventional view is that pre cise temporal information is gradually lost as information is passed through a layered network of mean-rate ""units."" We tested in hu mans whether neuronal populations at different processing stages behave like mean-rate ""units"". A blind source separation algorithm was applied to MEG signals from sensory-motor integration tasks. Response time latency and variability for multiple visual sources were estimated by detecting single-trial stimulus-locked events for each source. In two subjects tested on four visual reaction time tasks, we reliably identified sources belonging to early and late vi sual processing stages. The standard deviation of response latency was smaller for early rather than late processing stages. This sup ports ",NLP,02f039058bd48307e6f653a2005c9dd2-Paper.pdf,1999
The Entropy Regularization,"Effective methods of capacity control via uniform convergence bounds for function expansions have been largely limited to Support Vector ma chines, where good bounds are obtainable by the entropy number ap proach. We extend these methods to systems with expansions in terms of arbitrary (parametrized) basis functions and a wide range of regulariza tion methods covering the whole range of general linear additive models. This is achieved by a data dependent analysis of the eigenvalues of the corresponding design matrix. 1 INTRODUCTION Model selection criteria based on the Vapnik-Chervonenkis (VC) dimension are known to be difficult to obtain, worst case, and often not very tight. Yet they have the theoretical appeal of providing bounds, with few or no assumptions made. Recently new methods [8, 7, 6] have been developed which are able to provide a better characterization of the complexity of function classes than the VC dimension, and more over, are easily obtainable and take advantage of ",Optimization & Theoretical ML,08e6bea8e90ba87af3c9554d94db6579-Paper.pdf,1999
U nmixing Hyperspectral Data,"In hyperspectral imagery one pixel typically consists of a mixture of the reflectance spectra of several materials, where the mixture coefficients correspond to the abundances of the constituting ma terials. We assume linear combinations of reflectance spectra with some additive normal sensor noise and derive a probabilistic MAP framework for analyzing hyperspectral data. As the material re flectance characteristics are not know a priori, we face the problem of unsupervised linear unmixing. The incorporation of different prior information (e.g. positivity and normalization of the abun dances) naturally leads to a family of interesting algorithms, for example in the noise-free case yielding an algorithm that can be understood as constrained independent component analysis (ICA). Simulations underline the usefulness of our theory. 1 Introduction Current hyperspectral remote sensing technology can form images of ground surface reflectance at a few hundred wavelengths simultaneously, with w",Computer Vision,0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf,1999
Correctness of belief propagation in Gaussian,"Local ""belief propagation"" rules of the sort proposed by Pearl [15] are guaranteed to converge to the correct posterior probabilities in singly connected graphical models. Recently, a number of researchers have em pirically demonstrated good performance of ""loopy belief propagation"" using these same rules on graphs with loops. Perhaps the most dramatic instance is the near Shannon-limit performance of ""Turbo codes"", whose decoding algorithm is equivalent to loopy belief propagation. Except for the case of graphs with a single loop, there has been little theo retical understanding of the performance of loopy propagation. Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly Gaussian random variables. We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation. We give sufficient conditions for convergence and show that when belief propagation converges it gives the c",Optimization & Theoretical ML,10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf,1999
Population Decoding Based on,"We study a population decoding paradigm in which the maximum likeli hood inference is based on an unfaithful decoding model (UMLI). This is usually the case for neural population decoding because the encoding process of the brain is not exactly known, or because a simplified de coding model is preferred for saving computational cost. We consider an unfaithful decoding model which neglects the pair-wise correlation between neuronal activities, and prove that UMLI is asymptotically effi cient when the neuronal correlation is uniform or of limited-range. The performance of UMLI is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass de coding method. It turns out that UMLI has advantages of decreasing the computational complexity remarkablely and maintaining a high-level decoding accuracy at the same time. The effect of correlation on the decoding accuracy is also discussed. 1 Introduction Population coding is a method to encode a",Computer Vision,11c484ea9305ea4c7bb6b2e6d570d466-Paper.pdf,1999
Broadband Direction-Of-Arrival Estimation,"N wideband sources recorded using N closely spaced receivers can feasibly be separated based only on second order statistics when using a physical model of the mixing process. In this case we show that the parameter estimation problem can be essentially reduced to considering directions of arrival and attenuations of each signal. The paper presents two demixing methods operating in the time and frequency domain and experimentally shows that it is always possible to demix signals arriving at different angles. Moreover, one can use spatial cues to solve the channel selection problem and a post-processing Wiener filter to ameliorate the artifacts caused by demixing. 1 Introduction Blind source separation (BSS) is capable of dramatic results when used to separate mixtures of independent signals. The method relies on simultaneous recordings of signals from two or more input sensors and separates the original sources purely on the basis of statistical independence between them. Unfortunately",Computer Vision,11d0e6287202fced83f79975ec59a3a6-Paper.pdf,1999
Emergence of Topography and Complex,"Independent component analysis of natural images leads to emer gence of simple cell properties, Le. linear filters that resemble wavelets or Gabor functions. In this paper, we extend ICA to explain further properties of VI cells. First, we decompose natural images into independent subspaces instead of scalar components. This model leads to emergence of phase and shift invariant fea tures, similar to those in VI complex cells. Second, we define a topography between the linear components obtained by ICA. The topographic distance between two components is defined by their higher-order correlations, so that two components are close to each other in the topography if they are strongly dependent on each other. This leads to simultaneous emergence of both topography and invariances similar to complex cell properties. 1 Introduction A fundamental approach in signal processing is to design a statistical generative model of the observed signals. Such an approach is also useful for modeling the p",Computer Vision,148510031349642de5ca0c544f31b2ef-Paper.pdf,1999
Reinforcement Learning Using Approximate,"The problem of developing good policies for partially observable Markov decision problems (POMDPs) remains one of the most challenging ar eas of research in stochastic planning. One line of research in this area involves the use of reinforcement learning with belief states, probabil ity distributions over the underlying model states. This is a promis ing method for small problems, but its application is limited by the in tractability of computing or representing a full belief state for large prob lems. Recent work shows that, in many settings, we can maintain an approximate belief state, which is fairly close to the true belief state. In particular, great success has been shown with approximate belief states that marginalize out correlations between state variables. In this paper, we investigate two methods of full belief state reinforcement learning and one novel method for reinforcement learning using factored approximate belief states. We compare the performance of these algorithms ",Reinforcement Learning,158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf,1999
Building Predictive Models from Fractal,"We propose a novel approach for building finite memory predictive mod els similar in spirit to variable memory length Markov models (VLMMs). The models are constructed by first transforming the n-block structure of the training sequence into a spatial structure of points in a unit hypercube, such that the longer is the common suffix shared by any two n-blocks, the closer lie their point representations. Such a transformation embodies a Markov assumption - n-blocks with long common suffixes are likely to produce similar continuations. Finding a set of prediction contexts is formulated as a resource allocation problem solved by vector quantizing the spatial n-block representation. We compare our model with both the classical and variable memory length Markov models on three data sets with different memory and stochastic components. Our models have a superior performance, yet, their construction is fully automatic, which is shown to be problematic in the case of VLMMs. 1 Introduction Stat",Optimization & Theoretical ML,17ed8abedc255908be746d245e50263a-Paper.pdf,1999
Neural Computation with Winner-Take-All as,"Everybody ""knows"" that neural networks need more than a single layer of nonlinear units to compute interesting functions. We show that this is false if one employs winner-take-all as nonlinear unit: • Any boolean function can be computed by a single k-winner-take all unit applied to weighted sums of the input variables. • Any continuous function can be approximated arbitrarily well by a single soft winner-take-all unit applied to weighted sums of the input variables. • Only positive weights are needed in these (linear) weighted sums. This may be of interest from the point of view of neurophysiology, since only 15% of the synapses in the cortex are inhibitory. In addi tion it is widely believed that there are special microcircuits in the cortex that compute winner-take-all. • Our results support the view that winner-take-all is a very useful basic computational unit in Neural VLS!: o it is wellknown that winner-take-all of n input variables can be computed very efficiently with 2n trans",Computer Vision,1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf,1999
Support Vector Method for Multivariate,"A new method for multivariate density estimation is developed based on the Support Vector Method (SVM) solution of inverse ill-posed problems. The solution has the form of a mixture of den sities. This method with Gaussian kernels compared favorably to both Parzen's method and the Gaussian Mixture Model method. For synthetic data we achieve more accurate estimates for densities of 2, 6, 12, and 40 dimensions. 1 Introduction The problem of multivariate density estimation is important for many applications, in particular, for speech recognition [1] [7]. When the unknown density belongs to a parametric set satisfying certain conditions one can estimate it using the maximum likelihood (ML) method. Often these conditions are too restrictive. Therefore, non-parametric methods were proposed. The most popular of these, Parzen's method [5], uses the following estimate given data Xl, ... , Xl: (1) J = where K'Yl(t - Xi) is a smooth function such that K'Yl(t - xi)dt 1. Under some conditions on ""Y",Optimization & Theoretical ML,207f88018f72237565570f8a9e5ca240-Paper.pdf,1999
Leveraged Vector Machines,"We describe an iterative algorithm for building vector machines used in classification tasks. The algorithm builds on ideas from support vector machines, boosting, and generalized additive models. The algorithm can be used with various continuously differential functions that bound the discrete (0-1) classification loss and is very simple to implement. We test the proposed algorithm with two different loss functions on synthetic and natural data. We also describe a norm-penalized version of the algorithm for the exponential loss function used in AdaBoost. The performance of the algorithm on natural data is comparable to support vector machines while typically its running time is shorter than of SVM. 1 Introduction Support vector machines (SVM) [1, 13] and boosting [10, 3, 4, 11] are highly popular and effective methods for constructing linear classifiers. The theoretical basis for SVMs stems from Vapnik's seminal on learning and generalization [12] and has proved to be of great practic",Optimization & Theoretical ML,21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf,1999
Learning Factored Representations for Partially,"The problem of reinforcement learning in a non-Markov environment is explored using a dynamic Bayesian network, where conditional indepen dence assumptions between random variables are compactly represented by network parameters. The parameters are learned on-line, and approx imations are used to perform inference and to compute the optimal value function. The relative effects of inference and value function approxi mations on the quality of the final policy are investigated, by learning to solve a moderately difficult driving task. The two value function approx imations, linear and quadratic, were found to perform similarly, but the quadratic model was more sensitive to initialization. Both performed be low the level of human performance on the task. The dynamic Bayesian network performed comparably to a model using a localist hidden state representation, while requiring exponentially fewer parameters. 1 Introduction Reinforcement learning (RL) addresses the problem of learning to act",Reinforcement Learning,231141b34c82aa95e48810a9d1b33a79-Paper.pdf,1999
Variational Inference for Bayesian,"We present an algorithm that infers the model structure of a mix ture of factor analysers using an efficient and deterministic varia tional approximation to full Bayesian integration over model pa rameters. This procedure can automatically determine the opti mal number of components and the local dimensionality of each component (Le. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to overfitting. Using a stochastic procedure for adding components it is possible to per form the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exact predi",Optimization & Theoretical ML,2451041557a22145b3701b0184109cab-Paper.pdf,1999
Topographic Transformation as a,"Invariance to topographic transformations such as translation and shearing in an image has been successfully incorporated into feed forward mechanisms, e.g., ""convolutional neural networks"", ""tan gent propagation"". We describe a way to add transformation invari ance to a generative density model by approximating the nonlinear transformation manifold by a discrete set of transformations. An EM algorithm for the original model can be extended to the new model by computing expectations over the set of transformations. We show how to add a discrete transformation variable to Gaussian mixture modeling, factor analysis and mixtures of factor analysis. We give results on filtering microscopy images, face and facial pose clustering, and handwritten digit modeling and recognition. 1 Introduction Imagine what happens to the point in the N-dimensional space corresponding to an N-pixel image of an object, while the object is deformed by shearing. A very small amount of shearing will move the point",Computer Vision,25e2a30f44898b9f3e978b1786dcd85c-Paper.pdf,1999
Channel Noise in Excitable Neuronal,"Stochastic fluctuations of voltage-gated ion channels generate current and voltage noise in neuronal membranes. This noise may be a criti cal determinant of the efficacy of information processing within neural systems. Using Monte-Carlo simulations, we carry out a systematic in vestigation of the relationship between channel kinetics and the result ing membrane voltage noise using a stochastic Markov version of the Mainen-Sejnowski model of dendritic excitability in cortical neurons. Our simulations show that kinetic parameters which lead to an increase in membrane excitability (increasing channel densities, decreasing tem perature) also lead to an increase in the magnitude of the sub-threshold voltage noise. Noise also increases as the membrane is depolarized from rest towards threshold. This suggests that channel fluctuations may in terfere with a neuron's ability to function as an integrator of its synaptic inputs and may limit the reliability and precision of neural information pro",Optimization & Theoretical ML,2612aa892d962d6f8056b195ca6e550d-Paper.pdf,1999
Efficient Approaches to Gaussian Process,"We present three simple approximations for the calculation of the posterior mean in Gaussian Process classification. The first two methods are related to mean field ideas known in Statistical Physics. The third approach is based on Bayesian online approach which was motivated by recent results in the Statistical Mechanics of Neural Networks. We present simulation results showing: 1. that the mean field Bayesian evidence may be used for hyperparameter tuning and 2. that the online approach may achieve a low training error fast. 1 Introduction Gaussian processes provide promising non-parametric Bayesian approaches to re gression and classification [2, 1]. In these statistical models, it is assumed that the likelihood of an output or target variable y for a given input x E RN can be written as P(Yla(x)) where a : RN --+ R are functions which have a Gaussian prior distri bution, i.e. a is (a priori) assumed to be a Gaussian random field. This means that any finite set of field variables a(",Optimization & Theoretical ML,26751be1181460baf78db8d5eb7aad39-Paper.pdf,1999
Optimal sizes of dendritic and axonal arbors,"I consider a topographic projection between two neuronal layers with dif ferent densities of neurons. Given the number of output neurons con nected to each input neuron (divergence or fan-out) and the number of input neurons synapsing on each output neuron (convergence or fan-in) I determine the widths of axonal and dendritic arbors which minimize the total volume ofaxons and dendrites. My analytical results can be sum marized qualitatively in the following rule: neurons of the sparser layer should have arbors wider than those of the denser layer. This agrees with the anatomical data from retinal and cerebellar neurons whose morphol ogy and connectivity are known. The rule may be used to infer connec tivity of neurons from their morphology. 1 Introduction Understanding brain function requires knowing connections between neurons. However, experimental studies of inter-neuronal connectivity are difficult and the connectivity data is scarce. At the same time neuroanatomists possess much d",Optimization & Theoretical ML,270edd69788dce200a3b395a6da6fdb7-Paper.pdf,1999
v-Arc: Ensemble Learning,"AdaBoost and other ensemble methods have successfully been ap plied to a number of classification tasks, seemingly defying prob lems of overfitting. AdaBoost performs gradient descent in an error function with respect to the margin, asymptotically concentrating on the patterns which are hardest to learn. For very noisy prob lems, however, this can be disadvantageous. Indeed, theoretical analysis has shown that the margin distribution, as opposed to just the minimal margin, plays a crucial role in understanding this phe nomenon. Loosely speaking, some outliers should be tolerated if this has the benefit of substantially increasing the margin on the remaining points. We propose a new boosting algorithm which al lows for the possibility of a pre-specified fraction of points to lie in the margin area Or even on the wrong side of the decision boundary. 1 Introduction Boosting and related Ensemble learning methods have been recently used with great success in applications such as Optical Cha",Computer Vision,28dc6b0e1b33769b4b94685e4f4d1e5c-Paper.pdf,1999
Monte Carlo POMDPs,"We present a Monte Carlo algorithm for learning to act in partially observable Markov decision processes (POMDPs) with real-valued state and action spaces. Our approach uses importance sampling for representing beliefs, and Monte Carlo approximation for belief propagation. A reinforcement learning algorithm, value iteration, is employed to learn value functions over belief states. Finally, a sample based version of nearest neighbor is used to generalize across states. Initial empirical results suggest that our approach works well in practical applications. 1 Introduction POMDPs address the problem of acting optimally in partially observable dynamic environ ment [6]. In POMDPs, a learner interacts with a stochastic environment whose state is only partially observable. Actions change the state of the environment and lead to numerical penalties/rewards, which may be observed with an unknown temporal delay. The learner's goal is to devise a policy for action selection that maximizes the re",Reinforcement Learning,299570476c6f0309545110c592b6a63b-Paper.pdf,1999
A recurrent model of the interaction between,"A very simple model of two reciprocally connected attractor neural net works is studied analytically in situations similar to those encountered in delay match-to-sample tasks with intervening stimuli and in tasks of memory guided attention. The model qualitatively reproduces many of the experimental data on these types of tasks and provides a framework for the understanding of the experimental observations in the context of the attractor neural network scenario. 1 Introduction Working memory is usually defined as the capability to actively hold information in mem ory for short periods of time. In primates, visual working memory is usually studied in experiments in which, after the presentation of a given visual stimulus, the monkey has to withhold its response during a certain delay period in which no specific visual stimulus is shown. After the delay, another stimulus is presented and the monkey has to make a response which depends on the interaction between the two stimuli. In order ",Reinforcement Learning,2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf,1999
Information Factorization in,"We examine a psychophysical law that describes the influence of stimulus and context on perception. According to this law choice probability ratios factorize into components independently con trolled by stimulus and context. It has been argued that this pat tern of results is incompatible with feedback models of perception. In this paper we examine this claim using neural network models defined via stochastic differential equations. We show that the law is related to a condition named channel separability and has little to do with the existence of feedback connections. In essence, chan nels are separable if they converge into the response units without direct lateral connections to other channels and if their sensors are not directly contaminated by external inputs to the other chan nels. Implications of the analysis for cognitive and computational neurosicence are discussed. 1 Introduction We examine a psychophysical law, named the Morton-Massaro law, and its implica tions to connecti",Optimization & Theoretical ML,2cb6b10338a7fc4117a80da24b582060-Paper.pdf,1999
Hierarchical Image Probability (HIP) Models,"We formulate a model for probability distributions on image spaces. We show that any distribution of images can be factored exactly into condi tional distributions of feature vectors at one resolution (pyramid level) conditioned on the image information at lower resolutions. We would like to factor this over positions in the pyramid levels to make it tractable, but such factoring may miss long-range dependencies. To fix this, we in troduce hidden class labels at each pixel in the pyramid. The result is a hierarchical mixture of conditional probabilities, similar to a hidden Markov model on a tree. The model parameters can be found with max imum likelihood estimation using the EM algorithm. We have obtained encouraging preliminary results on the problems of detecting various ob jects in SAR images and target recognition in optical aerial images. 1 Introduction I Many approaches to object recognition in images estimate Pr(class image). By con trast, a model of the probability distributio",Computer Vision,365d17770080c807a0e47ae9118d8641-Paper.pdf,1999
Reinforcement Learning for,"Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs). However, the practical application ofMDP algorithms to dialogue systems faces a number of severe technical challenges. We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs. Our experiments demonstrate that RLDS holds promise as a tool for ""browsing"" and understanding correlations in complex, temporally dependent dialogue corpora. 1 Introduction Systems in which human users speak to a computer in order to achieve a goal are called spoken dialogue systems. Such systems are some of the few realized examples of open ended, real-time, goal-oriented interaction between humans and computers, and are therefore an important and exciting testbed for AI and machine learning research. Spoken dialogue systems typically integrate many",Reinforcement Learning,36d7534290610d9b7e9abed244dd2f28-Paper.pdf,1999
Distributed Synchrony of Spiking Neurons,"We investigate the behavior of a Hebbian cell assembly of spiking neurons formed via a temporal synaptic learning curve. This learn ing function is based on recent experimental findings. It includes potentiation for short time delays between pre- and post-synaptic neuronal spiking, and depression for spiking events occuring in the reverse order. The coupling between the dynamics of the synaptic learning and of the neuronal activation leads to interesting results. We find that the cell assembly can fire asynchronously, but may also function in complete synchrony, or in distributed synchrony. The latter implies spontaneous division of the Hebbian cell assem bly into groups of cells that fire in a cyclic manner. We invetigate the behavior of distributed synchrony both by simulations and by analytic calculations of the resulting synaptic distributions. 1 Introduction The Hebbian paradigm that serves as the basis for models of associative memory is often conceived as the statement that a gr",Optimization & Theoretical ML,375c71349b295fbe2dcdca9206f20a06-Paper.pdf,1999
Image representations for facial expression,"The Facial Action Coding System (FACS) (9) is an objective method for quantifying facial movement in terms of component actions. This system is widely used in behavioral investigations of emotion, cognitive processes, and social interaction. The cod ing is presently performed by highly trained human experts. This paper explores and compares techniques for automatically recog nizing facial actions in sequences of images. These methods include unsupervised learning techniques for finding basis images such as principal component analysis, independent component analysis and local feature analysis, and supervised learning techniques such as Fisher's linear discriminants. These data-driven bases are com pared to Gabor wavelets, in which the basis images are predefined. Best performances were obtained using the Gabor wavelet repre sentation and the independent component representation, both of which achieved 96% accuracy for classifying 12 facial actions. The ICA representation employs 2 orde",Computer Vision,393c55aea738548df743a186d15f3bef-Paper.pdf,1999
Algorithms for Independent Components,"A latent variable generative model with finite noise is used to de scribe several different algorithms for Independent Components Anal ysis (lCA). In particular, the Fixed Point ICA algorithm is shown to be equivalent to the Expectation-Maximization algorithm for maximum likelihood under certain constraints, allowing the conditions for global convergence to be elucidated. The algorithms can also be explained by their generic behavior near a singular point where the size of the opti mal generative bases vanishes. An expansion of the likelihood about this singular point indicates the role of higher order correlations in determin ing the features discovered by ICA. The application and convergence of these algorithms are demonstrated on a simple illustrative example. Introduction Independent Components Analysis (lCA) has generated much recent theoretical and prac tical interest because of its successes on a number of different signal processing problems. ICA attempts to decompose the obser",NLP,3c1e4bd67169b8153e0047536c9f541e-Paper.pdf,1999
A SNoW-Based Face Detector,"A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incremen tally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and oth ers. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods. 1 Introduction Growing interest in intelligent human computer interactions has motivated a recent surge in research on problems such as face trackin",Computer Vision,3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf,1999
A Winner-Take-All Circuit with,"I describe a silicon network consisting of a group of excitatory neu rons and a global inhibitory neuron. The output of the inhibitory neuron is normalized with respect to the input strengths. This out put models the normalization property of the wide-field direction selective cells in the fly visual system. This normalizing property is also useful in any system where we wish the output signal to code only the strength of the inputs, and not be dependent on the num ber of inputs. The circuitry in each neuron is equivalent to that in Lazzaro's winner-take-all (WTA) circuit with one additional tran sistor and a voltage reference. Just as in Lazzaro's circuit, the outputs of the excitatory neurons code the neuron with the largest input. The difference here is that multiple winners can be chosen. By varying the voltage reference of the neuron, the network can transition between a soft-max behavior and a hard WTA behav ior. I show results from a fabricated chip of 20 neurons in a 1.2J.Lm CM",Computer Vision,3e7e0224018ab3cf51abb96464d518cd-Paper.pdf,1999
Bayesian model selection for Support,"We present a variational Bayesian method for model selection over families of kernels classifiers like Support Vector machines or Gaus sian processes. The algorithm needs no user interaction and is able to adapt a large number of kernel parameters to given data without having to sacrifice training cases for validation. This opens the pos sibility to use sophisticated families of kernels in situations where the small ""standard kernel"" classes are clearly inappropriate. We relate the method to other work done on Gaussian processes and clarify the relation between Support Vector machines and certain Gaussian process models. 1 Introduction Bayesian techniques have been widely and successfully used in the neural networks and statistics community and are appealing because of their conceptual simplicity, generality and consistency with which they solve learning problems. In this paper we present a new method for applying the Bayesian methodology to Support Vector machines. We will briefly rev",Optimization & Theoretical ML,404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf,1999
Kirchoff Law Markov Fields for Analog,"Three contributions to developing an algorithm for assisting engi neers in designing analog circuits are provided in this paper. First, a method for representing highly nonlinear and non-continuous analog circuits using Kirchoff current law potential functions within the context of a Markov field is described. Second, a relatively effi cient algorithm for optimizing the Markov field objective function is briefly described and the convergence proof is briefly sketched. And third, empirical results illustrating the strengths and limita tions of the approach are provided within the context of a JFET transistor design problem. The proposed algorithm generated a set of circuit components for the JFET circuit model that accurately generated the desired characteristic curves. 1 Analog circuit design using Markov random fields 1.1 Markov random field models A Markov random field (MRF) is a generalization of the concept of a Markov chain. In a Markov field one begins with a set of random variab",Computer Vision,418ef6127e44214882c61e372e866691-Paper.pdf,1999
Can VI mechanisms account for,"When a visual image consists of a figure against a background, V1 cells are physiologically observed to give higher responses to image regions corresponding to the figure relative to their responses to the background. The medial axis of the figure also induces rela tively higher responses compared to responses to other locations in the figure (except for the boundary between the figure and the background). Since the receptive fields of V1 cells are very smal l compared with the global scale of the figure-ground and medial axis effects, it has been suggested that these effects may be caused by feedback from higher visual areas. I show how these effects can be accounted for by V1 mechanisms when the size of the figure is small or is of a certain scale. They are a manifestation of the processes of pre-attentive segmentation which detect and highlight the boundaries between homogeneous image regions. 1 Introduction Segmenting figure from ground is one of the most important visual tasks. We",Computer Vision,442cde81694ca09a626eeddefd1b74ca-Paper.pdf,1999
Policy Gradient Methods for,"Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and deter mining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, indepen dent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy. Large applications of reinforcement learning (RL) require the use of generalizing func tion approx",Reinforcement Learning,464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf,1999
Lower Bounds on the Complexity of,"We calculate lower bounds on the size of sigmoidal neural networks that approximate continuous functions. In particular, we show that for the approximation of polynomials the network size has to grow as O((logk)1/4) where k is the degree of the polynomials. This bound is valid for any input dimension, i.e. independently of the number of variables. The result is obtained by introducing a new method employing upper bounds on the Vapnik-Chervonenkis dimension for proving lower bounds on the size of networks that approximate continuous functions. 1 Introduction Sigmoidal neural networks are known to be universal approximators. This is one of the theoretical results most frequently cited to justify the use of sigmoidal neural networks in applications. By this statement one refers to the fact that sigmoidal neural networks have been shown to be able to approximate any continuous function arbitrarily well. Numerous results in the literature have established variants of this universal approxim",Optimization & Theoretical ML,4921f95baf824205e1b13f22d60357a1-Paper.pdf,1999
Evolv ..........,"Recent theories suggestthat languageacquisitionis assisted bythe evolution of languages towards forms that are easily learnable. In this paper, weevolvecombinatoriallanguageswhichcanbelearned by a recurrent neural network quickly and from relatively few ex amples. Additionally, we evolve languages for generalization in different ""worlds"", and for generalization from specific examples. We find that languages can be evolved to facilitate different forms of impressive generalization for a minimally biased, general pur pose learner. The results provide empirical support for the theory that the language itself, as well as the language environment of a learner, plays a substantial role in learning: that there is far more to language acquisition than the language acquisition device. 1 Introduction: Factors in language learnability In exploring issues oflanguage learnability, the special abilities ofhumans to learn complex languages have been much emphasized, with one dominant theory based on ",NLP,4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf,1999
Large Margin DAGs for,"We present a new learning architecture: the Decision Directed Acyclic Graph (DDAG), which is used to combine many two-class classifiers into a multiclass classifier. For an N -class problem, the DDAG con tains N(N - 1)/2 classifiers, one for each pair of classes. We present a VC analysis of the case when the node classifiers are hyperplanes; the re sulting bound on the test error depends on N and on the margin achieved at the nodes, but not on the dimension of the space. This motivates an algorithm, DAGSVM, which operates in a kernel-induced feature space and uses two-class maximal margin hyperplanes at each decision-node of the DDAG. The DAGSVM is substantially faster to train and evalu ate than either the standard algorithm or Max Wins, while maintaining comparable accuracy to both of these algorithms. 1 Introduction The problem of multiclass classificatIon, especially for systems like SVMs, doesn't present an easy solution. It is generally simpler to construct classifier theory and ",Optimization & Theoretical ML,4abe17a1c80cbdd2aa241b70840879de-Paper.pdf,1999
Approximate Planning in Large POMDPs,"We consider the problem of reliably choosing a near-best strategy from a restricted class of strategies TI in a partially observable Markov deci sion process (POMDP). We assume we are given the ability to simulate the POMDP, and study what might be called the sample complexity - that is, the amount of data one must generate in the POMDP in order to choose a good strategy. We prove upper bounds on the sample com plexity showing that, even for infinitely large and arbitrarily complex POMDPs, the amount of data needed can be finite, and depends only linearly on the complexity of the restricted strategy class TI, and expo nentially on the horizon time. This latter dependence can be eased in a variety of ways, including the application of gradient and local search algorithms. Our measure of complexity generalizes the classical super vised learning notion of VC dimension to the settings of reinforcement learning and planning. 1 Introduction Much recent attention has been focused on partially",Reinforcement Learning,4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf,1999
The Relaxed Online,"We describe a new incremental algorithm for training linear thresh old functions: the Relaxed Online Maximum Margin Algorithm, or ROMMA. ROMMA can be viewed as an approximation to the algorithm that repeatedly chooses the hyperplane that classifies previously seen ex amples correctly with the maximum margin. It is known that such a maximum-margin hypothesis can be computed by minimizing the length of the weight vector subject to a number of linear constraints. ROMMA works by maintaining a relatively simple relaxation of these constraints that can be efficiently updated. We prove a mistake bound for ROMMA that is the same as that proved for the perceptron algorithm. Our analysis implies that the more computationally intensive maximum-margin algo rithm also satisfies this mistake bound; this is the first worst-case perfor mance guarantee for this algorithm. We describe some experiments us ing ROMMA and a variant that updates its hypothesis more aggressively as batch algorithms to recogni",Optimization & Theoretical ML,515ab26c135e92ed8bf3a594d67e4ade-Paper.pdf,1999
Maximum entropy discrimination,"We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions. All calcula tions involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections. This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete. Support vector machines are naturally subsumed un der this class and we provide several extensions. We are also able to estimate exactly and efficiently discriminative distributions over tree structures of class-conditional models within this framework. Preliminary experimental results are indicative of the potential in these techniques. 1 Introduction Effective discrimination is essential in many application areas. Employing gener ative probability models such as mixture models in this context is attractive but",Optimization & Theoretical ML,4fa53be91b4933d536748a60458b9797-Paper.pdf,1999
Bayesian modelling of tMRI time series,"We present a Hidden Markov Model (HMM) for inferring the hidden psychological state (or neural activity) during single trial tMRI activa tion experiments with blocked task paradigms. Inference is based on Bayesian methodology, using a combination of analytical and a variety of Markov Chain Monte Carlo (MCMC) sampling techniques. The ad vantage of this method is that detection of short time learning effects be tween repeated trials is possible since inference is based only on single trial experiments. 1 Introduction Functional magnetic resonance imaging (tMRI) is a non-invasive technique that enables indirect measures of neuronal activity in the working human brain. The most common tMRI technique is based on an image contrast induced by temporal shifts in the relative concentration of oxyhemoglobin and deoxyhemoglobin (BOLD contrast). Since neuronal activation leads to an increased blood flow, the so-called hemodynamic response, the mea sured tMRI signal reflects neuronal activity. Henc",Optimization & Theoretical ML,52c5189391854c93e8a0e1326e56c14f-Paper.pdf,1999
Bayesian averaging is well-temperated,"Bayesian predictions are stochastic just like predictions of any other inference scheme that generalize from a finite sample. While a sim ple variational argument shows that Bayes averaging is generaliza tion optimal given that the prior matches the teacher parameter distribution the situation is less clear if the teacher distribution is unknown. I define a class of averaging procedures, the temperated likelihoods, including both Bayes averaging with a uniform prior and maximum likelihood estimation as special cases. I show that Bayes is generalization optimal in this family for any teacher dis tribution for two learning problems that are analytically tractable: learning the mean of a Gaussian and asymptotics of smooth learn ers. 1 Introduction Learning is the stochastic process of generalizing from a random finite sample of data. Often a learning problem has natural quantitative measure of generalization. If a loss function is defined the natural measure is the generalization error, i",Optimization & Theoretical ML,52d080a3e172c33fd6886a37e7288491-Paper.pdf,1999
Policy Search via Density Estimation,"We propose a new approach to the problem of searching a space of stochastic controllers for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP). Following several other authors, our approach is based on searching in parameterized families of policies (for example, via gradient descent) to optimize solution qual ity. However, rather than trying to estimate the values and derivatives of a policy directly, we do so indirectly using estimates for the proba bility densities that the policy induces on states at the different points in time. This enables our algorithms to exploit the many techniques for efficient and robust approximate density propagation in stochastic sys tems. We show how our techniques can be applied both to deterministic propagation schemes (where the MDP's dynamics are given explicitly in compact form,) and to stochastic propagation schemes (where we have access only to a generative model, or simulator, of the MDP). We present empiri",Optimization & Theoretical ML,54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf,1999
Low Power Wireless Communication via,"This paper examines the application of reinforcement learning to a wire less communication problem. The problem requires that channel util ity be maximized while simultaneously minimizing battery usage. We present a solution to this multi-criteria problem that is able to signifi cantly reduce power consumption. The solution uses a variable discount factor to capture the effects of battery usage. 1 Introduction Reinforcement learning (RL) has been applied to resource allocation problems in telecom munications, e.g., channel allocation in wireless systems, network routing, and admission control in telecommunication networks [1,2, 8, 10]. These have demonstrated reinforce ment learning can find good policies that significantly increase the application reward within the dynamics of the telecommunication problems. However, a key issue is how to treat the commonly occurring multiple reward and constraint criteria in a consistent way. This paper will focus on power management for wireless pac",Reinforcement Learning,54f5f4071faca32ad5285fef87b78646-Paper.pdf,1999
Learning to Parse Images,"We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recog nition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting hand written digits were obtained. 1 Introd uction The task of recognition has been the main focus of attention of statistical pattern recognition for the past 40 years. The paradigm problem is to classify an object from a vector of features extracted from the image. With the advent of backpropagation [1], the choice of features and the choice of weights to put on these features became part of a single, overall optimization and impressive performance was obtained for restricted but important tasks such as handwritten character identification [2]. A significant weakness of many current recognition systems is their reliance on a separate preprocessing stage that segment",Computer Vision,5a142a55461d5fef016acfb927fee0bd-Paper.pdf,1999
Robust Recognition of Noisy and Superimposed,"In many classification tasks, recognition accuracy is low because input patterns are corrupted by noise or are spatially or temporally overlapping. We propose an approach to overcoming these limitations based on a model of human selective attention. The model, an early selection filter guided by top-down attentional control, entertains each candidate output class in sequence and adjusts attentional gain coefficients in order to produce a strong response for that class. The chosen class is then the one that obtains the strongest response with the least modulation of attention. We present simulation results on classification of corrupted and superimposed handwritten digit patterns, showing a significant improvement in recognition rates. The algorithm has also been applied in the domain of speech recognition, with comparable results. 1 Introduction In many classification tasks, recognition accuracy is low because input patterns are corrupted by noise or are spatially or temporally overlap",Computer Vision,5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf,1999
Bayesian Network Induction via Local,"In recent years, Bayesian networks have become highly successful tool for di agnosis, analysis, and decision making in real-world domains. We present an efficient algorithm for learning Bayes networks from data. Our approach con structs Bayesian networks by first identifying each node's Markov blankets, then connecting nodes in a maximally consistent way. In contrast to the majority of work, which typically uses hill-climbing approaches that may produce dense and causally incorrect nets, our approach yields much more compact causal networks by heeding independencies in the data. Compact causal networks facilitate fast in ference and are also easier to understand. We prove that under mild assumptions, our approach requires time polynomial in the size of the data and the number of nodes. A randomized variant, also presented here, yields comparable results at much higher speeds. 1 Introduction A great number of scientific fields today benefit from being able to automatically estimate the ",Optimization & Theoretical ML,5d79099fcdf499f12b79770834c0164a-Paper.pdf,1999
Spiking Boltzmann Machines,"We first show how to represent sharp posterior probability distribu tions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to con vey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spik ing neurons learn to model an image sequence by fitting a dynamic generative model. 1 Population codes and energy landscapes A perceived object is represented in the brain by the activities of many neurons, but there is no general consensus on how the activities of individual neurons combine to represent the multiple properties of an object. We start by focussing on the case of a single object that has multiple instantiation parameters such as position, velocity, size and orientation. We assume that each neuron has an ideal stimulus in the space of instantiation parameters and that its activation rate or probability of activation falls off monotonically i",Computer Vision,62889e73828c756c961c5a6d6c01a463-Paper.pdf,1999
Actor-Critic Algorithms,"We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information pro vided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems. 1 Introduction The vast majority of Reinforcement Learning (RL) [9J and Neuro-Dynamic Pro gramming (NDP) [lJ methods fall into one of the following two categories: (a) Actor-only methods work with a parameterized family of policies. The gra dient of the performance, with respect to the actor parameters, is directly estimated by simulation, and the parameters are updated in a direction of improve",Reinforcement Learning,6449f44a102fde848669bdd9eb6b76fa-Paper.pdf,1999
Training Data Selection,"In this paper, we consider the problem of active learning in trigonomet ric polynomial networks and give a necessary and sufficient condition of sample points to provide the optimal generalization capability. By ana lyzing the condition from the functional analytic point of view, we clarify the mechanism of achieving the optimal generalization capability. We also show that a set of training examples satisfying the condition does not only provide the optimal generalization but also reduces the compu tational complexity and memory required for the calculation of learning results. Finally, examples of sample points satisfying the condition are given and computer simulations are performed to demonstrate the effec tiveness of the proposed active learning method. 1 Introduction Supervised learning is obtaining an underlying rule from training examples, and can be formulated as a function approximation problem. If sample points are actively designed, then learning can be performed more effici",Optimization & Theoretical ML,647c722bf90a49140184672e0d3723e3-Paper.pdf,1999
Image Recognition in Context: Application to,"We propose a new and efficient technique for incorporating contextual information into object classification. Most of the current techniques face the problem of exponential computation cost. In this paper, we propose a new general framework that incorporates partial context at a linear cost. This technique is applied to microscopic urinalysis image recognition, resulting in a significant improvement of recognition rate over the context free approach. This gain would have been impossible using conventional context incorporation techniques. 1 BACKGROUND: RECOGNITION IN CONTEXT There are a number of pattern recognition problem domains where the classification of an object should be based on more than simply the appearance of the object itself. In remote sensing image classification, where each pixel is part of ground cover, a pixel is more like ly to be a glacier if it is in a mountainous area, than if surrounded by pixels of residential areas. In text analysis, one can expect to find cer",Computer Vision,64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf,1999
Bayesian Map Learning in Dynamic,"We consider the problem of learning a grid-based map using a robot with noisy sensors and actuators. We compare two approaches: online EM, where the map is treated as a fixed parameter, and Bayesian inference, where the map is a (matrix-valued) random variable. We show that even on a very simple example, online EM can get stuck in local minima, which causes the robot to get ""lost"" and the resulting map to be useless. By contrast, the Bayesian approach, by maintaining multiple hypotheses, is much more ro bust. We then introduce a method for approximating the Bayesian solution, called Rao-Blackwellised particle filtering. We show that this approximation, when coupled with an active learning strategy, is fast but accurate. 1 Introduction The problem of getting mobile robots to autonomously learn maps of their envi ronment has been widely studied (see e.g., [9] for a collection of recent papers). The basic difficulty is that the robot must know exactly where it is (a problem called localiz",Computer Vision,66be31e4c40d676991f2405aaecc6934-Paper.pdf,1999
Better Generative Models for Sequential,"This paper describes bidirectional recurrent mixture density net works, which can model multi-modal distributions of the type P(Xt Iyf) and P(Xt lXI, X2, ... ,Xt-l, yf) without any explicit as sumptions about the use of context. These expressions occur fre quently in pattern recognition problems with sequential data, for example in speech recognition. Experiments show that the pro posed generative models give a higher likelihood on test data com pared to a traditional modeling approach, indicating that they can summarize the statistical properties of the data better. 1 Introduction Many problems of engineering interest can be formulated as sequential data prob lems in an abstract sense as supervised learning from sequential data, where an input vector (dimensionality D) sequence X = xf = {X!,X2, .. . ,XT_!,XT} liv ing in space X has to be mapped to an output vector (dimensionality J<) target sequence T = tf = {tl' t2, ... , tT -1 , tT} in space1 y, that often embodies cor relations bet",NLP,6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf,1999
Statistical Dynamics of Batch Learning,"An important issue in neural computing concerns the description of learning dynamics with macroscopic dynamical variables. Recen t progress on on-line learning only addresses the often unrealistic case of an infinite training set. We introduce a new framework to model batch learning of restricted sets of examples, widely applica ble to any learning cost function, and fully taking into account the temporal correlations introduced by the recycling of the examples. For illustration we analyze the effects of weight decay and early stopping during the learning of teacher-generated examples. 1 Introduction The dynamics of learning in neural computing is a complex multi-variate process. The interest on the macroscopic level is thus to describe the process with macro scopic dynamical variables. Recently, much progress has been made on modeling the dynamics of on-line learning, in which an independent example is generated for each learning step [1, 2]. Since statistical correlations among the e",Optimization & Theoretical ML,673271cc47c1a4e77f57e239ed4d28a7-Paper.pdf,1999
Scale Mixtures of Gaussians and the,"The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit vari ance dependencies not captured by second-order models. We ex amine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coeffi cients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear ""normalization"" procedure can be used to Gaussianize the coefficients. Recent years have witnessed a surge of interest in modeling the statistics of natural images. Such models are important for applications in image processing and com puter vis",Computer Vision,6a5dfac4be1502501489fc0f5a24b667-Paper.pdf,1999
Independent Factor Analysis with,"We present a new technique for time series analysis based on dy namic probabilistic networks. In this approach, the observed data are modeled in terms of unobserved, mutually independent factors, as in the recently introduced technique of Independent Factor Anal ysis (IFA). However, unlike in IFA, the factors are not Li.d.; each factor has its own temporal statistical characteristics. We derive a family of EM algorithms that learn the structure of the underlying factors and their relation to the data. These algorithms perform source separation and noise reduction in an integrated manner, and demonstrate superior performance compared to IFA. 1 Introduction The technique of independent factor analysis (IFA) introduced in [1] provides a tool for modeling L'-dim data in terms of L unobserved factors. These factors are mutually independent and combine linearly with added noise to produce the observed data. Mathematically, the model is defined by Yt = HXt + Ut, (1) where Xt is the vector of ",Optimization & Theoretical ML,6a81681a7af700c6385d36577ebec359-Paper.pdf,1999
Managing Uncertainty in Cue Combination,"We develop a hierarchical generative model to study cue combi nation. The model maps a global shape parameter to local cue specific parameters, which in tum generate an intensity image. Inferring shape from images is achieved by inverting this model. Inference produces a probability distribution at each level; using distributions rather than a single value of underlying variables at each stage preserves information about the validity of each local cue for the given image. This allows the model, unlike standard combination models, to adaptively weight each cue based on gen eral cue reliability and specific image context. We describe the results of a cue combination psychophysics experiment we con ducted that allows a direct comparison with the model. The model provides a good fit to our data and a natural account for some in teresting aspects of cue combination. Understanding cue combination is a fundamental step in developing computa tional models of visual perception, because many asp",Optimization & Theoretical ML,6e62a992c676f611616097dbea8ea030-Paper.pdf,1999
Potential Boosters ?,"Recent interpretations of the Adaboost algorithm view it as per forming a gradient descent on a potential function. Simply chang ing the potential function allows one to create new algorithms re lated to AdaBoost. However, these new algorithms are generally not known to have the formal boosting property. This paper ex amines the question of which potential functions lead to new al gorithms that are boosters. The two main results are general sets of conditions on the potential; one set implies that the resulting algorithm is a booster, while the other implies that the algorithm is not. These conditions are applied to previously studied potential functions, such as those used by LogitBoost and Doom II. 1 Introduction The first boosting algorithm appeared in Rob Schapire's thesis [1]. This algorithm was able to boost the performance of a weak PAC learner [2] so that the resulting algorithm satisfies the strong PAC learning [3] criteria. We will call any method that builds a strong PAC lea",Optimization & Theoretical ML,70ece1e1e0931919438fcfc6bd5f199c-Paper.pdf,1999
Resonance in a Stochastic Neuron Model,"We study here a simple stochastic single neuron model with delayed self-feedback capable of generating spike trains. Simulations show that its spike trains exhibit resonant behavior between ""noise"" and ""delay"". In order to gain insight into this resonance, we simplify the model and study a stochastic binary element whose transition probability depends on its state at a fixed interval in the past. With this simplified model we can analytically compute interspike interval histograms, and show how the resonance between noise and delay arises. The resonance is also observed when such elements are coupled through delayed interaction. 1 Introd uction ""Noise"" and ""delay"" are two elements which are associated with many natural and artificial systems and have been studied in diverse fields. Neural networks provide representative examples of information processing systems with noise and delay. Though much research has gone into the investigation of these two factors in the community, they have m",Optimization & Theoretical ML,712a3c9878efeae8ff06d57432016ceb-Paper.pdf,1999
Wiring optimization in the brain,"The complexity of cortical circuits may be characterized by the number of synapses per neuron. We study the dependence of complexity on the fraction of the cortical volume that is made up of ""wire"" (that is, ofaxons and dendrites), and find that complexity is maximized when wire takes up about 60% of the cortical volume. This prediction is in good agree ment with experimental observations. A consequence of our arguments is that any rearrangement of neurons that takes more wire would sacrifice computational power. Wiring a brain presents formidable problems because of the extremely large number of con nections: a microliter of cortex contains approximately 105 neurons, 109 synapses, and 4 km ofaxons, with 60% of the cortical volume being taken up with ""wire"", half of this by axons and the other half by dendrites. [ 1] Each cortical neighborhood must have exactly the right balance of components; if too many cell bodies were present in a particular mm cube, for example, insufficient space",Computer Vision,7137debd45ae4d0ab9aa953017286b20-Paper.pdf,1999
Learning from user feedback in image retrieval,"We formulate the problem of retrieving images from visual databases as a problem of Bayesian inference. This leads to natural and effective solutions for two of the most challenging issues in the design of a retrieval system: providing support for region-based queries without requiring prior image segmentation, and accounting for user-feedback during a retrieval session. We present a new learning algorithm that relies on belief propagation to account for both positive and negative examples of the user's interests. 1 Introduction Due to the large amounts of imagery that can now be accessed and managed via comput ers, the problem of content-based image retrieval (CBIR) has recently attracted significant interest among the vision community [1, 2, 5]. Unlike most traditional vision applications, very few assumptions about the content of the images to be analyzed are allowable in the context of CBIR. This implies that the space of valid image representations is restricted to those of a gene",Computer Vision,7283518d47a05a09d33779a17adf1707-Paper.pdf,1999
Online Independent Component Analysis,"Stochastic meta-descent (SMD) is a new technique for online adap tation of local learning rates in arbitrary twice-differentiable sys tems. Like matrix momentum it uses full second-order information while retaining O(n) computational complexity by exploiting the efficient computation of Hessian-vector products. Here we apply SMD to independent component analysis, and employ the result ing algorithm for the blind separation of time-varying mixtures. By matching individual learning rates to the rate of change in each source signal's mixture coefficients, our technique is capable of si multaneously tracking sources that move at very different, a priori unknown speeds. 1 Introduction Independent component analysis (ICA) methods are typically run in batch mode in order to keep the stochasticity of the empirical gradient low. Often this is combined with a global learning rate annealing scheme that negotiates the tradeoff between fast convergence and good asymptotic performance. For time-vary",Optimization & Theoretical ML,7437d136770f5b35194cb46c1653efaa-Paper.pdf,1999
A Variational Bayesian Framework for,"This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analyt ical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non Gaussian and no Hessian needs to be computed. Predictive quanti ties are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its conver gence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation. 1 Introduction A standard method to learn a graphical model 1 from data is maximum likelihood (ML). Given a training dataset, ML estimates a single optimal value for the model",Optimization & Theoretical ML,74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf,1999
Model selection in clustering by uniform,"Unsupervised learning algorithms are designed to extract struc ture from data samples. Reliable and robust inference requires a guarantee that extracted structures are typical for the data source, Le., similar structures have to be inferred from a second sample set of the same data source. The overfitting phenomenon in max imum entropy based annealing algorithms is exemplarily studied for a class of histogram clustering models. Bernstein's inequality for large deviations is used to determine the maximally achievable approximation quality parameterized by a minimal temperature. Monte Carlo simulations support the proposed model selection cri terion by finite temperature annealing. 1 Introduction Learning algorithms are designed to extract structure from data. Two classes of algorithms have been widely discussed in the literature - supervised and unsuper vised learning. The distinction between the two classes depends on supervision or teacher information which is either available to the ",Optimization & Theoretical ML,757f843a169cc678064d9530d12a1881-Paper.pdf,1999
Algebraic Analysis for Non-Regular,"Hierarchical learning machines are non-regular and non-identifiable statistical models, whose true parameter sets are analytic sets with singularities. Using algebraic analysis, we rigorously prove that the stochastic complexity of a non-identifiable learning machine is asymptotically equal to >'1 log n - (ml - 1) log log n + const., where n is the number of training samples. Moreover we show that the rational number >'1 and the integer ml can be algorithmically calculated using resolution of singularities in algebraic geometry. Also we obtain inequalities 0 < >'1 ~ d/2 and 1 ~ ml ~ d, where d is the number of parameters. 1 Introduction Hierarchical learning machines such as multi-layer perceptrons, radial basis func tions, and normal mixtures are non-regular and non-identifiable learning machines. If the true distribution is almost contained in a learning model, then the set of true parameters is not one point but an analytic variety [4][9][3][10]. This paper establishes the mathemati",Optimization & Theoretical ML,752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf,1999
U nmixing Hyperspectral Data,"In hyperspectral imagery one pixel typically consists of a mixture of the reflectance spectra of several materials, where the mixture coefficients correspond to the abundances of the constituting ma terials. We assume linear combinations of reflectance spectra with some additive normal sensor noise and derive a probabilistic MAP framework for analyzing hyperspectral data. As the material re flectance characteristics are not know a priori, we face the problem of unsupervised linear unmixing. The incorporation of different prior information (e.g. positivity and normalization of the abun dances) naturally leads to a family of interesting algorithms, for example in the noise-free case yielding an algorithm that can be understood as constrained independent component analysis (ICA). Simulations underline the usefulness of our theory. 1 Introduction Current hyperspectral remote sensing technology can form images of ground surface reflectance at a few hundred wavelengths simultaneously, with w",Computer Vision,798ed7d4ee7138d49b8828958048130a-Paper.pdf,1999
Some Theoretical Results Concerning the,"Recently, sample complexity bounds have been derived for problems in volving linear functions such as neural networks and support vector ma chines. In this paper, we extend some theoretical results in this area by deriving dimensional independent covering number bounds for regular ized linear functions under certain regularization conditions. We show that such bounds lead to a class of new methods for training linear clas sifiers with similar theoretical advantages of the support vector machine. Furthermore, we also present a theoretical analysis for these new meth ods from the asymptotic statistical point of view. This technique provides better description for large sample behaviors of these algorithms. 1 Introduction In this paper, we are interested in the generalization performance of linear classifiers ob tained from certain algorithms. From computational learning theory point of view, such performance measurements, or sample complexity bounds, can be described by a quanti ty calle",Optimization & Theoretical ML,7c4ede33a62160a19586f6e26eaefacf-Paper.pdf,1999
Inference for the Generalization Error,"In order to to compare learning algorithms, experimental results reported in the machine learning litterature often use statistical tests of signifi cance. Unfortunately, most of these tests do not take into account the variability due to the choice of training set. We perform a theoretical investigation of the variance of the cross-validation estimate of the gen eralization error that takes into account the variability due to the choice of training sets. This allows us to propose two new ways to estimate this variance. We show, via simulations, that these new statistics perform well relative to the statistics considered by Dietterich (Dietterich, 1998). 1 Introduction When applying a learning algorithm (or comparing several algorithms), one is typically interested in estimating its generalization error. Its point estimation is rather trivial through cross-validation. Providing a variance estimate of that estimation, so that hypothesis test ing and/or confidence intervals are possible,",Optimization & Theoretical ML,7d12b66d3df6af8d429c1a357d8b9e1a-Paper.pdf,1999
LTD Facilitates Learning In a Noisy,"Long-term potentiation (LTP) has long been held as a biological substrate for associative learning. Recently, evidence has emerged that long-term depression (LTD) results when the presynaptic cell fires after the postsynaptic cell. The computational utility of LTD is explored here. Synaptic modification kernels for both LTP and LTD have been proposed by other laboratories based studies of one postsynaptic unit. Here, the interaction between time-dependent LTP and LTD is studied in small networks. 1 Introduction Long term potentiation (LTP) is a neurophysiological phenomenon observed under laboratory conditions in which two neurons or neural populations are stimulated at a high frequency with a resulting measurable increase in synaptic efficacy between them that lasts for several hours or days [1]-[2] LTP thus provides direct evidence supporting the neurophysiological hypothesis articulated by Hebb [3]. This increase in synaptic strength must be countered by a mechanism for weakening th",Optimization & Theoretical ML,7dd0240cd412efde8bc165e864d3644f-Paper.pdf,1999
An Oeulo-Motor System with Multi-Chip,"A system emulating the functionality of a moving eye-hence the name oculo-motor system-has been built and successfully tested. It is made of an optical device for shifting the field of view of an image sensor by up to 45 ° in any direction, four neuromorphic analog VLSI circuits imple menting an oculo-motor control loop, and some off-the-shelf electronics. The custom integrated circuits communicate with each other primarily by non-arbitrated address-event buses. The system implements the behav iors of saliency-based saccadic exploration, and smooth pursuit of light spots. The duration of saccades ranges from 45 ms to 100 ms, which is comparable to human eye performance. Smooth pursuit operates on light sources moving at up to 500/s in the visual field. 1 INTRODUCTION Inspiration from biology has been recognized as a seminal approach to address some en gineering challenges, particularly in the computational domain [1]. Researchers have bor rowed architectures, operating principles and e",Computer Vision,7e230522657ecdc50e4249581b861f8e-Paper.pdf,1999
Understanding stepwise generalization of,"In this article we study the effects of introducing structure in the input distribution of the data to be learnt by a simple perceptron. We determine the learning curves within the framework of Statis tical Mechanics. Stepwise generalization occurs as a function of the number of examples when the distribution of patterns is highly anisotropic. Although extremely simple, the model seems to cap ture the relevant features of a class of Support Vector Machines which was recently shown to present this behavior. 1 Introduction A new approach to learning has recently been proposed as an alternative to feedfor ward neural networks: the Support Vector Machines (SVM) [1]. Instead of trying to learn a non linear mapping between the input patterns and internal representations, like in multilayered perceptrons, the SVMs choose a priori a non-linear kernel that transforms the input space into a high dimensional feature space. In binary classi fication tasks like those considered in the present paper",Optimization & Theoretical ML,7eb7eabbe9bd03c2fc99881d04da9cbd-Paper.pdf,1999
A Geometric Interpretation of v-SVM,"We show that the recently proposed variant of the Support Vector machine (SVM) algorithm, known as v-SVM, can be interpreted as a maximal separation between subsets of the convex hulls of the data, which we call soft convex hulls. The soft convex hulls are controlled by choice of the parameter v. If the intersection of the convex hulls is empty, the hyperplane is positioned halfway between them such that the distance between convex hulls, measured along the normal, is maximized; and if it is not, the hyperplane's normal is similarly determined by the soft convex hulls, but its position (perpendicular distance from the origin) is adjusted to minimize the error sum. The proposed geometric interpretation of v-SVM also leads to necessary and sufficient conditions for the existence of a choice of v for which the v-SVM solution is nontrivial. 1 Introduction Recently, SchOlkopf et al. [I) introduced a new class of SVM algorithms, called v-SVM, for both regression estimation and pattern recogn",Optimization & Theoretical ML,7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,1999
Robust Learning of Chaotic Attractors,"A fundamental problem with the modeling of chaotic time series data is that minimizing short-term prediction errors does not guarantee a match between the reconstructed attractors of model and experiments. We introduce a modeling paradigm that simultaneously learns to short-tenn predict and to locate the outlines of the attractor by a new way of nonlinear principal component analysis. Closed-loop predictions are constrained to stay within these outlines, to prevent divergence from the attractor. Learning is exceptionally fast: parameter estimation for the 1000 sample laser data from the 1991 Santa Fe time series competition took less than a minute on a 166 MHz Pentium PC. 1 Introduction We focus on the following objective: given a set of experimental data and the assumption that it was produced by a deterministic chaotic system, find a set of model equations that will produce a time-series with identical chaotic characteristics, having the same chaotic attractor. The common approach co",Optimization & Theoretical ML,81c650caac28cdefce4de5ddc18befa0-Paper.pdf,1999
Greedy importance sampling,"I present a simple variation of importance sampling that explicitly search es for important regions in the target distribution. I prove that the tech nique yields unbiased estimates, and show empirically it can reduce the variance of standard Monte Carlo estimators. This is achieved by con centrating samples in more significant regions of the sample space. 1 Introduction It is well known that general inference and learning with graphical models is computa tionally hard [1] and it is therefore necessary to consider restricted architectures [13], or approximate algorithms to perform these tasks [3, 7]. Among the most convenient and successful techniques are stochastic methods which are guaranteed to converge to a correct solution in the limit oflarge samples [10, 11, 12, 15]. These methods can be easily applied to complex inference problems that overwhelm deterministic approaches. The family of stochastic inference methods can be grouped into the independent Monte Carlo methods (importan",Optimization & Theoretical ML,8303a79b1e19a194f1875981be5bdb6f-Paper.pdf,1999
Recurrent cortical competition: Strengthen or,"We investigate the short term .dynamics of the recurrent competition and neural activity in the primary visual cortex in terms of information pro cessing and in the context of orientation selectivity. We propose that af ter stimulus onset, the strength of the recurrent excitation decreases due to fast synaptic depression. As a consequence, the network shifts from an initially highly nonlinear to a more linear operating regime. Sharp orientation tuning is established in the first highly competitive phase. In the second and less competitive phase, precise signaling of multiple ori entations and long range modulation, e.g., by intra- and inter-areal con nections becomes possible (surround effects). Thus the network first ex tracts the salient features from the stimulus, and then starts to process the details. We show that this signal processing strategy is optimal if the neurons have limited bandwidth and their objective is to transmit the maximum amount of information in any time interva",Computer Vision,831c2f88a604a07ca94314b56a4921b8-Paper.pdf,1999
Constrained Hidden Markov Models,"By thinking of each state in a hidden Markov model as corresponding to some spatial region of a fictitious topology space it is possible to naturally define neigh bouring states as those which are connected in that space. The transition matrix can then be constrained to allow transitions only between neighbours; this means that all valid state sequences correspond to connected paths in the topology space. I show how such constrained HMMs can learn to discover underlying structure in complex sequences of high dimensional data, and apply them to the problem of recovering mouth movements from acoustics in continuous speech. 1 Latent variable models for structured sequence data Structured time-series are generated by systems whose underlying state variables change in a continuous way but whose state to output mappings are highly nonlinear, many to one and not smooth. Probabilistic unsupervised learning for such sequences requires models with two essential features: latent (hidden) variable",Optimization & Theoretical ML,84c6494d30851c63a55cdb8cb047fadd-Paper.pdf,1999
Approximate inference algorithms for two-layer,"We present a class of approximate inference algorithms for graphical models of the QMR-DT type. We give convergence rates for these al gorithms and for the Jaakkola and Jordan (1999) algorithm, and verify these theoretical predictions empirically. We also present empirical re sults on the difficult QMR-DT network problem, obtaining performance of the new algorithms roughly comparable to the Jaakkola and Jordan algorithm. 1 Introduction The graphical models formalism provides an appealing framework for the design and anal ysis of network-based learning and inference systems. The formalism endows graphs with a joint probability distribution and interprets most queries of interest as marginal or con ditional probabilities under this joint. For a fixed model one is generally interested in the conditional probability of an output given an input (for prediction), or an input conditional on the output (for diagnosis or control). During learning the focus is usually on the like lihood (a margi",Optimization & Theoretical ML,84f0f20482cde7e5eacaf7364a643d33-Paper.pdf,1999
Perceptual Organization Based on,"A figure-ground segregation network is proposed based on a novel boundary pair representation. Nodes in the network are bound ary segments obtained through local grouping. Each node is ex citatorily coupled with the neighboring nodes that belong to the same region, and inhibitorily coupled with the corresponding paired node. Gestalt grouping rules are incorporated by modulating con nections. The status of a node represents its probability being figural and is updated according to a differential equation. The system solves the figure-ground segregation problem through tem poral evolution. Different perceptual phenomena, such as modal and amodal completion, virtual contours, grouping and shape de composition are then explained through local diffusion. The system eliminates combinatorial optimization and accounts for many psy chophysical results with a fixed set of parameters. 1 Introduction Perceptual organization refers to the ability of grouping similar features in sensory data. This, ",Computer Vision,851300ee84c2b80ed40f51ed26d866fc-Paper.pdf,1999
Learning Informative Statistics: A,"We discuss an information theoretic approach for categorizing and mod eling dynamic processes. The approach can learn a compact and informa tive statistic which summarizes past states to predict future observations. Furthermore, the uncertainty of the prediction is characterized nonpara metrically by a joint density over the learned statistic and present obser vation. We discuss the application of the technique to both noise driven dynamical systems and random processes sampled from a density which is conditioned on the past. In the first case we show results in which both the dynamics of random walk and the statistics of the driving noise are captured. In the second case we present results in which a summarizing statistic is learned on noisy random telegraph waves with differing de pendencies on past states. In both cases the algorithm yields a principled approach for discriminating processes with differing dynamics and/or de pendencies. The method is grounded in ideas from informatio",Optimization & Theoretical ML,8698ff92115213ab187d31d4ee5da8ea-Paper.pdf,1999
Rules and Similarity in Concept Learning,"This paper argues that two apparently distinct modes of generalizing con cepts - abstracting rules and computing similarity to exemplars - should both be seen as special cases of a more general Bayesian learning frame work. Bayes explains the specific workings of these two modes - which rules are abstracted, how similarity is measured - as well as why gener alization should appear rule- or similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning. 1 Introduction In domains ranging from reasoning to language acquisition, a broad view is emerging of cognition as a hybrid of two distinct modes of computation, one based on applying abstract rules and the other based on assessing similarity to stored exemplars [7]. Much support for this view comes from the study of concepts and categorization.",NLP,86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf,1999
Support Vector Method for Novelty Detection,"Suppose you are given some dataset drawn from an underlying probabil ity distribution P and you want to estimate a ""simple"" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified l/ between 0 and 1. We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a poten tially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data. 1 INTRODUCTION During recent years, a new set of kernel techniques for supervised learning has been devel oped [8]. Specifically, support vector (SV) algorithms for pattern recognition, regres",Optimization & Theoretical ML,8725fb777f25776ffa9076e44fcfd776-Paper.pdf,1999
Generalized Model Selection For Unsupervised,"We describe a Bayesian approach to model selection in unsupervised learning that determines both the feature set and the number of clusters. We then evaluate this scheme (based on marginal likelihood) and one based on cross-validated likelihood. For the Bayesian scheme we derive a closed-form solution of the marginal likelihood by assuming appropriate forms of the likelihood function and prior. Extensive experiments compare these approaches and all results are verified by comparison against ground truth. In these experiments the Bayesian scheme using our objective function gave better results than cross-validation. 1 Introduction Recent efforts define the model selection problem as one of estimating the number of clusters[ 10, 17]. It is easy to see, particularly in applications with large number of features, that various choices of feature subsets will reveal different structures underlying the data. It is our contention that this interplay between the feature subset and the number of",Optimization & Theoretical ML,89f03f7d02720160f1b04cf5b27f5ccb-Paper.pdf,1999
An Improved Decomposition Algorithm,"A new decomposition algorithm for training regression Support Vector Machines (SVM) is presented. The algorithm builds on the basic principles of decomposition proposed by Osuna et. al., and addresses the issue of optimal working set selection. The new criteria for testing optimality of a working set are derived. Based on these criteria, the principle of ""maximal inconsistency"" is pro posed to form (approximately) optimal working sets. Experimental results show superior performance of the new algorithm in compar ison with traditional training of regression SVM without decompo sition. Similar results have been previously reported on decomposi tion algorithms for pattern recognition SVM. The new algorithm is also applicable to advanced SVM formulations based on regression, such as density estimation and integral equation SVM. 1 Introd uction The increasing interest in applications of Support Vector Machines (SVM) to large scale problems ushers in new requirements for computational comple",Optimization & Theoretical ML,8b5700012be65c9da25f49408d959ca0-Paper.pdf,1999
An Analog VLSI Model of,"This paper presents an electronic system that extracts the periodicity of a sound. It uses three analogue VLSI building blocks: a silicon cochlea, two inner-hair-cell circuits and two spiking neuron chips. The silicon cochlea consists of a cascade of filters. Because of the delay between two outputs from the silicon cochlea, spike trains created at these outputs are synchronous only for a narrow range of periodicities. In contrast to traditional band pass filters, where an increase in' selectivity has to be traded off against a decrease in response time, the proposed system responds quickly, independent of selectivity. 1 Introduction The human ear transduces airborne sounds into a neural signal using three stages in the inner ear's cochlea: (i) the mechanical filtering of the Basilar Membrane (BM), (ii) the transduction of membrane vibration into neurotransmitter release by the Inner Hair Cells (IHCs), and (iii) spike generation by the Spiral Ganglion Cells (SGCs), whose axons form the",Computer Vision,8b6a80c3cf2cbd5f967063618dc54f39-Paper.pdf,1999
Data Visualization and Feature Selection:,"Data visualization and feature selection methods are proposed based on the )oint mutual information and ICA. The visualization methods can find many good 2-D projections for high dimensional data interpretation, which cannot be easily found by the other ex isting methods. The new variable selection method is found to be better in eliminating redundancy in the inputs than other methods based on simple mutual information. The efficacy of the methods is illustrated on a radar signal analysis problem to find 2-D viewing coordinates for data visualization and to select inputs for a neural network classifier. Keywords: feature selection, joint mutual information, ICA, vi sualization, classification. 1 INTRODUCTION Visualization of input data and feature selection are intimately related. A good feature selection algorithm can identify meaningful coordinate projections for low dimensional data visualization. Conversely, a good visualization technique can sug gest meaningful features to include",Computer Vision,8c01a75941549a705cf7275e41b21f0d-Paper.pdf,1999
From Coexpression to Coregulation: An,"We provide preliminary evidence that eXlstmg algorithms for inferring small-scale gene regulation networks from gene expression data can be adapted to large-scale gene expression data coming from hybridization microarrays. The essential steps are (1) clustering many genes by their expression time-course data into a minimal set of clusters of co-expressed genes, (2) theoretically modeling the various conditions under which the time-courses are measured using a continious-time analog recurrent neural network for the cluster mean time-courses, (3) fitting such a regulatory model to the cluster mean time courses by simulated annealing with weight decay, and (4) analysing several such fits for commonalities in the circuit parameter sets including the connection matrices. This procedure can be used to assess the adequacy of existing and future gene expression time-course data sets for determ ining transcriptional regulatory relationships such as coregulation. 1 Introduction In a cell, genes ",Optimization & Theoretical ML,8bb88f80d334b1869781beb89f7b73be-Paper.pdf,1999
Noisy Neural Networks and,"In this paper we define a probabilistic computational model which generalizes many noisy neural network models, including the recent work of Maass and Sontag [5]. We identify weak ergodicjty as the mechanism responsible for restriction of the computational power of probabilistic models to definite languages, independent of the characteristics of the noise: whether it is discrete or analog, or if it depends on the input or not, and independent of whether the variables are discrete or continuous. We give examples of weakly ergodic models including noisy computational systems with noise depending on the current state and inputs, aggregate models, and computational systems which update in continuous time. 1 Introduction Noisy neural networks were recently examined, e.g. in. [1,4, 5]. It was shown in [5] that Gaussian-like noise reduces the power of analog recurrent neural networks to the class of definite languages, which area strict subset of regular languages. Let E be an arbitrary alpha",Computer Vision,94e4451ad23909020c28b26ca3a13cb8-Paper.pdf,1999
An Information-Theoretic Framework for,"In this paper, we propose that information maximization can pro vide a unified framework for understanding saccadic eye move ments. In this framework, the mutual information among the cor tical representations of the retinal image, the priors constructed from our long term visual experience, and a dynamic short-term internal representation constructed from recent saccades provides a map for guiding eye navigation. By directing the eyes to loca tions of maximum complexity in neuronal ensemble responses at each step, the automatic saccadic eye movement system greedily collects information about the external world, while modifying the neural representations in the process. This framework attempts to connect several psychological phenomena, such as pop-out and inhibition of return, to long term visual experience and short term working memory. It also provides an interesting perspective on contextual computation and formation of neural representation in the visual system. 1 Introduction Whe",NLP,8d420fa35754d1f1c19969c88780314d-Paper.pdf,1999
Boosting Algorithms as Gradient Descent,"We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theo retical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions. 1 Introduction There has been considerable interest recently in voting methods for pattern classi fication, which predict the label of a particular example using a weighted vote over a set of base classifiers [10, 2, 6, 9, 16, 5, 3, 19, 12, 17, 7,",Optimization & Theoretical ML,96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf,1999
The Nonnegative Boltzmann Machine,"The nonnegative Boltzmann machine (NNBM) is a recurrent neural net work model that can describe multimodal nonnegative data. Application of maximum likelihood estimation to this model gives a learning rule that is analogous to the binary Boltzmann machine. We examine the utility of the mean field approximation for the NNBM, and describe how Monte Carlo sampling techniques can be used to learn its parameters. Reflec tive slice sampling is particularly well-suited for this distribution, and can efficiently be implemented to sample the distribution. We illustrate learning of the NNBM on a transiationally invariant distribution, as well as on a generative model for images of human faces. Introduction The multivariate Gaussian is the most elementary distribution used to model generic da ta. It represents the maximum entropy distribution under the constraint that the mean and covariance matrix of the distribution match that of the data. For the case of binary data, the maximum entropy distri",Optimization & Theoretical ML,955a1584af63a546588caae4d23840b3-Paper.pdf,1999
A MCMC approach to Hierarchical Mixture,"There are many hierarchical clustering algorithms available, but these lack a firm statistical basis. Here we set up a hierarchical probabilistic mixture model, where data is generated in a hierarchical tree-structured manner. Markov chain Monte Carlo (MCMC) methods are demonstrated which can be used to sample from the posterior distribution over trees containing variable numbers of hidden units. 1 Introduction Over the past decade or two mixture models have become a popular approach to clustering or competitive learning problems. They have the advantage of having a well-defined ob jective function and fit in with the general trend of viewing neural network problems in a statistical framework. However, one disadvantage is that they produce a ""flat"" cluster struc ture rather than the hierarchical tree structure that is returned by some clustering algorithms such as the agglomerative single-link method (see e.g. [12]). In this paper I formulate a hierarchical mixture model, which retains",Optimization & Theoretical ML,973a5f0ccbc4ee3524ccf035d35b284b-Paper.pdf,1999
Local probability propagation for factor,"Ever since Pearl's probability propagation algorithm in graphs with cycles was shown to produce excellent results for error-correcting decoding a few years ago, we have been curious about whether local probability propagation could be used successfully for ma chine learning. One of the simplest adaptive models is the factor analyzer, which is a two-layer network that models bottom layer sensory inputs as a linear combination of top layer factors plus in dependent Gaussian sensor noise. We show that local probability propagation in the factor analyzer network usually takes just a few iterations to perform accurate inference, even in networks with 320 sensors and 80 factors. We derive an expression for the algorithm's fixed point and show that this fixed point matches the exact solu tion in a variety of networks, even when the fixed point is unstable. We also show that this method can be used successfully to perform inference for approximate EM and we give results on an online face recog",Computer Vision,96de2547f44254c97f5f4f1f402711c1-Paper.pdf,1999
The Infinite Gaussian Mixture Model,"In a Bayesian mixture model it is not necessary a priori to limit the num ber of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of find ing the ""right"" number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling. 1 Introduction One of the major advantages in the Bayesian methodology is that ""overfitting"" is avoided; thus the difficult task of adjusting model complexity vanishes. For neural networks, this was demonstrated by Neal [1996] whose work on infinite networks led to the reinvention and popularisation of Gaussian Process models [Williams & Rasmussen, 1996]. In this paper a Markov Chain Monte Carlo (MCMC) implementation of a hierarchical infinite Gaussian mixture model is presented. Perhaps surprisingly, inference in such models is possible using finite amounts of computation. Similar models are known in statis",Optimization & Theoretical ML,97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf,1999
Reconstruction of Sequential Data with,"We consider the problem of reconstructing a temporal discrete sequence of multidimensional real vectors when part of the data is missing, under the assumption that the sequence was generated by a continuous pro cess. A particular case of this problem is multivariate regression, which is very difficult when the underlying mapping is one-to-many. We pro pose an algorithm based on a joint probability model of the variables of interest, implemented using a nonlinear latent variable model. Each point in the sequence is potentially reconstructed as any of the modes of the conditional distribution of the missing variables given the present variables (computed using an exhaustive mode search in a Gaussian mix ture). Mode selection is determined by a dynamic programming search that minimises a geometric measure of the reconstructed sequence, de rived from continuity constraints. We illustrate the algorithm with a toy example and apply it to a real-world inverse problem, the acoustic-to articula",Computer Vision,9a4400501febb2a95e79248486a5f6d3-Paper.pdf,1999
Learning the Similarity of Documents:,"The project pursued in this paper is to develop from first information-geometric principles a general method for learning the similarity between text documents. Each individual docu ment is modeled as a memoryless information source. Based on a latent class decomposition of the term-document matrix, a low dimensional (curved) multinomial subfamily is learned. From this model a canonical similarity function - known as the Fisher kernel - is derived. Our approach can be applied for unsupervised and supervised learning problems alike. This in particular covers inter esting cases where both, labeled and unlabeled data are available. Experiments in automated indexing and text categorization verify the advantages of the proposed method. 1 Introduction The computer-based analysis and organization of large document repositories is one oftoday's great challenges in machine learning, a key problem being the quantitative assessment of document similarities. A reliable similarity measure would pro",NLP,9d2682367c3935defcb1f9e247a97c0d-Paper.pdf,1999
from Single-Camera Video,"The three-dimensional motion of humans is underdetermined when the observation is limited to a single camera, due to the inherent 3D ambi guity of 2D video. We present a system that reconstructs the 3D motion of human subjects from single-camera video, relying on prior knowledge about human motion, learned from training data, to resolve those am biguities. After initialization in 2D, the tracking and 3D reconstruction is automatic; we show results for several video sequences. The results show the power of treating 3D body tracking as an inference problem. 1 Introduction We seek to capture the 3D motions of humans from video sequences. The potential appli cations are broad, including industrial computer graphics, virtual reality, and improved human-computer interaction. Recent research attention has focused on unencumbered tracking techniques that don't require attaching markers to the subject's body [4, 5], see [12] for a survey. Typically, these methods require simultaneous views from",Computer Vision,9fe97fff97f089661135d0487843108e-Paper.pdf,1999
Mixture Density Estimation,"Gaussian mixtures (or so-called radial basis function networks) for density estimation provide a natural counterpart to sigmoidal neu ral networks for function fitting and approximation. In both cases, it is possible to give simple expressions for the iterative improve ment of performance as components of the network are introduced one at a time. In particular, for mixture density estimation we show that a k-component mixture estimated by maximum likelihood (or by an iterative likelihood improvement that we introduce) achieves log-likelihood within order 1/k of the log-likelihood achievable by any convex combination. Consequences for approximation and es timation using Kullback-Leibler risk are also given. A Minimum Description Length principle selects the optimal number of compo nents k that minimizes the risk bound. 1 Introduction In density estimation, Gaussian mixtures provide flexible-basis representations for densities that can be used to model heterogeneous data in high dimensio",Optimization & Theoretical ML,a0f3601dc682036423013a5d965db9aa-Paper.pdf,1999
Information Capacity and Robustness of,"The reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes. Adding ion channel stochasticity to neuronal models results in a macroscopic behavior that replicates the input-dependent reliabili ty and precision of real neurons. We calculate the amount of infor mation that an ion channel based stochastic Hodgkin-Huxley (HH) neuron model can encode about a wide set of stimuli. We show that both the information rate and the information per spike of the s tochastic model are similar to the values reported experimentally. Moreover, the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input, and less so with the average firing rate of the neuron. We also show that for the HH ion channel density, the information capacity is robust to changes in the density of ion channels in the membrane, whereas changing the ratio between the Na+ and K+ ion channels has a considerable effect o",Computer Vision,a40511cad8383e5ae8ddd8b855d135da-Paper.pdf,1999
Bayesian Transduction,Transduction is an inference principle that takes a training sam ple and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for induction. Transduction provides a confidence measure on single predictions rather than classifiers - a feature particularly important for risk-sensitive applications. The possibly infinite number of functions is reduced to a finite number of equiv alence classes on the working sample. A rigorous Bayesian analysis reveals that for standard classification loss we cannot benefit from considering more than one test point at a time. The probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space. We con sider the PAC setting of binary classification by linear discriminant functions (perceptrons) in kernel space such that the probability of labels is determined by the volume ratio in version space. We su,Optimization & Theoretical ML,a51c896c9cb81ecb5a199d51ac9fc3c5-Paper.pdf,1999
Constructing Heterogeneous Committees,"The committee approach has been proposed for reducing model uncertainty and improving generalization performance. The ad vantage of committees depends on (1) the performance of individ ual members and (2) the correlational structure of errors between members. This paper presents an input grouping technique for de signing a heterogeneous committee. With this technique, all input variables are first grouped based on their mutual information. Sta tistically similar variables are assigned to the same group. Each member's input set is then formed by input variables extracted from different groups. Our designed committees have less error cor relation between its members, since each member observes different input variable combinations. The individual member's feature sets contain less redundant information, because highly correlated vari ables will not be combined together. The member feature sets con tain almost complete information, since each set contains a feature from each information g",Optimization & Theoretical ML,a588a6199feff5ba48402883d9b72700-Paper.pdf,1999
An Analysis of Turbo Decoding,"We provide an analysis of the turbo decoding algorithm (TDA) in a setting involving Gaussian densities. In this context, we are able to show that the algorithm converges and that - somewhat surprisingly - though the density generated by the TDA may differ significantly from the desired posterior density, the means of these two densities coincide. 1 Introd uction In many applications, the state of a system must be inferred from noisy observations. Examples include digital communications, speech recognition, and control with in complete information. Unfortunately, problems of inference are often intractable, and one must resort to approximation methods. One approximate inference method that has recently generated spectacular success in certain coding applications is the turbo decoding algorithm [1, 2], which bears a close resemblance to message-passing algorithms developed in the coding community a few decades ago [4]. It has been shown that the TDA is also related to well-understood exa",Optimization & Theoretical ML,a63fc8c5d915e1f1a40f40e6c7499863-Paper.pdf,1999
Probabilistic methods for Support Vector,"I describe a framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors. This can provide intuitive guidelines for choosing a 'good' SVM kernel. It can also assign (by evidence maximization) optimal values to parameters such as the noise level C which cannot be determined unambiguously from properties of the MAP solution alone (such as cross-validation er ror). I illustrate this using a simple approximate expression for the SVM evidence. Once C has been determined, error bars on SVM predictions can also be obtained. 1 Support Vector Machines: A probabilistic framework Support Vector Machines (SVMs) have recently been the subject of intense re search activity within the neural networks community; for tutorial introductions and overviews of recent developments see [1, 2, 3]. One of the open questions that remains is how to set the 'tunable' parameters of an SVM algorithm: While meth ods for choosing",Optimization & Theoretical ML,a941493eeea57ede8214fd77d41806bc-Paper.pdf,1999
Neural System Model of Human Sound,"This paper examines the role of biological constraints in the human audi tory localization process. A psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologi cally plausible ""realism constraints"". The directional acoustical cues, upon which sound localization is based, were derived from the human subject's head-related transfer functions (HRTFs). Sound stimuli were generated by convolving bandpass noise with the HRTFs and were pre sented to both the subject and the model. The input stimuli to the model was processed using the Auditory Image Model of cochlear processing. The cochlear data was then analyzed by a time-delay neural network which integrated temporal and spectral information to determine the spa tial location of the sound source. The combined cochlear model and neural network provided a system model of the sound localization pro cess. Human-like localization p",Computer Vision,ab2b41c63853f0a651ba9fbf502b0cd8-Paper.pdf,1999
Graded grammaticality in Prediction,"We introduce a novel method of constructing language models, which avoids some of the problems associated with recurrent neu ral networks. The method of creating a Prediction Fractal Machine (PFM) [1] is briefly described and some experiments are presented which demonstrate the suitability of PFMs for language modeling. PFMs distinguish reliably between minimal pairs, and their be havior is consistent with the hypothesis [4] that wellformedness is 'graded' not absolute. A discussion of their potential to offer fresh insights into language acquisition and processing follows. 1 Introduction Cognitive linguistics has seen the development in recent years of two important, related trends. Firstly, a widespread renewal of interest in the statistical, 'graded' nature of language (e.g. [2]-[4]) is showing that the traditional all-or-nothing no tion of well-formedness may not present an accurate picture of how the congruity of utterances is represented internally. Secondly, the analysis of stat",NLP,ae614c557843b1df326cb29c57225459-Paper.pdf,1999
A Neuromorphic VLSI System for Modeling,"We have developed and tested an analog/digital VLSI system that mod els the coordination of biological segmental oscillators underlying axial locomotion in animals such as leeches and lampreys. In its current form the system consists of a chain of twelve pattern generating circuits that are capable of arbitrary contralateral inhibitory synaptic coupling. Each pattern generating circuit is implemented with two independent silicon Morris-Lecar neurons with a total of 32 programmable (floating-gate based) inhibitory synapses, and an asynchronous address-event inter connection element that provides synaptic connectivity and implements axonal delay. We describe and analyze the data from a set of experi ments exploring the system behavior in terms of synaptic coupling. 1 Introduction In recent years, neuroscientists and modelers have made great strides towards illuminat ing structure and computational properties in biological motor systems. For example, much progress has been made toward und",Computer Vision,acab0116c354964a558e65bdd07ff047-Paper.pdf,1999
Learning sparse codes with a,"We describe a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images. The sparsity of the basis function coefficients is modeled with a mixture-of-Gaussians distribution. One Gaussian captures non active coefficients with a small-variance distribution centered at zero, while one or more other Gaussians capture active coefficients with a large-variance distribution. We show that when the prior is in such a form, there exist efficient methods for learning the basis functions as well as the parameters of the prior. The performance of the algorithm is demonstrated on a number of test cases and also on natural images. The basis functions learned on natural images are similar to those obtained with other methods, but the sparse form of the coefficient distribution is much better described. Also, since the parameters of the prior are adapted to the data, no assumption about sparse structure in the images need be made a priori, rather ",Computer Vision,b0f2ad44d26e1a6f244201fe0fd864d1-Paper.pdf,1999
Semiparametric Approach to Multichannel,"In this paper we discuss the semi parametric statistical model for blind deconvolution. First we introduce a Lie Group to the manifold of non causal FIR filters. Then blind deconvolution problem is formulated in the framework of a semiparametric model, and a family of estimating functions is derived for blind deconvolution. A natural gradient learn ing algorithm is developed for training noncausal filters. Stability of the natural gradient algorithm is also analyzed in this framework. 1 Introduction Recently blind separation/deconvolution has been recognized as an increasing important research area due to its rapidly growing applications in various fields, such as telecom munication systems, image enhancement and biomedical signal processing. Refer to re view papers [7] and [13] for details. A semi parametric statistical model treats a family of probability distributions specified by a finite-dimensional parameter of interest and an infinite-dimensional nuisance parameter [12]. Amari a",Optimization & Theoretical ML,b147a61c1d07c1c999560f62add6dbc7-Paper.pdf,1999
Application of Blind Separation of Sources to,"In the analysis of data recorded by optical imaging from intrinsic signals (measurement of changes of light reflectance from cortical tissue) the re moval of noise and artifacts such as blood vessel patterns is a serious problem. Often bandpass filtering is used, but the underlying assumption that a spatial frequency exists, which separates the mapping component from other components (especially the global signal), is questionable. Here we propose alternative ways of processing optical imaging data, us ing blind source separation techniques based on the spatial decorre1ation of the data. We first perform benchmarks on artificial data in order to select the way of processing, which is most robust with respect to sen sor noise. We then apply it to recordings of optical imaging experiments from macaque primary visual cortex. We show that our BSS technique is able to extract ocular dominance and orientation preference maps from single condition stacks, for data, where standard post-process",Computer Vision,b2531e7bb29bf22e1daae486fae3417a-Paper.pdf,1999
Spectral Cues in Human Sound Localization,"The differential contribution of the monaural and interaural spectral cues to human sound localization was examined using a combined psy chophysical and analytical approach. The cues to a sound's location were correlated on an individual basis with the human localization re sponses to a variety of spectrally manipulated sounds. The spectral cues derive from the acoustical filtering of an individual's auditory periphery which is characterized by the measured head-related transfer functions (HRTFs). Auditory localization performance was determined in virtual auditory space (VAS). Psychoacoustical experiments were conducted in which the amplitude spectra of the sound stimulus was varied indepen dentlyat each ear while preserving the normal timing cues, an impossibil ity in the free-field environment. Virtual auditory noise stimuli were gen erated over earphones for a specified target direction such that there was a ""false"" flat spectrum at the left eardrum. Using the subject's HRTFs, the ",Computer Vision,b29eed44276144e4e8103a661f9a78b7-Paper.pdf,1999
Robust Full Bayesian Methods for Neural,"In this paper, we propose a full Bayesian model for neural networks. This model treats the model dimension (number of neurons), model parameters, regularisation parameters and noise parameters as ran dom variables that need to be estimated. We then propose a re versible jump Markov chain Monte Carlo (MCMC) method to per form the necessary computations. We find that the results are not only better than the previously reported ones, but also appear to be robust with respect to the prior specification. Moreover, we present a geometric convergence theorem for the algorithm. 1 Introduction In the early nineties, Buntine and Weigend (1991) and Mackay (1992) showed that a principled Bayesian learning approach to neural networks can lead to many improve ments [1,2]. In particular, Mackay showed that by approximating the distributions of the weights with Gaussians and adopting smoothing priors, it is possible to obtain estimates of the weights and output variances and to automatically set the r",Optimization & Theoretical ML,b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf,1999
A Neurodynamical Approach to Visual Attention,"The psychophysical evidence for ""selective attention"" originates mainly from visual search experiments. In this work, we formulate a hierarchi cal system of interconnected modules consisting in populations of neu rons for modeling the underlying mechanisms involved in selective visual attention. We demonstrate that our neural system for visual search works across the visual field in parallel but due to the different intrinsic dynamics can show the two experimentally observed modes of visual attention, namely: the serial and the parallel search mode. In other words, neither explicit model of a focus of attention nor saliencies maps are used. The focus of attention appears as an emergent property of the dynamic behavior of the system. The neural population dynamics are handled in the framework of the mean-field approximation. Conse quently, the whole process can be expressed as a system of coupled dif ferential equations. 1 Introduction Traditional theories of human vision considers two ",Computer Vision,b3bbccd6c008e727785cb81b1aa08ac5-Paper.pdf,1999
Dynamics of Supervised Learning with,"We generalize a recent formalism to describe the dynamics of supervised learning in layered neural networks, in the regime where data recycling is inevitable, to the case of noisy teachers. Our theory generates reliable predictions for the evolution in time of training- and generalization er rors, and extends the class of mathematically solvable learning processes in large neural networks to those situations where overfitting can occur. 1 Introduction Tools from statistical mechanics have been used successfully over the last decade to study the dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The simplest theories result upon assuming the data set to be much larger than the number of weight updates made, which rules out recycling and ensures that any distribution of relevance will be Gaussian. Unfortunately, both in terms of applications and in terms of mathematical interest, this regime is not the most relevant one. Most complications and peculiaritie",Optimization & Theoretical ML,b59a51a3c0bf9c5228fde841714f523a-Paper.pdf,1999
Audio-Vision:,"Psychophysical and physiological evidence shows that sound local ization of acoustic signals is strongly influenced by their synchrony with visual signals. This effect, known as ventriloquism, is at work when sound coming from the side of a TV set feels as if it were coming from the mouth of the actors. The ventriloquism effect suggests that there is important information about sound location encoded in the synchrony between the audio and video signals. In spite of this evidence, audiovisual synchrony is rarely used as a source of information in computer vision tasks. In this paper we explore the use of audio visual synchrony to locate sound sources. We developed a system that searches for regions of the visual land scape that correlate highly with the acoustic signals and tags them as likely to contain an acoustic source. We discuss our experience implementing the system, present results on a speaker localization task and discuss potential applications of the approach. Introd uction W",Computer Vision,b618c3210e934362ac261db280128c22-Paper.pdf,1999
Predictive Sequence Learning in Recurrent,Neocortical circuits are dominated by massive excitatory feedback: more than eighty percent of the synapses made by excitatory cortical neurons are onto other excitatory cortical neurons. Why is there such massive re current excitation in the neocortex and what is its role in cortical compu tation? Recent neurophysiological experiments have shown that the plas ticity of recurrent neocortical synapses is governed by a temporally asym metric Hebbian learning rule. We describe how such a rule may allow the cortex to modify recurrent synapses for prediction of input sequences. The goal is to predict the next cortical input from the recent past based on previous experience of similar input sequences. We show that a temporal difference learning rule for prediction used in conjunction with dendritic back-propagating action potentials reproduces the temporally asymmet ric Hebbian plasticity observed physiologically. Biophysical simulations demonstrate that a network of cortical neurons can lea,Reinforcement Learning,b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf,1999
Differentiating Functions of the Jacobian,"For many problems, the correct behavior of a model depends not only on its input-output mapping but also on properties of its Jacobian matrix, the matrix of partial derivatives of the model's outputs with respect to its in puts. We introduce the J-prop algorithm, an efficient general method for computing the exact partial derivatives of a variety of simple functions of the Jacobian of a model with respect to its free parameters. The algorithm applies to any parametrized feedforward model, including nonlinear re gression, multilayer perceptrons, and radial basis function networks. 1 Introduction Let f (x, w) be an n input, m output, twice differentiable feedforward model parameterized by an input vector, x, and a weight vector w. Its Jacobian matrix is defined as ~ [ ()xl ~l a~"" = df(x, w) . J= : aim aim dx aXI ax"" The algorithm we introduce can be used to optimize functions of the form (1) or Ev(w) = 21 1 1Jv - bll 2 (2) where u, v, a, and b are user-defined constants. Our algorithm, w",Optimization & Theoretical ML,b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf,1999
Effects of Spatial and Temporal Contiguity on,"Spatial information comes in two forms: direct spatial information (for example, retinal position) and indirect temporal contiguity information, since objects encountered sequentially are in general spatially close. The acquisition of spatial information by a neural network is investigated here. Given a spatial layout of several objects, networks are trained on a prediction task. Networks using temporal sequences with no direct spa tial information are found to develop internal representations that show distances correlated with distances in the external layout. The influence of spatial information is analyzed by providing direct spatial information to the system during training that is either consistent with the layout or inconsistent with it. This approach allows examination of the relative contributions of spatial and temporal contiguity. 1 Introduction Spatial information is acquired by a process of exploration that is fundamentally tempo ral, whether it be on a small scale, such a",Computer Vision,ba1b3eba322eab5d895aa3023fe78b9c-Paper.pdf,1999
Agglomerative Information Bottleneck,"We introduce a novel distributional clustering algorithm that max imizes the mutual information per cluster between data and giv en categories. This algorithm can be considered as a bottom up hard version of the recently introduced ""Information Bottleneck Method"". The algorithm is compared with the top-down soft ver sion of the information bottleneck method and a relationship be tween the hard and soft results is established. We demonstrate the algorithm on the 20 Newsgroups data set. For a subset of two news groups we achieve compression by 3 orders of magnitudes loosing only 10% of the original mutual information. 1 Introduction The problem of self-organization of the members of a set X based on the similarity of the conditional distributions of the members of another set, Y, {p(Ylx)}, was first introduced in [8] and was termed ""distributional clustering"" . This question was recently shown in [9] to be a special case of a much more fun damental problem: What are the features of the v",Optimization & Theoretical ML,be3e9d3f7d70537357c67bb3f4086846-Paper.pdf,1999
Spike-based learning rules and stabilization of,"We analyze the conditions under which synaptic learning rules based on action potential timing can be approximated by learning rules based on firing rates. In particular, we consider a form of plasticity in which synapses depress when a presynaptic spike is followed by a postsynaptic spike, and potentiate with the opposite temporal ordering. Such differen tial anti-Hebbian plasticity can be approximated under certain conditions by a learning rule that depends on the time derivative of the postsynaptic firing rate. Such a learning rule acts to stabilize persistent neural activity patterns in recurrent neural networks. 1 INTRODUCTION Recent experiments have demonstrated types of synaptic plasticity that depend on the o~i =~=::====: ~re 11111111111 111111111111111111 11. temporal ordering of presynap A post 11111111111 11111111 •• _:3/_-----' tic and postsynaptic spiking. At \_:_; J ~ cortical [ I] and hippocampal[2] B 0L-.i t .:oLl ____ ° synapses, long-term potenti o ation is induced by",Computer Vision,c0560792e4a3c79e62f76cbf9fb277dd-Paper.pdf,1999
Nonlinear Discriminant Analysis using,"Fishers linear discriminant analysis (LDA) is a classical multivari ate technique both for dimension reduction and classification. The data vectors are transformed into a low dimensional subspace such that the class centroids are spread out as much as possible. In this subspace LDA works as a simple prototype classifier with lin ear decision boundaries. However, in many applications the linear boundaries do not adequately separate the classes. We present a nonlinear generalization of discriminant analysis that uses the ker nel trick of representing dot products by kernel functions. The pre sented algorithm allows a simple formulation of the EM-algorithm in terms of kernel functions which leads to a unique concept for un supervised mixture analysis, supervised discriminant analysis and semi-supervised discriminant analysis with partially unlabelled ob servations in feature spaces. 1 Introduction Classical linear discriminant analysis (LDA) projects N data vectors that belong to c differ",Optimization & Theoretical ML,c0d0e461de8d0024aebcb0a7c68836df-Paper.pdf,1999
On input selection with reversible jump,"In this paper we will treat input selection for a radial basis function (RBF) like classifier within a Bayesian framework. We approximate the a-posteriori distribution over both model coefficients and input subsets by samples drawn with Gibbs updates and reversible jump moves. Using some public datasets, we compare the classification accuracy of the method with a conventional ARD scheme. These datasets are also used to infer the a-posteriori probabilities of dif ferent input subsets. 1 Introduction Methods that aim to determine relevance of inputs have always interested re searchers in various communities. Classical feature subset selection techniques, as reviewed in [1], use search algorithms and evaluation criteria to determine one opti mal subset. Although these approaches can improve classification accuracy, they do not explore different equally probable subsets. Automatic relevance determination (ARD) is another approach which determines relevance of inputs. ARD is due to [6] who ",Optimization & Theoretical ML,c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf,1999
Uniqueness of the SVM Solution,"We give necessary and sufficient conditions for uniqueness of the support vector solution for the problems of pattern recognition and regression estimation, for a general class of cost functions. We show that if the solution is not unique, all support vectors are necessarily at bound, and we give some simple examples of non-unique solu tions. We note that uniqueness of the primal (dual) solution does not necessarily imply uniqueness of the dual (primal) solution. We show how to compute the threshold b when the solution is unique, but when all support vectors are at bound, in which case the usual method for determining b does not work. 1 Introduction Support vector machines (SVMs) have attracted wide interest as a means to imple ment structural risk minimization for the problems of classification and regression estimation. The fact that training an SVM amounts to solving a convex quadratic programming problem means that the solution found is global, and that if it is not unique, then th",Optimization & Theoretical ML,c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf,1999
The Parallel Problems Server: an Interactive Tool,"Imagine that you wish to classify data consisting of tens of thousands of ex amples residing in a twenty thousand dimensional space. How can one ap ply standard machine learning algorithms? We describe the Parallel Prob lems Server (PPServer) and MATLAB*P. In tandem they allow users of networked computers to work transparently on large data sets from within Matlab. This work is motivated by the desire to bring the many benefits of scientific computing algorithms and computational power to machine learning researchers. We demonstrate the usefulness of the system on a number of tasks. For example, we perform independent components analysis on very large text corpora consisting of tens of thousands of documents, making minimal changes to the original Bell and Sejnowski Matlab source (Bell and Se jnowski, 1995). Applying ML techniques to data previously beyond their reach leads to interesting analyses of both data and algorithms. 1 Introduction Real-world data sets are extremely large by t",Optimization & Theoretical ML,c59b469d724f7919b7d35514184fdc0f-Paper.pdf,1999
Search for Information Bearing,"In this paper, we use mutual information to characterize the dis tributions of phonetic and speaker/channel information in a time frequency space. The mutual information (MI) between the pho netic label and one feature, and the joint mutual information (JMI) between the phonetic label and two or three features are estimated. The Miller's bias formulas for entropy and mutual information es timates are extended to include higher order terms. The MI and the JMI for speaker/channel recognition are also estimated. The results are complementary to those for phonetic classification. Our results show how the phonetic information is locally spread and how the speaker/channel information is globally spread in time and frequency. 1 Introduction Speech signals typically carry information about number of target sources such as linguistic message, speaker identity, and environment in which the speech was produced. In most realistic applications of speech technology, only one or a few in formation ta",NLP,cc42acc8ce334185e0193753adb6cb77-Paper.pdf,1999
An Oscillatory Correlation Framework for,"A neural model is described which uses oscillatory correlation to segregate speech from interfering sound sources. The core of the model is a two-layer neural oscillator network. A sound stream is represented by a synchronized population of oscillators, and different streams are represented by desynchronized oscillator populations. The model has been evaluated using a corpus of speech mixed with interfering sounds, and produces an improvement in signal-to-noise ratio for every mixture. 1 Introduction Speech is seldom heard in isolation: usually, it is mixed with other environmental sounds. Hence, the auditory system must parse the acoustic mixture reaching the ears in order to retrieve a description of each sound source, a process termed auditory scene analysis (ASA) [2]. Conceptually, ASA may be regarded as a two-stage process. The first stage (which we term 'segmentation') decomposes the acoustic stimulus into a collection of sensory elements. In the second stage ('grouping'), elemen",Computer Vision,cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf,1999
Manifold Stochastic Dynamics,"We propose a new Markov Chain Monte Carlo algorithm which is a gen eralization of the stochastic dynamics method. The algorithm performs exploration of the state space using its intrinsic geometric structure, facil itating efficient sampling of complex distributions. Applied to Bayesian learning in neural networks, our algorithm was found to perform at least as well as the best state-of-the-art method while consuming considerably less time. 1 Introduction In the Bayesian framework predictions are made by integrating the function of interest over the posterior parameter distribution, the lattt~r being the normalized product of the prior distribution and the likelihood. Since in most problems the integrals are too complex to be calculated analytically, approximations are needed. Early works in Bayesian learning for nonlinear models [Buntineand Weigend 1991, MacKay 1992] used Gaussian approximations to the posterior parameter distribution. However, the Gaussian approximation may be poor, ",Optimization & Theoretical ML,d2cdf047a6674cef251d56544a3cf029-Paper.pdf,1999
Bifurcation Analysis of a Silicon Neuron,We have developed a VLSI silicon neuron and a corresponding mathe matical model that is a two state-variable system. We describe the cir cuit implementation and compare the behaviors observed in the silicon neuron and the mathematical model. We also perform bifurcation analy sis of the mathematical model by varying the externally applied current and show that the behaviors exhibited by the silicon neuron under corre sponding conditions are in good agreement to those predicted by the bifurcation analysis. 1 Introduction The use of hardware models to understand dynamical behaviors in biological systems is an approach that has a long and fruitful history [1 ][2]. The implementation in silicon of oscillatory neural networks that model rhythmic motor-pattern generation in animals is one recent addition to these modeling efforts [3][4]. The oscillatory patterns generated by these systems result from intrinsic membrane properties of individual neurons and their synaptic interactions within th,Computer Vision,d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf,1999
Speech Modelling Using Subspace and EM,"The speech waveform can be modelled as a piecewise-stationary linear stochastic state space system, and its parameters can be estimated using an expectation-maximisation (EM) algorithm. One problem is the ini tialisation of the EM algorithm. Standard initialisation schemes can lead to poor formant trajectories. But these trajectories however are impor tant for vowel intelligibility. The aim of this paper is to investigate the suitability of subspace identification methods to initialise EM. The paper compares the subspace state space system identification (4SID) method with the EM algorithm. The 4SID and EM methods are similar in that they both estimate a state sequence (but using Kalman fil ters and Kalman smoothers respectively), and then estimate parameters (but using least-squares and maximum likelihood respectively). The sim ilarity of 4SID and EM motivates the use of 4SID to initialise EM. Also, 4SID is non-iterative and requires no initialisation, whereas EM is itera tive and req",NLP,d860bd12ce9c026814bbdfc1c573f0f5-Paper.pdf,1999
Optimal Kernel Shapes for Local Linear,"Local linear regression performs very well in many low-dimensional forecasting problems. In high-dimensional spaces, its performance typically decays due to the well-known ""curse-of-dimensionality"". A possible way to approach this problem is by varying the ""shape"" of the weighting kernel. In this work we suggest a new, data-driven method to estimating the optimal kernel shape. Experiments us ing an artificially generated data set and data from the UC Irvine repository show the benefits of kernel shaping. 1 Introduction Local linear regression has attracted considerable attention in both statistical and machine learning literature as a flexible tool for nonparametric regression analysis [Cle79, FG96, AMS97]. Like most statistical smoothing approaches, local modeling suffers from the so-called ""curse-of-dimensionality"", the well-known fact that the proportion of the training data that lie in a fixed-radius neighborhood of a point decreases to zero at an exponential rate with increasing d",Optimization & Theoretical ML,d8d31bd778da8bdd536187c36e48892b-Paper.pdf,1999
Robust Neural Network Regression for Offline,"We replace the commonly used Gaussian noise model in nonlinear regression by a more flexible noise model based on the Student-t distribution. The degrees of freedom of the t-distribution can be chosen such that as special cases either the Gaussian distribution or the Cauchy distribution are realized. The latter is commonly used in robust regres sion. Since the t-distribution can be interpreted as being an infinite mix ture of Gaussians, parameters and hyperparameters such as the degrees of freedom of the t-distribution can be learned from the data based on an EM-learning algorithm. We show that modeling using the t-distribution leads to improved predictors on real world data sets. In particular, if outliers are present, the t-distribution is superior to the Gaussian noise model. In effect, by adapting the degrees of freedom, the system can ""learn"" to distinguish between outliers and non-outliers. Especially for online learning tasks, one is interested in avoiding inappropriate weight c",Optimization & Theoretical ML,db29450c3f5e97f97846693611f98c15-Paper.pdf,1999
Neural Network Based Model Predictive,"Model Predictive Control (MPC), a control algorithm which uses an optimizer to solve for the optimal control moves over a future time horizon based upon a model of the process, has become a stan dard control technique in the process industries over the past two decades. In most industrial applications, a linear dynamic model developed using empirical data is used even though the process it self is often nonlinear. Linear models have been used because of the difficulty in developing a generic nonlinear model from empirical data and the computational expense often involved in using non linear models. In this paper, we present a generic neural network based technique for developing nonlinear dynamic models from em pirical data and show that these models can be efficiently used in a model predictive control framework. This nonlinear MPC based approach has been successfully implemented in a number of indus trial applications in the refining, petrochemical, paper and food industries. Perform",Computer Vision,db957c626a8cd7a27231adfbf51e20eb-Paper.pdf,1999
Coastal Navigation with Mobile Robots,"The problem that we address in this paper is how a mobile robot can plan in order to arrive at its goal with minimum uncertainty. Traditional motion planning algo rithms often assume that a mobile robot can track its position reliably, however, in real world situations, reliable localization may not always be feasible. Partially Observable Markov Decision Processes (POMDPs) provide one way to maximize the certainty of reaching the goal state, but at the cost of computational intractability for large state spaces. The method we propose explicitly models the uncertainty of the robot's position as a state variable, and generates trajectories through the augmented pose-uncertainty space. By minimizing the positional uncertainty at the goal, the robot reduces the likelihood it becomes lost. We demonstrate experimentally that coastal navigation reduces the uncertainty at the goal, especially with degraded localization. 1 Introduction For an operational mobile robot, it is essential to preven",Computer Vision,df9028fcb6b065e000ffe8a4f03eeb38-Paper.pdf,1999
Boosting with Multi-Way Branching in,"It is known that decision tree learning can be viewed as a form of boosting. However, existing boosting theorems for decision tree learning allow only binary-branching trees and the generalization to multi-branching trees is not immediate. Practical decision tree al gorithms, such as CART and C4.5, implement a trade-off between the number of branches and the improvement in tree quality as measured by an index function. Here we give a boosting justifica tion for a particular quantitative trade-off curve. Our main theorem states, in essence, that if we require an improvement proportional to the log of the number of branches then top-down greedy con struction of decision trees remains an effective boosting algorithm. 1 Introduction Decision trees have been proved to be a very popular tool in experimental machine learning. Their popularity stems from two basic features - they can be constructed quickly and they seem to achieve low error rates in practice. In some cases the time required fo",Optimization & Theoretical ML,e1696007be4eefb81b1a1d39ce48681b-Paper.pdf,1999
Effects of Spatial and Temporal Contiguity on,"Spatial information comes in two forms: direct spatial information (for example, retinal position) and indirect temporal contiguity information, since objects encountered sequentially are in general spatially close. The acquisition of spatial information by a neural network is investigated here. Given a spatial layout of several objects, networks are trained on a prediction task. Networks using temporal sequences with no direct spa tial information are found to develop internal representations that show distances correlated with distances in the external layout. The influence of spatial information is analyzed by providing direct spatial information to the system during training that is either consistent with the layout or inconsistent with it. This approach allows examination of the relative contributions of spatial and temporal contiguity. 1 Introduction Spatial information is acquired by a process of exploration that is fundamentally tempo ral, whether it be on a small scale, such a",Computer Vision,e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf,1999
Model Selection for Support Vector Machines,"New functionals for parameter (model) selection of Support Vector Ma chines are introduced based on the concepts of the span of support vec tors and rescaling of the feature space. It is shown that using these func tionals, one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter. 1 Introduction Support Vector Machines (SVMs) implement the following idea: they map input vectors into a high dimensional feature space, where a maximal margin hyperplane is constructed [6]. It was shown that when training data are separable, the error rate for SVMs can be characterized by (1) where R is the radius ofthe smallest sphere containing the training data and M is the mar gin (the distance between the hyperplane and the closest training vector in feature space). This functional estimates the VC dimension of hyperplanes separating data with a given margin M. To perform the mapping and to calculate Rand M in the SVM technique. ",Optimization & Theoretical ML,e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf,1999
Memory Capacity of Linear vs. Nonlinear,"Previous biophysical modeling work showed that nonlinear interac tions among nearby synapses located on active dendritic trees can provide a large boost in the memory capacity of a cell (Mel, 1992a, 1992b). The aim of our present work is to quantify this boost by estimating the capacity of (1) a neuron model with passive den dritic integration where inputs are combined linearly across the entire cell followed by a single global threshold, and (2) an active dendrite model in which a threshold is applied separately to the output of each branch, and the branch subtotals are combined lin early. We focus here on the limiting case of binary-valued synaptic weights, and derive expressions which measure model capacity by estimating the number of distinct input-output functions available to both neuron types. We show that (1) the application of a fixed nonlinearity to each dendritic compartment substantially increases the model's flexibility, (2) for a neuron of realistic size, the capacity of ",Computer Vision,e4873aa9a05cc5ed839561d121516766-Paper.pdf,1999
State Abstraction in MAXQ Hierarchical,"Many researchers have explored methods for hierarchical reinforce ment learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state ab stractions, in which aspects of the state space are ignored. In previ ous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning. 1 Introduction Most work on hierarchical reinforcement learning has focused on temporal abstrac tion. For example, in the Options framework [1,2], the programmer defines a set of macro actions (""options"") and provides a policy for each. Learning algorithms (such as semi-Markov Q learni",Reinforcement Learning,e5a4d6bf330f23a8707bb0d6001dfbe8-Paper.pdf,1999
Modeling High-Dimensional Discrete Data with,"The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables ex plodes exponentially. In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow only at most as the square of the number of vari ables, using a multi-layer neural network to represent the joint distribu tion of the variables as the product of conditional distributions. The neu ral network can be interpreted as a graphical model without hidden ran dom variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables. Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can ",Computer Vision,e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf,1999
A generative model for attractor dynamics,"Attractor networks, which map an input space to a discrete out put space, are useful for pattern completion. However, designing a net to have a given set of attractors is notoriously tricky; training procedures are CPU intensive and often produce spurious afuac tors and ill-conditioned attractor basins. These difficulties occur because each connection in the network participates in the encod ing of multiple attractors. We describe an alternative formulation of attractor networks in which the encoding of knowledge is local, not distributed. Although localist attractor networks have similar dynamics to their distributed counterparts, they are much easier to work with and interpret. We propose a statistical formulation of localist attract or net dynamics, which yields a convergence proof and a mathematical interpretation of model parameters. Attractor networks map an input space, usually continuous, to a sparse output space composed of a discrete set of alternatives. Attractor networks ha",Computer Vision,e721a54a8cf18c8543d44782d9ef681f-Paper.pdf,1999
An Environment Model for N onstationary,"Reinforcement learning in nonstationary environments is generally regarded as an important and yet difficult problem. This paper partially addresses the problem by formalizing a subclass of nonsta tionary environments. The environment model, called hidden-mode Markov decision process (HM-MDP), assumes that environmental changes are always confined to a small number of hidden modes. A mode basically indexes a Markov decision process (MDP) and evolves with time according to a Markov chain. While HM-MDP is a special case of partially observable Markov decision processes (POMDP), modeling an HM-MDP environment via the more gen eral POMDP model unnecessarily increases the problem complex ity. A variant of the Baum-Welch algorithm is developed for model learning requiring less data and time. 1 Introduction Reinforcement Learning (RL) [7] is a learning paradigm based upon the framework of Markov decision process (MDP). Traditional RL research assumes that environ ment dynamics (i.e., MDP para",Reinforcement Learning,e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf,1999
Predictive Approaches For Choosing,"Gaussian Processes are powerful regression models specified by parametrized mean and covariance functions. Standard approaches to estimate these parameters (known by the name Hyperparam eters) are Maximum Likelihood (ML) and Maximum APosterior (MAP) approaches. In this paper, we propose and investigate pre dictive approaches, namely, maximization of Geisser's Surrogate Predictive Probability (GPP) and minimization of mean square er ror with respect to GPP (referred to as Geisser's Predictive mean square Error (GPE)) to estimate the hyperparameters. We also derive results for the standard Cross-Validation (CV) error and make a comparison. These approaches are tested on a number of problems and experimental results show that these approaches are strongly competitive to existing approaches. 1 Introduction Gaussian Processes (GPs) are powerful regression models that have gained popular ity recently, though they have appeared in different forms in the literature for years. They can be used ",Optimization & Theoretical ML,e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf,1999
Acquisition in Autoshaping,"Quantitative data on the speed with which animals acquire behav ioral responses during classical conditioning experiments should provide strong constraints on models of learning. However, most models have simply ignored these data; the few that have attempt ed to address them have failed by at least an order of magnitude. We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli playa crucial role. 1 Introduction Conditioning experiments probe the ways that animals make predictions about rewards and punishments and how those predictions are used to their advantage. Substantial quantitative data are available as to how pigeons and rats acquire con ditioned responses during autoshaping, which is one of the simplest paradigms of classical conditioning.4 These data are revealing about the statistical, and ulti mately also the neural, substrate underlying the ways that anima",Optimization & Theoretical ML,e9b73bccd1762555582b513ff9d02492-Paper.pdf,1999
Transductive Inference for Estimating,"We introduce an algorithm for estimating the values of a function at a set of test points Xe+!, ... , xl+m given a set of training points (XI,YI), ... ,(xe,Ye) without estimating (as an intermediate step) the regression function. We demonstrate that this direct (transduc ti ve) way for estimating values of the regression (or classification in pattern recognition) can be more accurate than the tradition alone based on two steps, first estimating the function and then calculating the values of this function at the points of interest. 1 Introduction Following [6] we consider a general scheme of transductive inference. Suppose there exists a function y* = fo(x) from which we observe the measurements corrupted with noise ((Xl, YI),"" . (xe, Ye)), Yi = Y; + ~i' (1) Find an algorithm A that using both the given set of training data (1) and the given set of test data (Xl+!,' .. , XHm) (2) selects from a set of functions {x t--+ f (x)} a function = = Y f(x) fA(xlxl,YI, ... ,Xl,Yl,XHI"""",XHm) (3) ",Optimization & Theoretical ML,ef2a4be5473ab0b3cc286e67b1f59f44-Paper.pdf,1999
Effective Learning Requires Neuronal,"This paper revisits the classical neuroscience paradigm of Hebbian learning. We find that a necessary requirement for effective as sociative memory learning is that the efficacies of the incoming synapses should be uncorrelated. This requirement is difficult to achieve in a robust manner by Hebbian synaptic learning, since it depends on network level information. Effective learning can yet be obtained by a neuronal process that maintains a zero sum of the in coming synaptic efficacies. This normalization drastically improves the memory capacity of associative networks, from an essentially bounded capacity to one that linearly scales with the network's size. It also enables the effective storage of patterns with heterogeneous coding levels in a single network. Such neuronal normalization can be successfully carried out by activity-dependent homeostasis of the neuron's synaptic efficacies, which was recently observed in cortical tissue. Thus, our findings strongly suggest that effective ",NLP,f0bda020d2470f2e74990a07a607ebd9-Paper.pdf,1999
The Relevance Vector Machine,"The support vector machine (SVM) is a state-of-the-art technique for regression and classification, combining excellent generalisation properties with a sparse kernel representation. However, it does suffer from a number of disadvantages, notably the absence of prob abilistic outputs, the requirement to estimate a trade-off parameter and the need to utilise 'Mercer' kernel functions. In this paper we introduce the Relevance Vector Machine (RVM), a Bayesian treat ment of a generalised linear model of identical functional form to the SVM. The RVM suffers from none of the above disadvantages, and examples demonstrate that for comparable generalisation per formance, the RVM requires dramatically fewer kernel functions. 1 Introd uction In supervised learning we are given a set of examples of input vectors {Xn}~=l along with corresponding targets {tn}~=l' the latter of which might be real values (in regression) or class labels (classification). From this 'training' set we wish to learn a mod",Optimization & Theoretical ML,f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf,1999
Learning Statistically Neutral Tasks,"In this paper, we question the necessity of levels of expert-guided tasks. We focus on two tasks, date calculation and parity-12, that are claimed to require intermediate levels of abstraction that must be defined by a human expert. We challenge this claim by demon strating empirically that a single hidden-layer BP-SOM network can learn both tasks without guidance. Moreover, we analyze the net work's solution for the parity-12 task and show that its solution makes use of an elegant intermediary checksum computation. 1 Introduction Breaking up a complex task into many smaller and simpler subtasks facilitates its solution. Such task decomposition has proved to be a successful technique in developing algorithms and in building theories of cognition. In their study and modeling of the human problem-solving process, Newell and Simon [1] employed protocol analysis to determine the subtasks human subjects employ in solving a complex task. Even nowadays, many cognitive scientists take task dec",Optimization & Theoretical ML,f63f65b503e22cb970527f23c9ad7db1-Paper.pdf,1999
Dual Estimation and the Unscented,"Dual estimation refers to the problem of simultaneously estimating the state of a dynamic system and the model which gives rise to the dynam ics. Algorithms include expectation-maximization (EM), dual Kalman filtering, and joint Kalman methods. These methods have recently been explored in the context of nonlinear modeling, where a neural network is used as the functional form of the unknown model. Typically, an ex tended Kalman filter (EKF) or smoother is used for the part of the al gorithm that estimates the clean state given the current estimated model. An EKF may also be used to estimate the weights of the network. This paper points out the flaws in using the EKF, and proposes an improve ment based on a new approach called the unscented transformation (UT) [3]. A substantial performance gain is achieved with the same order of computational complexity as that of the standard EKF. The approach is illustrated on several dual estimation methods. 1 Introduction We consider the problem of",Optimization & Theoretical ML,f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf,1999
Correctness of belief propagation in Gaussian,"Local ""belief propagation"" rules of the sort proposed by Pearl [15] are guaranteed to converge to the correct posterior probabilities in singly connected graphical models. Recently, a number of researchers have em pirically demonstrated good performance of ""loopy belief propagation"" using these same rules on graphs with loops. Perhaps the most dramatic instance is the near Shannon-limit performance of ""Turbo codes"", whose decoding algorithm is equivalent to loopy belief propagation. Except for the case of graphs with a single loop, there has been little theo retical understanding of the performance of loopy propagation. Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly Gaussian random variables. We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation. We give sufficient conditions for convergence and show that when belief propagation converges it gives the c",Optimization & Theoretical ML,f670ef5d2d6bdf8f29450a970494dd64-Paper.pdf,1999
A Multi-class Linear Learning Algorithm,"In this paper, we present Committee, a new multi-class learning algo rithm related to the Winnow family of algorithms. Committee is an al gorithm for combining the predictions of a set of sub-experts in the on line mistake-bounded model oflearning. A sub-expert is a special type of attribute that predicts with a distribution over a finite number of classes. Committee learns a linear function of sub-experts and uses this function to make class predictions. We provide bounds for Committee that show it performs well when the target can be represented by a few relevant sub-experts. We also show how Committee can be used to solve more traditional problems composed of attributes. This leads to a natural ex tension that learns on multi-class problems that contain both traditional attributes and sub-experts. 1 Introduction In this paper, we present a new multi-class learning algorithm called Committee. Committee learns a k class target function by combining information from a large set of sub-",Optimization & Theoretical ML,fc6709bfdf0572f183c1a84ce5276e96-Paper.pdf,1999
Churn Reduction in the Wireless Industry,"Competition in the wireless telecommunications industry is rampant. To main tain profitability, wireless carriers must control chum, the loss of subscribers who switch from one carrier to another. We explore statistical techniques for chum prediction and, based on these predictions. an optimal policy for identify ing customers to whom incentives should be offered to increase retention. Our experiments are based on a data base of nearly 47,000 U.S. domestic subscrib ers, and includes information about their usage, billing, credit, application, and complaint history. We show that under a wide variety of assumptions concerning the cost of intervention and the retention rate resulting from intervention, chum prediction and remediation can yield significant savings to a carrier. We also show the importance of a data representation crafted by domain experts. Competition in the wireless telecommunications industry is rampant. As many as seven competing carriers operate in each market. The ind",Optimization & Theoretical ML,fc9b003bb003a298c2ad0d05e4342bdc-Paper.pdf,1999
Recognizing Evoked Potentials in a Virtual,"Virtual reality (VR) provides immersive and controllable experimen tal environments. It expands the bounds of possible evoked potential (EP) experiments by providing complex, dynamic environments in or der to study cognition without sacrificing environmental control. VR also serves as a safe dynamic testbed for brain-computer .interface (BCl) research. However, there has been some concern about detecting EP sig nals in a complex VR environment. This paper shows that EPs exist at red, green, and yellow stop lights in a virtual driving environment. Ex perimental results show the existence of the P3 EP at ""go"" and ""stop"" lights and the contingent negative variation (CNY) EP at ""slow down"" lights. In order to test the feasibility of on-line recognition in VR, we looked at recognizing the P3 EP at red stop tights and the absence of this signal at yellow slow down lights. Recognition results show that the P3 may successfully be used to control the brakes of a VR car at stop lights. 1 Introdu",Computer Vision,fddd7938a71db5f81fcc621673ab67b7-Paper.pdf,1999
Reinforcement Learning with Function,"Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program. 1 Introduction Although there are convergent online algorithms (such as TD()') [1]) for learning the parameters of a linear approximation to the value function of a Markov process, no way is known to extend these convergence proofs to the task of online approxi mation of either the state-value (V*) or the action-value (Q*) function of a general Markov decision process. In fact, there are known counterexamples to many pro posed algorithms. For example, fitted value iteration can di",Reinforcement Learning,04df4d434d481c5bb723be1b6df1ee65-Paper.pdf,2000
Redundancy and Dimensionality Reduction in,"Low-dimensional representations are key to solving problems in high level vision, such as face compression and recognition. Factorial coding strategies for reducing the redundancy present in natural images on the basis of their second-order statistics have been successful in account ing for both psychophysical and neurophysiological properties of early vision. Class-specific representations are presumably formed later, at the higher-level stages of cortical processing. Here we show that when retinotopic factorial codes are derived for ensembles of natural objects, such as human faces, not only redundancy, but also dimensionality is re duced. We also show that objects are built from parts in a non-Gaussian fashion which allows these local-feature codes to have dimensionalities that are substantially lower than the respective Nyquist sampling rates. 1 Introduction Sensory systems must take advantage of the statistical structure of their inputs in order to process them efficiently, both t",Computer Vision,052335232b11864986bb2fa20fa38748-Paper.pdf,2000
Who Does What? A Novel Algorithm to,"We introduce a novel algorithm, termed PPA (Performance Prediction Algorithm), that quantitatively measures the contributions of elements of a neural system to the tasks it performs. The algorithm identifies the neurons or areas which participate in a cognitive or behavioral task, given data about performance decrease in a small set of lesions. It also allows the accurate prediction of performances due to multi-element lesions. The effectiveness of the new algorithm is demonstrated in two models of recurrent neural networks with complex interactions among the ele ments. The algorithm is scalable and applicable to the analysis of large neural networks. Given the recent advances in reversible inactivation techniques, it has the potential to significantly contribute to the under standing of the organization of biological nervous systems, and to shed light on the long-lasting debate about local versus distributed computa tion in the brain. 1 Introduction Even simple nervous systems are cap",Computer Vision,0533a888904bd4867929dffd884d60b8-Paper.pdf,2000
"Overfitting in Neural Nets: Backpropagation,","The conventional wisdom is that backprop nets with excess hidden units generalize poorly. We show that nets with excess capacity generalize well when trained with backprop and early stopping. Experiments sug gest two reasons for this: 1) Overfitting can vary significantly in different regions of the model. Excess capacity allows better fit to regions of high non-linearity, and backprop often avoids overfitting the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents in similar sequence. Big nets pass through stages similar to those learned by smaller nets. Early stopping can stop training the large net when it generalizes comparably to a smaller net. We also show that conjugate gradient can yield worse generalization because it overfits regions of low non-linearity when learning to fit regions of high non-linearity. 1 Introduction It is commonly believed that large multi-layer perceptrons (MLPs) generalize poorly: nets with too much capacity overfit the tr",Optimization & Theoretical ML,059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf,2000
Color Opponency Constitutes A Sparse,"The human visual system encodes the chromatic signals conveyed by the three types of retinal cone photoreceptors in an opponent fashion. This color opponency has been shown to constitute an efficient encoding by spectral decorrelation of the receptor signals. We analyze the spatial and chromatic structure of natural scenes by decomposing the spectral images into a set of linear basis functions such that they constitute a representation with minimal redun dancy. Independent component analysis finds the basis functions that transforms the spatiochromatic data such that the outputs (activations) are statistically as independent as possible, i.e. least redundant. The resulting basis functions show strong opponency along an achromatic direction (luminance edges), along a blue yellow direction, and along a red-blue direction. Furthermore, the resulting activations have very sparse distributions, suggesting that the use of color opponency in the human visual system achieves a highly efficient",Computer Vision,0609154fa35b3194026346c9cac2a248-Paper.pdf,2000
Learning Segmentation by Random Walks,"We present a new view of image segmentation by pairwise simi larities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This interpretation shows that spectral methods for clustering and segmentation have a probabilistic foun dation. In particular, we prove that the Normalized Cut method arises naturally from our framework. Finally, the framework pro vides a principled method for learning the similarity function as a combination of features. 1 Introduction This paper focuses on pairwise (or similarity-based) clustering and image segmen tation. In contrast to statistical clustering methods, that assume a probabilistic model that generates the observed data points (or pixels), pairwise clustering de fines a similarity function between pairs of points and then formulates a criterion (e.g. maximum total intracluster similarity) that the clustering must optimize. The optimality criteria quantify t",Computer Vision,069654d5ce089c13f642d19f09a3d1c0-Paper.pdf,2000
A PAC-Bayesian Margin Bound for Linear,"We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC-Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing com plexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same",Optimization & Theoretical ML,06a15eb1c3836723b53e4abca8d9b879-Paper.pdf,2000
Active Learning for Parameter Estimation,"Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations. 1 Introduction In many machine learning applications, the most time-consuming and costly task is the collection of a sufficiently larg",Optimization & Theoretical ML,0731460a8a5ce1626210cbf4385ae0ef-Paper.pdf,2000
A tighter bound for graphical models,"We present a method to bound the partition function of a Boltz mann machine neural network with any odd order polynomial. This is a direct extension of the mean field bound, which is first order. We show that the third order bound is strictly better than mean field. Additionally we show the rough outline how this bound is applicable to sigmoid belief networks. Numerical experiments in dicate that an error reduction of a factor two is easily reached in the region where expansion based approximations are useful. 1 Introduction Graphical models have the capability to model a large class of probability distri butions. The neurons in these networks are the random variables, whereas the connections between them model the causal dependencies. Usually, some of the nodes have a direct relation with the random variables in the problem and are called 'visibles'. The other nodes, known as 'hiddens', are used to model more complex probability distributions. Learning in graphical models can be done ",Optimization & Theoretical ML,07a4e20a7bbeeb7a736682b26b16ebe8-Paper.pdf,2000
Occam·s Razor,"The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work. 1 Introduction Occam's Razor is a well known principle of ""parsimony of explanations"" which is influen tial in scientific thinking in general and in problems of statistical inference in particular. In this paper we review its consequences for Bayesian statistical models, where its behaviour can be easily demonstrated and quantified. One might think that one has to build a prior over models which explicitly favours simpler models. But as we will see, Occam's Razor is in fact embodied in the application of Bayesian theory. This idea is k",NLP,0950ca92a4dcf426067cfd2246bb5ff3-Paper.pdf,2000
Exact Solutions to Time-Dependent MDPs,"We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public trans portation and telescope observation scheduling. 1 Introduction Imagine trying to plan a route from home to work that minimizes expected time. One approach is to use a tool such as ""Mapquest"", which annotates maps with information about estimated driving time, then applies a standard graph-search algorithm to produce a shortest route. Even if driving times are stochastic, the an notations can be expected times, so this presents no additional challenge. However, consider what happens if we would like to include public transportation in our route planning. Buses, trains, and subways vary in their expected travel time according to the time of day: buses and subways come mo",Optimization & Theoretical ML,09b15d48a1514d8209b192a8b8f34e48-Paper.pdf,2000
An Information Maximization Approach to,"The principle of maximizing mutual information is applied to learning overcomplete and recurrent representations. The underlying model con sists of a network of input units driving a larger number of output units with recurrent interactions. In the limit of zero noise, the network is de terministic and the mutual information can be related to the entropy of the output units. Maximizing this entropy with respect to both the feed forward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters. The conventional independent components (ICA) learning algorithm can be recovered as a special case where there is an equal number of output units and no recurrent con nections. The application of these new learning rules is illustrated on a simple two-dimensional input example. 1 Introduction Many unsupervised learning algorithms such as principal component analysis, vector quan tization, self-organizing feature maps, and others use the princi",Optimization & Theoretical ML,09fb05dd477d4ae6479985ca56c5a12d-Paper.pdf,2000
Programmable Reinforcement Learning Agents,"We present an expressive agent design language for reinforcement learn ing that allows the user to constrain the policies considered by the learn ing process.The language includes standard features such as parameter ized subroutines, temporary interrupts, aborts, and memory variables, but also allows for unspecified choices in the agent program. For learning that which isn't specified, we present provably convergent learning algo rithms. We demonstrate by example that agent programs written in the language are concise as well as modular. This facilitates state abstraction and the transferability of learned skills. 1 Introduction The field of reinforcement learning has recently adopted the idea that the application of prior knowledge may allow much faster learning and may indeed be essential if real world environments are to be addressed. For learning behaviors, the most obvious form of prior knowledge provides a partial description of desired behaviors. Several languages for partial de",Reinforcement Learning,11108a3dbfe4636cb40b84b803b2fff6-Paper.pdf,2000
A Linear Programming Approach to,Novelty detection involves modeling the normal behaviour of a sys tem hence enabling detection of any divergence from normality. It has potential applications in many areas such as detection of ma chine damage or highlighting abnormal features in medical data. One approach is to build a hypothesis estimating the support of the normal data i.e. constructing a function which is positive in the region where the data is located and negative elsewhere. Recently kernel methods have been proposed for estimating the support of a distribution and they have performed well in practice - training involves solution of a quadratic programming problem. In this pa per we propose a simpler kernel method for estimating the support based on linear programming. The method is easy to implement and can learn large datasets rapidly. We demonstrate the method on medical and fault detection datasets. 1 Introduction. An important classification task is the ability to distinguish between new instances similar to,Optimization & Theoretical ML,0e087ec55dcbe7b2d7992d6b69b519fb-Paper.pdf,2000
Learning Joint Statistical Models for,"People can understand complex auditory and visual information, often using one to disambiguate the other. Automated analysis, even at a low level, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sam pling rates. Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the com plex signal relationships. We learn the joint distribution of the visual and auditory signals using a non-parametric approach. First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation. We then model the complicated stochastic rela tionships between the signals using a nonparametric density estimator. These learned densities allow processing across signal modalities. We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of",Computer Vision,11f524c3fbfeeca4aa916edcb6b6392e-Paper.pdf,2000
Convergence of Large Margin Separable Linear,"Large margin linear classification methods have been successfully ap plied to many applications. For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed ""optimal hyperplane"" approaches zero at a rate propor tional to the inverse training sample size. This rate is usually charac terized by the margin and the maximum norm of the input data. In this paper, we argue that another quantity, namely the robustness of the in put data distribution, also plays an important role in characterizing the convergence behavior of expected misclassification error. Based on this concept of robustness, we show that for a large margin separable linear classification problem, the expected misclassification error may converge exponentially in the number of training sample size. 1 Introduction We consider the binary classification problem: to determine a label y E {-1, 1} associ ated with an input vector x. A useful method for solvi",Optimization & Theoretical ML,13168e6a2e6c84b4b7de9390c0ef5ec5-Paper.pdf,2000
Emergence of movement sensitive,"Olshausen & Field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised, oriented, bandpass receptive fields, similar to those of 'simple cells' in VI. This paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input. This algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds, similar to the receptive fields of the movement-sensitive cells observed in cortical visual areas. Furthermore, in contrast to previous approaches to learning direction selectivity, the timing of neuronal activity encodes the phase of the movement, so the precise timing of spikes is crucially important to the information encoding. 1 Introduction It was suggested by Barlow [3] that the goal of early sensory processing is to reduce redundancy in sensory information and the activity of sen",Computer Vision,136f951362dab62e64eb8e841183c2a9-Paper.pdf,2000
High-temperature expansions for learning,"Recent work has exploited boundedness of data in the unsupervised learning of new types of generative model. For nonnegative data it was recently shown that the maximum-entropy generative model is a Non negative Boltzmann Distribution not a Gaussian distribution, when the model is constrained to match the first and second order statistics of the data. Learning for practical sized problems is made difficult by the need to compute expectations under the model distribution. The computa tional cost of Markov chain Monte Carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods. Here I present a second order mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a ""high-temperature"" expansion. The theory is tested on learning a bimodal 2-dimensional model, a high-dimensional translationally invariant distribution, and a generative model for hand written digits. 1 Introduc",Optimization & Theoretical ML,139f0874f2ded2e41b0393c4ac5644f7-Paper.pdf,2000
A Support Vector Method for Clustering,"We present a novel method for clustering using the support vector ma chine approach. Data points are mapped to a high dimensional feature space, where support vectors are used to define a sphere enclosing them. The boundary of the sphere forms in data space a set of closed contours containing the data. Data points enclosed by each contour are defined as a cluster. As the width parameter of the Gaussian kernel is decreased, these contours fit the data more tightly and splitting of contours occurs. The algorithm works by separating clusters according to valleys in the un derlying probability distribution, and thus clusters can take on arbitrary geometrical shapes. As in other SV algorithms, outliers can be dealt with by introducing a soft margin constant leading to smoother cluster bound aries. The structure of the data is explored by varying the two parame ters. We investigate the dependence of our method on these parameters and apply it to several data sets. 1 Introduction Clustering i",Optimization & Theoretical ML,14cfdb59b5bda1fc245aadae15b1984a-Paper.pdf,2000
Incremental and Decremental Support Vector,"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is re versible, and decremental ""unlearning"" offers an efficient method to ex actly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data. 1 Introduction Training a support vector machine (SVM) requires solving a quadratic programming (QP) problem in a number of coefficients equal to the number of training examples. For very large datasets, standard numeric techniques for QP become infeasible. Practical techniques decompose the problem into manageable subproblems over part of the data [7, 5] or, in the limit, perform iterative pairwise [8] or component-wise [3] optimization. A disadvan",Optimization & Theoretical ML,155fa09596c7e18e50b58eb7e0c6ccb4-Paper.pdf,2000
Active inference in concept learning,"People are active experimenters, not just passive observers, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that maximize the expected information gain, given current beliefs (Fedorov 1972, MacKay 1992, Oaksford & Chater 1994). In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept learning task (Tenenbaum 2000). The results of the experiment are analyzed in terms of the expected information gain of the questions asked by subjects. In scientific inquiry and in everyday life, people seek out information relevant to perceptual and cognitive tasks. Scientists perform experiments to uncover causal relationships; people saccade to informative areas of visual scenes, turn their head towards surprising sounds, and ask questions to understand the meaning of concepts. Consider a person learning a forei",NLP,15d185eaa7c954e77f5343d941e25fbd-Paper.pdf,2000
The Interplay of Symbolic and Subsymbolic,"Although connectionist models have provided insights into the nature of perception and motor control, connectionist accounts of higher cognition seldom go beyond an implementation of traditional symbol-processing theories. We describe a connectionist constraint satisfaction model of how people solve anagram problems. The model exploits statistics of English orthography, but also addresses the interplay of sub symbolic and symbolic computation by a mechanism that extracts approximate symbolic representations (partial orderings of letters) from sub symbolic structures and injects the extracted representation back into the model to assist in the solution of the anagram. We show the computational benefit of this extraction-injection process and discuss its relationship to conscious mental processes and working memory. We also account for experimental data concerning the difficulty of anagram solution based on the orthographic structure of the anagram string and the target word. Historicall",NLP,17c3433fecc21b57000debdf7ad5c930-Paper.pdf,2000
Sex with Support Vector Machines,"Nonlinear Support Vector Machines (SVMs) are investigated for visual sex classification with low resolution ""thumbnail"" faces (21- by-12 pixels) processed from 1,755 images from the FER ET face database. The performance of SVMs is shown to be superior to traditional pattern classifiers (Linear, Quadratic, Fisher Linear Dis criminant, Nearest-Neighbor) as well as more modern techniques such as Radial Basis Function (RBF) classifiers and large ensemble RBF networks. Furthermore, the SVM performance (3.4% error) is currently the best result reported in the open literature. 1 Introduction In recent years, SVMs have been successfully applied to various tasks in com putational face-processing. These include face detection [14], face pose discrim ination [12] and face recognition [16]. Although facial sex classification has attracted much attention in the psychological literature [1, 4, 8, 15], relatively few computatinal learning methods have been proposed. We will briefly review and summari",Computer Vision,1e913e1b06ead0b66e30b6867bf63549-Paper.pdf,2000
% & ’ ( ) *,No abstract found,"Based on the title of the research paper, the",19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf,2000
Recognizing Hand-written Digits Using,"The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - l)th level model. After train ing, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logis tic classification network that is trained on sep",Computer Vision,1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,2000
APRICODD: Approximate Policy Construction,"We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions. 1 Introduction The last decade has seen much interest in structured approaches to solving planning prob lems under uncertainty formulated as Markov decision processes (MDPs). Structured algo rithms allow problems to be solved without explicit state-space enumeration by aggregating states of identical value. Structured approaches using decision trees have been applied to classical dynamic programming (DP) algorithms ",Optimization & Theoretical ML,201d7288b4c18a679e48b31c72c30ded-Paper.pdf,2000
Four-Iegged Walking Gait Control Using a,"To control the walking gaits of a four-legged robot we present a novel neuromorphic VLSI chip that coordinates the relative phasing of the robot's legs similar to how spinal Central Pattern Generators are believed to control vertebrate locomotion [3]. The chip controls the leg move ments by driving motors with time varying voltages which are the out puts of a small network of coupled oscillators. The characteristics of the chip's output voltages depend on a set of input parameters. The rela tionship between input parameters and output voltages can be computed analytically for an idealized system. In practice, however, this ideal re lationship is only approximately true due to transistor mismatch and off sets. Fine tuning of the chip's input parameters is done automatically by the robotic system, using an unsupervised Support Vector (SV) learning algorithm introduced recently [7]. The learning requires only that the description of the desired output is given. The machine learns from (un",Computer Vision,2647c1dba23bc0e0f9cdf75339e120d2-Paper.pdf,2000
A Mathematical Programming Approach to the,"We investigate a new kernel-based classifier: the Kernel Fisher Discrim inant (KFD). A mathematical programming formulation based on the ob servation that KFD maximizes the average margin permits an interesting modification of the original KFD algorithm yielding the sparse KFD. We find that both, KFD and the proposed sparse KFD, can be understood in an unifying probabilistic context. Furthermore, we show connections to Support Vector Machines and Relevance Vector Machines. From this understanding, we are able to outline an interesting kernel-regression technique based upon the KFD algorithm. Simulations support the use fulness of our approach. 1 Introduction Recent years have shown an enormous interest in kernel-based classification algorithms, primarily in Support Vector Machines (SVM) [2]. The success of SVMs seems to be trig gered by (i) their good generalization performance, (ii) the existence of a unique solution, and (iii) the strong theoretical background: structural risk minimi",Optimization & Theoretical ML,29530de21430b7540ec3f65135f7323c-Paper.pdf,2000
Using Free Energies to Represent Q-values in a,"The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Ac tions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation. 1 Introduction Online Reinforcement Learning (RL) algorithms try to find a policy which maximizes the expected time-discounted reward provided by the environment. They do this by performing sample backups to learn a value ",Reinforcement Learning,2d1b2a5ff364606ff041650887723470-Paper.pdf,2000
Dopamine Bonuses,"Substantial data support a temporal difference (TO) model of dopamine (OA) neuron activity in which the cells provide a global error signal for reinforcement learning. However, in certain cir cumstances, OA activity seems anomalous under the TO model, responding to non-rewarding stimuli. We address these anoma lies by suggesting that OA cells multiplex information about re ward bonuses, including Sutton's exploration bonuses and Ng et al's non-distorting shaping bonuses. We interpret this additional role for OA in terms of the unconditional attentional and psy chomotor effects of dopamine, having the computational role of guiding exploration. 1 Introduction Much evidence suggests that dopamine cells in the primate midbrain play an im portant role in reward and action learning. Electrophysiological studies support a theory that OA cells signal a global prediction error for summed future reward in appetitive conditioning tasks (Montague et al, 1996; Schultz et al, 1997), in the form of a",Reinforcement Learning,2f25f6e326adb93c5787175dda209ab6-Paper.pdf,2000
Sparse Greedy,"We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n2m), storage is O(nm), the cost for prediction is 0 (n ) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems. 1 Introduction Gaussian processes have become popular because they allow exact Bayesian analysis with simple matrix manipulations, yet provide good performance. They share with Support Vector machines and Regularization Networks the concept of regularization via Reproducing Kernel Hilbert spaces [3], that is, they allow the direct specification of the smoothness properties of the class of functions under consideration. However, Gaussian processes are not always the method of choice for large datasets, since they invo",Optimization & Theoretical ML,3214a6d842cc69597f9edf26df552e43-Paper.pdf,2000
Large Scale Bayes Point Machines,"The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has re cently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excel lent generalisation abilities. However, the billiard algorithm as pre sented in [4] is restricted to small sample size because it requires o (m2) of memory and 0 (N . m2) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is al gorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of hand written digits which show that Bayes point machines (BPMs) are competitive with the",Optimization & Theoretical ML,333222170ab9edca4785c39f55221fe7-Paper.pdf,2000
Efficient Learning of Linear Perceptrons,"We consider the existence of efficient algorithms for learning the class of half-spaces in ~n in the agnostic learning model (Le., mak ing no prior assumptions on the example-generating distribution). The resulting combinatorial problem - finding the best agreement half-space over an input sample - is NP hard to approximate to within some constant factor. We suggest a way to circumvent this theoretical bound by introducing a new measure of success for such algorithms. An algorithm is IL-margin successful if the agreement ratio of the half-space it outputs is as good as that of any half-space once training points that are inside the IL-margins of its separating hyper-plane are disregarded. We prove crisp computational com plexity results with respect to this success measure: On one hand, for every positive IL, there exist efficient (poly-time) IL-margin suc cessful learning algorithms. On the other hand, we prove that unless P=NP, there is no algorithm that runs in time polynomial in th",Optimization & Theoretical ML,39027dfad5138c9ca0c474d71db915c3-Paper.pdf,2000
Gaussianization,"High dimensional data modeling is difficult mainly because the so-called ""curse of dimensionality"". We propose a technique called ""Gaussianiza tion"" for high dimensional density estimation, which alleviates the curse of dimensionality by exploiting the independence structures in the data. Gaussianization is motivated from recent developments in the statistics literature: projection pursuit, independent component analysis and Gaus sian mixture models with semi-tied covariances. We propose an iter ative Gaussianization procedure which converges weakly: at each it eration, the data is first transformed to the least dependent coordinates and then each coordinate is marginally Gaussianized by univariate tech niques. Gaussianization offers density estimation sharper than traditional kernel methods and radial basis function methods. Gaussianization can be viewed as efficient solution of nonlinear independent component anal ysis and high dimensional projection pursuit. 1 Introduction Density E",Optimization & Theoretical ML,3c947bc2f7ff007b86a9428b74654de5-Paper.pdf,2000
Error-correcting Codes on a Bethe-like Lattice,"We analyze Gallager codes by employing a simple mean-field approxi mation that distorts the model geometry and preserves important interac tions between sites. The method naturally recovers the probability prop agation decoding algorithm as an extremization of a proper free-energy. We find a thermodynamic phase transition that coincides with informa tion theoretical upper-bounds and explain the practical code performance in terms of the free-energy landscape. 1 Introduction In the last years increasing interest has been devoted to the application of mean-field tech niques to inference problems. There are many different ways of building mean-field theo ries. One can make a perturbative expansion around a tractable model [1,2], or assume a tractable structure and variationally determine the model parameters [3]. Error-correcting codes (ECC) are particularly interesting examples of inference problems in loopy intractable graphs [4]. Recently the focus has been directed to the state-of-the",Optimization & Theoretical ML,3fab5890d8113d0b5a4178201dc842ad-Paper.pdf,2000
Homeostasis in a Silicon Integrate and Fire,"In this work, we explore homeostasis in a silicon integrate-and-fire neu ron. The neuron adapts its firing rate over long time periods on the order of seconds or minutes so that it returns to its spontaneous firing rate after a lasting perturbation. Homeostasis is implemented via two schemes. One scheme looks at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate. The second scheme adapts the synaptic ""threshold"" depending on the neuron's activity. The threshold is lowered if the neuron's activity decreases over a long time and is increased for prolonged increase in postsynaptic activity. Both these mechanisms for adaptation use floating-gate technology. The re sults shown here are measured from a chip fabricated in a 2-J.lm CMOS process. 1 Introduction We explored long-time constant adaptation mechanisms in a simple integrate-and-fire sili con neuron. Many researchers have postulated constant adaptation mechanisms which, for example, prese",Computer Vision,414e773d5b7e5c06d564f594bf6384d0-Paper.pdf,2000
Incorporating Second-Order Functional,"Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori con straints. 1 Introduction Incorporating a priori knowledge of a particular task into a learning algorithm helps re ducing the necessary complexity of the learner and generally improves performance, if the incorporated knowledge is relevant to the task and really corresponds t",Optimization & Theoretical ML,44968aece94f667e4095002d140b5896-Paper.pdf,2000
Spike-Timing-Dependent Learning for,"We apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to pre- and post-synaptic ac tivity, with a kernel A(r) measuring the effect for a postsynaptic spike a time r after the presynaptic one. The resulting synaptic ma trices have an outer-product form in which the oscillating patterns are represented as complex vectors. In a simple model, the even part of A(r) enhances the resonant response to learned stimulus by reducing the effective damping, while the odd part determines the frequency of oscillation. We relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations. 1 Introduction Recent studies of synapses between pyramidal neocortical and hippocampal neu rons [1, 2, 3, 4] have revealed that changes in synaptic efficacy can depend on the relative timing of pre- and postsynaptic spikes. Typically, a presynaptic spike fol lowed by a postsynaptic one leads to",Computer Vision,4496bf24afe7fab6f046bf4923da8de6-Paper.pdf,2000
On Reversing Jensen's Inequality,"Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen com putes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then per mits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statis tical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown. 1 1 Introduction Statistical model estimation a",Optimization & Theoretical ML,44a2e0804995faf8d2e3b084a1e2db1d-Paper.pdf,2000
From Mixtures of Mixtures to,"We establish a principled framework for adaptive transform cod ing. Transform coders are often constructed by concatenating an ad hoc choice of transform with suboptimal bit allocation and quan tizer design. Instead, we start from a probabilistic latent variable model in the form of a mixture of constrained Gaussian mixtures. From this model we derive a transform coding algorithm, which is a constrained version of the generalized Lloyd algorithm for vector quantizer design. A byproduct of our derivation is the introduc tion of a new transform basis, which unlike other transforms (PCA, DCT, etc.) is explicitly optimized for coding. Image compression experiments show adaptive transform coders designed with our al gorithm improve compressed image signal-to-noise ratio up to 3 dB compared to global transform coding and 0.5 to 2 dB compared to other adaptive transform coders. 1 Introduction Compression algorithms for image and video signals often use transform coding as a low-complexity alt",Computer Vision,4888241374e8c62ddd9b4c3cfd091f96-Paper.pdf,2000
Algorithmic Stability and Generalization,"We present a novel way of obtaining PAC-style bounds on the gen eralization error of learning algorithms, explicitly using their stabil ity properties. A stable learner is one for which the learned solution does not change much with small changes in the training set. The bounds we obtain do not depend on any measure of the complexity of the hypothesis space (e.g. VC dimension) but rather depend on how the learning algorithm searches this space, and can thus be applied even when the VC dimension is infinite. We demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance. 1 Introduction A key issue in computational learning theory is to bound the generalization error of learning algorithms. Until recently, most of the research in that area has focused on uniform a-priori bounds giving a guarantee that the difference between the training error and the test error is uniformly small for any hy",Optimization & Theoretical ML,49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf,2000
The Kernel Trick for Distances,"A method is described which, like the kernel trick in support vector ma chines (SVMs), lets us generalize distance-based algorithms to operate in feature spaces, usually nonlinearly related to the input space. This is done by identifying a class of kernels which can be represented as norm-based distances in Hilbert spaces. It turns out that common kernel algorithms, such as SVMs and kernel PCA, are actually really distance based algorithms and can be run with that class of kernels, too. As well as providing a useful new insight into how these algorithms work, the present work can form the basis for conceiving new algorithms. 1 Introduction One of the crucial ingredients of SVMs is the so-called kernel trick for the computation of dot products in high-dimensional feature spaces using simple functions defined on pairs of input patterns. This trick allows the formulation of nonlinear variants of any algorithm that can be cast in terms of dot products, SVMs being but the most prominent exa",Optimization & Theoretical ML,4e87337f366f72daa424dae11df0538c-Paper.pdf,2000
Higher-order Statistical Properties,"We present evidence that several higher-order statistical proper ties of natural images and signals can be explained by a stochastic model which simply varies scale of an otherwise stationary Gaus sian process. We discuss two interesting consequences. The first is that a variety of natural signals can be related through a com mon model of spherically invariant random processes, which have the attractive property that the joint densities can be constructed from the one dimensional marginal. The second is that in some cas es the non-stationarity assumption and only second order methods can be explicitly exploited to find a linear basis that is equivalent to independent components obtained with higher-order methods. This is demonstrated on spectro-temporal components of speech. 1 Introduction Recently, considerable attention has been paid to understanding and modeling the non-Gaussian or ""higher-order"" properties of natural signals, particularly images. Several non-Gaussian properties hav",Computer Vision,5103c3584b063c431bd1268e9b5e76fb-Paper.pdf,2000
Probabilistic Semantic Video Indexing,"We propose a novel probabilistic framework for semantic video in dexing. We define probabilistic multimedia objects (multijects) to map low-level media features to high-level semantic labels. A graphical network of such multijects (multinet) captures scene con text by discovering intra-frame as well as inter-frame dependency relations between the concepts. The main contribution is a novel application of a factor graph framework to model this network. We model relations between semantic concepts in terms of their co-occurrence as well as the temporal dependencies between these concepts within video shots. Using the sum-product algorithm [1] for approximate or exact inference in these factor graph multinets, we attempt to correct errors made during isolated concept detec tion by forcing high-level constraints. This results in a significant improvement in the overall detection performance. 1 Introduction Research in video retrieval has traditionally focussed on the paradigm of query-by ex",Computer Vision,52d2752b150f9c35ccb6869cbf074e48-Paper.pdf,2000
Accumulator networks: Suitors of local,"One way to approximate inference in richly-connected graphical models is to apply the sum-product algorithm (a.k.a. probabil ity propagation algorithm), while ignoring the fact that the graph has cycles. The sum-product algorithm can be directly applied in Gaussian networks and in graphs for coding, but for many condi tional probability functions - including the sigmoid function - di rect application of the sum-product algorithm is not possible. We introduce ""accumulator networks"" that have low local complexity (but exponential global complexity) so the sum-product algorithm can be directly applied. In an accumulator network, the probability of a child given its parents is computed by accumulating the inputs from the parents in a Markov chain or more generally a tree. After giving expressions for inference and learning in accumulator net works, we give results on the ""bars problem"" and on the problem of extracting translated, overlapping faces from an image. 1 Introduction Graphical pr",Computer Vision,537de305e941fccdbba5627e3eefbb24-Paper.pdf,2000
Sparse Representation for Gaussian Process,"We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online al gorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experi mental results on toy examples and large real-world data sets indicate the efficiency of the approach. 1 Introduction Gaussian processes (GP) [1; 15] provide promising non-parametric tools for modelling real-world statistical problems. Like other kernel based methods, e.g. Support Vector Ma chines (SVMs) [13], they combine a high flexibility ofthe model by working in high (often 00) dimensional feature spaces with the simplicity that all operations are ""kernelized"" i.e. they are performed in the (lower dimensional) input space using positive definite kernels. An important advantage of GPs over other non-Bayesian models is the explicit",Optimization & Theoretical ML,56f9f88906aebf4ad985aaec7fa01313-Paper.pdf,2000
Hippocampally-Dependent Consolidation in a,"In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it. Consolidation has been the focus of numerous experimental and qualita tive modeling studies, but only little quantitative exploration. We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic infor mation as well. The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also train ing the cortex to perform appropriate completion by itself. 1 Introduction The hippocampal formation and adjacent cortical areas have long been believed to be in volved in the acquisition and retrieval of long-term memory for events and other declarative information. Clinical studies in humans and animal experiments indicate that damage to these regions ",NLP,57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf,2000
Automated State Abstraction for Options using,"Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accom plish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hier archical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be au tomated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illus trate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective. 1 Introduction Researchers in the field of reinforcement learning have recently focused considerable at tention on temporally abstract act",Reinforcement Learning,596dedf4498e258e4bdc9fd70df9a859-Paper.pdf,2000
Structure learning in human causal induction,"We use graphical models to explore the question of how people learn sim ple causal relationships from data. The two leading psychological theo ries can both be seen as estimating the parameters of a fixed graph. We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference. Our argument is supported through the discussion of three data sets. 1 Introduction Causality plays a central role in human mental life. Our behavior depends upon our under standing of the causal structure of our environment, and we are remarkably good at infer ring causation from mere observation. Constructing formal models of causal induction is currently a major focus of attention in computer science [7], psychology [3,6], and philos ophy [5]. This paper attempts to connect these literatures, by framing the debate between two major psychological theories in the computational l",Optimization & Theoretical ML,59bcda7c438bad7d2afffe9e2fed00be-Paper.pdf,2000
Bayesian video shot segmentation,"Prior knowledge about video structure can be used both as a means to improve the peiformance of content analysis and to extract features that allow semantic classification. We introduce statistical models for two important components of this structure, shot duration and activity, and demonstrate the usefulness of these models by introducing a Bayesian formulation for the shot segmentation problem. The new formulations is shown to extend standard thresholding methods in an adaptive and intuitive way, leading to improved segmentation accuracy. 1 Introduction Given the recent advances on video coding and streaming technology and the pervasiveness of video as a form of communication, there is currently a strong interest in the development of techniques for browsing, categorizing, retrieving and automatically summarizing video. In this context, two tasks are of particular relevance: the decomposition of a video stream into its component units, and the extraction of features for the automati",Computer Vision,5b6ba13f79129a74a3e819b78e36b922-Paper.pdf,2000
Minimum Bayes Error Feature Selection for,"We consider the problem of designing a linear transformation () E lRPx n, of rank p ~ n, which projects the features of a classifier x E lRn onto y = ()x E lRP such as to achieve minimum Bayes error (or probabil ity of misclassification). Two avenues will be explored: the first is to maximize the ()-average divergence between the class densities and the second is to minimize the union Bhattacharyya bound in the range of (). While both approaches yield similar performance in practice, they out perform standard LDA features and show a 10% relative improvement in the word error rate over state-of-the-art cepstral features on a large vocabulary telephony speech recognition task. 1 Introduction Modern speech recognition systems use cepstral features characterizing the short-term spectrum of the speech signal for classifying frames into phonetic classes. These features are augmented with dynamic information from the adjacent frames to capture transient spectral events in the signal. What is ",Computer Vision,59e0b2658e9f2e77f8d4d83f8d07ca84-Paper.pdf,2000
Kernel expansions with unlabeled examples,"Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classi fication performance. We present a new tractable algorithm for exploit ing unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is read ily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely dis criminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed ap proach requires very few labeled examples for high classification accu racy. 1 Introduction In many modern classification problems such as text categorization, very few labeled ex amples are available but a large",Optimization & Theoretical ML,5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf,2000
Universality and individuality in a neural,"The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor out puts, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from informa tion theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that tempo ral patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality. 1 Introduction When two people look at the same",Computer Vision,5fa9e41bfec0725742cc9d15ef594120-Paper.pdf,2000
Generalized Belief Propagation,"Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. We show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statis tical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. More importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more ac curate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we de rive generalized belief propagation (GBP) versions ofthese Kikuchi approximations. These new message passing algo",Optimization & Theoretical ML,61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf,2000
Multiple timesc ales of adaptation in a neural,"Many neural systems extend their dynamic range by adaptation. We ex amine the timescales of adaptation in the context of dynamically mod ulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation. 1 Introduction Adaptation was one of the first phenomena discovered when Adrian recorded the responses of single sensory neurons [1, 2]. Since that time, many different forms of adaptation have been found in almost all sensory systems. The simplest forms of adaptation, such as light and dark adaptation in the visual system, seem to involve just discarding a large constant background signal so that the system can maintain sensitivity to small changes. The idea of Attneave [3] and Barlow [4] that the nerv",NLP,645098b086d2f9e1e0e939c27f9f2d6f-Paper.pdf,2000
Speech Denoising and Dereverberation Using,"This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denois ing and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple micro phones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods. 1 Introduction This paper presents a statistical-model-based algorithm for reconstructing a speech source from microphone signals recorded in a stationary noisy reverberant environment. Speech enhancement in a realistic environment is a challenging problem, which remains largely unsolved in spite of more than three decades of research. Speech enhancement has many applications",Computer Vision,65699726a3c601b9f31bf04019c8593c-Paper.pdf,2000
"Modelling spatial recall, mental imagery and","We present a computational model of the neural mechanisms in the pari etal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and im agery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients. 1 Introduction We perform spatial computations everday. Tasks such as reaching and ",Computer Vision,65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf,2000
Adaptive Object Representation with,"Theories of object recognition often assume that only one representa tion scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriat",Computer Vision,68148596109e38cf9367d27875e185be-Paper.pdf,2000
"Model Complexity, Goodness of Fit and","We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifi cally we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within first order as a function of model complexity. This general property of ""diminishing returns"" is illustrated on a number of real data sets and learning problems, including finite mixture modeling and multivariate linear regression. 1 Introduction, Motivation, and Related Work = Assume we have a data set D {Xl, X2, ... , xn}, where the Xi could be vectors, sequences, etc. We consider modeling the data set D using models indexed by a complexity index k, 1 :::; k :::; k max• For example, the models could be finite mixture probability density functions (PDFs) for vector Xi'S where model complexity is indexed by the number of components k in the mixture. Alternatively, the modeling task could be to fit a conditional regression model y =",Optimization & Theoretical ML,69d1fc78dbda242c43ad6590368912d4-Paper.pdf,2000
Text Classification using String Kernels,"We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered se quence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct compu tation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a stan dard word feature space kernel [6] is made showing encouraging results. 1 Introduction Standard learning systems (like neural networks or decision tree",NLP,68c694de94e6c110f42e587e8e48d852-Paper.pdf,2000
Fast Training of Support Vector Classifiers,"In this communication we present a new algorithm for solving Support Vector Classifiers (SVC) with large training data sets. The new algorithm is based on an Iterative Re-Weighted Least Squares procedure which is used to optimize the SVc. Moreover, a novel sample selection strategy for the working set is presented, which randomly chooses the working set among the training samples that do not fulfill the stopping criteria. The validity of both proposals, the optimization procedure and sample selection strategy, is shown by means of computer experiments using well-known data sets. 1 INTRODUCTION The Support Vector Classifier (SVC) is a powerful tool to solve pattern recognition prob lems [13, 14] in such a way that the solution is completely described as a linear combination of several training samples, named the Support Vectors. The training procedure for solving the SVC is usually based on Quadratic Programming (QP) which presents some inherent limitations, mainly the computational com",Optimization & Theoretical ML,6ae07dcb33ec3b7c814df797cbda0f87-Paper.pdf,2000
Some new bounds on the generalization error of,"In this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The theory of Gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes, the complexity of the class being measured via the so called Gaussian complexity func tions. As a simple application of our results, we obtain the bounds of Schapire, Freund, Bartlett and Lee for the generalization error of boost ing. We also substantially improve the results of Bartlett on bounding the generalization error of neural networks in terms of h -norms of the weights of neurons. Furthermore, under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee. 1 Introduction and margin type inequalities f",Optimization & Theoretical ML,6be5336db2c119736cf48f475e051bfe-Paper.pdf,2000
A Neural Probabilistic Language Model,"A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed rep resentation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these repre sentations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly im proves on a state-of-the-art trigram model. 1 Introduction A fundamental problem that makes language modeling and other learning problems diffi cult is the curse of dimensionality. It is particularly",NLP,728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf,2000
Beyond maximum likelihood and density,"The goal of many unsupervised learning procedures is to bring two probability distributions into alignment. Generative models such as Gaussian mixtures and Boltzmann machines can be cast in this light, as can recoding models such as ICA and projection pursuit. We propose a novel sample-based error measure for these classes of models, which applies even in situations where maximum likelihood (ML) and probability density estimation-based formulations can not be applied, e.g., models that are nonlinear or have intractable posteriors. Furthermore, our sample-based error measure avoids the difficulties of approximating a density function. We prove that with an unconstrained model, (1) our approach converges on the correct solution as the number of samples goes to infinity, and (2) the expected solution of our approach in the generative framework is the ML solution. Finally, we evaluate our approach via simula tions of linear and nonlinear models on mixture of Gaussians and ICA problems. The",Optimization & Theoretical ML,6e79ed05baec2754e25b4eac73a332d2-Paper.pdf,2000
.N-Body. Problems in Statistical Learning,"We present efficient algorithms for all-point-pairs problems, or 'N body'-like problems, which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection, and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric techniques which are applicable in principle to any 'N-body' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more effi cient either empirically or theoretically. In additio",Optimization & Theoretical ML,7385db9a3f11415bc0e9e2625fae3734-Paper.pdf,2000
A silicon primitive for competitive learning,"Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11- transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit per forms a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task. 1 Introduction Competitive learning is a family of neural learning algorithms that has proved use ful for training many classification and clustering networks [1]. In these networks, a neuron's synaptic weight vector typically represents a tight cluster of data points. Upon presentation of a new input to the network, the neuron representing the closest cluster adapts its weight vector, decreasing the difference between the weight vector and present input. Details ",Optimization & Theoretical ML,73e0f7487b8e5297182c5a711d20bf26-Paper.pdf,2000
Support Vector Novelty Detection,"A system has been developed to extract diagnostic information from jet engine carcass vibration data. Support Vector Machines applied to nov elty detection provide a measure of how unusual the shape of a vibra tion signature is, by learning a representation of normality. We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the appli cation to Jet Engine vibration analysis. 1 Introduction Jet engines have a number of rigorous pass-off tests before they can be delivered to the customer. The main test is a vibration test over the full range of operating speeds. Vibration gauges are attached to the casing of the engine and the speed of each shaft is measured using a tachometer. The engine on the test bed is slowly accelerated from idle to full speed and then gradually decelerated back to idle. As the engine accelerates, the rotation frequency of the two (or three) shafts increases and so does the fre",Optimization & Theoretical ML,7302e3f5e7c072aea8801faf8a492be0-Paper.pdf,2000
Automatic choice of dimensionality for peA,"A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to es timate the true dimensionality of the data. The resulting estimate is sim ple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accu rate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster. 1 Introduction Recovering the intrinsic dimensionality of a data set is a classic and fundamental problem in data analysis. A popular method for doing this is PCA or localized PCA. Modeling the data manifold with localized PCA dates back to [4]. Since then, the prob",Optimization & Theoretical ML,7503cfacd12053d309b6bed5c89de212-Paper.pdf,2000
Propagation Algorithms for Variational,"Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoret ical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these re sults to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smooth ing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimen sionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set. 1 Introduction Bayesian approaches to machine learning have several desirable properties. Bayesian integration does not suffer overfitting (since nothing is fit to the data). Prior knowl edge can be incorporated naturally and all uncertaint",Optimization & Theoretical ML,77369e37b2aa1404f416275183ab055f-Paper.pdf,2000
An Adaptive Metric Machine for Pattern,"Nearest neighbor classification assumes locally constant class con ditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for pro ducing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the mod ified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data. 1 Introduction In classification, a feature vector x = (Xl,···, Xqy E lRq, representing an object, is assumed to be in one of J classes {i}{=l'",Optimization & Theoretical ML,7e9e346dc5fd268b49bf418523af8679-Paper.pdf,2000
On iterative Krylov-dogleg trust-region,"This paper describes a method of dogleg trust-region steps, or re stricted Levenberg-Marquardt steps, based on a projection pro cess onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the lin earized Gauss-Newton normal equation, whereas the outer nonlin ear algorithm repeatedly takes so-called ""Krylov-dogleg"" steps, re lying only on matrix-vector multiplication without explicitly form ing the Jacobian matrix or the Gauss-Newton model Hessian. That is, our iterative dogleg algorithm can reduce both operational counts and memory space by a factor of O(n) (the number of pa rameters) in comparison with a direct linear-equation solver. This memory-less property is useful for large-scale problems. 1 Introduction We consider the so-called neural networks nonlinear least squares prob lem 1 wherein the objective is to optimize the n weight parameters of neura",Optimization & Theoretical ML,7ffd85d93a3e4de5c490d304ccd9f864-Paper.pdf,2000
Stability and noise in biochemical switches,"Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed. 1 Introduction The problem of building a reliable switch arises in several different biological con texts. The classical example is the switching on and off of gene expression during development [1], or in simpler systems such as phage .A [2]. It is likely that the cell cycle should also be viewed as a sequence of switching events among discrete states, rather than as a continuously running clock [3]. The stable switching of a specific class of kinase molecules between active",Optimization & Theoretical ML,82cadb0649a3af4968404c9f6031b233-Paper.pdf,2000
Computing with Finite and Infinite Networks,"Using statistical mechanics results, I calculate learning curves (average generalization error) for Gaussian processes (GPs) and Bayesian neural networks (NNs) used for regression. Applying the results to learning a teacher defined by a two-layer network, I can directly compare GP and Bayesian NN learning. I find that a GP in general requires CJ (d S )-training examples to learn input features of order s (d is the input dimension), whereas a NN can learn the task with order the number of adjustable weights training examples. Since a GP can be considered as an infinite NN, the results show that even in the Bayesian approach, it is important to limit the complexity of the learning machine. The theoretical findings are confirmed in simulations with analytical GP learning and a NN mean field algorithm. 1 Introduction Non-parametric kernel methods such as Gaussian Processes (GPs) and Support Vector Ma chines (SVMs) are closely related to neural networks (NNs). These may be considered as sin",Optimization & Theoretical ML,8562ae5e286544710b2e7ebe9858833b-Paper.pdf,2000
Hierarchical Memory-Based,"A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hier archy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher lev els in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task. 1 Introduction Reinforcement learning encompasses a class of machine learning problems in which an agent learns from experience as it interacts with its environment. One funda mental challenge faced by reinforcement learning agents in real-world problems is that the sta",Reinforcement Learning,85f007f8c50dd25f5a45fca73cad64bd-Paper.pdf,2000
Second order approximations for probability,"In this paper, we derive a second order mean field theory for directed graphical probability models. By using an information theoretic argu ment it is shown how this can be done in the absense of a partition function. This method is a direct generalisation of the well-known TAP approximation for Boltzmann Machines. In a numerical example, it is shown that the method greatly improves the first order mean field ap proximation. For a restricted class of graphical models, so-called single overlap graphs, the second order method has comparable complexity to the first order method. For sigmoid belief networks, the method is shown to be particularly fast and effective. 1 Introduction Recently, a number of authors have proposed deterministic methods for approximate infer ence in large graphical models. The simplest approach gives a lower bound on the prob ability of a subset of variables using Jenssen's inequality (Saul et aI., 1996). The method involves the minimization of the KL divergence b",Optimization & Theoretical ML,865dfbde8a344b44095495f3591f7407-Paper.pdf,2000
Feature Selection for SVMs,"We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA micro array data. 1 Introduction In many supervised learning problems feature selection is important for a variety of rea sons: generalization performance, running time requirements, and constraints and interpre tational issues imposed by the problem itself. In classification problems we are given f data points Xi E ~n labeled Y E ±1 drawn i.i.d from a probability distribution P(x, y). We would like to select a subset of features while preserving or improving the discriminative ability of a classifier. As a brute force search of all possible features is a combinato",Optimization & Theoretical ML,8c3039bd5842dca3d944faab91447818-Paper.pdf,2000
Direct Classification with Indirect Data,"We classify an input space according to the outputs of a real-valued function. The function is not given, but rather examples of the function. We contribute a consistent classifier that avoids the un necessary complexity of estimating the function. 1 Introduction In this paper, we consider a learning problem that combines elements of regression and classification. Suppose there exists an unknown real-valued property of the feature space, p(¢), that maps from the feature space, ¢ ERn, to R. The property function and a positive set A c R, define the desired classifier as follows: C*(¢) = { ~~ if p(¢) E A (1) otherwise Though p(¢) is unknown, measurements, p"" associated with p(¢) at different fea tures, ¢, are available in a data set X = {(¢i,P,i)} of size IXI = N. Each sample is i.i.d. with unknown distribution f(¢,p,). This data is indirect in that p, may be an input to a sufficient statistic for estimating p( ¢) but in itself does not directly indicate C*(¢) in (1). Figure 1 gives a sc",Optimization & Theoretical ML,8c8a58fa97c205ff222de3685497742c-Paper.pdf,2000
Constrained Independent Component,"The paper presents a novel technique of constrained independent component analysis (CICA) to introduce constraints into the clas sical ICA and solve the constrained optimization problem by using Lagrange multiplier methods. This paper shows that CICA can be used to order the resulted independent components in a specific manner and normalize the demixing matrix in the signal separation procedure. It can systematically eliminate the ICA's indeterminacy on permutation and dilation. The experiments demonstrate the use of CICA in ordering of independent components while providing normalized demixing processes. Keywords: Independent component analysis, constrained indepen dent component analysis, constrained optimization, Lagrange mul tiplier methods 1 Introduction Independent component analysis (ICA) is a technique to transform a multivari ate random signal into a signal with components that are mutually independent in complete statistical sense [1]. There has been a growing interest in res",Optimization & Theoretical ML,8d55a249e6baa5c06772297520da2051-Paper.pdf,2000
A Gradient-Based Boosting Algorithm for,"In adaptive boosting, several weak learners trained sequentially are combined to boost the overall algorithm performance. Re cently adaptive boosting methods for classification problems have been derived as gradient descent algorithms. This formulation jus tifies key elements and parameters in the methods, all chosen to optimize a single common objective function. We propose an anal ogous formulation for adaptive boosting of regression problems, utilizing a novel objective function that leads to a simple boosting algorithm. We prove that this method reduces training error, and compare its performance to other regression methods. The aim of boosting algorithms is to ""boost"" the small advantage that a hypothesis produced by a weak learner can achieve over random guessing, by using the weak learning procedure several times on a sequence of carefully constructed distribu tions. Boosting methods, notably AdaBoost (Freund & Schapire, 1997), are sim ple yet powerful algorithms that are easy t",Optimization & Theoretical ML,8d9fc2308c8f28d2a7d2f6f48801c705-Paper.pdf,2000
The Early Word Catches the Weights,"The strong correlation between the frequency of words and their naming latency has been well documented. However, as early as 1973, the Age of Acquisition (AoA) of a word was alleged to be the actual variable of interest, but these studies seem to have been ignored in most of the lit erature. Recently, there has been a resurgence of interest in AoA. While some studies have shown that frequency has no effect when AoA is con trolled for, more recent studies have found independent contributions of frequency and AoA. Connectionist models have repeatedly shown strong effects of frequency, but little attention has been paid to whether they can also show AoA effects. Indeed, several researchers have explicitly claimed that they cannot show AoA effects. In this work, we explore these claims using a simple feed forward neural network. We find a sig nificant contribution of AoA to naming latency, as well as conditions under which frequency provides an independent contribution. 1 Background Namin",NLP,90599c8fdd2f6e7a03ad173e2f535751-Paper.pdf,2000
The Manhattan World Assumption:,"Preliminary work by the authors made use of the so-called ""Man hattan world"" assumption about the scene statistics of city and indoor scenes. This assumption stated that such scenes were built on a cartesian grid which led to regularities in the image edge gra dient statistics. In this paper we explore the general applicability of this assumption and show that, surprisingly, it holds in a large variety of less structured environments including rural scenes. This enables us, from a single image, to determine the orientation of the viewer relative to the scene structure and also to detect target ob jects which are not aligned with the grid. These inferences are performed using a Bayesian model with probability distributions (e.g. on the image gradient statistics) learnt from real data. 1 Introduction In recent years, there has been growing interest in the statistics of natural images (see Huang and Mumford [4] for a recent review). Our focus, however, is on the discovery of scene statist",Computer Vision,90e1357833654983612fb05e3ec9148c-Paper.pdf,2000
Processing of Time Series by Neural Circuits,"Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their ""weight"" changes on a short time scale by several hundred percent in dependence of the past input to the synapse. In this article we explore the consequences that these synaptic dynamics entail for the computational power of feedforward neural networks. We show that gradient descent suffices to approximate a given (quadratic) filter by a rather small neural system with dynamic synapses. We also compare our network model to artificial neural net works designed for time series processing. Our numerical results are complemented by theoretical analysis which show that even with just a single hidden layer such networks can approximate a surprisingly large large class of nonlinear filters: all filters that can be characterized by Volterra series. This result is robust with regard to various changes in the ",Optimization & Theoretical ML,944626adf9e3b76a3919b50dc0b080a4-Paper.pdf,2000
Machine Learning for Video-Based,"We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video tex tures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of inter est is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated veloc ity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to cre ate such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search",Computer Vision,958adb57686c2fdec5796398de5f317a-Paper.pdf,2000
Discovering Hidden Variables:,"A serious problem in learning probabilistic models is the presence of hid den variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex de pendencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this pa per, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for ""structural signatures"" of hidden variables - substructures in the learned network that tend to suggest the presence of a hidden variable. We make this basic idea concrete, and show how to integrate it with structure-search algorithms. We evaluate this ",Computer Vision,95e6834d0a3d99e9ea8811855ae9229d-Paper.pdf,2000
Natural sound statistics and divisive,"We explore the statistical properties of natural sound stimuli pre processed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially re duced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the recti fied responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natu ral sounds. We demonstrate that the resulting model accounts for non linearities in the response characteristics of the auditory nerve, by com paring model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in p",NLP,96c5c28becf18e71190460a9955aa4d8-Paper.pdf,2000
Regularized Winnow Methods,"In theory, the Winnow multiplicative update has certain advantages over the Perceptron additive update when there are many irrelevant attributes. Recently, there has been much effort on enhancing the Perceptron algo rithm by using regularization, leading to a class of linear classification methods called support vector machines. Similarly, it is also possible to apply the regularization idea to the Winnow algorithm, which gives meth ods we call regularized Winnows. We show that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron. We investigate algorithmic is sues and learning properties of the derived methods. Some experimental results will also be provided to illustrate different methods. 1 Introduction In this paper, we consider the binary classification problem that is to determine a label y E {-1, 1} associated with an input vector x. A useful method for solving this problem is through linear discriminant",Optimization & Theoretical ML,9a1de01f893e0d2551ecbb7ce4dc963e-Paper.pdf,2000
FaceSync: A linear operator for measuring,"FaceSync is an optimal linear algorithm that finds the degree of syn chronization between the audio and image recordings of a human speaker. Using canonical correlation, it finds the best direction to com bine all the audio and image data, projecting them onto a single axis. FaceSync uses Pearson's correlation to measure the degree of synchro nization between the audio and image data. We derive the optimal linear transform to combine the audio and visual information and describe an implementation that avoids the numerical problems caused by comput ing the correlation matrices. 1 Motivation In many applications, we want to know about the synchronization between an audio signal and the corresponding image data. In a teleconferencing system, we might want to know which of the several people imaged by a camera is heard by the microphones; then, we can direct the camera to the speaker. In post-production for a film, clean audio dialog is often dubbed over the video; we want to adjust the au",Computer Vision,9f6992966d4c363ea0162a056cb45fe5-Paper.pdf,2000
What can a single neuron compute?,"In this paper we formulate a description of the computation per formed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the Hodgkin Huxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dy namic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrate and-fire or perceptron model. 1 Introduction Classical neural network models approximate neurons as devices that sum their inputs and generate a nonzero output if the sum exceeds a threshold. From our current state of knowledge in neurobiology it is easy to criticize these models as over simplified: where is the complex geometry of neurons, or the many different kinds of ion channel, each with its own intricate multistate kinetics? Indeed, progress at this more microscopic level of de",Optimization & Theoretical ML,a19acd7d2689207f9047f8cb01357370-Paper.pdf,2000
Mixtures of Gaussian Processes,"We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes -in particular in form of Gaussian process classification, the support vector machine and the MGP model can be used for quantifying the dependencies in graphical models. 1 Introduction Gaussian processes are typically used for regression where it is assumed that the underly ing function is generated by one infinite-dimensional Gaussian distribution (i.e. we assume a Gaussian prior distribution). In Gaussian process regression (GPR) we further assume that output data are generated by additive Gaussian noise, i.e. we assume a Gaussian like lihood model. GPR can be generalized by using likelihood models from the exponential family of distributions which is useful for classificat",Optimization & Theoretical ML,9fdb62f932adf55af2c0e09e55861964-Paper.pdf,2000
Tree-Based Modeling and Estimation of,"We present the embedded trees algorithm, an iterative technique for estimation of Gaussian processes defined on arbitrary graphs. By exactly solving a series of modified problems on embedded span ning trees, it computes the conditional means with an efficiency comparable to or better than other techniques. Unlike other meth ods, the embedded trees algorithm also computes exact error co variances. The error covariance computation is most efficient for graphs in which removing a small number of edges reveals an em bedded tree. In this context, we demonstrate that sparse loopy graphs can provide a significant increase in modeling power rela tive to trees, with only a minor increase in estimation complexity. 1 Introduction Graphical models are an invaluable tool for defining and manipulating probability distributions. In modeling stochastic processes with graphical models, two basic problems arise: (i) specifying a class of graphs with which to model or approximate the process; and (ii) de",Optimization & Theoretical ML,a3545bd79d31f9a72d3a78690adf73fc-Paper.pdf,2000
Learning winner-take-all competition between,"It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of poten tially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analy sis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some de generate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computa tional function of our network is to store and retrieve memories as per mitted sets of coactive neurons. In traditional winner-take-all networks, lateral inhibition is used to enforce a localized, or ""g",Computer Vision,aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf,2000
Bayes Networks on Ice:,"A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad. 1 Introduction Figure 1 : Human meteorite search with snowmobiles on the Antarctic ice sheets, and on foot in the moraines. Antarctica contains the most fertile meteorite hunting grounds on Earth. The pristine, dry and cold environment ensures that meteorites deposited there are preserved for long periods. Subsequent glacial flow of the ice sheets where they land concentrates them in particular areas. To date, most meteorites recovered throughout history have been done so in Antarctica in the last 20 years. Furthermore, they are less likel",Computer Vision,ac5dab2e99eee9cf9ec672e383691302-Paper.pdf,2000
Foundations for a Circuit Complexity Theory of,"We introduce total wire length as salient complexity measure for an anal ysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering. This new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation-and scale-invariant sensory process ing. We exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds, in particular with linear or almost linear total wire length. 1 Introduction Circuit complexity theory is a classical area of theoretical computer science, that provides estimates for the complexity of circuits for computing specific benchmark functions, such as binary addition, multiplication and sorting (see, e.g. (Savage, 1998». In recent years interest has grown in understanding the complexity of circuits for early sensory processing, both from the biological point of view and from the po",Computer Vision,ab7314887865c4265e896c6e209d1cd6-Paper.pdf,2000
The Use of MDL to Select among,"How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling. 1 Model Selection and Model Complexity The development and testing of computational models of cognitive processing are a central focus in cognitive science. A model embodies a solution to a problem whose adequacy is evaluated by its ability to mimic behavior by capturing the regularities underlying observed data. This enterprise of model selection is challenging because of the competing goals that must be satisfied. Traditionally, computational models of cognition have been compared using one",NLP,af3303f852abeccd793068486a391626-Paper.pdf,2000
Learning and Tracking Cyclic Human,"We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into ""cycles"". Then the mean and the princi pal components of the cycles are computed using a new algorithm that accounts for missing information and enforces smooth tran sitions between cycles. The learned temporal model provides a prior probability distribution over human motions that can be used in a Bayesian framework for tracking human subjects in complex monocular video sequences and recovering their 3D motion. 1 Introduction The modeling and tracking of human motion in video is important for problems as varied as animation, video database search, sports medicine, and human-computer interaction. Technically, the human body can be approximated by a collection of articulated limbs and its motion can be thought of as a collection of time-series describing the joint angles as ",Computer Vision,ad4cc1fb9b068faecfb70914acc63395-Paper.pdf,2000
Decomposition of Reinforcement Learning,"This paper presents predictive gain scheduling, a technique for simplify ing reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-fu ture call arrival rates, and precomputation of policies for Poisson call ar rival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in sig nificantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem. 1 Introduction In multi-service communications networks, such as Asynchronous Transfer Mode (ATM) networks, resource control is of crucial importance for the network operator as well as for the users. The objective is to maintain the service quality while maximizing the operator's revenue. At the call level, service quality (Grade of Service) is measur",Reinforcement Learning,b432f34c5a997c8e7c806a895ecc5e25-Paper.pdf,2000
Active Support Vector Machine,"An active set strategy is applied to the dual of a simple reformula tion of the standard quadratic program of a linear support vector machine. This application generates a fast new dual algorithm that consists of solving a finite number of linear equations, with a typically large dimensionality equal to the number of points to be classified. However, by making novel use of the Sherman-Morrison Woodbury formula, a much smaller matrix of the order of the orig inal input space is inverted at each step. Thus, a problem with a 32-dimensional input space and 7 million points required inverting positive definite symmetric matrices of size 33 x 33 with a total run ning time of 96 minutes on a 400 MHz Pentium II. The algorithm requires no specialized quadratic or linear programming code, but merely a linear equation solver which is publicly available. 1 Introduction Support vector machines (SVMs) [23, 5, 14, 12] are powerful tools for data classifi cation. Classification is achieved by a linear",Optimization & Theoretical ML,b139e104214a08ae3f2ebcce149cdf6e-Paper.pdf,2000
Vicinal Risk Minimization,"The Vicinal Risk Minimization principle establishes a bridge between generative models and methods derived from the Structural Risk Mini mization Principle such as Support Vector Machines or Statistical Reg ularization. We explain how VRM provides a framework which inte grates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop. We then show how the approach implies new algorithm s for solving problems usually associated with generative models. New algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data. Preliminary empirical results are presented. 1 Introduction Structural Risk Minimisation (SRM) in a learning system can be achieved using constraints on the parameter vectors, using regularization terms in the cost function, or using Support Vector Machines (SVM). All these principles have lead to well",Optimization & Theoretical ML,ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf,2000
Generalizable Singular Value,"We demonstrate that statistical analysis of ill-posed data sets is subject to a bias, which can be observed when projecting indepen dent test set examples onto a basis defined by the training exam ples. Because the training examples in an ill-posed data set do not fully span the signal space the observed training set variances in each basis vector will be too high compared to the average vari ance of the test set projections onto the same basis vectors. On basis of this understanding we introduce the Generalizable Singu lar Value Decomposition (GenSVD) as a means to reduce this bias by re-estimation of the singular values obtained in a conventional Singular Value Decomposition, allowing for a generalization perfor mance increase of a subsequent statistical model. We demonstrate that the algorithm succesfully corrects bias in a data set from a functional PET activation study of the human brain. 1 Ill-posed Data Sets An ill-posed data set has more dimensions in each example than there ar",Optimization & Theoretical ML,b4568df26077653eeadf29596708c94b-Paper.pdf,2000
Sparsity of data representation of optimal kernel,"Vapnik's result that the expectation of the generalisation error ofthe opti mal hyperplane is bounded by the expectation of the ratio of the number of support vectors to the number of training examples is extended to a broad class of kernel machines. The class includes Support Vector Ma chines for soft margin classification and regression, and Regularization Networks with a variety of kernels and cost functions. We show that key inequalities in Vapnik's result become equalities once ""the classification error"" is replaced by ""the margin error"", with the latter defined as an in stance with positive cost. In particular we show that expectations of the true margin error and the empirical margin error are equal, and that the sparse solutions for kernel machines are possible only if the cost function is ""partially"" insensitive. 1 Introduction Minimization of regularized risk is a backbone of several recent advances in machine learn ing, including Support Vector Machines (SVM) [13], Regulariz",Optimization & Theoretical ML,c164bbc9d6c72a52c599bbb43d8db8e1-Paper.pdf,2000
Sparse Kernel,"'Kernel' principal component analysis (PCA) is an elegant non linear generalisation of the popular linear data analysis method, where a kernel function implicitly defines a nonlinear transforma tion into a feature space wherein standard PCA is performed. Un fortunately, the technique is not 'sparse', since the components thus obtained are expressed in terms of kernels associated with ev ery training vector. This paper shows that by approximating the covariance matrix in feature space by a reduced number of exam ple vectors, using a maximum-likelihood approach, we may obtain a highly sparse form of kernel PCA without loss of effectiveness. 1 Introduction Principal component analysis (PCA) is a well-established technique for dimension ality reduction, and examples of its many applications include data compression, image processing, visualisation, exploratory data analysis, pattern recognition and time series prediction. Given a set of N d-dimensional data vectors Xn, which we take to hav",Optimization & Theoretical ML,bf201d5407a6509fa536afc4b380577e-Paper.pdf,2000
Rate-coded Restricted Boltzmann Machines for,"We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by com paring the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations. 1 Introduction Face recognition is difficult when th",Computer Vision,c366c2c97d47b02b24c3ecade4c40a01-Paper.pdf,2000
Shape Context: A new descriptor for,"We develop an approach to object recognition based on match ing shapes and using a resulting measure of similarity in a nearest neighbor classifier. The key algorithmic problem here is that of finding pointwise correspondences between an image shape and a stored prototype shape. We introduce a new shape descriptor, the shape context, which makes this possible, using a simple and robust algorithm. The shape context at a point captures the distri bution over relative positions of other shape points and thus sum marizes global shape in a rich, local descriptor. We demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes. Once shapes are aligned, shape contexts are used to define a robust score for measuring shape sim ilarity. We have used this score in a nearest-neighbor classifier for recognition of hand written digits as well as 3D objects, using exactly the same distance function. On the benchmark MNIST dataset of handwritten digit",Computer Vision,c44799b04a1c72e3c8593a53e8000c78-Paper.pdf,2000
"Position Variance, Recurrence and Perceptual","Stimulus arrays are inevitably presented at different positions on the retina in visual tasks, even those that nominally require fixation. In par ticular, this applies to many perceptual learning tasks. We show that per ceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli, involving a particular, quadratic, non-linearity rather than a purely lin ear discrimination. We show the advantage taking this non-linearity into account has for discrimination, and suggest it as a role for recurrent con nections in area VI, by demonstrating the superior discrimination perfor mance of a recurrent network. We propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks. 1 Introduction The field of perceptual learning in simple, but high precision, visual tasks (such as vernier acuity t",Computer Vision,c45008212f7bdf6eab6050c2a564435a-Paper.pdf,2000
Permitted and Forbidden Sets in,"Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmet ric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks com posed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigen modes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbid den sets that cannot. Permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synap",Optimization & Theoretical ML,c8cbd669cfb2f016574e9d147092b5bb-Paper.pdf,2000
Competition and Arbors in Ocular Dominance,"Hebbian and competitive Hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development. We analyse in the oretical detail a particular model (adapted from Piepenbrock & Ober mayer, 1999) for the development of Id stripe-like patterns, which places competitive and interactive cortical influences, and free and restricted ini tial arborisation onto a common footing. 1 Introduction Cats, many species of monkeys, and humans exibit ocular dominance stripes, which are alternating areas of primary visual cortex devoted to input from (the thalamic relay associ ated with) just one or the other eye (see Erwin et aI, 1995; Miller, 1996; Swindale, 1996 for reviews of theory and data). These well-known fingerprint patterns have been a seduc tive target for models of cortical pattern formation because of the mix of competition and cooperation they suggest. A wealth of synaptic adaptation algorithms has been suggested to account for them (and also the concomitant refine",Computer Vision,c91591a8d461c2869b9f535ded3e213e-Paper.pdf,2000
Learning Switching Linear Models of Human,"The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. Effective models of human dynamics can be learned from motion capture data using switching linear dynamic system (SLDS) models. We present results for human motion synthe sis, classification, and visual tracking using learned SLDS models. Since exact inference in SLDS is intractable, we present three approximate in ference algorithms and compare their performance. In particular, a new variational inference algorithm is obtained by casting the SLDS model as a Dynamic Bayesian Network. Classification experiments show the superiority of SLDS over conventional HMM's for our problem domain. 1 Introduction The human figure exhibits complex and rich dynamic behavior. Dynamics are essential to the classification of human motion (e.g. gesture recognition) as well as to the synthesis of realistic figure motion for computer graphics. In visual tracking applications, dynamics can provide a powerful",Computer Vision,ca460332316d6da84b08b9bcf39b687b-Paper.pdf,2000
New Approaches Towards Robust and,"In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent) ""experts"", each expert focusing on a different char acteristic of the signal, and that the different stream likelihoods (or posteriors) are combined at some (temporal) stage to yield a global recognition output. As a further extension to multi-stream ASR, we will finally introduce a new approach, referred to as HMM2, where the HMM emission probabilities are estimated via state spe cific feature based HMMs responsible for merging the stream infor mation and modeling their possible correlation. 1 Multi-Channel Processing in ASR Curr",NLP,cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf,2000
Place Cells and Spatial Navigation based on,"We model hippocampal place cells and head-direction cells by combin ing allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual in put, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsu pervised Hebbian learning is employed to incrementally build a popula tion of localized overlapping place fields. Place cells serve as basis func tions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented. 1 Introduction In order to achieve spatial learning, both animals and artificial agents need to autonomously locate themselves based on available sensory information. Neurophysiological findings suggest the spatial self-localization of rodents is supported by place-sensitive and direction sensitive cells. Place cells in the rat Hippocampus provide a spatial representation in allo centric coordinates [1]. A place cell exhibits a high firing rat",Computer Vision,cd14821dab219ea06e2fd1a2df2e3582-Paper.pdf,2000
Kernel-Based Reinforcement Learning in,"Many approaches to reinforcement learning combine neural net works or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those pro cedures is that the resulting learning algorithms are frequently un stable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical ap plication to the optimal portfolio choice problem. 1 Introduction Temporal-difference (TD) learning has been applied successfully to many real-world applications that can be formulated as discrete state Markov Decision Processes (MDPs) with unknown transition probabilities. If th",Reinforcement Learning,cd63a3eec3319fd9c84c942a08316e00-Paper.pdf,2000
A New Approximate Maximal Margin,"A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p ~ 2 for a set of linearly separable data. Our algorithm, called ALMAp (Approximate Large Mar- gin algorithm w.r.t. norm p), takes 0 ((P~21;;2) corrections to sepa rate the data with p-norm margin larger than (1 - 0:) ,,(, where,,( is the p-norm margin of the data and X is a bound on the p-norm of the in stances. ALMAp avoids quadratic (or higher-order) programming meth ods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's perceptron. We report on some experiments comparing ALMAp to two incremental algorithms: Perceptron and Li and Long's ROMMA. Our algorithm seems to perform quite better than both. The accuracy levels achieved by ALMAp are slightly inferior to those obtained by Support vector Machines (SVMs). On the other hand, ALMAp is quite faster and easier to implement than standard SVMs training algorithms. 1 Introduction A great d",Optimization & Theoretical ML,d072677d210ac4c03ba046120f0802ec-Paper.pdf,2000
Ensemble Learning and Linear Response Theory,"We propose a general Bayesian framework for performing independent component analysis (leA) which relies on ensemble learning and lin ear response theory known from statistical physics. We apply it to both discrete and continuous sources. For the continuous source the underde termined (overcomplete) case is studied. The naive mean-field approach fails in this case whereas linear response theory-which gives an improved estimate of covariances-is very efficient. The examples given are for sources without temporal correlations. However, this derivation can eas ily be extended to treat temporal correlations. Finally, the framework offers a simple way of generating new leA algorithms without needing to define the prior distribution of the sources explicitly. 1 Introduction Reconstruction of statistically independent source signals from linear mixtures is an active research field. For historical background and early references see e.g. [I]. The source separation problem has a Bayesian formul",Optimization & Theoretical ML,d1e946f4e67db4b362ad23818a6fb78a-Paper.pdf,2000
Regularization with Dot-Product Kernels,"In this paper we give necessary and sufficient conditions under which kernels of dot product type k(x, y) = k(x . y) satisfy Mer cer's condition and thus may be used in Support Vector Ma chines (SVM), Regularization Networks (RN) or Gaussian Pro cesses (GP). In particular, we show that if the kernel is analytic (i.e. can be expanded in a Taylor series), all expansion coefficients have to be nonnegative. We give an explicit functional form for the feature map by calculating its eigenfunctions and eigenvalues. 1 Introduction Kernel functions are widely used in learning algorithms such as Support Vector Ma chines, Gaussian Processes, or Regularization Networks. A possible interpretation of their effects is that they represent dot products in some feature space :7, i.e. k(x,y) = ¢(x)· ¢(y) (1) where ¢ is a map from input (data) space X into:7. Another interpretation is to connect ¢ with the regularization properties of the corresponding learning algorithm [8]. Most popular kernels can be d",Optimization & Theoretical ML,d25414405eb37dae1c14b18d6a2cac34-Paper.pdf,2000
From Margin To Sparsity,"We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation er ror bound for the classifier learned by the perceptron learning algo rithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are cur rently available for the support vector solution itself. 1 Introduction In the last few years there has been a large controversy about the significance of the attained margin, i.e. the smallest real valued output of a classifiers before thresholding, as an indicator of generalisation performance. Results in the YC, PAC and luckiness frameworks seem to indicate that a large margin is a pre-requisite for small generalisation error bounds (see [14, 12]). These results caused many researchers to focus on large margin methods such as the",Optimization & Theoretical ML,d305281faf947ca7acade9ad5c8c818c-Paper.pdf,2000
On a Connection between Kernel PCA,"In this paper we show that the kernel peA algorithm of Sch6lkopf et al (1998) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on Ilx - yll. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed. 1 Introduction Suppose we are given n objects, and for each pair (i,j) we have a measurement of the ""dissimilarity"" Oij between the two objects. In multidimensional scaling (MDS) the aim is to place n points in a low dimensional space (usually Euclidean) so that the interpoint distances dij have a particular relationship to the original dissimilarities. In classical scaling we would like the interpoint distances to be equal to the dissimilarities. For example, classical scaling can be used to reconstruct a map ",Optimization & Theoretical ML,d4b2aeb2453bdadaa45cbe9882ffefcf-Paper.pdf,2000
Interactive Parts Model: an Application,"In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We tested both models on a database of on-line cursive script. We show that im plementations of HMMs and the IP model, in which all letters are assumed to have the same average width, give comparable results. However, in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity. 1 Introduction Hidden Markov models [9] have been a dominant paradigm in speech and handwrit ing recognition over the past several decades. The success of HMMs is primarily due to their ability to model the statistical and sequential nature of speech and handwriting data. However, HMMs have a number of weaknesses [2]. First, dis criminative powers of HMMs are weak since the training algorithm is based on a Maximum Likelihood Estimate (MLE) criterion, whereas the optimal training should be based on a Maximum a Posteriori (MAP) criterion [2]. Second, in mos",NLP,d54e99a6c03704e95e6965532dec148b-Paper.pdf,2000
One Microphone Source Separation,"Source separation, or computational auditory scene analysis, attempts to extract individual acoustic objects from input which contains a mixture of sounds from different sources, altered by the acoustic environment. Unmixing algorithms such as lCA and its extensions recover sources by reweighting multiple obser vation sequences, and thus cannot operate when only a single observation signal is available. I present a technique called refiltering which recovers sources by a nonstationary reweighting (""masking"") of frequency sub-bands from a single recording, and argue for the application of statistical algorithms to learning this masking function. I present results of a simple factorial HMM system which learns on recordings of single speakers and can then separate mixtures using only one observation signal by computing the masking function and then refiltering. 1 Learning from data in computational auditory scene analysis Imagine listening to many pianos being played simultaneously. If ea",Computer Vision,d523773c6b194f37b938d340d5d02232-Paper.pdf,2000
A new model of spatial representations In,"Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocen tric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sen sory prediction, multi-sensory integration, sensory-motor transfor mation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsycholog ical data. In order to perform an action directed toward an object, it is necessary to have a representation of its spatial location. The brain must be able to use spatial cues coming from different modalities (e.g. vision, audition, touch, proprioception), combine them to i",Computer Vision,d757719ed7c2b66dd17dcee2a3cb29f4-Paper.pdf,2000
Feature Correspondence:,"When trying to recover 3D structure from a set of images, the most difficult problem is establishing the correspondence between the measurements. Most existing approaches assume that features can be tracked across frames, whereas methods that exploit rigidity constraints to facilitate matching do so only under restricted cam era motion. In this paper we propose a Bayesian approach that avoids the brittleness associated with singling out one ""best"" cor respondence, and instead consider the distribution over all possible correspondences. We treat both a fully Bayesian approach that yields a posterior distribution, and a MAP approach that makes use of EM to maximize this posterior. We show how Markov chain Monte Carlo methods can be used to implement these techniques in practice, and present experimental results on real data. 1 Introduction Structure from motion (SFM) addresses the problem of simultaneously recovering camera pose and a three-dimensional model from a collection of images. ",Computer Vision,d7657583058394c828ee150fada65345-Paper.pdf,2000
Stagewise processing in error-correcting,"We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and sim ulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation. 1 Introduction In error-correcting codes [1] and image restoration [2], the choice of the so-called hyperparameters is an important factor in determining their performances. Hyper parameters refer to the coefficients weighing the biases and variances of the tasks. In error correction, they determine the statistical significance given to the parity checking terms and the received bits. Similarly in image restoration, they determine the statistical weights given to the prior knowledge and the received data. It was shown, by the use of inequalities, that the choice of the hyperparameters is opti mal when there is a match between",Optimization & Theoretical ML,d7fd118e6f226a71b5f1ffe10efd0a78-Paper.pdf,2000
The Kernel Gibbs Sampler,"We present an algorithm that samples the hypothesis space of ker nel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piece wise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contam inated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS out performs an SVM that is incapable of taking into account label noise. 1 Introduction Two great ideas have dominated recent developments in machine learning: the ap plication of kernel methods and the popularisation of B",Optimization & Theoretical ML,dc40b7120e77741d191c0d2b82cea7be-Paper.pdf,2000
Learning Sparse Image Codes using a,"We show how a wavelet basis may be adapted to best represent natural images in terms of sparse coefficients. The wavelet basis, which may be either complete or overcomplete, is specified by a small number of spatial functions which are repeated across space and combined in a recursive fashion so as to be self-similar across scale. These functions are adapted to minimize the estimated code length under a model that assumes images are composed of a linear superposition of sparse, independent components. When adapted to natural images, the wavelet bases take on different orientations and they evenly tile the orientation domain, in stark contrast to the standard, non-oriented wavelet bases used in image compression. When the basis set is allowed to be overcomplete, it also yields higher coding efficiency than standard wavelet bases. 1 Introduction The general problem we address here is that of learning efficient codes for represent ing natural images. Our previous work in this area has foc",Computer Vision,dc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf,2000
Keeping flexible active contours on track using,"Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained ""active"" con tours in video sequences. However, when the contours are highly flexible (e.g. for tracking fingers of a hand), a computationally burdensome num ber of particles is needed to successfully approximate the contour distri bution. We show how the Metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence. We compare this method to condensation using a video sequence that requires highly flexible contours, and show that the new algorithm performs dramatically better that the condensation algorithm. We discuss the incorporation of this method into the ""active contour"" framework where a shape-subspace is used constrain shape variation. 1 Introduction Tracking objects with flexible shapes in video sequences is currently an important topic in the vision community. Methods inclu",Computer Vision,dfce06801e1a85d6d06f1fdd4475dacd-Paper.pdf,2000
Weak Learners and Improved Rates of,"The problem of constructing weak classifiers for boosting algo rithms is studied. We present an algorithm that produces a linear classifier that is guaranteed to achieve an error better than random guessing for any distribution on the data. While this weak learner is not useful for learning in general, we show that under reasonable conditions on the distribution it yields an effective weak learner for one-dimensional problems. Preliminary simulations suggest that similar behavior can be expected in higher dimensions, a result which is corroborated by some recent theoretical bounds. Addi tionally, we provide improved convergence rate bounds for the gen eralization error in situations where the empirical error can be made small, which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established. 1 Introduction The recently introduced boosting approach to classification (e.g., [10]) has been shown to be a highly e",Optimization & Theoretical ML,dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf,2000
Data clustering by Markovian relaxation,"We introduce a new, non-parametric and principled, distance based clustering method. This method combines a pairwise based ap proach with a vector-quantization method which provide a mean ingful interpretation to the resulting clusters. The idea is based on turning the distance matrix into a Markov process and then examine the decay of mutual-information during the relaxation of this process. The clusters emerge as quasi-stable structures dur ing this relaxation, and then are extracted using the information bottleneck method. These clusters capture the information about the initial point of the relaxation in the most effective way. The method can cluster data with no geometric or other bias and makes no assumption about the underlying distribution. 1 Introduction Data clustering is one of the most fundamental pattern recognition problems, with numerous algorithms and applications. Yet, the problem itself is ill-defined: the goal is to find a ""reasonable"" partition of data points into c",Optimization & Theoretical ML,e06f967fb0d355592be4e7674fa31d26-Paper.pdf,2000
Balancing Multiple Sources of Reward in,"For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar com ponents. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by com bining the multiple components can throwaway vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforce ment learning. We then present an new algorithm for finding a solution and results on simulated environments. 1 Introduction In the traditional reinforcement learning framework, the learning agent is given a single scalar value of reward at each time step. The goal is for the agent to optimize the sum of these rewards over time (the return). For many applications, there is more information available. Consider the case of a home entertainment system designed to sense which residents are currently in the ",Reinforcement Learning,e0ab531ec312161511493b002f9be2ee-Paper.pdf,2000
Analysis of Bit Error Probability of,"We analyze the bit error probability of multiuser demodulators for direct sequence binary phase-shift-keying (DSIBPSK) CDMA channel with ad ditive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is ap plied to evaluate the performance of the resulting MPM (Marginal Pos terior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementa tion of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the per formance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, espe cially in the cases of small information bit rate and low noise level. 1 Introduction The CDMA (Code-Division-Multiple-Access) technique [1] is important as a fundame",Optimization & Theoretical ML,e5b294b70c9647dcf804d7baa1903918-Paper.pdf,2000
Temporally Dependent Plasticity:,"The paradigm of Hebbian learning has recently received a novel in terpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic princi ple of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the ex perimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters 1 Introduction Hebbian plasticity, the major paradigm for learning in computational neuroscience, was until a few years ago interpreted as learning ",Optimization & Theoretical ML,e1314fc026da60d837353d20aefaf054-Paper.pdf,2000
Robust Reinforcement Learning,"This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as mod eling errors. The use of environmental models in RL is quite pop ular for both off-line learning by simulations and for on-line ac tion planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of Hoocontrol, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a min max solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in refer ence to the value function. We tested the paradigm, which we call ""Robu",Reinforcement Learning,e8dfff4676a47048d6f0c4ef899593dd-Paper.pdf,2000
A variational mean-field theory for,"A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite com petitive. 1 Introduction Application of mean-field theory to solve the problem of inference in Belief Net works(BNs) is well known [1]. In this paper we will discuss a variational mean-field theory and its application to BNs, sigmoidal BNs in particular. We present a variational derivation of the mean-field theory, proposed by Plefka[2]. The theory will be developed for a stochastic system, consistin~ of N binary random variables, Si E {O, I}, described by the energy function E(S), and the following Boltzmann Gibbs distribution at a temperature T: _ -e-z~ T - , z """"' E(S) P(S) = = ~ e-----;y-. S The application of this mean-field method to Boltzmann Machines(BMs) is already done [3]. A large class of BN s are described by the f",Optimization & Theoretical ML,e74c0d42b4433905293aab661fcf8ddb-Paper.pdf,2000
Explaining Away in Weight Space,"Explaining away has mostly been considered in terms of inference of states in belief networks. We show how it can also arise in a Bayesian context in inference about the weights governing relationships such as those between stimuli and reinforcers in conditioning experiments such as bacA',Ward blocking. We show how explaining away in weight space can be accounted for using an extension of a Kalman filter model; pro vide a new approximate way of looking at the Kalman gain matrix as a whitener for the correlation matrix of the observation process; suggest a network implementation of this whitener using an architecture due to Goodall; and show that the resulting model exhibits backward blocking. 1 Introduction The phenomenon of explaining away is commonplace in inference in belief networks. In this, an explanation (a setting of activities of unobserved units) that is consistent with cer tain observations is accorded a low posterior probability if another explanation for the same observati",Optimization & Theoretical ML,eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf,2000
Improved Output Coding for Classification,"Output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems. Previous re search on output coding has employed, almost solely, predefined discrete codes. We describe an algorithm that improves the performance of output codes by relaxing them to continuous codes. The relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines. We describe experiments with the proposed algorithm, comparing it to standard discrete output codes. The experimental results indicate that continuous relaxations of output codes often improve the generalization performance, especially for short codes. 1 Introduction The problem of multiclass categorization is about assigning labels to instances where the la bels are drawn from some finite set. Many machine learning problems include a multiclass categorization component in them. Examples for such applications are text classification,",Optimization & Theoretical ML,ecd62de20ea67e1c2d933d311b08178a-Paper.pdf,2000
Learning curves for Gaussian processes,"Based on a statistical mechanics approach, we develop a method for approximately computing average case learning curves for Gaus sian process regression models. The approximation works well in the large sample size limit and for arbitrary dimensionality of the input space. We explain how the approximation can be systemati cally improved and argue that similar techniques can be applied to general likelihood models. 1 Introduction Gaussian process (GP) models have gained considerable interest in the Neural Com putation Community (see e.g.[I, 2, 3, 4] ) in recent years. Being non-parametric models by construction their theoretical understanding seems to be less well devel oped compared to simpler parametric models like neural networks. We are especially interested in developing theoretical approaches which will at least give good approx imations to generalization errors when the number of training data is sufficiently large. In this paper we present a step in this direction which is based",Optimization & Theoretical ML,ed519dacc89b2bead3f453b0b05a4a8b-Paper.pdf,2000
"Sequentially fitting ""inclusive"" trees for","An important class of problems can be cast as inference in noisy OR Bayesian networks, where the binary state of each variable is a logical OR of noisy versions of the states of the variable's par ents. For example, in medical diagnosis, the presence of a symptom can be expressed as a noisy-OR of the diseases that may cause the symptom - on some occasions, a disease may fail to activate the symptom. Inference in richly-connected noisy-OR networks is in tractable, but approximate methods (e.g., variational techniques) are showing increasing promise as practical solutions. One prob lem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior, ig noring other plausible configurations of the hidden variables. We introduce a new sequential variational method for bipartite noisy OR networks, that favors including all modes of the true posterior and models the posterior distribution as a tree. We compare this method with other appro",Computer Vision,f0bbac6fa079f1e00b2c14c1d3c6ccf0-Paper.pdf,2000
Whence Sparseness?,"It has been shown that the receptive fields of simple cells in VI can be ex plained by assuming optimal encoding, provided that an extra constraint of sparseness is added. This finding suggests that there is a reason, in dependent of optimal representation, for sparseness. However this work used an ad hoc model for the noise. Here I show that, if a biologically more plausible noise model, describing neurons as Poisson processes, is used sparseness does not have to be added as a constraint. Thus I con clude that sparseness is not a feature that evolution has striven for, but is simply the result of the evolutionary pressure towards an optimal repre sentation. 1 Introduction Recently there has been an resurgence of interest in using optimal coding strategies to 'explain' the response properties of neuron in the primary sensory areas [1]. Notably this approach was used Olshausen and Field [2] to infer the receptive field of simple cells in the primary visual cortex. To arrive at the corre",Optimization & Theoretical ML,f0fcf351df4eb6786e9bb6fc4e2dee02-Paper.pdf,2000
Periodic Component Analysis:,"An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent com ponent analysis (ICA). Our method-called periodic component analysis (1l""CA)-uses constructive interference to enhance periodic components of the frequency spectrum and destructive interference to cancel noise. The front end emulates important aspects of auditory processing, such as cochlear filtering, nonlinear compression, and insensitivity to phase, with the aim of approaching the robustness of human listeners. The method avoids the inefficiencies of autocorrelation at the pitch period: it does not require long delay lines, and it correlates signals at a clock rate on the order of the actual pitch, as opposed to the original sampling rate. We derive its cost function and present some experimental results. 1 Introduction Periodic structure in the time waveform conveys important ",NLP,f22e4747da1aa27e363d86d40ff442fe-Paper.pdf,2000
Partially Observable SDE Models for,"This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of contin uous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models. 1 Introduction This paper explores a framework for recognition of image sequences using partially observable stochastic differential equations (SDEs). In particular we use SDE mod els of low-power non-linear RC circuits with a significant thermal noise component. We call them diffusion networks. A diffusi",Computer Vision,f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf,2000
Combining ICA and top-down attention,"We present an algorithm which compensates for the mismatches between characteristics of real-world problems and assumptions of independent component analysis algorithm. To provide additional information to the ICA network, we incorporate top-down selec tive attention. An MLP classifier is added to the separated signal channel and the error of the classifier is backpropagated to the ICA network. This backpropagation process results in estimation of expected ICA output signal for the top-down attention. Then, the unmixing matrix is retrained according to a new cost function representing the backpropagated error as well as independence. It modifies the density of recovered signals to the density appropriate for classification. For noisy speech signal recorded in real environ ments, the algorithm improved the recognition performance and showed robustness against parametric changes. 1 Introduction Independent Component Analysis (ICA) is a method for blind signal separation. ICA linearly tra",Optimization & Theoretical ML,f2d887e01a80e813d9080038decbbabb-Paper.pdf,2000
Learning continuous distributions:,"Learning of a smooth but nonparametric probability density can be reg ularized using methods of Quantum Field Theory. We implement a field theoretic prior numerically, test its efficacy, and show that the free pa rameter of the theory (,smoothness scale') can be determined self con sistently by the data; this forms an infinite dimensional generalization of the MDL principle. Finally, we study the implications of one's choice of the prior and the parameterization and conclude that the smoothness scale determination makes density estimation very weakly sensitive to the choice of the prior, and that even wrong choices can be advantageous for small data sets. One of the central problems in learning is to balance 'goodness of fit' criteria against the complexity of models. An important development in the Bayesian approach was thus the realization that there does not need to be any extra penalty for model complexity: if we compute the total probability that data are generated by a model, the",Optimization & Theoretical ML,f39ae9ff3a81f499230c4126e01f421b-Paper.pdf,2000
A comparison of Image Processing,"We examine eight different techniques for developing visual rep resentations in machine vision tasks. In particular we compare different versions of principal component and independent com ponent analysis in combination with stepwise regression methods for variable selection. We found that local methods, based on the statistics of image patches, consistently outperformed global meth ods based on the statistics of entire images. This result is consistent with previous work on emotion and facial expression recognition. In addition, the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance. 1 Introduction We study the performance of eight different methods for developing image repre sentations based on the statistical properties of the images at hand. These methods are compared on their performance on a visual speech recognition task. While the representations developed are specific to visual speech recognition, the meth o",Computer Vision,f31b20466ae89669f9741e047487eb37-Paper.pdf,2000
Algebraic Information Geometry for,"Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold, since Fisher in formation matrices are singular. In this paper, the rigorous asymp totic form of the stochastic complexity is clarified based on resolu tion of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIO, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordi nate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIO. It is useful for model selection, but not for generalization. 1 Introduction The Fisher information matrix determines a metric of the set of all parameters of a learning machine [2]. If it is positive definite, ",Optimization & Theoretical ML,f442d33fa06832082290ad8544a8da27-Paper.pdf,2000
The Missing Link - A Probabilistic Model of,"We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambigua tion, focused web crawling, web authoring, and bibliometric analysis. 1 Introduction No text, no paper, no book can be isolated from the all-embracing corpus of documents it is embedded in. Ideas, thoughts, and work described in a document inevitably relate to and build upon previously published material. 1 Traditionally, this interdependency has been represented by citations, which allow authors to explicitly make refer",NLP,f45a1078feb35de77d26b3f7a52ef502-Paper.pdf,2000
The Use of Classifiers in Sequential Inference,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an im portant subproblem - identifying phrase structure. The first is a Marko vian approach that extends standard HMMs to allow the use of a rich ob servation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction for malisms. We develop efficient combination algorithms under both mod els and study them experimentally in the context of shallow parsing. 1 Introduction In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some con straints - the sequential nature of the data or other domain specific constraints. Consider, for example, the problem of chunking natural language sentences wher",NLP,f4a4da9aa7eadfd23c7bdb7cf57b3112-Paper.pdf,2000
Finding the Key to a Synapse,"Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for com putations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynap tic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature. 1 Introduction A large number of experimental studies have shown that biological synapses have an in herent dynamics, which controls how the pattern of amplitudes of postsynaptic responses depends on the temporal pattern of the incom",Optimization & Theoretical ML,f542eae1949358e25d8bfeefe5b199f1-Paper.pdf,2000
The Unscented Particle Filter,"In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses a bank of unscented fil ters to obtain the importance proposal distribution. This proposal has two very ""nice"" properties. Firstly, it makes efficient use of the latest available information and, secondly, it can have heavy tails. As a result, we find that the algorithm outperforms stan dard particle filtering and other nonlinear filtering methods very substantially. This experimental finding is in agreement with the theoretical convergence proof for the algorithm. The algorithm also includes resampling and (possibly) Markov chain Monte Carlo (MCMC) steps. 1 Introduction Filtering is the problem of estimating the states (parameters or hidden variables) of a system as a set of observations becomes available on-line. This problem is of paramount importance in many fields of science, engineering and finance. To solve it, one begins by modelling the evolution of the system and the no",Optimization & Theoretical ML,f5c3dd7514bf620a1b85450d2ae374b1-Paper.pdf,2000
Algorithms for Non-negative Matrix,"Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multi plicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the Expectation Maximization algorithm. The algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence. 1 Introduction Unsupervised learning algorithms such as principal components analysis and vector quan tization can be understood as factorizing a data matrix subject to different constraints. De pending upon the constraints utilized, the resulting factors can be",Optimization & Theoretical ML,f9d1152547c0bde01830b7e8bd60024c-Paper.pdf,2000
Factored Semi-Tied Covariance Matrices,"A new form of covariance modelling for Gaussian mixture models and hidden Markov models is presented. This is an extension to an efficient form of covariance modelling used in speech recognition, semi-tied co variance matrices. In the standard form of semi-tied covariance matrices the covariance matrix is decomposed into a highly shared decorrelating transform and a component-specific diagonal covariance matrix. The use of a factored decorrelating transform is presented in this paper. This fac toring effectively increases the number of possible transforms without in creasing the number of free parameters. Maximum likelihood estimation schemes for all the model parameters are presented including the compo nent/transform assignment, transform and component parameters. This new model form is evaluated on a large vocabulary speech recognition task. It is shown that using this factored form of covariance modelling reduces the word error rate. 1 Introduction A standard problem in machine lea",Optimization & Theoretical ML,fb8feff253bb6c834deb61ec76baa893-Paper.pdf,2000
Divisive and Subtractive Mask Effects:,"We describe an analogy between psychophysically measured effects in contrast masking, and the behavior of a simple integrate-and fire neuron that receives time-modulated inhibition. In the psy chophysical experiments, we tested observers ability to discriminate contrasts of peripheral Gabor patches in the presence of collinear Gabor flankers. The data reveal a complex interaction pattern that we account for by assuming that flankers provide divisive inhibi tion to the target unit for low target contrasts, but provide sub tractive inhibition to the target unit for higher target contrasts. A similar switch from divisive to subtractive inhibition is observed in an integrate-and-fire unit that receives inhibition modulated in time such that the cell spends part of the time in a high-inhibition state and part of the time in a low-inhibition state. The simi larity between the effects suggests that one may cause the other. The biophysical model makes testable predictions for physiological sin",Computer Vision,faacbcd5bf1d018912c116bf2783e9a1-Paper.pdf,2000
Noise suppression based on,"A novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically-motivated estimation of the local signal-to-noise ratio (SNR) in different frequency chan nels. For SNR-estimation, the input signal is transformed into so-called Amplitude Modulation Spectrograms (AMS), which rep resent both spectral and temporal characteristics of the respective analysis frame, and which imitate the representation of modula tion frequencies in higher stages of the mammalian auditory sys tem. A neural network is used to analyse AMS patterns generated from noisy speech and estimates the local SNR. Noise suppres sion is achieved by attenuating frequency channels according to their SNR. The noise suppression algorithm is evaluated in speaker independent digit recognition experiments and compared to noise suppression by Spectral Subtraction. 1 Introduction One of the major problems in automatic speech recognition (ASR) systems is their lack of robustness in noise, whic",NLP,fc4ddc15f9f4b4b06ef7844d6bb53abf-Paper.pdf,2000
Dendritic compartmentalization could underlie,"Neurons in area V4 have relatively large receptive fields (RFs), so multi ple visual features are simultaneously ""seen"" by these cells. Recordings from single V 4 neurons suggest that simultaneously presented stimuli compete to set the output firing rate, and that attention acts to isolate individual features by biasing the competition in favor of the attended object. We propose that both stimulus competition and attentional bias ing arise from the spatial segregation of afferent synapses onto different regions of the excitable dendritic tree of V 4 neurons. The pattern of feed forward, stimulus-driven inputs follows from a Hebbian rule: excitatory afferents with similar RFs tend to group together on the dendritic tree, avoiding randomly located inhibitory inputs with similar RFs. The same principle guides the formation of inputs that mediate attentional mod ulation. Using both biophysically detailed compartmental models and simplified models of computation in single neurons, we demons",Computer Vision,ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf,2000
Smart Vision Chip Fabricated Using Three,"The smart VISIOn chip has a large potential for application in general purpose high speed image processing systems. In order to fabricate smart vision chips including photo detector compactly, we have proposed the application of three dimensional LSI technology for smart vision chips. Three dimensional technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure. In this paper, we describe our three dimensional LSI technology for neuromorphic circuits and the design of smart vision chips. 1 Introduction Recently, the demand for very fast image processing systems with real time operation capability has significantly increased. Conventional image processing systems based on the system level integration of a camera and a digital processor, do not have the potential for application in general purpose consumer electronic products. This is simply due to the cost, size and complexity of these systems. Therefo",Computer Vision,ff7d0f525b3be596a51fb919492c099c-Paper.pdf,2000
"A productive, systematic framework for","We describe a unified framework for the understanding of struc ture representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common ""middle-scale"" parts, repre sented as image fragments. The model addresses the same concerns as previous work on compositional representation through the use of what+where receptive fields and attentional gain modulation. It does not require prior exposure to the individual parts, and avoids the need for abstract symbolic binding. 1 The problem of structure representation The focus of theoretical discussion in visual object processing has recently started to shift from problems of recognition and categorization to the representation of object structure. Although view- or appearance-based solutions for these problems proved effective on a variety of object classes [1], the ""holistic"" nature of ",Computer Vision,fface8385abbf94b4593a0ed53a0c70f-Paper.pdf,2000
Fragment completion in humans and machines,"Partial informationcan trigger a complete memory. At the same time, humanmemoryisnotperfect. Acuecancontainenoughinformationto specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patternsinhumanmemoryerrors.Weusecuesthatconsistofwordfrag- ments. Weshowthatshortandlongcuesarecompletedmoreaccurately thanmediumlengthonesandstudysomeofthefactorsthatleadtothis behavior.Wethenpresentanovelcomputationalmodelthatshowssome oftheflexibilityandpatternsoferrorsthatoccurinhumanmemory.This model iterates between bottom-up and top-down computations. These aretiedtogetherusingaMarkovmodelofwordsthatallowsmemoryto beaccessedwithasimplefeatureset,andenablesabottom-upprocessto computeaprobabilitydistributionofpossiblecompletionsofwordfrag- ments,inamannersimilartomodelsofvisualperceptualcompletion. 1 Introduction Thispaperaddressestheproblemofretrievingitemsinmemoryfrompartialinformation. Human memoryis remarkablefor ",NLP,0070d23b06b1486a538c0eaa45dd167a-Paper.pdf,2001
Means. Correlations and Bounds,"The partition function for a Boltzmann machine can be bounded from above and below. We can use this to bound the means and the correlations. For networks with small weights, the values of these statistics can be restricted to non-trivial regions (i.e. a subset of [-1, 1]). Experimental results show that reasonable bounding occurs for weight sizes where mean field expansions generally give good results. 1 Introduction Over the last decade, bounding techniques have become a popular tool to deal with graphical models that are too complex for exact computation. A nice property of bounds is that they give at least some information you can rely on. For instance, one may find that a correlation is definitely between 0.4 and 0.6. An ordinary ap proximation might be more accurate, but in practical situations there is absolutely no warranty for that. The best known bound is probably the mean field bound, which has been described for Boltzmann machines in [1] and later for sigmoid belief networks",Optimization & Theoretical ML,0004d0b59e19461ff126e3a08a814c33-Paper.pdf,2001
Eye movements and the maturation of cortical,"Neuralactivityappearstobeacrucialcomponentforshapingtherecep- tivefieldsofcorticalsimplecellsintoadjacent,orientedsubregionsalter- natelyreceivingON-andOFF-centerexcitatorygeniculateinputs. Itis knownthattheorientationselectiveresponsesofV1neuronsarerefined byvisualexperience. Aftereyeopening,thespatiotemporalstructureof neuralactivityintheearlystagesofthevisualpathwaydependsbothon thevisualenvironmentandonhowtheenvironmentisscanned.Wehave used computationalmodelingto investigate how eyemovements might affecttherefinementoftheorientationtuningofsimplecellsinthepres- enceofaHebbianschemeofsynapticplasticity.Levelsofcorrelationbe- tweentheactivityofsimulatedcellswereexaminedwhilenaturalscenes were scanned so as to model sequences of saccades and fixational eye movements,suchasmicrosaccades,tremorandoculardrift.Thespecific patterns of activity required for a quantitatively accurate development ofsimple cell receptive fields withsegregatedON andOFF subregions wereobservedduringfixationaley",Computer Vision,008bd5ad93b754d500338c253d9c1770-Paper.pdf,2001
Orientation-Selective aVLSI Spiking Neurons,"We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiverchipwhoseintegrate-and-fireneuronsareconnectedinasoft winner-take-all architecture. The circuit on this multi-neuron chip ap- proximates a cortical microcircuit. The neurons can be configured for different computational properties by the virtual connections of a se- lected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication pro- tocol that uses asynchronous digital pulses, similar to spikes in a neu- ronal system. We used the multi-chip spike-based system to synthe- size orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking modelmatchedtheexperimentalobservationsanddigitalsimulationsof continuous-valuedneurons.Themulti-chipVLSIsy",Computer Vision,0189caa552598b845b29b17a427692d1-Paper.pdf,2001
On the Generalization Ability,"In this paper we show that on-line algorithms for classification and re- gression can be naturally used to obtain hypotheses with good data- dependenttail boundson their risk. Our results are provenwithout re- quiringcomplicatedconcentration-of-measureargumentsandtheyhold forarbitraryon-linelearningalgorithms. Furthermore,whenappliedto concrete on-line algorithms, our results yield tail bounds that in many casesarecomparableorbetterthanthebestknownbounds. 1 Introduction One of the main contributions of the recent statistical theories for regression and classi- fication problems [21, 19] is the derivation of functionals of certain empirical quantities (suchasthesampleerrororthesamplemargin)thatprovideuniformriskboundsforallthe hypothesesinacertainclass. Thisapproachhassomeknownweakpoints.First,obtaining tight uniformrisk bounds in terms of meaningfulempirical quantities is generallya dif- ficulttask. Second,searchingforthehypothesisminimizingagivenempiricalfunctional isoftencomputationa",Optimization & Theoretical ML,01931a6925d3de09e5f87419d9d55055-Paper.pdf,2001
Analysis of Sparse Bayesian Learning,"The recent introduction of the 'relevance vector machine' has effec tively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective func tion, and demonstrates that conditioned on an individual hyper parameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model. 1 Introduction We consider the approximation, from a training sample, of real-valued functions, a task variously referred to as prediction, regression, interpolation or function ap proximation. Given a set of data {xn' tn};;=l the 'target' samples tn = f(xn) + En are conventionally conside",Optimization & Theoretical ML,02b1be0d48924c327124732726097157-Paper.pdf,2001
Motivated Reinforcement Learning,"The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning in cludes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine, 12,13 into the neurobiology and psychol ogy of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many dif ferent actions at a given state, but rather whether a single re sponse is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psy chological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact. 1 Introduction Reinforcement learning (RL 28) bears a tortuous relationship with historical and conte",Reinforcement Learning,051928341be67dcba03f0e04104d9047-Paper.pdf,2001
A hierarchical model of complex cells in,"A cortical model for motion-in-depth selectivity of complex cells in the visual cortex is proposed. The model is based on a time ex tension of the phase-based techniques for disparity estimation. We consider the computation of the total temporal derivative of the time-varying disparity through the combination of the responses of disparity energy units. To take into account the physiological plau sibility, the model is based on the combinations of binocular cells characterized by different ocular dominance indices. The resulting cortical units of the model show a sharp selectivity for motion-in depth that has been compared with that reported in the literature for real cortical cells. 1 Introduction The analysis of a dynamic scene implies estimates of motion parameters to infer spatio-temporal information about the visual world. In particular, the perception of motion-in-depth (MID), i.e. the capability of discriminating between forward and backward movements of objects from an observer,",Computer Vision,03e7d2ebec1e820ac34d054df7e68f48-Paper.pdf,2001
Categorization by Learning,"Wedescribeanalgorithmforautomaticallylearningdiscriminativecom- ponentsofobjects with SVM classifiers. It is based ongrowingimage parts by minimizing theoretical bounds on the error probability of an SVM.Component-basedfaceclassifiers arethencombinedina second stagetoyieldahierarchicalSVMclassifier. Experimentalresultsinface classificationshowconsiderablerobustnessagainstrotationsindepthand suggestperformanceatsignificantlybetterlevelthanotherfacedetection systems. Novel aspects of our approach are: a) an algorithm to learn component-basedclassificationexpertsandtheircombination,b)theuse of3-Dmorphablemodelsfortraining,andc)amaximumoperationon theoutputofeachcomponentclassifier whichmayberelevantforbio- logicalmodelsofvisualrecognition. 1 Introduction Westudytheproblemofautomaticallysynthesizinghierarchicalclassifiersbylearningdis- criminativeobjectpartsinimages. Ourmotivationisthatmostobjectclasses(e.g. faces, cars)seemtobenaturallydescribedbyafewcharacteristicpartsorcomponentsandthei",Computer Vision,05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf,2001
Group Redundancy Measures Reveal,"The way groups of auditory neurons interact to code acoustic in formation is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hy pothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are signif icant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest",Optimization & Theoretical ML,05a5cf06982ba7892ed2a6d38fe832d6-Paper.pdf,2001
Probabilistic Inference of Hand Motion from Neural,"Statisticallearningandprobabilisticinferencetechniquesareusedtoin- ferthehandpositionofasubjectfrommulti-electroderecordingsofneu- ralactivityinmotorcortex. First,anarrayofelectrodesprovidestrain- ingdataofneuralfiringconditionedonhandkinematics.Welearnanon- parametricrepresentationofthisfiringactivityusingaBayesianmodel andrigorouslycompareit with previousmodelsusingcross-validation. Second, we infer a posterior probability distribution over handmotion conditionedonasequenceofneuraltestdatausingBayesianinference. The learned firing models of multiple cells are used to define a non- Gaussianlikelihoodtermwhichiscombinedwithapriorprobabilityfor thekinematics. Aparticlefilteringmethodisusedtorepresent,update, andpropagatetheposteriordistributionovertime.Theapproachiscom- paredwithtraditionallinearfilteringmethods;theresultssuggestthatit maybeappropriateforneuralprostheticapplications. 1 Introduction Thispaperexplorestheuseofstatisticallearningmethodsandprobabilisticinferencetech- niques ",NLP,06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf,2001
The Noisy Euclidean Traveling Salesman,"We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure. Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances. This procedure requires identifying the exact relationship between permutations and tours. In a learning setting, the average trajec tory is used as a model to construct solutions to new instances sampled from the same source. Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance. 1 Introduction The approach in combinatorial optimization is traditionally single-instance and worst-case-oriented. An algorithm is tested against the worst possible single in stance. In reality, algorithms are often applied to a large number of related instances, the average-case performance being the measurement of in",Optimization & Theoretical ML,06b1338ba02add2b5d2da67663b19ebe-Paper.pdf,2001
Incorporating Invariances in Nonlinear,"The choice of an SVM kernel corresponds to the choice of a rep resentation of the data in a feature space and, to improve per formance, it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in non linear kernels. We show on a digit recognition task that the pro posed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1 Introduction In some classification tasks, an a priori knowledge is known about the invariances related to the task. For instance, in image classification, we know that the label of a given image should not change after a small translation or rotation. More generally, we assume we know a local transformation Lt depending on a parameter t (for instance, a vertical translation of t pixels) such that any point x should be considered equivalent to LtX, the transformed point. Ideally, the output of ",Computer Vision,07811dc6c422334ce36a09ff5cd6fe71-Paper.pdf,2001
Sampling Techniques for Kernel Methods,"We propose randomized techniques for speeding up Kernel Principal ComponentAnalysis on three levels: sampling and quantizationof the Gram matrix in training, randomizedroundingin evaluatingthe kernel expansions,andrandomprojectionsinevaluatingthekernelitself. Inall three cases, we give sharp bounds on the accuracy of the obtained ap- proximations. Ratherintriguingly,allthreetechniquescanbeviewedas instantiations of the following idea: replace the kernel function (cid:0) by a “randomizedkernel”whichbehaveslike (cid:0) inexpectation. 1 Introduction Givenacollection (cid:1) oftrainingdata (cid:2)(cid:4) (cid:3)(cid:6) (cid:5)(cid:8) (cid:7)(cid:9) (cid:7)(cid:8) (cid:7)(cid:10) (cid:5)(cid:11) (cid:2)(cid:13) (cid:12) ,techniquessuchaslinearSVMs[13]and PCA extract features from (cid:1) by computinglinear functions of this data. However, it is oftenthecasethatthestructurepresentinthetrainingdataisnotsimplyalinearfunction ofthedatarepresentation. Worse,manydatasetsdonotreadilysupportlinearo",Optimization & Theoretical ML,07cb5f86508f146774a2fac4373a8e50-Paper.pdf,2001
Reinforcement Learning and Time,"Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1 Introduction An aspect of delayed-reward reinforcement learning problem which has a long his tory of study in animal experiments, but has been overlooked by theorists, is the learning of the expected time to the reward. In a number of animal experiments, animals need to wait a given time interval after a stimulus before performing an action in or",Reinforcement Learning,08f90c1a417155361a5c4b8d297e0d78-Paper.pdf,2001
Probabilistic Abstraction Hierarchies,"Manydomainsarenaturallyorganizedinanabstractionhierarchyortaxonomy, wheretheinstancesin“nearby”classesinthetaxonomyaresimilar. Inthispa- per, weprovide ageneralprobabilistic framework forclustering dataintoaset ofclassesorganizedasataxonomy,whereeachclassisassociatedwithaprob- abilistic model from which the data was generated. The clustering algorithm simultaneouslyoptimizesthreethings: theassignmentofdatainstancestoclus- ters,themodelsassociatedwiththeclusters,andthestructureoftheabstraction hierarchy.Auniquefeatureofourapproachisthatitutilizesglobaloptimization algorithmsforbothofthelasttwosteps,reducingthesensitivitytonoiseandthe propensitytolocalmaximathatarecharacteristicofalgorithmssuchashierarchi- calagglomerativeclusteringthatonlytakelocalsteps. Weprovideatheoretical analysisforouralgorithm,showingthatitconvergestoalocalmaximumofthe jointlikelihoodofmodelanddata. Wepresentexperimentalresultsonsynthetic data,andonrealdatainthedomainsofgeneexpressionandtext. 1 Introduction Manydo",NLP,0ae3f79a30234b6c45a6f7d298ba1310-Paper.pdf,2001
Hyperbolic Self-Organizing Maps for Semantic,"We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyper- bolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane,whichisanon-euclideanspacecharacterizedbyconstantnegative gaussiancurvature.Theexponentiallyincreasingsizeofaneighborhood around a point in hyperbolic space provides more freedom to map the complexinformationspacearisingfromlanguageintospatialrelations. We describe experiments, showingthat the HSOM can successfully be appliedtotextcategorizationtasksandyieldsresultscomparabletoother state-of-the-artmethods. 1 Introduction FormanytasksofexploratydataanalysistheSelf-OrganizingMaps(SOM),asintroduced byKohonenmorethana decadeago, havebecomea widelyused tool[1, 2]. So far, the overwhelmingmajority of SOM approacheshave taken it for grantedto use a flat space as theirdata modeland, motivatedby its conveniencefor visualization, havefavoredthe (suitablydiscretized)euclideanplaneastheirchie",Computer Vision,093b60fd0557804c8ba0cbf1453da22f-Paper.pdf,2001
Fast and Robust Classification using Asymmetric,"Thispaperdevelopsanewapproachforextremelyfastdetectionindo- mainswherethedistributionofpositiveandnegativeexamplesishighly skewed (e.g. face detection or database retrieval). In such domains a cascadeofsimpleclassifierseachtrainedtoachievehighdetectionrates andmodestfalsepositiveratescanyieldafinaldetectorwithmanydesir- ablefeatures:includinghighdetectionrates,verylowfalsepositiverates, andfast performance. Achieving extremely highdetectionrates, rather thanlowerror,isnotatasktypicallyaddressedbymachinelearningal- gorithms. We propose a new variant of AdaBoost as a mechanism for trainingthesimpleclassifiersusedinthecascade. Experimentalresults in the domain of face detectionshow the trainingalgorithmyields sig- nificantimprovementsinperformanceoverconventionalAdaBoost. The final face detection system can process 15 frames per second, achieves over90%detection,andafalsepositiverateof1ina1,000,000. 1 Introduction In many applications fast classification is almost as important as accurate",Computer Vision,0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf,2001
K-Local Hyperplane and Convex Distance,"Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometricalintuitionas to whyK-NearestNeighbor(KNN)algorithms oftenperformmorepoorlythanSVMsonclassificationtasks. We then proposemodifiedK-NearestNeighboralgorithmsto overcomethe per- ceivedproblem.TheapproachissimilarinspirittoTangentDistance,but with invariancesinferredfromthe localneighborhoodratherthanprior knowledge. Experimentalresultsonrealworldclassificationtaskssug- gest that the modified KNN algorithms oftengive a dramatic improve- mentoverstandardKNNandperformaswellorbetterthanSVMs. 1 Motivation Thenotionof marginforclassificationtasks hasbeenlargelypopularizedbythesuccess oftheSupportVectorMachine(SVM)[2,15]approach. The marginofSVMshasanice geometric interpretation1: it can be defined informallyas (twice) the smallest Euclidean distancebetweenthedecisionsurfaceandtheclosesttrainingpoint. Thedecisionsurface produced by the original SVM ",Optimization & Theoretical ML,1359aa933b48b754a2f54adb688bfa77-Paper.pdf,2001
Agglomerative Multivariate Information,"The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution peA, B), this method con structs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. In a recent paper, we introduced a general princi pled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets. 1 Introduction The information bottleneck (IB) method of Tishby et al [14] is an unsupervised non parametric data organization technique. Given a joint distribution P(A, B), this method constructs a new variable T that represents partitions of A which are (locally) maximizing the mutual information",Optimization & Theoretical ML,1113d7a76ffceca1bb350bfe145467c6-Paper.pdf,2001
Thomas L.. Griffiths & Joshua B. Tenenbaum,"Estimating the parameters of sparse multinomial distributions is animportant componentofmanystatisticallearningtasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distributionas a means ofaccountingfor sparsity. We present a Bayesianapproachthat allows weakpriorknowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compres sion and estimating distributions over words in newsgroup data. 1 Introduction Sparse multinomial distributions arise in many statistical domains, including nat ural language processing and graphical models. Consequently, a number of ap proaches to parameter estimation for sparse multinomial distributions have been suggested [3]. These approaches tend to be domain-independent: they make little use ofprior knowledge about a specific domain. In many domains where multino mial distributions",NLP,16ba72172e6a4f1de54d11ab6967e371-Paper.pdf,2001
Bayesian time series classification,"This paperproposesan approachto classification of adjacentsegments ofatimeseriesasbeingeitherof (cid:0) classes.Weuseahierarchicalmodel thatconsistsofafeatureextractionstageandagenerativeclassifierwhich isbuiltontopofthesefeatures.Suchtwostageapproachesareoftenused insignalandimageprocessing.Thenovelpartofourworkisthatwelink thesestagesprobabilisticallybyusingalatentfeaturespace. Touseone jointmodelisaBayesianrequirement,whichhastheadvantagetofuse informationaccordingtoitscertainty. The classifier is implemented as hidden Markov model with Gaussian andMultinomialobservationdistributionsdefinedona suitably chosen representationofautoregressivemodels.TheMarkovdependencyismo- tivated by the assumption that successive classifications will be corre- lated. InferenceisdonewithMarkovchainMonteCarlo(MCMC)tech- niques. Weapplytheproposedapproachtosyntheticdataandtoclassi- ficationofEEGthatwasrecordedwhilethesubjectsperformeddifferent cognitivetasks. All experimentsshow that usinga latent featur",Optimization & Theoretical ML,194cf6c2de8e00c05fcf16c498adc7bf-Paper.pdf,2001
Computing Time Lower Bounds for,"Recurrent neural networks of analog units are computers for real valued functions. We study the time complexity of real computa tion in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no re strictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational lim itations that time-bounded analog recurrent neural networks are subject to. 1 Introduction Analog recurrent neural networks are known to have computational capabilities that exceed those of classical Turing machines (see, e.g., Siegelmann and Sontag, 1995; Kilian and Siegelmann, 1996; Siegelmann, 1999). Very little, however, is known about their limitations. Among the rare results in this direction, for instance, is the one of Sima and Orponen (2001) showing that continuo",Optimization & Theoretical ML,1a0a283bfe7c549dee6c638a05200e32-Paper.pdf,2001
Playing is believing:,"Weproposeanewclassificationformulti-agentlearningalgorithms,with eachleagueofplayerscharacterizedbyboththeirpossiblestrategiesand possiblebeliefs.Usingthisclassification,wereviewtheoptimalityofex- istingalgorithms,includingthecaseofinterleagueplay. Weproposean incrementalimprovementtotheexistingalgorithmsthatseemstoachieve averagepayoffsthatareatleasttheNashequilibriumpayoffsinthelong- runagainstfairopponents. 1 Introduction Thetopicoflearninginmulti-agentenvironmentshasreceivedincreasingattentionoverthe pastseveralyears.Gametheoristshavebeguntoexaminelearningmodelsintheirstudyof repeatedgames,andreinforcementlearningresearchershavebeguntoextendtheirsingle- agentlearningmodelstothemultiple-agentcase. Astraditionalmodelsandmethodsfrom thesetwofieldsareadaptedtotackletheproblemofmulti-agentlearning,thecentralissue ofoptimalityisworthrevisiting.Whatdoweexpectasuccessfullearnertodo? MatrixgamesandNashequilibrium. Fromthegametheoryperspective,therepeated game is a generalization of the trad",Reinforcement Learning,1b36ea1c9b7a1c3ad668b8bb5df7963f-Paper.pdf,2001
Probabilistic principles in unsupervised learning,"To findouthowthe representationsofstructuredvisual objectsdepend ontheco-occurrencestatisticsoftheirconstituents,weexposedsubjects toasetofcompositeimageswithtightcontrolexertedover(1)thecondi- tionalprobabilitiesoftheconstituentfragments,and(2)thevalueofBar- low’scriterionof“suspiciouscoincidence”(theratioofjointprobability totheproductofmarginals). Wethencomparedthepartverificationre- sponsetimesforvariousprobe/targetcombinationsbeforeandafterthe exposure. For composite probes, the speedupwas much larger for tar- getsthatcontainedpairsoffragmentsperfectlypredictiveofeachother, comparedto those that did not. This effect was modulatedby the sig- nificanceoftheirco-occurrenceasestimatedbyBarlow’scriterion. For lone-fragmentprobes,thespeedupinallconditionswasgenerallylower thanforcomposites.Theseresultsshedlightonthebrain’sstrategiesfor unsupervisedacquisitionofstructuralinformationinvision. 1 Motivation How does the human visual system decide for which objects it should maintain distinc",Computer Vision,1b5230e3ea6d7123847ad55a1e06fffd-Paper.pdf,2001
Predictive Representations of State,"We show that states of a dynamical system can be usefully repre sented by multi-step, action-conditional predictions of future ob servations. State representations that are groundedin data in this way may be easier to learn, generalize better, and be less depen dent on accurate prior models than, for example, POMDP state representations. Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear spe cialization of the predictive approach with the state representa tions used in POMDPs and in k-orderMarkovmodels. Oursis the first specific formulation ofthe predictive idea that includes both stochasticity and actions (controls). We showthat any system has a linear predictive state representation with number ofpredictions nogreaterthanthenumberofstatesinitsminimalPOMDPmodel. In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise. There have been two dominant approaches. The genera",Optimization & Theoretical ML,1e4d36177d71bbb3558e43af9577d70e-Paper.pdf,2001
Discriminative Direction for Kernel Classifiers,"In many scientific and engineering applications, detecting and under- standing differences between two groups of examples can be reduced toaclassicalproblemoftrainingaclassifierforlabelingnewexamples while making as few mistakes as possible. In the traditional classifi- cation setting, the resulting classifier is rarely analyzedin terms of the propertiesoftheinputdatacapturedbythediscriminativemodel. How- ever,suchanalysisiscrucialifwewanttounderstandandvisualizethe detecteddifferences.Weproposeanapproachtointerpretationofthesta- tisticalmodelintheoriginalfeaturespacethatallowsustoargueabout themodelintermsoftherelevantchangestotheinputvectors.Foreach point in the input space, we define a discriminative direction to be the directionthatmovesthepointtowardstheotherclasswhileintroducing aslittleirrelevantchangeaspossiblewithrespecttotheclassifierfunc- tion. Wederivethediscriminativedirectionforkernel-basedclassifiers, demonstratethetechniqueonseveralexamplesandbrieflydiscussitsuse inthes",Optimization & Theoretical ML,1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf,2001
Contextual Modulation of Target Saliency,"Themost popularalgorithmsfor objectdetectionrequiretheuseof exhaustivespatialandscalesearchprocedures. Insuchapproaches, an object is defined by means of local features. fu this paper we showthatincludingcontextualinformationinobjectdetectionpro cedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that theproposedschemeis able to accuratelypredict likelyobject classes, locations and sizes. 1 Introduction Although there is growing evidence ofthe role ofcontextual information in human perception [1], research in computational vision is dominated by object-based rep resentations [5,9,10,15]. In real-world scenes, intrinsic object information is often degradedduetoocclusion,lowcontrast, andpoorresolution. Insuchsituations, the object recognitionproblem based on intrinsic object representations is ill-posed. A more comprehensiverepresentationofan object should include contextualinforma tion [11,13]: Obj. representatian",Computer Vision,1f4fe6a4411edc2ff625888b4093e917-Paper.pdf,2001
On Kernel-Target Alignment,"We introduce the notion of kernel-alignment, a measure of similar ity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpre tations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of per formance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results show ing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1 Introduction Kernel base",Optimization & Theoretical ML,1f71e393b3809197ed66df836fe833e5-Paper.pdf,2001
Escaping the Convex Hull with,"Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vec tor Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while mini mizing the intra-class disparity. 1 Introduction Both intuition and theory [9] seem to support that the best linear separation be tween two classes is the one that maximizes the margin. But is this always true? In the example shown in Fig.(l), the maximum margin hyperplane is Wo; however, most observers would say that the separating hyperplane WI has better chances to generalize, as it takes into account the expected location of additional training sam- ••••••••••• f\J:-•••••••••• •• ..,- -.Q",Computer Vision,23d2e1578544b172cca332ff74bddf5f-Paper.pdf,2001
Algorithmic Luckiness,"In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used. Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypothe ses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space. 1 Introduction Statistical learning theory is mainly concerned with the study of uniform bounds on the expected error of hypotheses from a given hypothesis ",Optimization & Theoretical ML,24f0d2c90473b2bc949ad962e61d9bcb-Paper.pdf,2001
Optimising Synchronisation Times for,"With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for exam ple news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisa tion timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1 Introduction As the available bandwidth for wireless devices increases, new challenges are pre sented in the utilisation of such bandwidth. Given that always up connections are generally considered infe",Optimization & Theoretical ML,253614bbac999b38b5b60cae531c4969-Paper.pdf,2001
Spike timing and the coding of naturalistic,"Innature,animalsencounterhighdimensionalsensorystimulithathave complexstatisticalanddynamicalstructure. Attemptstostudytheneu- ralcodingofthesenaturalsignalsfacechallengesbothintheselectionof thesignalensembleandintheanalysisoftheresultingneuralresponses. Forzebrafinches,naturalisticstimulicanbedefinedassoundsthatthey encounterinacolonyofconspecificbirds.Weassembledanensembleof thesesoundsbyrecordinggroupsof10-40zebrafinches,andthenana- lyzedtheresponseofsingleneuronsinthesongbirdcentralauditoryarea (field L) to continuousplayback of long segments from this ensemble. Followingmethodsdevelopedintheflyvisualsystem,wemeasuredthe information that spike trains provide about the acoustic stimulus with- out any assumptions about which features of the stimulus are relevant. Preliminaryresultsindicatethatlargeamountsofinformationarecarried by spike timing, with roughly half of the information accessible only at timeresolutionsbetterthan10ms; additionalinformationis still be- ingrevealedastimere",Computer Vision,2557911c1bf75c2b643afb4ecbfc8ec2-Paper.pdf,2001
A Variational Approach to Learning Curves,"We combine the replica approachfrom statistical physics with a varia- tional approach to analyze learning curves analytically. We apply the methodtoGaussianprocessregression. Asamainresultwederiveap- proximativerelationsbetweenempiricalerrormeasures,thegeneraliza- tionerrorandtheposteriorvariance. 1 Introduction Approximate expressions for generalization errors for finite dimensional statistical data models can be often obtained in the large data limit using asymptotic expansions. Such methodscanyieldapproximaterelationsforempiricalandtrueerrorswhichcanbeused to assess the quality of the trained model see e.g. [1]. Unfortunately, such an approxi- mation scheme does not seem to be easily applicable to popular non-parametric models likeGaussianprocess(GP)modelsandSupportVectorMachines(SVMs). We applythe replica approach of statistical physics to asses the average case learning performanceof these kernel machines. So far, the tools of statistical physics have been successfully ap- pliedto",Optimization & Theoretical ML,26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf,2001
Why neuronal dynamics should control,"Hebbian learning rules are generally formulated as static rules. Un der changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more ro bust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by post synaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the additive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type",Optimization & Theoretical ML,277a78fc05c8864a170e9a56ceeabc4c-Paper.pdf,2001
Latent Dirichlet Allocation,"We propose a generative model for text and other collections of dis crete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hof mann's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present em pirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1 Introduction Recent years have seen the development and successful application of several latent factor models for discrete data. One notable example, Hofmann's pLSI/aspect model [3], has received the attention of many researchers, and applications have emerged in text modeling [3], c",NLP,296472c9542ad4d4788d543508116cbc-Paper.pdf,2001
Efficiency versus Convergence of Boolean,"WestudyonlinelearninginBooleandomainsusingkernelswhichcap- turefeatureexpansionsequivalenttousingconjunctionsoverbasicfea- tures. We demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization abil- ity of the resulting classifier. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Percep- tronalgorithmoveranexponentialnumberofconjunctions;howeverwe also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple func- tions. Wealsoconsiderananalogoususeofkernelfunctionstorunthe multiplicative-updateWinnowalgorithmoveranexpandedfeaturespace ofexponentiallymanyconjunctions. Whileknownupperboundsimply thatWinnowcanlearnDNFformulaewithapolynomialmistakebound inthissetting,weprovethatitiscomputationallyhardtosimulateWin- now’s behavior for ",Computer Vision,2cad8fa47bbef282badbb8de5374b894-Paper.pdf,2001
A Bayesian Network for Real-Time,"We describe a computer system that provides a real-time musi cal accompaniment for a live soloist in a piece of non-improvised music for soloist and accompaniment. A Bayesian network is devel oped that represents the joint distribution on the times at which the solo and accompaniment notes are played, relating the two parts through a layer of hidden variables. The network is first con structed using the rhythmic information contained in the musical score. The network is then trained to capture the musical interpre tations of the soloist and accompanist in an off-line rehearsal phase. During live accompaniment the learned distribution of the network is combined with a real-time analysis of the soloist's acoustic sig nal, performed with a hidden Markov model, to generate a musi cally principled accompaniment that respects all available sources of knowledge. A live demonstration will be provided. 1 Introduction We discuss our continuing work in developing a computer system that plays the ",Computer Vision,2b0f658cbffd284984fb11d90254081f-Paper.pdf,2001
Prodding the ROC Curve: Constrained,"When designing a two-alternative classifier, one ordinarily aims to maximize the classifier’s ability to discriminate between members of the two classes. We describe a situation in a real-world business application of machine-learning prediction in which an additional constraint is placed on the nature of the solu- tion: that the classifier achieve a specified correct acceptance or correct rejection rate (i.e., that it achieve a fixed accuracy on members of one class or the other). Our domain is predicting churn in the telecommunications industry. Churn refers to customers who switch from one service provider to another. We pro- pose four algorithms for training a classifier subject to this domain constraint, and present results showing that each algorithm yields a reliable improvement in performance. Although the improvement is modest in magnitude, it is nonethe- less impressive given the difficulty of the problem and the financial return that it achieves to the service provider. When",Optimization & Theoretical ML,2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf,2001
Natural Language Grammar Induction using a,"Thispaperpresentsanovelapproachtotheunsupervisedlearningofsyn- tacticanalysesofnaturallanguagetext.Mostpreviousworkhasfocused onmaximizinglikelihoodaccordingtogenerative PCFGmodels.Incon- trast,weemployasimpler probabilisticmodelovertreesbaseddirectly on constituent identity and linear context, and use an EM-like iterative proceduretoinducestructure. Thismethodproducesmuchhigherqual- ityanalyses,givingthebestpublishedresultsonthe ATISdataset. 1 Overview Toenableawiderangeofsubsequenttasks,humanlanguagesentencesarestandardlygiven tree-structure analyses, wherein the nodes in a tree dominate contiguousspans of words called constituents, as in figure1(a). Constituents are the linguistically coherentunits in thesentence,andareusuallylabeledwithaconstituentcategory,suchasnounphrase(NP) orverbphrase(VP). Anaimofgrammarinductionsystemsistofigureout,givenjustthe sentencesinacorpusS,whattreestructurescorrespondtothem.Inthissense,thegrammar inductionproblemis an incompletedataproblem, wherethe c",NLP,2d00f43f07911355d4151f13925ff292-Paper.pdf,2001
Estimating Car Insurance Premia:,"Estimating insurance premia from data is a difficult regression problemfor severalreasons: thelargenumberofvariables, manyof which are.discrete, and the very peculiar shape ofthe noise distri bution, asymmetricwithfat tails, withalargemajorityzerosanda few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that func tion approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Comparedmethods include decisiontrees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reducethe median premium by chargingmore tothe most risky customers. 1 Introduction The mainmathematical problemfaced by actuaries is that ofestimatinghowmuch eachinsurancecontractisexpectedtocost. Thisconditional",Optimization & Theoretical ML,2d405b367158e3f12d7c1e31a96b3af3-Paper.pdf,2001
Classifying Single Trial EEG:,"Drivenbytheprogressinthefieldofsingle-trialanalysisofEEG,thereis agrowinginterestinbraincomputerinterfaces(BCIs),i.e.,systemsthat enablehumansubjectstocontrolacomputeronlybymeansoftheirbrain signals. Inapseudo-onlinesimulationourBCIdetectsupcomingfinger movementsinanaturalkeyboardtypingconditionandpredictstheirlat- erality. This can be doneon average 100–230ms before the respective keyisactuallypressed,i.e.,longbeforetheonsetofEMG.Ourapproach isappealingforitsshortresponsetimeandhighclassificationaccuracy (>96%)ina binarydecisionwherenohumantrainingis involved. We comparediscriminativeclassifierslikeSupportVectorMachines(SVMs) anddifferentvariantsofFisherDiscriminantthatpossessfavorablereg- ularizationpropertiesfordealingwith highnoise cases (inter-trialvari- ablity). 1 Introduction The online analysis of single-trial electroencephalogram(EEG) measurements is a chal- lengeforsignalprocessingandmachinelearning. Oncethehighinter-trialvariability(see Figure 1) of this complex multivariate",NLP,2d579dc29360d8bbfbb4aa541de5afa9-Paper.pdf,2001
Kernel Logistic Regression and the Import,"Thesupportvectormachine(SVM)isknownforitsgoodperformancein binaryclassification,butitsextensiontomulti-classclassificationisstill an on-goingresearch issue. In this paper, we propose a new approach forclassification,calledtheimportvectormachine(IVM),whichisbuilt onkernellogisticregression(KLR).WeshowthattheIVMnotonlyper- formsaswellastheSVMinbinaryclassification,butalsocannaturally begeneralizedtothemulti-classcase.Furthermore,theIVMprovidesan estimateoftheunderlyingprobability.Similartothe“supportpoints”of theSVM,theIVMmodelusesonlyafractionofthetrainingdatatoindex kernelbasisfunctions,typicallyamuchsmallerfractionthantheSVM. ThisgivestheIVMacomputationaladvantageovertheSVM,especially whenthesizeofthetrainingdatasetislarge. 1 Introduction Instandardclassificationproblems, we aregivena set oftrainingdata (cid:0)(cid:2) (cid:1)(cid:4) (cid:3)(cid:6) (cid:5)(cid:8) (cid:7)(cid:9) (cid:3)(cid:11) (cid:10) , (cid:0)(cid:12) (cid:1)(cid:14) (cid:13)(cid:15) (cid:5)(cid:16) (cid:7)(cid:15) (",Optimization & Theoretical ML,2eace51d8f796d04991c831a07059758-Paper.pdf,2001
A Model of the Phonological Loop:,"We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phono logical loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially ar bitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural rep resentations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexi ble, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel se quences after modera",NLP,312351bff07989769097660a56395065-Paper.pdf,2001
Thin Junction Trees,"Wepresentanalgorithmthatinducesaclassofmodelswiththinjunction trees—models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junctiontreeisthin,inferenceinourmodelsremainstractablethroughout the learning process. This allows both an efficient implementation of aniterativescalingparameterestimationalgorithmandalsoensuresthat inferencecanbeperformedefficientlywiththefinalmodel.Weillustrate theapproachwithapplicationsinhandwrittendigitrecognitionandDNA splicesitedetection. Introduction Many learning problems in complex domains such as bioinformatics, vision, and infor- mationretrievalinvolvelargecollectionsofinterdependentvariables,noneofwhichhasa privilegedstatusasaresponsevariableorclasslabel. Insuchproblems,thegoalisgener- allythatofcharacterizingtheprincipaldependenciesinthedata,aproblemwhichisoften castwithintheframeworkofmultivariatedensityestimation.Simplemodelsareoftenpre- ferredinthissetting, bothfortheir",Optimization & Theoretical ML,325995af77a0e8b06d1204a171010b3a-Paper.pdf,2001
Linking motor learning to function,"Reaching movements require the brain to generate motor com- mandsthatrelyonaninternal modelofthetask’sdynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Us- ing a framework from function approximation, we argue that the sequence of errors should re(cid:13)ect the process of gradient descent. If so,thenthesequenceoferrorsshouldobeyhiddenstatetransitions of a simple dynamical system. Fitting the system to human data, we (cid:12)nd a surprisingly good (cid:12)t accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis ele- ments used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force (cid:12)eld, and in a random sequenceofforce(cid:12)eldswherelearningisnotpossible. Remarkably, we (cid",Optimization & Theoretical ML,3683af9d6f6c06acee72992f2977f67e-Paper.pdf,2001
Switch Packet Arbitration via Queue-Learning,"Inpacketswitches,packetsqueueatswitchinputsandcontendforout- puts. The contention arbitration policy directly affects switch perfor- mance. The best policydepends on the current state of the switch and current traffic patterns. This problem is hard because the state space, possible transitions, and set of actions all grow exponentially with the size of the switch. We present a reinforcementlearningformulationof the problemthat decomposesthe value functioninto many small inde- pendentvaluefunctionsandenablesanefficientactionselection. 1 Introduction Reinforcementlearning(RL)hasbeenappliedtoresourceallocationproblemsintelecom- munications. e.g., channel allocation in wireless systems, network routing, and admis- sioncontrolintelecommunicationnetworks[1,3, 7,11]. Thesehavedemonstratedrein- forcementlearningcanfindgoodpoliciesthatsignificantlyincreasetheapplicationreward withinthedynamicsofthetelecommunicationsproblems. However,akeyissueishowto scaletheseproblemswhenthestatespacegrowsquick",Optimization & Theoretical ML,378a063b8fdb1db941e34f4bde584c7d-Paper.pdf,2001
A Parallel Mixture of SVMs for Very Large Scale,"Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their train ing algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database, yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples). In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1 Introduction Recently a lot of work has been done around Support Vector Machines [9], mainly due to their impressive generalization performances on cla",Optimization & Theoretical ML,36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf,2001
Multi Dimensional ICA to Separate,"We present a new method for the blind separation of sources, which do not fulfill the independence assumption. In contrast to standard methods we consider groups of neighboring samples (""patches"") within the observed mixtures. First we extract independent features from the observed patches. It turns out that the average dependencies between these features in different sources is in general lower than the dependencies be tween the amplitudes of different sources. We show that it might be the case that most of the dependencies is carried by only a small number of features. Is this case - provided these features can be identified by some heuristic - we project all patches into the subspace which is orthogonal to the subspace spanned by the ""correlated"" features. Standard ICA is then performed on the elements of the transformed patches (for which the independence assumption holds) and ro bustly yields a good estimate of the mixing matrix. 1 Introduction ICA as a method for blind source sep",Optimization & Theoretical ML,3875115bacc48cca24ac51ee4b0e7975-Paper.pdf,2001
Effective size of receptive fields of inferior,"Inferiortemporalcortex(IT)neuronshavelargereceptivefieldswhena singleeffectiveobjectstimulusisshownagainstablankbackground,but havemuchsmallerreceptivefieldswhentheobjectisplacedinanatural scene.Thus,translationinvariantobjectrecognitionisreducedinnatural scenes,andthismayhelpobjectselection. We describeamodelwhich accountsforthisbycompetitionwithinanattractorinwhichtheneurons are tuned to different objects in the scene, and the fovea has a higher cortical magnification factor than the peripheral visual field. Further- more,weshowthattop-downobjectbiascanincreasethereceptivefield size,facilitatingobjectsearchincomplexvisualscenes,andprovidinga modelofobject-basedattention. Themodelleadstothepredictionthat introductionofasecondobjectintoascenewithblankbackgroundwill reducethereceptivefieldsizetovaluesthatdependontheclosenessof thesecondobjecttothetargetstimulus. Wesuggestthatmechanismsof thistypeenabletheoutputofITtobeprimarilyaboutoneobject,sothat theareas thatreceivefromITcanselect th",Computer Vision,3937230de3c8041e4da6ac3246a888e8-Paper.pdf,2001
A kernel method for multi-labelled classification,"ThisarticlepresentsaSupportVectorMachine(SVM)likelearningsys- temtohandlemulti-labelproblems. Suchproblemsareusuallydecom- posedintomanytwo-classproblemsbuttheexpressivepowerofsucha systemcanbeweak[5,7].Weexploreanewdirectapproach.Itisbased on a large margin ranking system that shares a lot of common proper- ties with SVMs. We tested it on a Yeast gene functional classification problemwithpositiveresults. 1 Introduction Manyproblemsin Text Miningor Bioinformaticsare multi-labelled. That is, eachpoint in a learning set is associated to a set of labels. Consider for instance the classification task of determining the subjects of a document, or of relating one protein to its many effectsonacell. Ineithercase,thelearningtaskwouldbetooutputasetoflabelswhose size is not known in advance: one document can for instance be about food, meat and finance,althoughanotheronewouldconcernonlyfoodandfat. Two-classandmulti-class classification or ordinal regression problems can all be cast into multi-la",NLP,39dcaf7a053dc372fbc391d4e6b5d693-Paper.pdf,2001
Learning Lateral Interactions for,"We present a new approach to the supervised learning of lateral inter- actionsforthecompetitivelayermodel(CLM)dynamicfeaturebinding architecture.Themethodisbasedonconsistencyconditions,whichwere recentlyshowntocharacterizetheattractorstatesofthislinearthreshold recurrentnetwork.Foragivensetoftrainingexamplesthelearningprob- lemisformulatedasaconvexquadraticoptimizationprobleminthelat- eralinteractionweights.Anefficientdimensionreductionofthelearning problemcan beachievedbyusinga linearsuperpositionofbasis inter- actions. Weshowthesuccessfulapplicationofthemethodtoamedical imagesegmentationproblemoffluorescencemicroscopecellimages. 1 Introduction Featurebindinghasbeenproposedtoprovideelegantsolutionstrategiestothesegmenta- tion problemin perception[11, 12, 14]. A lot of feature bindingmodels have thus tried toreproducegropingmechanismsliketheGestaltlawsofvisualperception,e.g. connect- ednessandgoodcontinuation,usingtemporalsynchronization[12]orspatialcoactivation [9,14]forbinding. Quite",Computer Vision,3a824154b16ed7dab899bf000b80eeee-Paper.pdf,2001
Very loopy belief propagation for,"Since the discovery that the best error-correcting decoding algo rithm can be viewed as belief propagation in a cycle-bound graph, researchers have been trying to determine under what circum stances ""loopy belief propagation"" is effective for probabilistic infer ence. Despite several theoretical advances in our understanding of loopy belief propagation, to our knowledge, the only problem that has been solved using loopy belief propagation is error-correcting decoding on Gaussian channels. We propose a new representation for the two-dimensional phase unwrapping problem, and we show that loopy belief propagation produces results that are superior to existing techniques. This is an important result, since many imag ing techniques, including magnetic resonance imaging and interfer ometric synthetic aperture radar, produce phase-wrapped images. Interestingly, the graph that we use has a very large number of very short cycles, supporting evidence that a large minimum cycle length is not need",Computer Vision,3b92d18aa7a6176dd37d372bc2f1eb71-Paper.pdf,2001
Modularity in the motor system: decomposition,"Thequestionofwhetherthenervoussystemproducesmovementthrough thecombinationofafewdiscreteelementshaslongbeencentraltothe study of motor control. Muscle synergies, i.e. coordinatedpatterns of muscleactivity,havebeenproposedaspossiblebuildingblocks.Herewe proposeamodelbasedoncombinationsofmusclesynergieswithaspe- cificamplitudeandtemporalstructure. Time-varyingsynergiesprovide arealisticbasisforthedecompositionofthecomplexpatternsobserved innaturalbehaviors. Toextracttime-varyingsynergiesfromsimultane- ousrecordingofEMGactivitywedevelopedanalgorithmwhichextends existingnon-negativematrixfactorizationtechniques. 1 Introduction Inordertoproducemovement,everyvertebratehastocoordinatethelargenumberofde- greesoffreedominthemusculoskeletalapparatus.Howthiscoordinationisaccomplished by the central nervous system is a long standing question in the study of motor control. Accordingtoonecommonproposal,thistaskmightbesimplifiedbyamodularorganiza- tionoftheneuralsystemscontrollingmovement[1,2,3,4].In",Optimization & Theoretical ML,3d863b367aa379f71c7afc0c9cdca41d-Paper.pdf,2001
The Method of Quantum Clustering,"Weproposeanovelclusteringmethodthatisanextensionofideasinher- enttoscale-spaceclusteringandsupport-vectorclustering. Likethelat- ter,itassociateseverydatapointwithavectorinHilbertspace,andlike theformeritputsemphasisontheirtotalsum,thatisequaltothescale- spaceprobabilityfunction. Thenoveltyofourapproachisthestudyof anoperatorinHilbertspace,representedbytheSchro¨dingerequationof whichtheprobabilityfunctionisasolution. ThisSchro¨dingerequation contains a potential function that can be derived analytically from the probabilityfunction. We associate minimaofthe potentialwith cluster centers.Themethodhasonevariableparameter,thescaleofitsGaussian kernel.Wedemonstrateitsapplicabilityonknowndatasets. Bylimiting theevaluationoftheSchro¨dingerpotentialtothelocationsofdatapoints, wecanapplythismethodtoproblemsinhighdimensions. 1 Introduction Methodsofdataclusteringareusuallybasedongeometricorprobabilisticconsiderations [1,2,3]. Theproblemofunsupervisedlearningofclustersbasedonlocationsofpointsin ",Optimization & Theoretical ML,3e9e39fed3b8369ed940f52cf300cf88-Paper.pdf,2001
Generating velocity tuning by asymmetric,"Asymmetriclateralconnectionsareonepossiblemechanismthatcanac- countforthedirectionselectivityofcorticalneurons.Wepresentamath- ematical analysis for a class of these models. Contrasting with earlier theoreticalworkthathasreliedonmethodsfromlinearsystems theory, westudythenetwork’snonlineardynamicpropertiesthatarisewhenthe threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that areappropriateformodelingtheresponsesofdirectionselectivecortical neurons. In addition, our analysis shows that outside a certain regime ofstimulusspeedsthestabilityofthissolutionsbreaksdowngivingrise to another class of solutions that are characterized by specific spatio- temporalperiodicity.Thispredictsthatifdirectionselectivityinthecor- texismainlyachievedbyasymmetriclateralconnectionslurchingactiv- itywavesmightbeobservableinensemblesofdirectionselectivecortical neuronswithinappropriateregimesofthestimulusspeed. 1 Introduction",Optimization & Theoretical ML,3f088ebeda03513be71d34d214291986-Paper.pdf,2001
Incremental Learning and Selective,"We propose a framework based on a parametric quadratic program ming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias prob lem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other ex isting technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the com plete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selec tive Sampling techniques may further boost convergence. 1 Introduction SVM training is a convex optimization problem which scales with the training set size rather t",Optimization & Theoretical ML,405e28906322882c5be9b4b27f4c35fd-Paper.pdf,2001
Bayesian Predictive Proflles with,"Massive transaction data sets are recorded in a routine manner in telecommunications, retail commerce, and Web site management. In this paper we address the problem of inferring predictive in- dividual proflles from such historical transaction data. We de- scribe a generative mixture model for count data and use an an approximate Bayesian estimation framework that efiectively com- bines an individual’s speciflc history with more general population patterns. We use a large real-world retail transaction data set to illustrate how these proflles consistently outperform non-mixture and non-Bayesian techniques in predicting customer behavior in out-of-sample data. 1 Introduction Transactiondatasetsconsistofrecordsofpairsofindividualsandevents,e.g.,items purchased (market basket data), telephone calls made (call records), or Web pages visited (from Web logs). Of signiflcant practical interest in many applications is the ability to derive individual-speciflc (or personalized) models for each ",NLP,44cd7a8f7f9f85129b9953950665064d-Paper.pdf,2001
A Rational Analysis of Cognitive Control,"We areinterestedinthemechanismsbywhichindividualsmonitorand adjusttheirperformanceofsimplecognitivetasks. Wemodelaspeeded discriminationtaskinwhichindividualsareaskedtoclassifyasequence of stimuli (Jones & Braver, 2001). Response conflict arises when one stimulus class is infrequentrelative to another,resultingin more errors andslowerreactiontimesfortheinfrequentclass. Howdocontrolpro- cesses modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individualsasminimizingacostthatdependsbothonerrorrateandre- actiontime. With twoadditionalassumptionsofrationality—thatclass priorprobabilitiesareaccuratelyestimatedandthatinferenceisoptimal subjectto limitationsonrate ofinformationtransmission—weobtaina goodfittooverallRTanderrordata,aswellastrial-by-trialvariationsin performance. Considerthefollowingscenario:Whiledriving,youapproachanintersectionatwhichthe trafficlighthasalreadyturnedyellow,signalingthatitisaboutt",Reinforcement Learning,4a3e00961a08879c34f91ca0070ea2f5-Paper.pdf,2001
Learning spike-based correlations and,"We have designed and fabricated a VLSI synapse that can learn a conditional probability or correlation between spike-based inputs and feedback signals. The synapse is low power, compact, provides nonvolatile weight storage, and can perform simultaneous multipli- cation and adaptation. We can calibrate arrays of synapses to en- sure uniform adaptation characteristics. Finally, adaptation in our synapse does not necessarily depend on the signals used for com- putation. Consequently, our synapse can implement learning rules that correlate past and present synaptic activity. We provide analy- sis and experimental chip results demonstrating the operation in learning and calibration mode, and show how to use our synapse to implement various learning rules in silicon. 1 Introduction Computation with conditional probabilities and correlations underlies many models of neurally inspired information processing. For example, in the sequence-learning neural network models proposed by Levy [1], syna",Computer Vision,4afd521d77158e02aed37e2274b90c9c-Paper.pdf,2001
Spectral Kernel Methods for Clustering,"In this paper we introduce new algorithms for unsupervised learn ing based on the use of a kernel matrix. All the information re quired by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but re lated cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clus ters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1 Introduction Kernel based learning provides a modular approach to learning system desig",Optimization & Theoretical ML,4ba29b9f9e5732ed33761840f4ba6c53-Paper.pdf,2001
A Natural Policy Gradient,"We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the param eter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradi ent is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sut ton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1 Introduction There has been a growing interest in direct policy-gradient methods for approximate planning in large Markov decision problems (MDPs). Such methods seek to find a good policy 7r among some restricted class of policies, by following the gradient of the future reward. Unfortunately, the standard gradient descent rule is non covariant. Crudely spea",Optimization & Theoretical ML,4b86abe48d358ecf194c56c69108433e-Paper.pdf,2001
Batch Value Function Approximation via,"We present three ways of combining linear programming with the kerneltricktofind valuefunction approximationsfor reinforcement learning. Oneformulation is based on SVM regression; the second isbasedontheBellmanequation; andthethirdseeksonlytoensure that good moves have an advantage over bad moves. All formu lations attempt to minimize the number of support vectors while fittingthedata. Experimentsina difficult, syntheticmazeproblem showthatallthreeformulationsgiveexcellentperformance,butthe advantageformulation is much easier to train. Unlike policygradi ent methods, the kernel methods described here can easily'adjust the complexity ofthe function approximator to fit the complexity ofthe value function. 1 Introduction Virtually all existing work on value function approximation and policy-gradient methods starts with a parameterized formula for the value function or policy and thenseekstofindthebestpolicythatcanberepresentedinthatparameterizedform. This can give rise to very difficult s",Reinforcement Learning,4ba3c163cd1efd4c14e3a415fa0a3010-Paper.pdf,2001
PAC Generalization Bounds for Co-training,"The rule-based bootstrapping introduced by Yarowsky, and its co- trainingvariantbyBlumandMitchell,havemetwithconsiderableem- piricalsuccess. Earlierworkonthetheoryofco-traininghasbeenonly looselyrelatedtoempiricallyusefulco-trainingalgorithms.Herewegive a newPAC-style boundongeneralizationerrorwhichjustifies boththe useofconfidences— partialrulesandpartiallabelingoftheunlabeled data — and the use of an agreement-based objective function as sug- gestedbyCollinsandSinger. Ourboundsapplytothemulticlasscase, i.e.,whereinstancesaretobeassignedoneof (cid:0) labelsfor (cid:0)(cid:2) (cid:1)(cid:4) (cid:3) . 1 Introduction In this paper, we study bootstrapping algorithms for learning from unlabeled data. The generalideainbootstrappingistousesomeinitiallabeleddatatobuilda(possiblypartial) predictivelabelingprocedure;thenusethelabelingproceduretolabelmoredata;thenuse the newly labeled data to build a new predictive procedureand so on. This process can beiterateduntilafixedpointisreachedorsomeoth",NLP,4c144c47ecba6f8318128703ca9e2601-Paper.pdf,2001
Face Recognition Using Kernel Methods,"Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recog nition, andtracking. Therepresentationinthesesubspacemethods is based on second order statistics ofthe image set, and does not address higher order statistical dependencies such as the relation ships amongthreeormorepixels. RecentlyHigher OrderStatistics andIndependent Component Analysis (ICA) havebeenused as in formative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Compo nent Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representationsfor face recognition, which we call Kernel Eigenface and Kernel Fisherfacemethods. While Eigenface and Fisherface methods aim to find projection directions based on the secondorder correlationofsamples, Kernel Eigenface and Ker nel Fisherface methods provide generalizations which take higher order correlations into account. We compar",Computer Vision,4d6b3e38b952600251ee92fe603170ff-Paper.pdf,2001
"Keywords:portfoliomanagement,financialforecasting,recurrentneuralnetworks.","Thispaperdealswithaneuralnetworkarchitecturewhichestablishesa portfoliomanagementsystemsimilartotheBlack/Littermanapproach. This allocationscheme distributes funds across various securities or fi- nancialmarketswhilesimultaneouslycomplyingwithspecificallocation constraintswhichmeettherequirementsofaninvestor. Theportfoliooptimizationalgorithmismodeledbyafeedforwardneural network. The underlying expected return forecasts are based on error correctionneuralnetworks(ECNN),whichutilizethelastmodelerroras anauxiliaryinputtoevaluatetheirownmisspecification. Theportfoliooptimizationisimplementedsuchthat(i.) theallocations comply with investor’s constraints and that (ii.) the risk of the portfo- liocanbecontrolled. We demonstratetheprofitabilityofourapproach byconstructinginternationallydiversifiedportfoliosacross (cid:1)(cid:3) (cid:2) different financial markets of the G7 contries. It turns out, that our approach is superiortoapresetbenchmarkportfolio. 1 Introduction: Portfolio-Management We",Optimization & Theoretical ML,4d8556695c262ab91ff51a943fdd6058-Paper.pdf,2001
Generalization Performance of Some Learning,"We investigate the generalization performance of some learning prob- lemsinHilbertfunctionalSpaces.Weintroduceanotionofconvergence oftheestimatedfunctionalpredictortothebestunderlyingpredictor,and obtainanestimateontherateoftheconvergence. Thisestimateallows ustoderivegeneralizationboundsonsomelearningformulations. 1 Introduction Inmachinelearning,ourgoalisoftentopredictanunobservedoutputvalue (cid:0) basedonan observedinputvector (cid:1) . This requiresus to estimate a functionalrelationship (cid:0)(cid:3) (cid:2)(cid:5) (cid:4)(cid:7) (cid:6) (cid:1)(cid:9) (cid:8) from a set of example pairs of (cid:6) (cid:1)(cid:7) (cid:10) (cid:0) (cid:8) . Usually the quality of the predictor (cid:4)(cid:11) (cid:6) (cid:1)(cid:9) (cid:8) can be measuredby a loss function (cid:12) (cid:6)(cid:13)(cid:4)(cid:7) (cid:6) (cid:1)(cid:14) (cid:8)(cid:15) (cid:10) (cid:0) (cid:8) that is problemdependent. In machinelearning, weassumethatthedata (cid:6) (cid:1)(cid:16) (cid:10) (cid:0) (cid:8) aredrawn",Optimization & Theoretical ML,4e62e752ae53fb6a6eebd0f6146aa702-Paper.pdf,2001
Analog Soft-Pattern-Matching Classifier,"A flexible pattern-matching analog classifier is presented in con- junction with a robust image representation algorithm called Prin- cipal Axes Projection (PAP). In the circuit, the functional form of matching is configurable in terms of the peak position, the peak height and the sharpness of the similarity evaluation. The test chip was fabri- cated in a 0.6-m m CMOS technology and successfully applied to hand-written pattern recognition and medical radiograph analysis using PAP as a feature extraction pre-processing step for robust image coding. The separation and classification of overlapping patterns is also ex- perimentally demonstrated. 1 Introduction Pattern classification using template matching techniques is a powerful tool in im- plementing human-like intelligent systems. However, the processing is computa- tionally very expensive, consuming a lot of CPU time when implemented as soft- ware running on general-purpose computers. Therefore, software approaches are not practical ",Computer Vision,51174add1c52758f33d414ceaf3fe6ba-Paper.pdf,2001
On the Concentration of Spectral,"We consider the problem of measuring the eigenvalues of a ran domly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Fur thermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results. 1 Introduction A number of learning algorithms rely on estimating spectral data on a sample of training points and using this data as input to further analyses. For example in Principal Component Analysis (PCA) the subspace spanned by the first k eigen vectors is used to give a k dimensional model of the data with minimal residual, hence forming a low dimensional representation of the data for analysis or clus tering. Recently the approach has been applied in kernel defined feature spaces in what has become known as kernel-PCA [5]. This representation has also been related to an Information Retrieval algorithm known as l",Optimization & Theoretical ML,5227b6aaf294f5f027273aebf16015f2-Paper.pdf,2001
Generalizable Relational Binding from,"We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctiverepresentationsinsteadofmorepopulartemporalsynchrony mechanisms. Supportersof temporal synchronyarguethat conjunctive representationslackbothefficiency(i.e.,combinatorialnumbersofunits are required) and systematicity (i.e., the resulting representations are overly specific and thus do not support generalization to novel exem- plars). To counter these claims, we show that our model: a) uses far fewerhiddenunitsthanthenumberofconjunctionsrepresented,byus- ingcoarse-coded,distributedrepresentationswhereeachunithasabroad tuningcurvethroughhigh-dimensionalconjunctionspace,andb)isca- pableofconsiderablegeneralizationtonovelinputs. 1 Introduction The binding problem as it is classically conceived arises when different pieces of infor- mation are processed by entirely separate units. For example, we can imagine there are neuronsthatseparatelyco",Computer Vision,519c84155964659375821f7ca576f095-Paper.pdf,2001
Stochastic Mixed-Signal VLSI Architecture for,"Amixed-signalparadigmispresentedforhigh-resolutionparallelinner- productcomputationinveryhighdimensions, suitable forefficientim- plementationofkernelsinimageprocessing.Atthecoreoftheexternally digitalarchitectureisahigh-density,low-poweranalogarrayperforming binary-binarypartialmatrix-vectormultiplication.Fulldigitalresolution ismaintainedevenwithlow-resolutionanalog-to-digitalconversion,ow- ingtorandomstatistics intheanalogsummationofbinaryproducts. A random modulation scheme produces near-Bernoulli statistics even for highlycorrelatedinputs.Theapproachisvalidatedwithrealimagedata, andwithexperimentalresultsfromaCID/DRAManalogarrayprototype in0.5 (cid:2) mCMOS. 1 Introduction Analogcomputationalarrays[1,2,3,4]forneuralinformationprocessingofferverylarge integration density and throughputas needed for real-time tasks in computer vision and patternrecognition[5]. Despitethesuccessofadaptivealgorithmsandarchitecturesinre- ducingtheeffectofanalogcomponentmismatchandnoiseonsystemperforman",Computer Vision,5352696a9ca3397beb79f116f3a33991-Paper.pdf,2001
Small-World Phenomena and the,No abstract found,Optimization & Theoretical ML,52dbb0686f8bd0c0c757acf716e28ec0-Paper.pdf,2001
The Emergence of Multiple Movement Units in,"Tangential hand velocity profiles of rapid human arm movements of- tenappearassequencesofseveralbell-shapedacceleration-deceleration phasescalledsubmovementsormovementunits. Thissuggestshowthe nervoussystemmightefficientlycontrolamotorplantinthepresenceof noiseandfeedbackdelay.Anothercriticalobservationisthatstochastic- ity ina motorcontrolproblemmakesthe optimalcontrolpolicyessen- tiallydifferentfromtheoptimalcontrolpolicyforthedeterministiccase. We useasimplifieddynamicmodelofanarmandaddressrapidaimed armmovements.Weusereinforcementlearningasatooltoapproximate the optimal policy in the presence of noise and feedbackdelay. Using a simplified modelwe show that multiple submovementsemergeas an optimalpolicyinthepresenceofnoiseandfeedbackdelay. Theoptimal policyinthissituationistodrivethearm’sendpointclosetothetargetby onefastsubmovementandthenapplyafewslowsubmovementstoaccu- ratelydrivethearm’sendpointintothetargetregion.Inoursimulations, thecontrollersometimesgeneratescorrectivesubmove",Reinforcement Learning,54ff9e9e3a2ec0300d4ce11261f5169f-Paper.pdf,2001
Pranking with Ranking,"We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k. Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance's true rank. We describe a simple and efficient online al gorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outper forms online algorithms for regression and classification applied to ranking. 1 Introduction The ranking problem we discuss in this paper shares common properties with both classification and regression problems. As in classification problems the goal is to assign one of k possible labels to a new instance. Similar to regression problems, the set of k labels is structured as there is a total order relation be",Optimization & Theoretical ML,5531a5834816222280f20d1ef9e95f69-Paper.pdf,2001
Variance Reduction Techniques for Gradient,"We consider the use of two additive control variate methods to reduce the variance of performancegradient estimates in reinforcement learn- ing problems. The first approach we consider is the baseline method, inwhichafunctionofthecurrentstateisaddedtothediscountedvalue estimate. We relate theperformanceofthesemethods, whichuse sam- plepaths, tothevarianceofestimates basedoniiddata. We derivethe baselinefunctionthatminimizesthisvariance,andweshowthatthevari- anceforanybaselineisthesumoftheoptimalvarianceandaweighted squareddistancetotheoptimalbaseline. Weshowthatthewidelyused averagediscountedvaluebaseline(wheretherewardis replacedbythe difference between the reward and its expectation) is suboptimal. The second approachwe consideris the actor-critic method, which uses an approximate value function. We give bounds on the expected squared errorofitsestimates.Weshowthatminimizingdistancetothetruevalue functionissuboptimalingeneral;weprovideanexampleforwhichthe truevaluefunctiongivesanesti",Reinforcement Learning,584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf,2001
Asymptotic Universality for Learning,"Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal general ization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1 Introduction Powerful systems for data inference, like neural networks implement complex input output relations by learning from example data. The price one has to pay for the flexibility of these models is the need to choose the proper model complexity for a given task, i.e. the system architecture which gives good generalization ability for novel data. This has become an important problem also for support vector machines [1]. The main advantage of SVMs is that",Optimization & Theoretical ML,5a1e3a5aede16d438c38862cac1a78db-Paper.pdf,2001
Active Information Retrieval,"In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the sys tem responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of infor mation that can be presented to the user per iteration. 1 Introduction An IR system consists of a collection of documents and an engine",Optimization & Theoretical ML,5a7f963e5e0504740c3a6b10bb6d4fa5-Paper.pdf,2001
Tempo Tracking Rhythm,"We present a probabilistic generative model for timing deviations in expressive music.performance. The structure of the proposed model is equivalent to a switching state space model. We formu late two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as fil tering and maximum a posteriori (MAP) state estimation tasks. The inferences are carried out using sequential Monte Carlo in tegration (particle filtering) techniques. For this purpose, we have deriveda novelViterbi algorithmfor Rao-Blackwellizedparticlefil ters, where a subset ofthe hidden variables is integrated out. The resulting model is suitable for realtime tempo tracking and tran scription and hence useful in a number ofmusic applications such as adaptive automatic accompaniment and score typesetting. 1 Introduction Automatic music transcription refers to extraction ofa high level description from musical performance, for example in form ofa music notation. Music no",Computer Vision,5ec829debe54b19a5f78d9a65b900a39-Paper.pdf,2001
Learning a Gaussian Process Prior,"ThispaperpresentsAutoDJ:asystemforautomaticallygeneratingmu- sicplaylistsbasedononeormoreseedsongsselectedbyauser.AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper furtherintroducesKernelMeta-Training,whichisamethodoflearning aGaussianProcesskernelfromadistributionoffunctionsthatgenerates thelearnedfunction.Forplaylistgeneration,AutoDJlearnsakernelfrom alargesetofalbums. Thislearnedkernelisshowntobemoreeffective atpredictingusers’playliststhanareasonablehand-designedkernel. 1 Introduction Digitalmusicisbecomingverywidespread,aspersonalcollectionsofmusicgrowtothou- sandsofsongs. Onetypicalwayforausertointeractwithapersonalmusiccollectionis to specify a playlist, an ordered list of music to be played. Using existing digital music software,ausercanmanuallyconstructaplaylistbyindividuallychoosingeachsong. Al- ternatively,playlistscanbegeneratedbytheuserspecifyingasetofrulesaboutsongs(e.g., genre=",Optimization & Theoretical ML,6351bf9dce654515bf1ddbd6426dfa97-Paper.pdf,2001
Exact differential equation population,"Mesoscopical, mathematical descriptions of dynamics of popula tions of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simula tions. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spik ing neurons. For Integrate-and-Fire type neurons, these formula tions were only approximately correct. Here, we derive a math ematically compact, exact population dynamics formulation for Integrate-and-Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscop ically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the col lective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observa",Optimization & Theoretical ML,6492d38d732122c58b44e3fdc3e9e9f3-Paper.pdf,2001
Improvisation and Learning,"Thisarticlepresentsa2-phasecomputationallearningmodelandappli- cation. Asademonstration,asystemhasbeenbuilt,calledCHIMEfor ComputerHumanInteractingMusicalEntity. Inphase1oftraining,re- currentback-propagationtrainsthemachinetoreproduce3jazzmelodies. Therecurrentnetworkisexpandedandisfurthertrainedinphase2witha reinforcementlearningalgorithmandacritiqueproducedbyasetofbasic rulesforjazzimprovisation. AftereachphaseCHIMEcaninteractively improvisewithahumaninrealtime. 1 Foundations Jazzimprovisationisthecreationofajazzmelodyinrealtime. CharlieParker,DizzyGille- spie,MilesDavis,JohnColtrane,CharlesMingus,TheloniousMonk,andSonnyRollinset al. werethefoundersofbebopandpostbopjazz[9]wheredrummers,bassists,andpianists keepthebeatandmaintainharmonicstructure.Otherplayersimproviseoverthisstructure andeventaketurnsimprovisingfor4barsatatime. Thisiscalledtradingfours. Meanwhile,artificialneuralnetworkshavebeenusedincomputermusic[4,12]. Inpartic- ular,theworkof(Todd[11])isthebasisforphase1ofCHIME,an",Computer Vision,654ad60ebd1ae29cedc37da04b6b0672-Paper.pdf,2001
Geometrical Singularities in the,"Singularities are ubiquitous in the parameter space of hierarchical models such as multilayer perceptrons. At singularities, the Fisher information matrix degenerates, and the Cramer-Rao paradigm does no more hold, implying that the classical model selection the ory such as AIC and MDL cannot be applied. It is important to study the relation between the generalization error and the training error at singularities. The present paper demonstrates a method of analyzing these errors both for the maximum likelihood estima tor and the Bayesian predictive distribution in terms of Gaussian random fields, by using simple models. 1 Introduction A neural network is specified by a number of parameters which are synaptic weights and biases. Learning takes place by modifying these parameters from observed input-output examples. Let us denote these parameters by a vector () = (0 1, ... , On). Then, a network is represented by a point in the parameter space S, where () plays the role of a coordinate s",Computer Vision,65d2ea03425887a717c435081cfc5dbb-Paper.pdf,2001
A Sequence Kernel and its Application to,"A novel approach for comparing sequences of observations using an explicit-expansionkernel is demonstrated. The kernel is derived using theassumptionoftheindependenceofthesequenceofobservationsand a mean-squared error training criterion. The use of an explicit expan- sionkernelreducesclassifier modelsize andcomputationdramatically, resultinginmodelsizes andcomputationone-hundredtimes smaller in ourapplication.Theexplicitexpansionalsopreservesthecomputational advantagesofanearlierarchitecturebasedonmean-squarederrortrain- ing. Trainingusingstandardsupportvectormachinemethodologygives accuracy that significantly exceeds the performance of state-of-the-art mean-squarederrortrainingforaspeakerrecognitiontask. 1 Introduction Comparison of sequences of observationsis a natural and necessary operationin speech applications.Severalrecentapproachesusingsupportvectormachines(SVM’s)havebeen proposedin theliterature. Thefirst set ofapproachesattempts tomodelemissionproba- bilities forhiddenMarkovm",Computer Vision,6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf,2001
Convergence of Optimistic and,"Vie sho,v the convergence of tV/O deterministic variants of Q learning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guaranteesthe convergesto an E optimalpolicy. Thesecondis a newandnovelalgorithm incremen tal Q-learning, whichgraduallypromotesthe values ofactionsthat are not taken. We show that incremental Q-learningconverges, in the limit, to the optimal policy. Our incremental Q-learning algo rithmcanbeviewedasderandomizationoftheE-greedyQ-learning. 1 Introduction One of the challenges of Reinforcement Learning is learning in an unknown envi ronment. The environment is modeled by an MDP and we can only observe the trajectory ofstates, actions and rewards generated by the agent wandering in the MDP. Therearetwo basicconceptualapproachestothelearningproblem. Thefirst is model base, where we first reconst",Reinforcement Learning,6f2688a5fce7d48c8d19762b88c32c3b-Paper.pdf,2001
Rao-Blackwellised Particle Filtering,"In this paper, we extend the Rao-Blackwellised particle filtering method to more complex hybrid models consisting of Gaussian la tent variables and discrete observations. This is accomplished by augmenting the models with artificial variables that enable us to apply Rao-Blackwellisation. Other improvements include the de sign of an optimal importance proposal distribution and being able to swap the sampling an selection steps to handle outliers. We focus on sequential binary classifiers that consist of linear combinations of basis functions, whose coefficients evolve according to a Gaussian smoothness prior. Our results show significant improvements. 1 Introduction Sequential Monte Carlo (SMC) particle methods go back to the first publically available paper in the modern field of Monte Carlo simulation (Metropolis and Ulam 1949); see (Doucet, de Freitas and Gordon 2001) for a comprehensive review. SMC is often referred to as particle filtering (PF) in the context of computing filtering",Optimization & Theoretical ML,6f4920ea25403ec77bee9efce43ea25e-Paper.pdf,2001
Boosting and Maximum Likelihood for,"We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between mini- mizingtheexponentiallossusedbyAdaBoostandmaximumlikelihood forexponentialmodelsisthatthelatterrequiresthemodeltobenormal- ized to forma conditionalprobabilitydistributionoverlabels. In addi- tiontoestablishingasimpleandeasilyunderstoodconnectionbetween thetwomethods,thisframeworkenablesustoderivenewregularization proceduresforboostingthatdirectlycorrespondtopenalizedmaximum likelihood. ExperimentsonUCIdatasets supportourtheoreticalanaly- sisandgiveadditionalinsightintotherelationshipbetweenboostingand logisticregression. 1 Introduction Severalrecentpapersinstatisticsandmachinelearninghavebeendevotedtotherelation- shipbetweenboostingandmorestandardstatisticalproceduressuchaslogisticregression. Inspiteofthisactivity,aneasy-to-understandandcleanconnectionbetweenthesediffer- ent techniques has not emerged. Friedman, Hastie and Tibshirani [7] note the simil",Optimization & Theoretical ML,71e09b16e21f7b6919bbfc43f6a5b2f0-Paper.pdf,2001
Iterative Double Clustering for,"Wepresentapowerfulmeta-clusteringtechniquecalledIterativeDou- ble Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that ex- hibited impressive performance on text categorization tasks [12]. Us- ingsyntheticallygenerateddataweempiricallyflndthatwheneverthe DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiflcantly more accurate classiflcation. IDC is especially advan- tageous when the data exhibits high attribute noise. Our simulation results also show the efiectiveness of IDC in text categorization prob- lems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we proposea simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unla- beled examples. 1 Introduction Data clustering is a fu",Optimization & Theoretical ML,798cebccb32617ad94123450fd137104-Paper.pdf,2001
Multiagent Planning with Factored MDPs,"Wepresentaprincipledandefficientplanningalgorithmforcooperativemultia- gentdynamicsystems. Astrikingfeatureofourmethodisthatthecoordination andcommunicationbetweentheagentsisnotimposed,butderiveddirectlyfrom thesystemdynamicsandfunctionapproximationarchitecture. Weviewtheen- tiremultiagentsystemasasingle,largeMarkovdecisionprocess(MDP),which weassumecanberepresentedinafactoredwayusingadynamicBayesiannet- work(DBN).TheactionspaceoftheresultingMDPisthejointactionspaceof theentiresetofagents.Ourapproachisbasedontheuseoffactoredlinearvalue functionsasanapproximationtothejointvaluefunction. Thisfactorizationof thevaluefunctionallowstheagentstocoordinatetheiractionsatruntimeusing a natural message passing scheme. We provide a simple and efficient method forcomputingsuchanapproximatevaluefunctionbysolvingasinglelinearpro- gram, whose size is determined by the interaction between the value function structureandtheDBN.Wetherebyavoidtheexponentialblowupinthestateand actionspace. Weshowthatourapp",Optimization & Theoretical ML,7af6266cc52234b5aa339b16695f7fc4-Paper.pdf,2001
On Discriminative vs. Generative,"We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of per formance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. 1 Introduction Generative classifiers learn a model of the joint probability, p( x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(ylx), and then picking the most likely label y. Discriminative classifiers model the pos terior p(ylx) directly, or learn a direct map from inputs x to the class labels. There are several compelling reasons for using discriminative rather ",NLP,7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf,2001
"ACh, Uncertainty, and Cortical Inference","Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that AChreportsonuncertaintyandcontrolshippocampal,corticaland cortico-amygdalar plasticity. We extend this view and consider itseffectsoncorticalrepresentationalinference,arguingthatACh controls thebalancebetweenbottom-up inference,in(cid:3)uenced by input stimuli, and top-down inference, in(cid:3)uenced by contextual information. We illustrate our proposalusing ahierarchicalhid- denMarkovmodel. 1 Introduction The individual and joint computational roles of neuromodulators such as dopamine,serotonin,norepinephrineandacetylcholinearecurrentlythefocusof intensivestudy.5,7,9(cid:150)11,16,27 Arichunderstandingoftheeffectsofneuromodulators on the dynamics of networks has come about through work in invertebrate sys- tems.21 Further, some general computational ideashave been advanced, such as thattheychangethe",NLP,7b1ce3d73b70f1a7246e7b76a35fb552-Paper.pdf,2001
Risk Sensitive Particle Filters,"Weproposeanewparticlefilterthatincorporatesamodelofcostswhen generatingparticles. Theapproachis motivatedbytheobservationthat thecostsofaccidentallynottrackinghypothesesmightbesignificantin someareasofstatespace,andnexttoirrelevantinothers.Byincorporat- ingacostmodelintoparticlefiltering,statesthataremorecriticaltothe systemperformancearemorelikelytobetracked.Automaticcalculation ofthecostmodelisimplementedusinganMDPvaluefunctioncalcula- tionthatestimatesthevalueoftrackingaparticularstate.Experimentsin twomobilerobotdomainsillustratetheappropriatenessoftheapproach. 1 Introduction Inrecentyears,particlefilters[3,7,8]havefoundwidespreadapplicationindomainswith noisy sensors, such as computer vision and robotics [2, 5]. Particle filters are powerful toolsforBayesianstateestimationinnon-linearsystems. Thekeyideaofparticlefiltersis toapproximateaposteriordistributionoverunknownstatevariablesbyasetofparticles, drawnfromthisdistribution. Thispaperaddressesaprimarydeficiencyofparticlefilters: ",Optimization & Theoretical ML,7ca57a9f85a19a6e4b9a248c1daca185-Paper.pdf,2001
Characterizing neural gain control using,"Spike-triggeredaveragingtechniquesareeffectiveforlinearcharacteri- zationofneuralresponses. Butneuronsexhibitimportantnonlinearbe- haviors, such as gain control, that are not captured by such analyses. Wedescribeaspike-triggeredcovariancemethodforretrievingsuppres- sivecomponentsofthegaincontrolsignalinaneuron. Wedemonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals significant suppressive axes and explains neuralnonlinearities.Thismethodshouldbeapplicabletoothersensory areasandmodalities. Whitenoiseanalysishasemergedasapowerfultechniqueforcharacterizingresponseprop- ertiesofspikingneurons.Asequenceofstimuliaredrawnrandomlyfromanensembleand presentedinrapidsuccession,andoneexaminesthesubsetthatelicitactionpotentials.This “spike-triggered”stimulusensemblecanprovideinformationabouttheneuron’sresponse characteristics. Inthemostwidelyusedformofthisanalysis,oneestimatesanexcitatory linearkernelby computingthe spike-triggeredaverage(ST",NLP,7d2b92b6726c241134dae6cd3fb8c182-Paper.pdf,2001
Speech Recognition with Missing Data using,"Inthe(cid:212)missingdata(cid:213)approachtoimprovingtherobustnessofautomatic speechrecognitiontoaddednoise,aninitialprocessidenti(cid:222)esspectral- temporal regions which are dominated by the speech source. The remaining regions are considered to be (cid:212)missing(cid:213). In this paper we develop a connectionist approach to the problem of adapting speech recognitiontothemissingdatacase,usingRecurrentNeuralNetworks. In contrast to methods based on Hidden Markov Models, RNNs allow ustomakeuseoflong-termtimeconstraintsandtomaketheproblems of classi(cid:222)cation with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task.",NLP,7f16109f1619fd7a733daf5a84c708c1-Paper.pdf,2001
On Spectral Clustering:,"Despite many empirical successes of spectral clustering methods algorithms that cluster points using eigenvectors of matrices de rived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1 Introduction The task of finding good clusters has been the focus of considerable research in machine learning and pattern recognition. For clustering points in Rn-a main ap plication focus of this paper- one standard approach is based on generative mod e",Optimization & Theoretical ML,801272ee79cfde7fa5960571fee36b9b-Paper.pdf,2001
Grouping with Bias,"With the optimization of pattern discrimination as a goal, graph partitioning approaches often lackthe capabilityto integrateprior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one ofsmooth solutions. A subspace projectionmethodisdevelopedtosolvethis constrainedeigenprob lema We demonstratethat simplepriors cangreatlyimproveimage segmentation results. 1"" Introduction Grouping is often thought of as the process of finding intrinsic clusters or group structures within a data set. In image segmentation, it means finding objects or object segments by clustering pixels and segregating them from background. It is often considered a bottom-up process. Although never explicitly stated, higher level ofknowledge, such as familiar object shapes, is to be ",Computer Vision,8038da89e49ac5eabb489cfc6cea9fc1-Paper.pdf,2001
Learning hierarchical structures with,"WepresentLinearRelationalEmbedding(LRE),anewmethodoflearn- ing a distributed representation of concepts from data consisting of in- stances ofrelationsbetweengiven concepts. Its finalgoalis tobe able togeneralize,i.e. infernewinstancesoftheserelationsamongthecon- cepts. On a task involving family relationships we show that LRE can generalize betterthananypreviouslypublishedmethod. We thenshow howLREcanbeusedeffectivelytofindcompactdistributedrepresenta- tionsforvariable-sizedrecursivedatastructures,suchastreesandlists. 1 Linear RelationalEmbedding Ouraimistotakealargesetoffactsaboutadomainexpressedastuplesofarbitrarysym- bolsinasimpleandrigidsyntacticformatandtobeabletoinferother“common-sense” factswithouthavinganypriorknowledgeaboutthedomain. Letusimagineasituationin whichwehaveasetofconceptsandasetofrelationsamongtheseconcepts,andthatour data consists of few instances of these relations that hold amongthe concepts. We want tobeabletoinferotherinstancesoftheserelations. Forexample, if",NLP,814a9c18f5abff398787c9cfcbf3d80c-Paper.pdf,2001
Constructing Distributed Representations,"Ifthepromiseofcomputationalmodelingistobefullyrealizedinhigher- levelcognitivedomainssuchaslanguageprocessing,principledmethods mustbedevelopedtoconstructthesemanticrepresentationsusedinsuch models. In this paper, we proposethe use of an established formalism frommathematicalpsychology, additiveclustering,asameansofauto- maticallyconstructingbinaryrepresentationsforobjectsusingonlypair- wise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large prob- lems. We present a new algorithm for additive clustering, based on a novelheuristictechniqueforcombinatorialoptimization. Thealgorithm issimplerthanpreviousformulationsandmakesfewerindependenceas- sumptions. Extensiveempiricaltests onbothhumanandsyntheticdata suggest that it is more effective than previous methods and that it also scalesbettertolargerproblems.Bymakingadditiveclusteringpractical, we take a significant step toward scaling connectionist models beyond ",Optimization & Theoretical ML,819e3d6c1381eac87c17617e5165f38c-Paper.pdf,2001
The Fidelity of Local Ordinal Encoding,"A key question in neuroscience is how to encode sensory stimuli suchasimagesandsounds. Motivatedbystudiesofresponseprop- ertiesofneuronsintheearlycorticalareas,weproposeanencoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures. In this scheme,thestructureofasignalisrepresentedbyasetofequalities and inequalities across adjacent regions. In this paper, we focus on characterizing the (cid:12)delity of this representation strategy. We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal repre- sentation scheme can faithfully encode signal structure. We also present a neurally plausible implementation of this computation thatusesonlylocalupdate rules. Theresultshighlighttherobust- ness and generalization ability of local ordinal encodings for the task of pattern classi(cid:12)cation. 1 Introduction Biologicaland arti(cid:12)cialrecognitionsystemsf",Computer Vision,84ddfb34126fc3a48ee38d7044e87276-Paper.pdf,2001
Products of Gaussians,"Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Be low we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaus sian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(l) process. Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. In this paper we consider PoE models in which each expert is a Gaussian. It is easy to see that in this case the product model will also be Gaussian. However, if each Gaussian has a simple structure, the product can hav",Optimization & Theoretical ML,8232e119d8f59aa83050a741631803a6-Paper.pdf,2001
Global Coordination of Local Linear Models,"Highdimensionaldatathatliesonornearalowdimensionalmanifoldcanbede- scribedbyacollectionoflocallinearmodels. Suchadescription,however,does not provide a global parameterization of the manifold—arguably an important goalofunsupervisedlearning. Inthispaper,weshowhowtolearnacollection of local linear models that solves this more difficultproblem. Our local linear modelsarerepresentedbyamixtureoffactoranalyzers,andthe“globalcoordi- nation”ofthesemodelsisachievedbyaddingaregularizingtermtothestandard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model’s parameter space, favoring models whose internal coor- dinate systems are aligned in a consistent way. As a result, the internal coor- dinateschangesmoothlyandcontinuouslyasonetraversesaconnectedpathon themanifold—even whenthepathcrossesthedomains ofmanydifferentlocal models. The regularizer takes the form of a Kullback-Leibler divergence and illustratesanunexpectedapplicationofvariationalmethods: no",Optimization & Theoretical ML,850af92f8d9903e7a4e0559a98ecc857-Paper.pdf,2001
Correlation Codes in Neuronal Populations,"Population codes oftenrelyon the tuning of the meanresponses to the stimulus parameters. However, this information can be greatly sup- pressedbylongrangecorrelations. Herewestudytheefficiencyofcod- inginformationinthesecondorderstatisticsofthepopulationresponses. WeshowthattheFisherInformationofthissystemgrowslinearlywith thesizeofthesystem. Weproposeabilinearreadoutmodelforextract- inginformationfromcorrelationcodes, andevaluateitsperformancein discriminationandestimationtasks. Itisshownthatthemainsourceof informationinthissystemisthestimulusdependenceofthevariancesof thesingleneuronresponses. 1 Introduction Experiments in the last years have shown that in many cortical areas, the fluctuations in the responses of neurons to external stimuli are significantly correlated [1, 2, 3, 4], rais- ingimportantquestionsregardingthecomputationalimplicationsofneuronalcorrelations. Recenttheoreticalstudieshaveaddressedtheissueofhowneuronalcorrelationsaffectthe efficiency of population coding [4, 5",Optimization & Theoretical ML,88ef51f0bf911e452e8dbb1d807a81ab-Paper.pdf,2001
Grammatical Bigrams,"Unsupervised learning algorithms have been derived for several sta tistical models of English grammar, but their computational com plexity makes applying them to large data sets intractable. This paper presents a probabilistic model of English grammar that is much simpler than conventional models, but which admits an effi cient EM training algorithm. The model is based upon grammat ical bigrams, i.e., syntactic relationships between pairs of words. We present the results of experiments that quantify the represen tational adequacy of the grammatical bigram model, its ability to generalize from labelled data, and its ability to induce syntactic structure from large amounts of raw text. 1 Introduction One of the most significant challenges in learning grammars from raw text is keep ing the computational complexity manageable. For example, the EM algorithm for the unsupervised training of Probabilistic Context-Free Grammars- known as the Inside-Outside algorithm- has been found in practice",NLP,89885ff2c83a10305ee08bd507c1049c-Paper.pdf,2001
The Steering Approach for Multi-Criteria,"We consider the problem of learning to attain multiple goals in a dynamic envi- ronment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary movesofNature. Thisproblemismodelled asastochastic(Markov) gamebetween the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which isbasedonthetheoryofapproachabilityforstochasticgames. Thisalgorithmcom- bines, in an appropriate way, a flnite set of standard, scalar-reward learning algo- rithms. Su–cientconditionsaregivenfortheconvergenceofthelearningalgorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1 Introduction This paper considers an on-line learning problem for Mark",Optimization & Theoretical ML,8c249675aea6c3cbd91661bbae767ff1-Paper.pdf,2001
Stabilizing Value Function,"Weaddresstheproblemofnon-convergenceofonlinereinforcement learning algorithms (e.g., Q learning and SARSA(A)) by adopt ing an incremental-batch approach that separates the exploration process from the function fitting process. Our BFBP (Batch Fit to Best Paths) algorithm alternates between an exploration phase (during which trajectories are generated to try to find fragments of the optimal policy) and a function fitting phase (during which a function approximator is fit to the best known paths from start states to terminal states). An advantage ofthis approach is that batch value-function fitting is a global process, which allows it to address the tradeoffs in function approximation that cannot be handled by local, online algorithms. This approach was pioneered by Boyan and Moore with their GROWSUPPORT and ROUT al gorithms. We show how to improve upon their work by applying a better exploration process and by enriching the function fitting procedure to incorporate Bellman error and adv",Reinforcement Learning,8d8818c8e140c64c743113f563cf750f-Paper.pdf,2001
Cobot: A Social Reinforcement Learning Agent,"We report on the use of reinforcement learning with Cobot, a software agent residinginthewell-knownonlinecommunityLambdaMOO.Ourinitialworkon Cobot(Isbelletal.2000)providedhimwiththeabilitytocollectsocialstatistics andreportthemtousers.HerewedescribeanapplicationofRLallowingCobot totakeproactiveactionsinthiscomplexsocialenvironment,andadaptbehavior from multiple sourcesofhumanreward. After 5monthsoftraining, and3171 reward and punishment events from 254 different LambdaMOO users, Cobot learnednontrivialpreferencesforanumberofusers,modifinghisbehaviorbased on his current state. Here we describeLambdaMOOand the state andaction spacesofCobot,andreportthestatisticalresultsofthelearningexperiment. 1 Introduction While most applications of reinforcement learning (RL) to date have been to problems ofcontrol,gameplayingandoptimization(SuttonandBarto1998),therehasbeenarecent handfulofapplicationstohuman-computerinteraction.Suchapplicationspresentanumber ofinterestingchallengestoRLmethodology(suc",Reinforcement Learning,92bbd31f8e0e43a7da8a6295b251725f-Paper.pdf,2001
Tree-based reparameterization for,"We develop a tree-based reparameterization framework that pro vides a new conceptual view of a large class of iterative algorithms for computing approximate marginals in graphs with cycles. It includes belief propagation (BP), which can be reformulated as a very local form of reparameterization. More generally, we consider algorithms that perform exact computations over spanning trees of the full graph. On the practical side, we find that such tree reparameterization (TRP) algorithms have convergence properties superior to BP. The reparameterization perspective also provides a number of theoretical insights into approximate inference, in cluding a new characterization of fixed points; and an invariance intrinsic to TRP /BP. These two properties enable us to analyze and bound the error between the TRP /BP approximations and the actual marginals. While our results arise naturally from the TRP perspective, most of them apply in an algorithm-independent manner to any local minimum of the B",Optimization & Theoretical ML,9185f3ec501c674c7c788464a36e7fb3-Paper.pdf,2001
Semi-Supervised MarginBoost,"In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1 Introduction In semi-supervised classification tasks, a concept is to be learnt using both labeled and unlabeled examples. Such problems arise frequently in data-mining where the cost of the labeling process can be prohibitive because it requires human help as in v",Optimization & Theoretical ML,931af583573227f0220bc568c65ce104-Paper.pdf,2001
ALGONQUIN - Learning dynamic noise,"A challenging, unsolved problem in the speech recognition com munity is recognizing speech signals that are corrupted by loud, highly nonstationary noise. One approach to noisy speech recog nition is to automatically remove the noise from the cepstrum se quence before feeding it in to a clean speech recognizer. In previous work published in Eurospeech, we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of estimating the noise free speech from the noisy speech. We showed how an iterative 2nd order vector Taylor series approximation could be used for prob abilistic inference in this model. In many circumstances, it is not possible to obtain examples of noise without speech. Noise statis tics may change significantly during an utterance, so that speech free frames are not sufficient for estimating the noise model. In this paper, we show how the noise model can be learned even when the data contains ",Computer Vision,95192c98732387165bf8e396c0f2dad2-Paper.pdf,2001
Adaptive N earest Neighbor Classification,"The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of exam ples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets. 1 Introduction In a classification problem, we are given J classes and l training observations. The training observations consist of n feature measurements x = (Xl,'"" ,Xn)T E ~n and the known clas",Optimization & Theoretical ML,955cb567b6e38f4c6b3f28cc857fc38c-Paper.pdf,2001
"Fast, large-scale transformation-invariant","In previous work on \transformed mixtures of Gaussians"" and \transformed hidden Markov models"", we showed how the EM al- gorithm in a discrete latent variable model can be used to jointly normalizedata(e.g., centerimages, pitch-normalizespectrograms) and learn a mixture model of the normalized data. The only input tothealgorithmisthedata, alistofpossibletransformations, and the number of clusters to (cid:12)nd. The main criticism of this work was that the exhaustive computation of the posterior probabili- ties over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we de- scribe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. ForN(cid:2)N images,learningC clustersunderN rotations,N scales, N x-translations and N y-translations takes only (C+2logN)N2 scalar operations per iteration. In",Computer Vision,95f6870ff3dcd442254e334a9033d349-Paper.pdf,2001
A Rotation and Translation Invariant Discrete,"We describe a neural network which enhances and completes salient closed contours. Our work is differentfromall previouswork in three important ways. First, like the input provided to V1 by LGN, the in- put to our computation is isotropic. That is, the input is composed of spotsnotedges. Second,ournetworkcomputesawelldefinedfunction oftheinputbasedonadistributionofclosedcontourscharacterizedby arandomprocess. Third,eventhoughourcomputationis implemented inadiscretenetwork,its outputisinvarianttocontinuousrotationsand translationsoftheinputpattern. 1 Introduction Thereis a longhistoryofresearchonneuralnetworksinspiredbythe structureofvisual cortexwhosefunctionshavebeendescribedascontourcompletion,saliencyenhancement, orientationsharpening,orsegmentation[6,7,8, 9,12]. A similiarnetworkhas beenpro- posedasamodelofvisualhallucinations[1]. Inthispaper,wedescribeaneuralnetwork whichenhancesandcompletessalientclosedcontours.Ourworkisdifferentfromallpre- viousworkinthreeimportantways.First,lik",Computer Vision,96055f5b06bf9381ac43879351642cf5-Paper.pdf,2001
A theory of neural integration in the,"Integrationinthehead-directionsystemisacomputationbywhichhor- izontal angular head velocity signals from the vestibular nuclei are in- tegratedtoyieldaneuralrepresentationofheaddirection. Inthethala- mus, the postsubiculumand the mammillarynuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their firing is maximal [BlairandSharp,1995, Blairetal.,1998,?]. Integrationisadifficultcomputation,giventhathead-velocitiescanvary overalargerange. Previousmodelsofthehead-directionsystemrelied ontheassumptionthattheintegrationis achievedin afiring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocitysignalsduringhigh-speedheadrotations,veryfastsynaptic dynamicshadtobeassumed. Here we addressthe question whetherintegrationin the head-direction system is possible with slow synapses, for example excitatory NMDA andinhibitoryGABA(B)typesynapses. Forneuralnetworkswithsuch slow synapses, rate",Neurobiology of Behavior,978d76676f5e7918f81d28e7d092ca0d-Paper.pdf,2001
(Not) Bounding the True Error,"Wepresentanewapproachtoboundingthetrueerrorrateofacontinuous valuedclassifierbaseduponPAC-Bayesbounds. Themethodfirstcon- structsa distribution overclassifiers bydetermininghowsensitiveeach parameterinthemodelis tonoise. Thetrueerrorrateofthestochastic classifierfoundwiththesensitivityanalysiscanthenbetightlybounded usingaPAC-Bayesbound. Inthispaperwedemonstratethemethodon artificialneuralnetworkswithresultsofa (cid:0)(cid:2) (cid:1)(cid:4) (cid:3) orderofmagnitudeim- provementvs. thebestdeterministicneuralnetbounds. 1 Introduction Inmachinelearningitis importanttoknowthetrueerrorrateaclassifierwillachieveon future test cases. Estimatingthis errorrate can be suprisinglydifficult. For example, all knownboundsonthetrueerrorrateofartificialneuralnetworkstendtobeextremelyloose andoftenresultinthemeaninglessboundof“alwayserr”(errorrate=1.0). Inthispaper,wedonotboundthetrueerrorrateofaneuralnetwork.Instead,webound thetrueerrorrateofadistributionoverneuralnetworkswhichwecreatebyanalysingone n",Optimization & Theoretical ML,98c7242894844ecd6ec94af67ac8247d-Paper.pdf,2001
Novel iteration schemes for the Cluster,"The Cluster Variation method is a class of approximation meth ods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Vari ation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP meth ods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1 Introduction Belief Propagation (BP) is a message passing scheme, which is known to yield exact inference in tree structured graphical models [1]. It has been noted by several authors that Belief Propagation can can also give impressive results for graphs that are not trees [2]. The Cluster Variation Method (CVM), is a method that has been developed in the physics community for approximate inference in",Optimization & Theoretical ML,9a49a25d845a483fae4be7e341368e36-Paper.pdf,2001
Infinite Mixtures of Gaussian Process Experts,"We presentan extension to the Mixture ofExperts (ME)model, where theindividualexpertsareGaussianProcess(GP)regressionmodels.Us- ing an input-dependent adaptation of the Dirichlet Process, we imple- mentagatingnetworkforaninfinitenumberofExperts.Inferenceinthis model maybedoneefficientlyusinga Markov Chainrelying onGibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially over- coming two of the biggest hurdles with GP models. Simulations show theviabilityofthisapproach. 1 Introduction GaussianProcesses[Williams&Rasmussen,1996]haveproventobeapowerfultoolfor regression. Theycombinetheflexibilityofbeingabletomodelarbitrarysmoothfunctions ifgivenenoughdata,withthesimplicityofaBayesianspecificationthatonlyrequiresin- ferenceoverasmallnumberofreadilyinterpretablehyperparameters–suchasthelength scalesbywhichthefunctionvariesalongdifferentdimensions,thecontributionsofsignal andnoisetothevarianceinthedata,etc. Ho",Optimization & Theoretical ML,9afefc52942cb83c7c1f14b2139b09ba-Paper.pdf,2001
Relative Density Nets: A New Way to,"Logistic units in the first hidden layer of a feedforward neural net work compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1 Introduction A standard way of performing classification using a generative model is to divide the training cases into their respective classes and then train a set of class conditional models. This unsupervised approach to classification is appealing for two reasons. It is possible to reduce overfitting, because the model learns the class-conditional input densities P(xlc) rather than the input-conditional class probabilities P(clx). Also, provided that the model density is a good match to the underlying data density then the decision provided by a p",Computer Vision,9bb6dee73b8b0ca97466ccb24fff3139-Paper.pdf,2001
Neural Implementation of Bayesian,"This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accu racy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is con veyed by the change in recurrent interactions as a result of Hebbian learning. 1 Introduction Information in the brain is not processed by a single neuron, but rather by a popu lation of them. Such a coding strategy is called population coding. It is conceivable that population coding has advantage of being robust to the fluctuation in a single neuron's activity. However, people argue that population coding may have other computationally desirable properties. One such property is to provide a framework for encoding complex objects by using basis functions [1]. This is inspired by the recent progresses in ",Computer Vision,9d7311ba459f9e45ed746755a32dcd11-Paper.pdf,2001
"TAP Gibbs Free Energy, Belief Propagation and","The adaptive TAP Gibbs free energy for a general densely connected probabilisticmodel with quadraticinteractions and arbritarysingle site constraintsisderived. Weshowhowaspecificsequentialminimization ofthefreeenergyleadstoageneralizationofMinka’sexpectationpropa- gation.Lastly,wederiveasparserepresentationversionofthesequential algorithm.Theusefulnessoftheapproachisdemonstratedonclassifica- tionanddensityestimationwithGaussianprocessesandonanindepen- dentcomponentanalysisproblem. 1 Introduction Thereisanincreasinginterestinmethodsforapproximateinferenceinprobabilistic(graph- ical)models.Suchapproximationsmayusuallybegroupedinthreeclasses.Inthefirstcase weapproximateself-consistencyrelations formarginalprobabilitiesbyasetofnonlinear equations. Meanfield(MF)approximationsandtheiradvancedextensionsbelongtothis group. However,it is notclear in general, howto solvethese equationsefficiently. This latterproblemis ofcentralconcerntothe secondclass, the Message passingalgorithms, likeBayesian",Optimization & Theoretical ML,9f62b8625f914a002496335037e9ad97-Paper.pdf,2001
Quantizing Density Estimators,"We suggest a nonparametric framework for unsupervised learning of projection models in terms of density estimation on quantized sample spaces. The objective is not to optimally reconstruct the data but in- steadthequantizeris chosentooptimallyreconstructthe densityofthe data. Fortheresultingquantizingdensityestimator (QDE)wepresenta generalmethodforparameterestimationandmodelselection. Weshow howprojectionsetswhichcorrespondtotraditionalunsupervisedmeth- odslikevectorquantizationorPCAappearinthenewframework.Fora principalcomponentquantizerwepresentresultsonsyntheticandreal- worlddata,whichshowthattheQDEcanimprovethegeneralizationof thekerneldensityestimatoralthoughitsestimateisbasedonsignificantly lower-dimensionalprojectionindicesofthedata. 1 Introduction Unsupervisedlearningisessentiallyconcernedwithfindingalternativerepresentationsfor unlabeleddata.Thesealternativerepresentationsusuallyreflectsomeimportantproperties oftheunderlyingdistributionandusuallytheytrytoexploitsomeredundancy",Optimization & Theoretical ML,a00e5eb0973d24649a4a920fc53d9564-Paper.pdf,2001
The Concave-Convex Procedure (CCCP),"We introduce the Concave-Convex procedure (CCCP) which con structs discrete time iterative dynamical systems which are guar anteed to monotonically decrease global optimization/energy func tions. It can be applied to (almost) any optimization problem and many existing algorithms can be interpreted in terms of CCCP. In particular, we prove relationships to some applications of Legendre transform techniques. We then illustrate CCCP by applications to Potts models, linear assignment, EM algorithms, and Generalized Iterative Scaling (GIS). CCCP can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms. 1 Introduction There is a lot of interest in designing discrete time dynamical systems for inference and learning (see, for example, [10], [3], [7], [13]). This paper describes a simple geometrical Concave-Convex procedure (CCCP) for constructing discrete time dynamical systems which can be guaranteed to decrease almost any ",Optimization & Theoretical ML,a012869311d64a44b5a0d567cd20de04-Paper.pdf,2001
Active Learning,"We investigate the following data miningproblem fromComputational Chemistry: Fromalargedatasetofcompounds,findthosethatbindto atargetmoleculeinasfewiterationsofbiologicaltestingaspossible.In eachiterationacomparativelysmallbatchofcompoundsisscreenedfor bindingtothetarget. We applyactivelearningtechniques forselecting thesuccessivebatches. Oneselectionstrategypicksunlabeledexamplesclosesttothemaximum marginhyperplane. Anotherproducesmanyweightvectorsbyrunning perceptronsovermultiplepermutationsofthedata. Eachweightvector voteswithits (cid:7) predictionandwepicktheunlabeledexamplesforwhich thepredictionismostevenlysplitbetween (cid:8) and (cid:9) . Forathirdselec- tionstrategynotethateachunlabeledexamplebisectstheversionspace of consistent weight vectors. We estimate the volume on both sides of thesplitbybouncingabilliardthroughtheversionspaceandselectun- labeledexamplesthatcausethemostevensplitoftheversionspace. We demonstrate that ontwo data sets provided byDuPont Pharmaceu- ticalsthat",NLP,a03fa30821986dff10fc66647c84c9c3-Paper.pdf,2001
Intransitive Likelihood-Ratio Classifiers,"Inthiswork,weintroduceaninformation-theoreticbasedcorrectionterm tothelikelihoodratioclassificationmethodformultipleclasses. Under certainconditions,thetermissufficientforoptimallycorrectingthedif- ferencebetweenthetrueandestimatedlikelihoodratio,andweanalyze this in the Gaussian case. We find that the new correctionterm signif- icantly improves the classification results when tested on medium vo- cabulary speech recognitiontasks. Moreover,the additionof this term makesthe class comparisonsanalogoustoan intransitivegameandwe therefore use several tournament-likestrategies to deal with this issue. Wefindthatfurthersmallimprovementsareobtainedbyusinganappro- priatetournament.Lastly,wefindthatintransitivityappearstobeagood measureofclassificationconfidence. 1 Introduction An important aspect of decision theory is multi-way pattern classification whereby one mustdeterminetheclass (cid:2)(cid:4) (cid:3) foragivendatavector (cid:5) thatminimizestheoverallrisk: (cid:2) (cid:3)(cid:7) (cid:6) ",Optimization & Theoretical ML,a088ea2078cd92b0b8a0e78a32c5c082-Paper.pdf,2001
Transform-invariant image decomposition,"Recent work has shown impressive transform-invariant modeling andclusteringforsetsofimagesofobjectswithsimilarappearance. We seek toexpand these capabilitiesto sets of imagesof anobject class that show considerable variation across individual instances (e.g.pedestrianimages)usingarepresentationbasedonpixel-wise similarities, similarity templates. Because of its invariance to the colorsofparticularcomponentsofanobject,thisrepresentationen- ablesdetectionofinstancesofanobjectclassandenablesalignment ofthoseinstances. Further,thismodelimplicitlyrepresentsthere- gions of color regularity in the class-speci(cid:12)c image set enabling a decomposition of that object class into component regions. 1 Introduction Images of a class of objects are often not e(cid:11)ectively characterized by a Gaussian distribution or even a mixture of Gaussians. In particular, we are interested in modeling classes of objects that are characterized by similarities and di(cid:11)erences betweenimagepixelsrathertha",Computer Vision,a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf,2001
Activity Driven Adaptive Stochastic,"Cortical neurons might be considered as threshold elements inte grating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We an alytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more re alistic neuron",Optimization & Theoretical ML,a29d1598024f9e87beab4b98411d48ce-Paper.pdf,2001
Reinforcement Learning,"This paper presents reinforcement learning with a Long Short Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies be tween relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation ofthe pole balancingtask. 1 Introduction Reinforcement learning (RL) is a way oflearning how to behave based on delayed reward signals [12]. Among the more important challenges for RL are tasks where part ofthe state ofthe environment is hidden from the agent. Suchtasks are called non-MarkoviantasksorPartiallyObservableMarkovDecisionProcesses. Manyreal world tasks have this problem of hidden state. For instance, in a navigation task different positions in the environment may look the same, but one and the same action may lead to different next states or rewards. Thus, hidden state makes RL more realistic. However, it also makes it more difficult, becau",Reinforcement Learning,a38b16173474ba8b1a95bcbc30d3b8a5-Paper.pdf,2001
The Intelligent Surfer:,"The PageRank algorithm, used in the Google search engine, greatly improves the results of Web search by taking into account the link structure of the Web. PageRank assigns to a page a score propor- tional to the number of times a random surfer would visit that page, if it surfed indefinitely from page to page, following all outlinks from a page with equal probability. We propose to improve Page- Rank by using a more intelligent surfer, one that is guided by a probabilistic model of the relevance of a page to a query. Efficient execution of our algorithm at query time is made possible by pre- computing at crawl time (and thus once for all queries) the neces- sary terms. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (hu- man-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. 1 Introduction Traditional information retrieval techniques can give poor results on",Optimization & Theoretical ML,a501bebf79d570651ff601788ea9d16d-Paper.pdf,2001
ADynamic HMM for On-line,"We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other ap proaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dy namically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to pro cess incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1 Introduction Abrupt changes can occur in many different real-world systems like, for example, in speech, in climatological or industri",Computer Vision,a48564053b3c7b54800246348c7fa4a0-Paper.pdf,2001
S.KoenigandM.Likhachev,"Incrementalsearchtechniquesfindoptimalsolutionstoseriesofsimilar search tasks much faster than is possible by solving each search task fromscratch. Whileresearchershavedevelopedincrementalversionsof uninformedsearchmethods, we developan incrementalversionof A*. The first search of LifelongPlanning A* is the same as that of A* but allsubsequentsearchesaremuchfasterbecauseitreusesthosepartsof theprevioussearchtreethatareidenticaltothenewsearchtree.Wethen presentexperimentalresultsthatdemonstratetheadvantagesofLifelong PlanningA*forsimplerouteplanningtasks. 1 Overview Artificial intelligencehas investigatedknowledge-basedsearchtechniquesthat allowone tosolvesearchtasksinlargedomains. Mostoftheresearchonthesemethodshasstudied how to solve one-shot search problems. However, search is often a repetitive process, whereoneneedstosolveaseriesofsimilarsearchtasks,forexample,becausetheactual situation turns out to be slightly different from the one initially assumed or because the situation chang",Optimization & Theoretical ML,a591024321c5e2bdbd23ed35f0574dde-Paper.pdf,2001
Grouping and dimensionality reduction by,"Locally Linear Embedding (LLE) is an elegant nonlinear dimensionality-reduction technique recently introduced by Roweis and Saul [2]. It fails when the data is divided into separate groups. We study a variant of LLE that can simultaneously group the data and calculate local embedding of each group. An estimate for the upper bound on the intrinsic dimension of the data set is obtained automatically. 1 Introduction Consider a collection of N data points Xi E ]RD. Suppose that, while the dimension D is large, we have independent information suggesting that the data are distributed on a manifold of dimension d < < D. In many circumstances it is beneficial to calculate the coordinates Yi E ]Rd of the data on the lower-dimensional manifold, both because the shape of the manifold may yield some insight in the process that produced the data, and because it is cheaper to store and manipulate the data when it is embedded in fewer dimensions. How can we compute such coordinates? Principal compone",Optimization & Theoretical ML,a5a61717dddc3501cfdf7a4e22d7dbaa-Paper.pdf,2001
Partially labeled classification with Markov,"To classify a large number of unlabeled examples we combine a lim- itednumberoflabeledexampleswithaMarkovrandomwalkrepresen- tationovertheunlabeledexamples.Therandomwalkrepresentationex- ploitsanylowdimensionalstructureinthedatainarobust,probabilistic manner. Wedevelopandcompareseveralestimationcriteria/algorithms suitedtothisrepresentation. Thisincludesinparticularmulti-wayclas- sificationwithanaveragemargincriterionwhichpermitsaclosedform solution. Thetimescaleoftherandomwalkregularizestherepresenta- tionandcanbesetthroughamargin-basedcriterionfavoringunambigu- ousclassification. We also extendthis basic regularizationbyadapting time scales for individual examples. We demonstrate the approach on syntheticexamplesandontextclassificationproblems. 1 Introduction Classificationwithpartiallylabeledexamplesinvolvesalimiteddatasetoflabeledexam- plesaswellasalargeunlabeleddataset. Theunlabeledexamplestobeclassifiedprovide informationaboutthestructureofthedomainwhilethefewlabeledexamplesident",NLP,a82d922b133be19c1171534e6594f754-Paper.pdf,2001
Dynamic Time-Alignment Kernel in,"A new class of Support Vector Machine (SVM) that is applica- ble to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, stan- dard SVM training and classi(cid:12)cation algorithms can be employed without further modi(cid:12)cations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimen- tal results show comparable recognition performance with hidden Markov models (HMMs). 1 Introduction SupportVectorMachine(SVM)[1]isoneofthelatestandmostsuccessfulstatistical pattern classi(cid:12)er that utilizes a kernel technique [2, 3]. The basic form of SVM classi(cid:12)er which classi(cid:12)es an input vector x2Rn is expressed as N N g(x)= (cid:11) y (cid:30)(x )(cid:1)(cid:30)(x)+b= (cid:11) y K(x ;x)+b; ",Optimization & Theoretical ML,a869ccbcbd9568808b8497e28275c7c8-Paper.pdf,2001
Associative memory in realistic neuronal,"Almost two decades ago, Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typ ically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2,3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to under stand how the behavior of the network scales with its parameters, and simula",Computer Vision,a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf,2001
A General Greedy Approximation Algorithm,"Greedy approximation algorithms have been frequently used to obtain sparsesolutionstolearningproblems.Inthispaper,wepresentageneral greedy algorithm for solving a class of convex optimization problems. We deriveaboundontherateofapproximationforthisalgorithm,and show that ouralgorithmincludes a numberof earlier studies as special cases. 1 Introduction Thegoalofmachinelearningistoobtainacertaininput/outputfunctionalrelationshipfrom asetoftrainingexamples.Inordertodoso,weneedtostartwithamodelofthefunctional relationship.Inpractice,itisoftendesirabletofindthesimplestmodelthatcanexplainthe data.Thisisbecausesimplemodelsareofteneasiertounderstandandcanhavesignificant computationaladvantagesovermorecomplicatedmodels. Inaddition,thephilosophyof Occam’sRazorimpliesthatthesimplestsolutionislikelytobethebestsolutionamongall possiblesolutions, Inthispaper,weareinterestedincompositemodelsthatcanbeexpressedaslinearcom- binationsofbasic models. Inthis framework,it is naturalto measurethesimplicity of",Optimization & Theoretical ML,a9813e9550fee3110373c21fa012eee7-Paper.pdf,2001
Reducing multiclass to binary by,"This paper presents a method for obtaining class membership probability esti- matesformulticlassclassificationproblemsbycouplingtheprobabilityestimates produced bybinaryclassifiers. Thisisanextensionforarbitrarycodematrices of a method due to Hastie and Tibshirani for pairwise coupling of probability estimates.ExperimentalresultswithBoostedNaiveBayesshowthatourmethod producescalibratedclassmembershipprobabilityestimates,whilehavingsimilar classificationaccuracyasloss-baseddecoding,amethodforobtainingthemost likelyclassthatdoesnotgenerateprobabilityestimates. 1 Introduction Thetwomostwell-knownapproachesforreducinga multiclassclassificationproblemto a set of binaryclassification problemsare knownas one-against-alland all-pairs. In the one-against-allapproach,wetrainaclassifierforeachoftheclassesusingaspositiveex- amplesthetrainingexamplesthatbelongtothatclass,andasnegativesalltheothertraining examples. Intheall-pairsapproach,wetraina classifierforeachpossiblepairofclasses ignoringtheexa",NLP,abdbeb4d8dbe30df8430a8394b7218ef-Paper.pdf,2001
Linear Time Inference in Hierarchical HMMs,"ThehierarchicalhiddenMarkovmodel(HHMM)is a generalizationof thehiddenMarkovmodel(HMM)thatmodelssequenceswithstructure at many length/time scales [FST98]. Unfortunately, the original infer- encealgorithmisrathercomplicated,andtakes (cid:2)(cid:4) (cid:3)(cid:6) (cid:5)(cid:8) (cid:7)(cid:10) (cid:9) time,where (cid:5) is thelengthofthesequence,makingitimpracticalformanydomains. In thispaper,weshowhowHHMMsareaspecialkindofdynamicBayesian network(DBN),andtherebyderiveamuchsimplerinferencealgorithm, which only takes (cid:2)(cid:4) (cid:3)(cid:6) (cid:5)(cid:11) (cid:9) time. Furthermore, by drawingthe connection between HHMMs and DBNs, we enable the application of many stan- dardapproximationtechniquestofurtherspeedupinference. 1 Introduction The Hierarchical HMM [FST98] is an extension of the HMM that is designed to modeldomainswithhierarchicalstructure,e.g.,naturallanguage,XML,DNAsequences [HIM (cid:12) 00], handwriting [FST98], plan recognition [BVW00], visual action recogntion [IB00, M",NLP,aebf7782a3d445f43cf30ee2c0d84dee-Paper.pdf,2001
Sequential noise compensation by,"We present a sequential Monte Carlo method applied to additive noise compensation for robust speech recognition in time-varying noise. Themethodgeneratesasetofsamplesaccordingtotheprior distribution given by clean speech models and noise prior evolved from previous estimation. An explicit model representing noise ef- fects on speech features is used, so that an extended Kalman fllter is constructed for each sample, generating the updated continuous stateestimateastheestimationofthenoiseparameter,andpredic- tion likelihood for weighting each sample. Minimum mean square error(MMSE)inferenceofthetime-varyingnoiseparameteriscar- riedoutoverthesesamplesbyfusiontheestimationofsamplesac- cording to their weights. A residual resampling selection step and aMetropolis-Hastingssmoothingstepareusedtoimprovecalcula- tion e–ciency. Experiments were conducted on speech recognition in simulated non-stationary noises, where noise power changed ar- tiflcially, and highly non-stationary Machinegun noise.",NLP,b2ab001909a8a6f04b51920306046ce5-Paper.pdf,2001
Direct value-approxiIllation for factored MDPs,"We present a simple approach for computing reasonable policies for factored Markov decision processes (MDPs), when the opti malvaluefunction canbe approximatedbya compact linearform. Our method is based on solving a single linear program that ap proximates the best linear fit to the optimal value function. By applying an efficient constraint generation procedure we obtain an iterativesolutionmethodthattacklesconciselinearprograms. This direct linear programmingapproach experimentallyyields a signif icant reduction in computation time over approximate value- and policy-iteration methods (sometimes reducing several hours to a few seconds). However, the quality of the solutions produced by linear programming is weaker-usually about twice the approxi mation error for the same approximating class. Nevertheless, the speed advantage allows one to use larger approximationclasses to achieve similarerror in reasonable time. 1 Introduction Markov decision processes (MDPs) form a foundation for co",Reinforcement Learning,b3b4d2dbedc99fe843fd3dedb02f086f-Paper.pdf,2001
Perceptual Metamers,"Theories of cue combination suggest the possibility of constructing visual stimuli that evoke different patterns of neural activity in sensory areas of the brain, but that cannot be distinguished by any behavioral measure of perception. Such stimuli, if they exist, would be interesting for two reasons. First, one could know that none of the differences between the stimuli survive past the computations used to build the percepts. Second, it can be difficult to distinguish stimulus-driven components of measured neural activity from top-down components (such as those due to the interestingness of the stimuli). Changing the stimulus without changing the percept could be exploited to measure the stimulus- driven activity. Here we describe stimuli in which vertical and horizontal disparities trade during the construction of percepts of slanted surfaces, yielding stimulus equivalence classes. Equivalence class membership changed after a change of vergence eye posture alone, without changes to",Computer Vision,b5f1e8fb36cd7fbeb7988e8639ac79e9-Paper.pdf,2001
Scaling laws and local minima in Hebbian ICA,"We study the dynamics of a Hebbian ICA algorithm extracting a sin- gle non-Gaussian component from a high-dimensionalGaussian back- ground. Forbothon-lineandbatchlearningwefindthatasurprisingly largenumberofexamplesarerequiredtoavoidtrappinginasub-optimal state close to the initial conditions. To extract a skewed signal at least (cid:0)(cid:2) (cid:1)(cid:4) (cid:3)(cid:6) (cid:5)(cid:8) (cid:7) examplesarerequiredfor (cid:3) -dimensionaldataand (cid:0)(cid:2) (cid:1)(cid:9) (cid:3)(cid:11) (cid:10)(cid:12) (cid:7) exam- plesarerequiredtoextractasymmetricalsignalwithnon-zerokurtosis. 1 Introduction Independentcomponentanalysis (ICA)is a statistical modellingtechniquewhichhas at- tractedasignificantamountofresearchinterestinrecentyears(forareview,seeHyva¨rinen, 1999).ThegoalofICAistofindarepresentationofdataintermsofacombinationofsta- tisticallyindependentvariables.Anumberofneurallearningalgorithmshavebeenapplied tothisproblem,asdetailedintheaforementionedreview. Theoretical studies of ",Optimization & Theoretical ML,b8b4b727d6f5d1b61fff7be687f7970f-Paper.pdf,2001
Online Learning with Kernels,"WeconsideronlinelearninginaReproducingKernelHilbertSpace.Our method is computationallyefficient and leads to simple algorithms. In particularwe deriveupdateequationsforclassification, regression,and noveltydetection. Theinclusionofthe (cid:0) -trickallowsustogivearobust parameterization. Moreover,unlikein batchlearningwherethe (cid:0) -trick onlyappliestothe (cid:1) -insensitivelossfunctionweareabletoderivegen- eraltrimmed-meantypesofestimatorssuchasforHuber’srobustloss. 1 Introduction Whilekernelmethodshaveproventobesuccessfulinmanybatchsettings(SupportVector Machines,GaussianProcesses,RegularizationNetworks)theextensiontoonlinemethods hasprovento providesomeunsolvedchallenges. Firstly, thestandardonlinesettingsfor linear methods are in dangerof overfitting, when appliedto an estimator using a feature spacemethod. Thiscallsforregularization(orpriorprobabilitiesinfunctionspaceifthe GaussianProcessviewistaken). Secondly,thefunctionalrepresentationoftheestimatorbecomesmorecomplexasthenum",Optimization & Theoretical ML,bd5af7cd922fd2603be4ee3dc43b0b77-Paper.pdf,2001
Adam Kepecs* and Sridhar Raghavachari,"Neurons receive excitatory inputs via both fast AMPA and slow NMDA type receptors. We find that neurons receiving input via NMDA receptors can have two stable membrane states which are input dependent. Action potentials can only be initiated from the higher voltage state. Similar observations have been made in sev eral brain areas which might be explained by our model. The in teractions between the two kinds of inputs lead us to suggest that some neurons may operate in 3 states: disabled, enabled and fir ing. Such enabled, but non-firing modes can be used to introduce context-dependent processing in neural networks. We provide a simple example and discuss possible implications for neuronal pro cessing and response variability. 1 Introduction Excitatory interactions between neurons are mediated by two classes of synapses: AMPA and NMDA. AMPA synapses act on a fast time scale (TAMPA'"" 5ms), and their role in shaping network dynamics has been extensively studied. The NMDA type receptors a",NLP,bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf,2001
Convolution Kernels for Natural Language,"WedescribetheapplicationofkernelmethodstoNaturalLanguagePro- cessing(NLP)problems. InmanyNLPtaskstheobjectsbeingmodeled arestrings,trees,graphsorotherdiscretestructureswhichrequiresome mechanismtoconvertthemintofeaturevectors.Wedescribekernelsfor variousnaturallanguagestructures,allowingrich,highdimensionalrep- resentations of these structures. We show how a kernel over trees can beappliedtoparsingusingthevotedperceptronalgorithm,andwegive experimentalresultsontheATIScorpusofparsetrees. 1 Introduction Kernelmethodshavebeenwidelyusedtoextendtheapplicabilityofmanywell-knownal- gorithms,suchasthePerceptron[1],SupportVectorMachines[6],orPrincipalComponent Analysis[15]. Akeypropertyofthese algorithmsis thattheonlyoperationtheyrequire is the evaluation of dot products between pairs of examples. One may thereforereplace thedotproductwithaMercerkernel,implicitlymappingfeaturevectorsin (cid:0)(cid:2) (cid:1) intoanew space (cid:0)(cid:4) (cid:3) ,andapplyingtheoriginalalgorithminthisnewfeatures",NLP,bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf,2001
On the Convergence of Leveraging,"We give an unified convergence analysis of ensemble learning meth- odsincludinge.g.AdaBoost,LogisticRegressionandtheLeast-Square- Boost algorithm for regression. These methods have in common that theyiterativelycallabaselearningalgorithmwhichreturnshypotheses thatarethenlinearlycombined. Weshowthatthesemethodsarerelated totheGauss-Southwellmethodknownfromnumericaloptimizationand state non-asymptotical convergence results for all these methods. Our analysis includes -normregularizedcost functions leading to a clean andgeneralwaytoregularizeensemblelearning. 1 Introduction WeshowconvergenceratesofensemblelearningmethodssuchasAdaBoost[10],Logistic Regression(LR)[11, 5]andtheLeast-Square(LS)regressionalgorithmcalledLS-Boost [12]. Thesealgorithmshaveincommonthattheyiterativelycallabaselearningalgorithm (alsocalledweaklearner)onaweightedtrainingsample. Thebaselearnerisexpected toreturnineachiteration ahypothesis fromsomehypothesissetofweakhypotheses thathassmallweightedtrainingerror. Thisist",Optimization & Theoretical ML,c215b446bcdf956d848a8419c1b5a920-Paper.pdf,2001
A Quantitative Model of Counterfactual,"Inthispaperweexploretwoquantitativeapproachestothemodellingof counterfactualreasoning–alinearandanoisy-ORmodel–basedonin- formationcontainedinconceptualdependencynetworks.Empiricaldata isacquiredinastudyandthefitofthemodelscomparedtoit. Wecon- cludebyconsideringtheappropriatenessofnon-parametricapproaches tocounterfactualreasoning,andexaminingtheprospectsforotherpara- metricapproachesinthefuture. 1 Introduction Ifrobinsdidn’thavewingswouldtheystillbeabletofly,eatwormsorbuildnests? Pre- viousworkoncounterfactualreasoninghastendedtocharacterisetheprocessesbywhich questions such as these are answered in purely qualitative terms, either focusing on the factorsdeterminingtheironsetandconsequences(seeRoese,1997,forareview);thequal- itative outline of their psychological characteristics (Kahnemanand Miller, 1986; Byrne and Tasso, 1999); or else their logical or schematic properties (Lewis, 1973; Goodman, 1983). And althoughPearl (2000)has describeda formalismaddressingquantitativeas- pectsofc",NLP,c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf,2001
A Neural Oscillator Model of Auditory,"A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. 1 Introduction In virtually all listening situations, we are exposed to a mixture of sound energy from multiple sources. Hence, the auditory system must separate an acoustic mixture in order to create a perceptual description of each sound source. It has been proposed that this process of auditory scene analysis (ASA) [2] takes place in two conceptual stages: segmentation in which the acoustic mixture is separated into its constituent ‘atomi",Computer Vision,c3535febaff29fcb7c0d20cbe94391c7-Paper.pdf,2001
Self-regulation Mechanism of Temporally,"Recentbiologicalexperimental(cid:12)ndingshaveshownthatthesynap- tic plasticity depends on the relative timing of the pre- and post- synaptic spikes whichdetermines whether LongTermPotentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called \Temporally Asymmetric Hebbian plas- ticity (TAH)"". Many authors have numerically shown that spatio- temporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal pat- terns is stillunknown, especially the e(cid:11)ects of LTD.In this paper, we employ a simple neural network model and show that inter- ference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is in- dispensable for storing sparse patterns. We also show that TAH qualitatively has the same e(cid:11)ect as the covariance learning when spatio-temporal patterns are embedded in the network. 1 Introduction Recent biologicalexpe",Computer Vision,c4de8ced6214345614d33fb0b16a8acd-Paper.pdf,2001
"An Efficient, Exact Algorithm for Solving","We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games. 1 Introduction Seeking to replicate the representational and computational benefits that graph ical models have provided to probabilistic inference, several recent works have introduced graph-theoretic frameworks for the study of multi-agent sys tems (La Mura 2000; Koller and Milch 2001; Kearns et al. 2001). In the simplest of these formalisms, each vertex represents a single agent, and the edges represent pairwise interaction between agents. As with many familiar network models, the macroscopic behavior of a large system is thus implicitly described by its local inter actions, and the computational challenge is to extract the global states of interest. Classical game theory is typically used to model ",Optimization & Theoretical ML,c5866e93cab1776890fe343c9e7063fb-Paper.pdf,2001
KLD-Sampling: Adaptive Particle Filters,"Overthelastyears,particlefiltershavebeenappliedwithgreatsuccessto avarietyofstateestimationproblems.Wepresentastatisticalapproachto increasingtheefficiencyofparticlefilters byadaptingthesize ofsample setson-the-fly.ThekeyideaoftheKLD-samplingmethodistoboundthe approximationerrorintroducedbythesample-basedrepresentationofthe particlefilter. ThenameKLD-samplingisduetothefactthatwemeasure theapproximationerrorbytheKullback-Leiblerdistance. Ouradaptation approachchoosesasmallnumberofsamplesifthedensityisfocusedon a small partof thestate space, andit choosesa largenumberof samples ifthestateuncertaintyishigh. Boththeimplementationandcomputation overheadofthisapproacharesmall. Extensiveexperimentsusingmobile robotlocalizationasatestapplicationshowthatourapproachyieldsdrastic improvementsover particle filters with fixed sample set sizes and overa previouslyintroducedadaptationtechnique. 1 Introduction Estimatingthestateofadynamicsystembasedonnoisysensormeasurementsisextremely importantinareas",Optimization & Theoretical ML,c5b2cebf15b205503560c4e8e6d1ea78-Paper.pdf,2001
Causal Categorization with Bayes Nets,"A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a categorys causal model. On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e.g., correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships. Research on the topic of categorization has traditionally focused on the problem of learning new categories given observations of category members. In contrast, the theory-based view of categories emphasizes the influe",NLP,c5a4e7e6882845ea7bb4d9462868219b-Paper.pdf,2001
Unsupervised Learning of Human Motion,"This paper presents an unsupervisedlearningalgorithmthat can derive theprobabilisticdependencestructureofpartsofanobject(amovinghu- manbodyinourexamples)automaticallyfromunlabeleddata. Thedis- tinguishedpartofthisworkisthatitisbasedonunlabeleddata,i.e.,the training features include both useful foreground parts and background clutter and the correspondencebetween the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algo- rithmisapplied. Agreedyalgorithmisdevelopedtoselectpartsandto searchfortheoptimalstructurebasedonthedifferentialentropyofthese variables. The success of ouralgorithmis demonstratedbyapplyingit togeneratemodelsofhumanmotionautomaticallyfromunlabeledreal imagesequences. 1 Introduction Humanmotiondetectionandlabelingisaveryi",Computer Vision,c6335734dbc0b1ded766421cfc611750-Paper.pdf,2001
Orientational and geometric,"Wepresent a modelofthefiring ofplace andhead-direction cellsin rat hippocampus. The modelcanpredict theresponse ofindividual cells and populationsto parametricmanipulations ofboth geomet ric (e.g. O'Keefe & Burgess, 1996) and orientational (Fenton et aI., 2000a) cues, extending a previous geometric model (Hartley et al., 2000). It provides a functional description of how these cells' spatial responses are derived from the rat's environment and makeseasilytestablequantitativepredictions. Considerationofthe phenomenon ofremapping (Muller & Kubie, 1987; Bostock et aI., 1991) indicates that the model may also be consistent with non parametricchanges infiring, andprovides constraints for itsfuture development. 1 Introduction 'Place cells' recorded in the hippocampus of freely moving rats encode the rat's current location (O'Keefe & Dostrovsky, 1971; Wilson & McNaughton, 1993). In open environments a place cell will fire whenever the rat enters a specific portion ofthe environment (the 'plac",Computer Vision,c8758b517083196f05ac29810b924aca-Paper.pdf,2001
Covariance Kernels from Bayesian,"We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this frame work which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1 Introduction Kernel machines, such as Support Vector machines or Gaussian processes, are pow erful and frequently used tools for solving statistical learning problems. They are based on the use of a kernel function which encodes task prior knowledge in a Bayesian manner. In this paper, we propose the framework of mutual informa tion (MI) kernels for learning covariance kernels from unlabeled task data using Bayesian techniques. This section introduces terms and concepts. We also discuss some general ideas for discriminative semi-supervised l",Optimization & Theoretical ML,c902b497eb972281fb5b4e206db38ee6-Paper.pdf,2001
Modeling Temporal Structure in Classical,"The Temporal Coding Hypothesis of Miller and colleagues [7] sug gests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a con strained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order condition ing experiments of Miller et al. [1, 2, 3], which other models are unable to explain. 1 Introduction Animal learning involves more than just predicting reinforcement. The well-known phenomena of latent learning and sensory preconditioning indicate that animals learn about stimuli in their environment before any reinforcement is supplied. More recently, a series of experiments by R. R. Miller and colleagues has demonstrated that in classical conditioning paradigms, animals appear to learn the temporal struc ture of the stimuli [8]. We will review three of these experiments. We then present a model of condit",Optimization & Theoretical ML,c92a10324374fac681719d63979d00fe-Paper.pdf,2001
Modeling the Modulatory Effect of,"We present new simulation results, in which a computational model of interacting visual neurons simultaneously predicts the modula tion of spatial vision thresholds by focal visual attention, for five dual-task human psychophysics experiments. This new study com plements our previous findings that attention activates a winner take-all competition among early visual neurons within one cortical hypercolumn. This ""intensified competition"" hypothesis assumed that attention equally affects all neurons, and yielded two single unit predictions: an increase in gain and a sharpening of tuning with attention. While both effects have been separately observed in electrophysiology, no single-unit study has yet shown them si multaneously. Hence, we here explore whether our model could still predict our data if attention might only modulate neuronal gain, but do so non-uniformly across neurons and tasks. Specifically, we investigate whether modulating the gain of only the neurons that are loudest, be",Computer Vision,cbef46321026d8404bc3216d4774c8a9-Paper.pdf,2001
Approximate Dynamic Programming,"The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such prob lems. The approach ""fits"" a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop bounds on the approximation error and present experi mental results in the domain of queueing network control, providing empirical support for the methodology. 1 Introduction Dynamic programming offers a unified approach to solving problems of stochastic control. Central to the methodology is the cost-to-go function, which can obtained via solving Bellman's equation. The domain of the cost-to-go function is the state space of the system to be controlled, and dynamic programming algorithms com pute and store a table consisting of one cost-to-go value per state. Unfortunately, the size of a s",Optimization & Theoretical ML,cd4bb35c75ba84b4f39e547b1416fd35-Paper.pdf,2001
A New Discriminative Kernel From Probabilistic Models,"Recently, Jaakkola and Haussler proposed a method for construct ing kernel functions from probabilistic models. Their so called ""Fisher kernel"" has been combined with discriminative classifiers such as SVM and applied successfully in e.g. DNA and protein analysis. Whereas the Fisher kernel (FK) is calculated from the marginal log-likelihood, we propose the TOP kernel derived from Tangent vectors Of Posterior log-odds. Furthermore we develop a theoretical framework on feature extractors from probabilistic models and use it for analyzing FK and TOP. In experiments our new discriminative TOP kernel compares favorably to the Fisher kernel. 1 Introduction In classification tasks, learning enables us to predict the output y E {-1, + 1} of some unknown system given the input a! E X based on the training examples {a!i' y;}i=l' The purpose of a feature extractor f : X --+ ]RD is to convert the representation of data without losing the information needed for classification [3]. When X is a vecto",Optimization & Theoretical ML,cee8d6b7ce52554fd70354e37bbf44a2-Paper.pdf,2001
Kernel Feature Spaces and,"Inkernelbasedlearningthedataismappedtoakernelfeaturespaceof a dimensionthat correspondsto the numberof trainingdata points. In practice,however,thedataformsasmallersubmanifoldinfeaturespace, afactthathas beenusede.g.byreducedset techniquesforSVMs. We proposeanewmathematicalconstructionthatpermitstoadapttothein- trinsicdimensionandtofindanorthonormalbasisofthissubmanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experimentsdemonstratethegoodperformanceandhighcomputational efficiencyofourkTDSEPalgorithmfortheproblemofnonlinearBSS. 1 Introduction Inawidespreadareaofapplicationskernelbasedlearningmachines,e.g.SupportVector Machines(e.g.[19,6])giveexcellentsolutions.Thisholdsbothforproblemsofsupervised andunsupervisedlearning(e.g.[3, 16,12]). Thegeneralideais tomapthedata x (i = i 1;:::;T)intosomekernelfeaturespace F bysomem",Computer Vision,cf2226ddd41b1a2d0ae51dab54d32c36-Paper.pdf,2001
The Unified Propagation and Scaling Algorithm,"In this paper we will show that a restricted class of constrained mini- mumdivergenceproblems,namedgeneralizedinferenceproblems,can besolvedbyapproximatingtheKLdivergencewithaBethefreeenergy. Thealgorithmwederiveiscloselyrelatedtobothloopybeliefpropaga- tionanditerativescaling.Thisunifiedpropagationandscalingalgorithm reducestoaconvergentalternativetoloopybeliefpropagationwhenno constraintsarepresent.Experimentsshowtheviabilityofouralgorithm. 1 Introduction Formanyinterestingmodels,exactinferenceis intractible. Treesareanotableexception whereBeliefPropagation(BP)canbeemployedtocomputetheposteriordistribution[1]. BP on loopy graphscan still be understoodas a form of approximateinferencesince its fixedpointsarestationarypointsoftheBethefreeenergy[2].Aseeminglyunrelatedprob- lemisthatoffindingthedistributionwithminimimKLdivergencetoapriordistribution subjecttosomeconstraints. Thisproblemcanbesolvedthroughtheiterativescaling(IS) procedure[3].Althoughalotofworkhasbeendoneonapproximateinfere",Optimization & Theoretical ML,d0fb963ff976f9c37fc81fe03c21ea7b-Paper.pdf,2001
Receptive field structure of flow detectors,"Observer translation relative to the world creates image flow that expands from the observer's direction of translation (heading) from which the observer can recover heading direction. Yet, the image flow is often more complex, depending on rotation of the eye, scene layout and translation velocity. A number of models [1-4] have been proposed on how the human visual system extracts heading from flow in a neurophysiologic ally plausible way. These models represent heading by a set of neurons that respond to large image flow patterns and receive input from motion sensed at different im age locations. We analysed these models to determine the exact receptive field of these heading detectors. We find most models predict that, contrary to widespread believe, the contributing mo tion sensors have a preferred motion directed circularly rather than radially around the detector's preferred heading. Moreover, the re sults suggest to look for more refined structure within the circular flow, such ",Computer Vision,d198bd736a97e7cecfdf8f4f2027ef80-Paper.pdf,2001
Bayesian morphometry of hippocampal cells,"Visual inspection of neurons suggests that dendritic orientation may be determined both by internal constraints (e.g. membrane tension) and by external vector fields (e.g. neurotrophic gradients). For example, basal dendrites of pyramidal cells appear nicely fan-out. This regular orientation is hard to justify completely with a general tendency to grow straight, given the zigzags observed experimentally. Instead, dendrites could (A) favor a fixed (“external”) direction, or (B) repel from their own soma. To investigate these possibilities quantitatively, reconstructed hippocampal cells were subjected to Bayesian analysis. The statistical model combined linearly factors A and B, as well as the tendency to grow straight. For all morphological classes, B was found to be significantly positive and consistently greater than A. In addition, when dendrites were artificially re-oriented according to this model, the resulting structures closely resembled real morphologies. These results suggest ",Optimization & Theoretical ML,d3a7f48c12e697d50c8a7ae7684644ef-Paper.pdf,2001
Audio-Visual Sound Separation Via,"It is well known that under noisy conditions we can hear speech much more clearly when we read the speaker's lips. This sug gests the utility of audio-visual information for the task of speech enhancement. We propose a method to exploit audio-visual cues to enable speech separation under non-stationary noise and with a single microphone. We revise and extend HMM-based speech enhancement techniques, in which signal and noise models are fac tori ally combined, to incorporate visual lip information and em ploy novel signal HMMs in which the dynamics of narrow-band and wide band components are factorial. We avoid the combina torial explosion in the factorial model by using a simple approxi mate inference technique to quickly estimate the clean signals in a mixture. We present a preliminary evaluation of this approach using a small-vocabulary audio-visual database, showing promising improvements in machine intelligibility for speech enhanced using audio and visual information. 1 Introductio",Computer Vision,d47268e9db2e9aa3827bba3afb7ff94a-Paper.pdf,2001
"Entropy and Inference, Revisited","Westudypropertiesofpopularnear–uniform(Dirichlet)priorsforlearn- ingundersampledprobabilitydistributionsondiscretenonmetricspaces andshowthattheyleadtodisastrousresults. However,anOccam–style phasespaceargumentexpandsthepriorsintotheirinfinitemixtureand resolvesmostoftheobservedproblems.Thisleadstoasurprisinglygood estimatorofentropiesofdiscretedistributions. Learning a probability distribution from examples is one of the basic problems in data analysis.Commonpracticalapproachesintroduceafamilyofparametricmodels,leadingto questionsaboutmodelselection. InBayesianinference,computingthetotalprobabilityof thedataarisingfromamodelinvolvesanintegrationoverparameterspace,andtheresulting “phasespacevolume”automaticallydiscriminatesagainstmodelswithlargernumbersof parameters—hencethedescriptionofthesevolumetermsasOccamfactors[1,2]. Aswe movefromfiniteparameterizationstomodelsthataredescribedbysmoothfunctions,the integrals over parameter space become functional integrals and methods from quantum",Optimization & Theoretical ML,d46e1fcf4c07ce4a69ee07e4134bcef1-Paper.pdf,2001
Spectral Relaxation for K-means,"The popular K-means clustering partitions a data set by minimiz ing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by com puting a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by comput ing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function. 1 Introduction K-means is a very popular method for general clustering [6]. In K-means clusters are represented by centers of mass of their members, and it can be shown that the K-means algorithm of alternating between assigning cluster membership for each data ",Optimization & Theoretical ML,d5c186983b52c4551ee00f72316c6eaa-Paper.pdf,2001
Gaussian Process Regression with,"Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learn ing with excessively strong smoothness assumptions can be partic ularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher func tio",Optimization & Theoretical ML,d68a18275455ae3eaa2c291eebb46e6d-Paper.pdf,2001
Kernel Machines and Boolean Functions,"Wegiveresultsaboutthelearnabilityandrequiredcomplexityoflogical formulaetosolveclassificationproblems. Theseresultsareobtainedby linkingpropositionallogicwithkernelmachines. Inparticularweshow that decision trees and disjunctive normal forms (DNF) can be repre- sentedbythehelpofaspecialkernel,linkingregularizedrisktosepara- tionmargin. Subsequentlywederivea numberof lower boundsonthe requiredcomplexityoflogicformulaeusingpropertiesofalgorithmsfor generationoflinearestimators,suchasperceptronandmaximalpercep- tronlearning. 1 Introduction The question of how many Boolean primitives are needed to learn a logical formula is typicallyanNP-hardproblem,especiallywhenlearningfromnoisydata. Likewise,when dealingwithdecisiontrees,thequestionwhatdepthandcomplexityofatreeisrequiredto learnacertainmappinghasproventobeadifficulttask. WeaddressthisissueinthepresentpaperandgivelowerboundsonthenumberofBoolean functionsrequiredtolearnamapping. Thisisachievedbyaconstructivealgorithmwhich can be carried o",Optimization & Theoretical ML,d77f00766fd3be3f2189c843a6af3fb2-Paper.pdf,2001
Information Geometrical Framework for,"Themysteryofbeliefpropagation(BP)decoder,especiallyoftheturbo decoding,isstudiedfrominformationgeometricalviewpoint.Theloopy belief network (BN) of turbocodes makes it difficult to obtain the true “belief” by BP, and the characteristics of the algorithmand its equilib- riumarenotclearlyunderstood.Ourstudygivesanintuitiveunderstand- ingofthemechanism,andanewframeworkfortheanalysis. Basedon theframework,werevealbasicpropertiesoftheturbodecoding. 1 Introduction Since the proposal of turbo codes[2], they have been attracting a lot of interests because oftheirhighperformanceoferrorcorrection. Althoughthethoroughexperimentalresults strongly support the potential of this iterative decoding method, the mathematical back- ground is not sufficiently understood. McEliece et al.[5] have shown its relation to the Pearl’sBP, buttheBN fortheturbodecodingis loopy,andtheBP solutiongivesonlyan approximation. Theproblemoftheturbodecodingisaspecificexampleofageneralproblemofmarginaliz- inganexponentialfam",Computer Vision,d7a84628c025d30f7b2c52c958767e76-Paper.pdf,2001
Efficient Resources Allocation,"Itis desirablethat a complexdecision-makingprobleminanuncer tain world be adequately modeled by a Markov Decision Process (MDP) whosestructuralrepresentationis adaptivelydesignedbya parsimonious resources allocation process. Resources include time andcostofexploration,amountofmemoryandcomputationaltime allowed for the policy or value function representation. Concerned about making the best use of the available resources, we address the problem ofefficiently estimating where adding extraresources is highly needed in order to improve the expected performance of the resultingpolicy. Possible application in reinforcement learning (RL),whenreal-worldexplorationis highlycostly, concernsthede tection ofthose areas ofthe state-space that need primarily to be explored in order to improve the policy. Another application con cerns approximation of continuous state-space stochastic control problems using adaptive discretization techniques for which highly efficient gridpoints allocationismandatory",Optimization & Theoretical ML,d79c6256b9bdac53a55801a066b70da3-Paper.pdf,2001
Speech Recognition using SVMs,"An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional gen erative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1 Introduction Speech recognition is a complex, dynamic classification task. State-of-the-art sys tems use Hidden Markov Models (HMMs), either trained to maximise likelihood or discriminatively, to achieve good levels of performance. One of the reasons for the popularity of HMMs is that they readily handle the va",NLP,d8330f857a17c53d217014ee776bfd50-Paper.pdf,2001
Learning Discriminative Feature Transforms,"ThemarriageofRenyientropywithParzendensityestimationhasbeen showntobe a viabletoolin learningdiscriminativefeaturetransforms. However, it suffers from computational complexityproportionalto the squareofthenumberofsamplesinthetrainingdata.Thissetsapractical limittousinglargedatabases. Wesuggestimmediatedivorceofthetwo methodsandremarriageofRenyientropywithasemi-parametricdensity estimationmethod,suchasaGaussianMixtureModels(GMM).Thisal- lows all of the computationto take place in the low dimensionaltarget space, and it reduces computational complexity proportional to square of the number of components in the mixtures. Furthermore, a conve- nientextensiontoHiddenMarkovModelsascommonlyusedinspeech recognitionbecomespossible. 1 Introduction Featureselectionorfeaturetransformsareimportantaspectsofanypatternrecognitionsys- tem. Optimalfeatureselectioncoupledwithaparticularclassifiercanbedonebyactually training and evaluating the classifier using all combinations of available features. Obvi- ",Optimization & Theoretical ML,d860edd1dd83b36f02ce52bde626c653-Paper.pdf,2001
Information-Geometrical Significance of,"Wereportaresultofperturbationanalysisondecodingerrorofthebelief propagationdecoderforGallagercodes. Theanalysisisbasedoninfor- mationgeometry,anditshowsthattheprincipaltermofdecodingerror atequilibriumcomesfromthe m-embeddingcurvatureofthelog-linear submanifoldspannedbytheestimatedpseudoposteriors,oneforthefull marginal,and K forpartialposteriors,eachofwhichtakesasinglecheck intoaccount,where K isthenumberofchecksintheGallagercode.Itis thenshownthattheprincipalerrortermvanisheswhentheparity-check matrixofthecodeissosparsethattherearenotwocolumnswithoverlap greaterthan1. 1 Introduction Recent progress on error-correcting codes has attracted much attention because their de- coders,exhibitingperformanceveryclosetoShannon'slimit,canbeimplementedasneu- ralnetworks. Important examples areturbocodesandGallager codes[1]. Itisnowwell understood that application of belief propagation to the respective graphical representa- tions of the decoding problems for both codes yields practically efficien",Optimization & Theoretical ML,dc513ea4fbdaa7a14786ffdebc4ef64e-Paper.pdf,2001
Adaptive Sparseness Using Jeffreys Prior,"Inthispaperweintroduceanewsparsenessinducingpriorwhichdoesnotinvolveany(hy- per)parametersthatneedtobeadjustedorestimated.Althoughotherapplicationsarepossi- ble,wefocushereonsupervisedlearningproblems:regressionandclassification. Experi- mentswithseveralpubliclyavailablebenchmarkdatasetsshowthattheproposedapproach yieldsstate-of-the-artperformance. Inparticular,ourmethodoutperformssupportvector machines and performs competitivelywith the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparseness- controllinghyper-parameters. 1 Introduction The goal of supervised learning is to infer a functional relation (cid:0)(cid:2) (cid:1)(cid:4) (cid:3)(cid:6) (cid:5)(cid:8) (cid:7)(cid:10) (cid:9) , based on a set of(maybe noisy)trainingexamples (cid:11) (cid:1)(cid:13) (cid:12)(cid:14) (cid:5)(cid:8) (cid:7)(cid:6) (cid:15)(cid:17) (cid:16)(cid:18) (cid:0)(cid:14) (cid:15)(cid:19) (cid:9)(cid:20) (cid:16)(cid:22) (cid:21)(c",Optimization & Theoretical ML,dd055f53a45702fe05e449c30ac80df9-Paper.pdf,2001
An Efficient Clustering Algorithm Using,"Thispaperdescribesaclusteringalgorithmforvectorquantizersusinga “stochasticassociationmodel”. Itoffersanewsimpleandpowerfulsoft- max adaptation rule. The adaptation process is the same as the on-line K-meansclusteringmethodexceptforaddingrandomfluctuationinthe distortion error evaluation process. Simulation results demonstrate that thenewalgorithmcanachieveefficientadaptationashighasthe“neural gas”algorithm,whichisreportedasoneofthemostefficientclustering methods. Itisakeytoadduncorrelatedrandomfluctuationinthesimi- larityevaluationprocessforeachreferencevector. Forhardwareimple- mentationofthisprocess,weproposeananostructure, whoseoperation isdescribedbyasingle-electroncircuit. Itpositivelyusesfluctuationin quantummechanicaltunnelingprocesses. 1 Introduction Vector quantization (VQ) techniques are used in a wide range of applications, including speech and image processing, data compression. VQ techniques encode a data manifold (cid:0)(cid:2) (cid:1)(cid:4) (cid:3)(cid:6) (cid:5) using",Optimization & Theoretical ML,de03beffeed9da5f3639a621bcab5dd4-Paper.pdf,2001
Grammar Transfer in a Second Order,"It has been known that people, after being exposed to sentences generated by an artificial grammar, acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modified grammar. We show that a second order recurrent neural network is able to transfer grammatical knowledge from one language (generated by a Finite State Machine) to another language which differ both in vocabularies and syntax. Representa tion of the grammatical knowledge in the network is analyzed using linear discriminant analysis. 1 Introduction In the field of artificial grammar learning, people are known to be able to transfer grammatical knowledge to a new language which consists of a new vocabulary [6]. Furthermore, this effect persists even when the new strings violate the syntactic rule slightly as long as they are similar to the old strings [1]. It has been shown in the past studies that recurrent neural networks also have the ability to generalize previously a",NLP,de73998802680548b916f1947ffbad76-Paper.pdf,2001
Prod uct Analysis:,"Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear com bination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis, called ""product analy sis"", that models the observed variables as a linear combination of products of normally distributed hidden variables. Just as fac tor analysis can be viewed as unsupervised linear regression on unobserved, normally distributed hidden variables, product anal ysis can be viewed as unsupervised linear regression on products of unobserved, normally distributed hidden variables. The map ping between the data and the hidden space is nonlinear, so we use an approximate variational technique for inference and learn ing. Since product analysis is a generalization of factor analysis, product analysis always fi",Optimization & Theoretical ML,deb54ffb41e085fd7f69a75b6359c989-Paper.pdf,2001
Matching Free Trees with Replicator Equations,"Motivatedbyourrecentworkonrootedtreematching,inthispaperwe providea solution to the problem of matchingtwo free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are inone-to-onecorrespondencewithmaximalcommonsubtrees. Wethen solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of thesesimpledynamicstoescapefromlocaloptima,theyalwaysreturned agloballyoptimalsolution. 1 Introduction Graphmatchingisaclassicproblemincomputervisionandpatternrecognition,instances ofwhichariseinareasasdiverseasobjectrecognition,motionandstereoanalysis[1]. In many problems (e.g., [2, 11, 19]) the graphs at hand have a peculiar structure: they are connected and acyclic, i.e. they are free trees. Note that, unlike “rooted” trees, in free treesthereisnodistinguishednodeplayingtheroleoftheroot,andhencenohierarchyis imposedonthem.St",Computer Vision,df4fe8a8bcd5c95cdb640aa9793bb32b-Paper.pdf,2001
Multiplicative Updates for Classification,"Weinvestigatealearningalgorithmfortheclassificationofnonnegativedataby mixture models. Multiplicative updaterules are derivedthat directly optimize theperformanceofthesemodelsasclassifiers. Theupdateruleshaveasimple closedform andanintuitive appeal. Ouralgorithm retainsthemainvirtuesof theExpectation-Maximization(EM)algorithm—itsguaranteeofmonotonicim- provement,anditsabsenceoftuningparameters—withtheaddedadvantageof optimizingadiscriminativeobjectivefunction. Thealgorithmreducesasaspe- cialcasetothemethodofgeneralizediterativescalingforlog-linearmodels.The learningrateofthealgorithmiscontrolledbythesparsenessofthetrainingdata. Weusethemethodofnonnegativematrixfactorization(NMF)todiscoversparse distributed representations of the data. This form of feature selection greatly accelerateslearningandmakesthealgorithm practicalonlargeproblems. Ex- perimentsshowthatdiscriminativelytrainedmixturemodelsleadtomuchbetter classificationthancomparablysizedmodelstrainedbyEM. 1 Introduction Mixturemo",Optimization & Theoretical ML,e0a209539d1e74ab9fe46b9e01a19a97-Paper.pdf,2001
The Infinite Hidden Markov Model,"We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transitionparameters,leavingonlythreehyperparameterswhichcanbe learned from data. These three hyperparameters define a hierarchical Dirichletprocesscapableofcapturingarichsetoftransitiondynamics. The three hyperparameterscontrol the time scale of the dynamics, the sparsityoftheunderlyingstate-transitionmatrix,andtheexpectednum- ber of distinct hidden states in a finite sequence. In this framework it isalsonaturaltoallowthealphabetofemittedsymbolstobeinfinite— consider, for example, symbols being possible words appearingin En- glishtext. 1 Introduction Hidden Markov models (HMMs) are one of the most popular methods in machine learning and statistics for modelling sequences such as speech and proteins. An HMM defines a probability distribution over sequences of observations (symbols)",Optimization & Theoretical ML,e3408432c1a48a52fb6c74d926b38886-Paper.pdf,2001
EM-DD: An Improved Multiple-Instance,"We present a new multiple-instance (MI) learning technique (EM DD) that combines EM with the diverse density (DD) algorithm. EM-DD is a general-purpose MI algorithm that can be applied with boolean or real-value labels and makes real-value predictions. On the boolean Musk benchmarks, the EM-DD algorithm without any tuning significantly outperforms all previous algorithms. EM-DD is relatively insensitive to the number of relevant attributes in the data set and scales up well to large bag sizes. Furthermore, EM DD provides a new framework for MI learning, in which the MI problem is converted to a single-instance setting by using EM to estimate the instance responsible for the label of the bag. 1 Introduction The multiple-instance (MI) learning model has received much attention. In this model, each training example is a set (or bag) of instances along with a single label equal to the maximum label among all instances in the bag. The individual instances within the bag are not given labels",Computer Vision,e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf,2001
Fast Parameter Estimation,"We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cav ity method, and an empirical measurement of the Green's func tion. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1 Introduction It is well known that correct choices of hyperparameters in classification and regres sion tasks can optimize the complexity of the data model, and hence achieve the best generalization [1]. In recent years various methods have been proposed to esti mate the optimal hyperparameters in different contexts, such as neural networks [2], support vector machines [3, 4, 5] and Gaussian processes [5]. Most of these meth ods are inspired by the technique of cross-validation or its variant, leave-one-out validation. While the leave-one-out procedure gives an almost unbiased estimate of the generalization error, it is nevertheless very tedious",Optimization & Theoretical ML,e7e23670481ac78b3c4122a99ba60573-Paper.pdf,2001
Learning Body Pose via Specialized Maps,"A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse map ping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an op timal way, as well as performing inference given inputs and knowl edge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional indepen dence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion. 1 Introduction In everyday life, humans can easily estimate body part locations (body pose) from relatively low-resolution images of the projected 3D world (e.g., when viewing a ",Computer Vision,ea6b2efbdd4255a9f1b3bbc6399b58f4-Paper.pdf,2001
Information-geometric decomposition In,"We present an information-geometric measure to systematically investigate neuronal firing patterns, taking account not only of the second-order but also of higher-order interactions. We begin with the case of two neurons for illustration and show how to test whether or not any pairwise correlation in one period is significantly different from that in the other period. In order to test such a hy pothesis of different firing rates, the correlation term needs to be singled out 'orthogonally' to the firing rates, where the null hypoth esis might not be of independent firing. This method is also shown to directly associate neural firing with behavior via their mutual information, which is decomposed into two types of information, conveyed by mean firing rate and coincident firing, respectively. Then, we show that these results, using the 'orthogonal' decompo sition, are naturally extended to the case of three neurons and n neurons in general. 1 Introduction Based on the theory of hierarchic",Optimization & Theoretical ML,ea5a486c712a91e48443cd802642223d-Paper.pdf,2001
Circuits for VLSI Implementation of,Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike tim ing differences between presynaptic and postsynaptic spikes. Sev eral temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an eas ily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1 Introduction Hebbian learning rules modify weights of synapses according to correlations between activity at the input and the output of neurons. Most artificial neural networks using Hebbian learning are based on pulse-rate correlations between continuous valued signals; they reduce the neural spike trains to mean firing rates and thus precise timing does not carry information. With this approach the spiking nature,Computer Vision,ef41d488755367316f04fc0e0e9dc9fc-Paper.pdf,2001
Laplacian Eigenmaps and Spectral,"Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low di mensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to non linear dimensionality reduction that has locality preserving prop erties and a natural connection to clustering. Several applications are considered. In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high di mensional space. For example, gray scale n x n images of a fixed object taken with a moving camera yield data points in rn: n2. However, the intrinsic dimensionality of the space of all images of the same object is the number of degrees of freedom of the camera - in fact the space has the natural",Optimization & Theoretical ML,f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf,2001
A Bayesian Model Predicts Human Parse,"Narayanan andJurafsky(1998)proposedthathumanlanguagecompre- hensioncan bemodeledbytreatinghuman comprehenders asBayesian reasoners, and modeling the comprehension process with Bayesian de- cisiontrees. InthispaperweextendtheNarayananandJurafskymodel to make furtherpredictionsabout readingtime given the probabilityof difference parses or interpretations, and test the model against reading timedatafromapsycholinguisticexperiment. 1 Introduction NarayananandJurafsky(1998)proposedthathumanlanguagecomprehensioncanbemod- eledbytreatinghumancomprehenders as Bayesianreasoners, andmodelingthecompre- hensionprocess withBayesian decisiontrees. In thispaper, we show thatthe model ac- countsforparse-preference andreading timedata from a psycholinguisticexperimenton readingtimeinambiguoussentences. Parsing,(generallycalled‘sentenceprocessing’whenwearereferringtohumanparsing), istheprocessofbuildingupsyntacticinterpretationsforasentencefromaninputsequence of written or spoken words. Ambiguity is extr",NLP,f15d337c70078947cfe1b5d6f0ed3f13-Paper.pdf,2001
MIME: Mutual Information Minimization,"Bayesian belief propagation in graphical models has been recently shown to havevery close ties to inference methods based in statis- tical physics. After Yedidia et al. demonstrated that belief prop- agation (cid:12)xed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guar- anteed to converge to a local minimum of the Bethe free energy. Yuille’salgorithmisbasedonacertaindecompositionoftheBethe free energy and he mentions that other decompositions are possi- ble and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpre- tation as pairwise mutual information minimization and marginal entropymaximization(MIME).Next,weconstructafamilyoffree energyfunctionsfromaspectrumofdecompositionsoftheoriginal Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local min- imum. Preliminary computer simulatio",Optimization & Theoretical ML,f1981e4bd8a0d6d8462016d2fc6276b3-Paper.pdf,2001
Rates of Convergence of Performance Gradient,"WeaddresstwoopentheoreticalquestionsinPolicyGradientReinforce- mentLearning.Thefirstconcernstheefficacyofusingfunctionapprox- imation to represent the state action value function, (cid:0) . Theory is pre- sentedshowingthatlinearfunctionapproximationrepresentationsof (cid:0) can degrade the rate of convergenceof performancegradientestimates byafactorof (cid:1)(cid:3) (cid:2)(cid:5) (cid:4)(cid:7) (cid:6)(cid:9) (cid:8) relativetowhennofunctionapproximationof (cid:0) isused,where (cid:4) isthenumberofpossibleactionsand (cid:6) isthenumber ofbasisfunctionsinthefunctionapproximationrepresentation.Thesec- ondconcernstheuseofa biasterminestimatingthestate actionvalue function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by (cid:1)(cid:3) (cid:2)(cid:11) (cid:10)(cid:13) (cid:12)(cid:14) (cid:2)(cid:15) (cid:10)(cid:17) (cid:16)(cid:18) (cid:4)(cid:19) (cid:8)(cid:20) (cid:8) ,where (cid:4) isthenumberofpossibleac",Optimization & Theoretical ML,f3e52c300b822a8123e7ace55fe15c08-Paper.pdf,2001
A Generalization of Principal Component,"Principal component analysis (PCA) is a commonly applied technique fordimensionalityreduction. PCA implicitlyminimizesa squaredloss function, which may be inappropriate for data that is not real-valued, suchasbinary-valueddata.ThispaperdrawsonideasfromtheExponen- tialfamily,Generalizedlinearmodels,andBregmandistances,togivea generalizationofPCAtolossfunctionsthatwearguearebettersuitedto otherdatatypes. We describealgorithmsforminimizingtheloss func- tions,andgiveexamplesonsimulateddata. 1 Introduction Principal component analysis (PCA) is a hugely popular dimensionality reduction tech- nique that attempts to find a low-dimensional subspace passing close to a given set of points (cid:3)(cid:5) (cid:4)(cid:7) (cid:6)(cid:9) (cid:8)(cid:10) (cid:8)(cid:10) (cid:8)(cid:9) (cid:6)(cid:11) (cid:3)(cid:13) (cid:12)(cid:15) (cid:14)(cid:17) (cid:16)(cid:19) (cid:18) .Morespecifically,inPCA,wefindalowerdimensionalsubspace thatminimizesthesumofthesquareddistancesfromthedatapoints (cid:3)(cid:5) ",Optimization & Theoretical ML,f410588e48dc83f2822a880a68f78923-Paper.pdf,2001
Minimax Probability Machine,"When constructing a classifier, the probability of correct classifi cation of future data points should be maximized. In the current paper this desideratum is translated in a very direct way into an optimization problem, which is solved using methods from con vex optimization. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries. A worst-case bound on the probability of misclassification of future data is ob tained explicitly. 1 Introduction Consider the problem of choosing a linear discriminant by minimizing the probabil ities that data vectors fall on the wrong side of the boundary. One way to attempt to achieve this is via a generative approach in which one makes distributional as sumptions about the class-conditional densities and thereby estimates and controls the relevant probabilities. The need to make distributional assumptions, however, casts doubt on the generality and validity of such an approach, and in discrimina tive solutions",Optimization & Theoretical ML,f48c04ffab49ff0e5d1176244fdfb65c-Paper.pdf,2001
"Duality, Geometry, and Support Vector","We develop an intuitive geometric framework for support vector regression (SVR). By examining when (cid:15)-tubes exist, we show that SVR can be regarded as a classi(cid:12)cationproblem inthe dualspace. Hard and soft (cid:15)-tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variableshifted up anddown by(cid:15). AnovelSVR modelis proposedbasedonchoosingthemax-marginplanebetween thetwo shifted datasets. Maximizing the margin corresponds to shrinking the e(cid:11)ective (cid:15)-tube. In the proposed approach the e(cid:11)ects of the choices of all parameters become clear geometrically. 1 Introduction Support Vector Machines (SVMs) [6] are a very robust methodology for inference with minimal parameter choices. Intuitive geometric formulations exist for the classi(cid:12)cation case addressing both the error metric and capacity control [1, 2]. For linearly separable classi(cid:12)cation, the primal SVM (cid:12)nds ",Optimization & Theoretical ML,f6e794a75c5d51de081dbefa224304f9-Paper.pdf,2001
BLIND SOURCE SEPARATION VIA,"We consider a problem of blind source separation from a set of instan taneous linear mixtures, where the mixing matrix is unknown. It was discovered recently, that exploiting the sparsity of sources in an appro priate representation according to some signal dictionary, dramatically improves the quality of separation. In this work we use the property of multi scale transforms, such as wavelet or wavelet packets, to decompose signals into sets of local features with various degrees of sparsity. We use this intrinsic property for selecting the best (most sparse) subsets of features for further separation. The performance of the algorithm is ver ified on noise-free and noisy data. Experiments with simulated signals, musical sounds and images demonstrate significant improvement of sep aration quality over previously reported results. 1 Introduction In the blind source separation problem an N-channel sensor signal x(~) is generated by M unknown scalar source signals srn(~), linearly mixed to",Optimization & Theoretical ML,f80bf05527157a8c2a7bb63b22f49aaa-Paper.pdf,2001
Estimating the Reliability of leA,"When applying unsupervised learning techniques like ICA or tem poral decorrelation, a key question is whether the discovered pro jections are reliable. In other words: can we give error bars or can we assess the quality of our separation? We use resampling meth ods to tackle these questions and show experimentally that our proposed variance estimations are strongly correlated to the sepa ration error. We demonstrate that this reliability estimation can be used to choose the appropriate ICA-model, to enhance signifi cantly the separation performance, and, most important, to mark the components that have a actual physical meaning. Application to 49-channel-data from an magneto encephalography (MEG) ex periment underlines the usefulness of our approach. 1 Introduction Blind source separation (BSS) techniques have found wide-spread use in various application domains, e.g. acoustics, telecommunication or biomedical signal pro cessing. (see e.g. [9, 5, 6, 1, 2, 4, 14, 8]). BSS is a statistic",Optimization & Theoretical ML,f80ff32e08a25270b5f252ce39522f72-Paper.pdf,2001
Learning from Infinite Data,"We propose the following general method for scaling learning algorithms to arbitrarily large data sets. Consider the model Mii learned by the algorithm using ni examples in step i (ii = (nl, ... ,nm)), and the model Moo that would be learned using in finite examples. Upper-bound the loss L(Mii' Moo) between them as a function of ii, and then minimize the algorithm's time com plexity f(ii) subject to the constraint that L(Moo, Mii) be at most f with probability at most 8. We apply this method to the EM algorithm for mixtures of Gaussians. Preliminary experiments on a series of large data sets provide evidence of the potential of this approach. 1 An Approach to Large-Scale Learning Large data sets make it possible to reliably learn complex models. On the other hand, they require large computational resources to learn from. While in the past the factor limiting the quality of learnable models was typically the quantity of data available, in many domains today data is super-abundant, and t",Optimization & Theoretical ML,f8bf09f5fceaea80e1f864a1b48938bf-Paper.pdf,2001
Features to Distributions on Images,"We describe the g-factor, which relates probability distributions on image features to distributions on the images themselves. The g-factor depends only on our choice of features and lattice quanti zation and is independent of the training image data. We illustrate the importance of the g-factor by analyzing how the parameters of Markov Random Field (i.e. Gibbs or log-linear) probability models of images are learned from data by maximum likelihood estimation. In particular, we study homogeneous MRF models which learn im age distributions in terms of clique potentials corresponding to fea ture histogram statistics (d. Minimax Entropy Learning (MEL) by Zhu, Wu and Mumford 1997 [11]). We first use our analysis of the g-factor to determine when the clique potentials decouple for different features. Second, we show that clique potentials can be computed analytically by approximating the g-factor. Third, we demonstrate a connection between this approximation and the Generalized Iterative Sca",Computer Vision,f8c0c968632845cd133308b1a494967f-Paper.pdf,2001
Distribution of Mutual Information,"The mutual information of two random variables z and J with joint probabilities {7rij} is commonly used in learning Bayesian nets as well as in many other fields. The chances 7rij are usually estimated by the empirical sampling frequency nij In leading to a point es timate J(nij In) for the mutual information. To answer questions like ""is J (nij In) consistent with zero?"" or ""what is the probability that the true mutual information is much larger than the point es timate?"" one has to go beyond the point estimate. In the Bayesian framework one can answer these questions by utilizing a (second order) prior distribution p( 7r) comprising prior information about 7r. From the prior p(7r) one can compute the posterior p(7rln), from which the distribution p(Iln) of the mutual information can be cal culated. We derive reliable and quickly computable approximations for p(Iln). We concentrate on the mean, variance, skewness, and kurtosis, and non-informative priors. For the mean we also give an ",Optimization & Theoretical ML,fb2e203234df6dee15934e448ee88971-Paper.pdf,2001
A Maximum-Likelihood Approach to,"Multisensory response enhancement (MRE) is the augmentation of the response of a neuron to sensory input of one modality by si multaneous input from another modality. The maximum likelihood (ML) model presented here modifies the Bayesian model for MRE (Anastasio et al.) by incorporating a decision strategy to maximize the number of correct decisions. Thus the ML model can also deal with the important tasks of stimulus discrimination and identifi cation in the presence of incongruent visual and auditory cues. It accounts for the inverse effectiveness observed in neurophysiolog ical recording data, and it predicts a functional relation between uni- and bimodal levels of discriminability that is testable both in neurophysiological and behavioral experiments. 1 Introduction In a typical environment stimuli occur at various positions in space and time. In order to produce a coherent assessment of the external world an individual must constantly discriminate between signals relevant for acti",Computer Vision,fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf,2001
Model Based Population Tracking and,"Probabilistic mixture models are used for a broad range of data anal- ysis tasks such as clustering, classification, predictive modeling, etc. Duetotheirinherentprobabilisticnature, mixturemodelscaneasilybe combined with other probabilistic or non-probabilistic techniques thus formingmorecomplexdataanalysissystems. Inthecaseofonlinedata (wherethereisastreamofdataavailable)modelscanbeconstantlyup- datedtoreflectthemostcurrentdistributionoftheincomingdata. How- ever, in many business applications the models themselves represent a parsimonious summary of the data and therefore it is not desirable to changemodelsfrequently,muchlesswitheverynewdatapoint. Insuch aframeworkitbecomescrucialtotracktheapplicabilityofthemixture model and detect the point in time when the model fails to adequately represent the data. In this paper we formulate the problem of change detectionandproposeaprincipledsolution. Empiricalresultsoverboth syntheticandreal-lifedatasetsarepresented. 1 IntroductionandNotation ",Optimization & Theoretical ML,ef8446f35513a8d6aa2308357a268a7e-Paper.pdf,2001
Model-Free Least Squares Policy Iteration,"We proposea newapproachto reinforcementlearningwhichcombines leastsquaresfunctionapproximationwithpolicyiteration. Ourmethod is model-freeandcompletelyoff policy. We aremotivatedbythe least squarestemporaldifferencelearningalgorithm(LSTD),whichisknown for its efficient use of sample experiences compared to pure temporal differencealgorithms.LSTDisidealforpredictionproblems,howeverit heretoforehasnothadastraightforwardapplicationtocontrolproblems. Moreover,approximationslearnedbyLSTDarestronglyinfluencedby thevisitationdistributionoverstates. Ournewalgorithm,LeastSquares PolicyIteration(LSPI)addressestheseissues.Theresultisanoff-policy methodwhichcanuse(orreuse)datacollectedfromanysource.Wehave testedLSPIonseveralproblems,includingabicyclesimulatorinwhich itlearnstoguidethebicycletoagoalefficientlybymerelyobservinga relativelysmallnumberofcompletelyrandomtrials. 1 Introduction Linear least squares function approximatorsoffer many advantages in the context of re- inforcement learning. Wh",Optimization & Theoretical ML,fca0789e7891cbc0583298a238316122-Paper.pdf,2001
"Temporal Coherence, Natural Image Sequences,","We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural imagesequences.Thepropertiesaresimple-cell-likereceptivefieldsand complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the first approachweextractreceptivefieldswhoseoutputsareastemporallyco- herentaspossible. Thisapproachyieldssimple-cell-likereceptivefields (oriented,localized,multiscale).Thus,temporalcoherenceisanalterna- tivetosparsecodinginmodelingtheemergenceofsimplecellreceptive fields.Thesecondapproachisbasedonatwo-layerstatisticalgenerative modelofnaturalimagesequences. Inadditiontomodelingthetemporal coherenceofindividualsimplecells,thismodelincludesinter-celltem- poral dependencies. Estimation of this model from natural data yields both simple-cell-like receptive fields, and complex-cell-like pooling of simplecelloutputs. Inthiscompletelyunsupervisedlearning,bothlay-",Computer Vision,00a03ec6533ca7f5c644d198d815329c-Paper.pdf,2002
Nonparametric Representation of Policies and,"A longstanding goal of reinforcement learning is to develop non- parametric representations of policies and value functions that support rapid learning without suffering from interference or the curse of di- mensionality. Wehavedevelopedatrajectory-basedapproach,inwhich policiesandvaluefunctionsarerepresentednonparametricallyalongtra- jectories.Thesetrajectories,policies,andvaluefunctionsareupdatedas thevaluefunctionbecomesmoreaccurateorasamodelofthetaskisup- dated. Wehaveappliedthisapproachtoperiodictaskssuchashopping and walking, which requiredhandling discount factors and discontinu- itiesinthetaskdynamics,andusingfunctionapproximationtorepresent valuefunctionsatdiscontinuities.Wealsodescribeextensionsoftheap- proach to make the policies more robust to modeling error and sensor noise. 1 Introduction Thewidespreadapplicationofreinforcementlearningishinderedbyexcessivecostinterms ofoneormoreofrepresentationalresources,computationtime,oramountoftrainingdata. Thegoalofourresearchprogram",Reinforcement Learning,01894d6f048493d2cacde3c579c315a3-Paper.pdf,2002
A Difierential Semantics for Jointree,"A new approach to inference in belief networks has been recently proposed, which is based on an algebraic representation of belief networksusingmulti{linearfunctions. Accordingtothisapproach, thekeycomputationalquestionisthatofrepresentingmulti{linear functions compactly, since inference reduces to a simple process of evaluating and difierentiating such functions. W e show here that mainstream inference algorithms based on jointrees are a special case of this approach in a very precise sense. W e use this result to prove new properties of jointree algorithms, and then discuss some of its practical and theoretical implications. 1 Introduction It was recently shown that the probability distribution of a belief network can be represented using a multi{linear function, and that most probabilistic queries of interest can be retrieved directly from the partial derivatives of this function [2]. Although the multi{linear function has an exponential number of terms, it can 1 be represented usin",Optimization & Theoretical ML,01eee509ee2f68dc6014898c309e86bf-Paper.pdf,2002
“Name That Song!”: A Probabilistic Approach,"Wepresentanovel,flexiblestatisticalapproachformodellingmusicand textjointly. Theapproachisbasedonmulti-modalmixturemodelsand maximumaposterioriestimationusingEM.Thelearnedmodelscanbe usedtobrowsedatabaseswithdocumentscontainingmusicandtext,to search for music using queriesconsisting of music and text (lyrics and other contextual information), to annotate text documents with music, andtoautomaticallyrecommendoridentifysimilarsongs. 1 Introduction Variationson“namethatsong”-typesofgamesarepopularonradioprograms. DJsplaya shortexcerptfromasongandlistenersphoneintoguessthenameofthesong. Ofcourse, callersoftengetitrightwhenDJsprovideextracontextualclues(suchaslyrics,orapiece oftriviaaboutthesongorband).Weareattemptingtoreproducethisabilityinthecontext ofinformationretrieval(IR).Inthispaper,wepresentamethodforqueryingwithwords and/ormusic. Wefocusonmonophonicandpolyphonicmusicalpiecesofknownstructure(MIDIfiles, full music notation, etc.). Retrieving these pieces in multimedia databases, such",Computer Vision,0233f3bb964cf325a30f8b1c2ed2da93-Paper.pdf,2002
Automatic Derivation of Statistical Algorithms:,"Machine learning has reached a point where many probabilistic meth- ods can be understood as variations, extensions and combinations of a much smaller set of abstract themes, e.g., as different instances of the EMalgorithm.Thisenablesthesystematicderivationofalgorithmscus- tomizedfordifferentmodels. Here, we describethe AUTOBAYES sys- temwhichtakesahigh-levelstatisticalmodelspecification,usespower- fulsymbolictechniquesbasedon schema-basedprogramsynthesis and computeralgebratoderiveanefficientspecializedalgorithmforlearning thatmodel,andgeneratesexecutablecodeimplementingthatalgorithm. ThiscapabilityisfarbeyondthatofcodecollectionssuchasMatlabtool- boxesoreventoolsformodel-independentoptimizationsuchas BUGS for Gibbs sampling: complex new algorithms can be generated with- outnewprogramming,algorithmscanbehighlyspecializedandtightly crafted for the exact structure of the model and data, and efficient and commented code can be generated for different languages or systems. We presentautom",Computer Vision,0234c510bc6d908b28c70ff313743079-Paper.pdf,2002
Going Metric: Denoising Pairwise Data,"Pairwise data in empirical sciences typically violate metricity, ei ther due to noise or due to fallible estimates, and therefore are hard to analyze by conventional machine learning technology. In this paper we therefore study ways to work around this problem. First, we present an alternative embedding to multi-dimensional scaling (MDS) that allows us to apply a variety of classical ma chine learning and signal processing algorithms. The class of pair wise grouping algorithms which share the shift-invariance property is statistically invariant under this embedding procedure, leading to identical assignments of objects to clusters. Based on this new vectorial representation, denoising methods are applied in a sec ond step. Both steps provide a theoretically well controlled setup to translate from pairwise data to the respective denoised met ric representation. We demonstrate the practical usefulness of our theoretical reasoning by discovering structure in protein sequence data bases, v",Optimization & Theoretical ML,02e656adee09f8394b402d9958389b7d-Paper.pdf,2002
"String Kernels, Fisher Kernels and Finite","In this paper we show how the generation of documents can be thought of as a k-stage Markov process, which leads to a Fisher ker nel from which the n-gram and string kernels can be re-constructed. The Fisher kernel view gives a more flexible insight into the string kernel and suggests how it can be parametrised in a way that re flects the statistics of the training corpus. Furthermore, the prob abilistic modelling approach suggests extending the Markov pro cess to consider sub-sequences of varying length, rather than the standard fixed-length approach used in the string kernel. We give a procedure for determining which sub-sequences are informative features and hence generate a Finite State Machine model, which can again be used to obtain a Fisher kernel. By adjusting the parametrisation we can also influence the weighting received by the features. In this way we are able to obtain a logarithmic weighting in a Fisher kernel. Finally, experiments are reported comparing the different ker",Computer Vision,03255088ed63354a54e0e5ed957e9008-Paper.pdf,2002
Extracting Relevant Structures with,"Theproblemofextractingtherelevantaspectsofdata,infaceofmultiple conflictingstructures,isinherenttomodelingofcomplexdata. Extract- ingstructureinonerandomvariablethatisrelevantforanothervariable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliaryvariables oftencontainmore in- formation than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also mini- mizingtheinformationaboutanother,irrelevant,variable. Inthispaper we givea generalformulationof thisproblemandderiveits formal,as wellasalgorithmic,solution.Itsoperationisdemonstratedinasynthetic exampleandintworealworldproblemsinthecontextoftextcategoriza- tionandfaceimages.Whiletheoriginalinformationbottleneckproblem is relatedto ratedistortiontheory,with thedistortionmeasur",Computer Vision,04048aeca2c0f5d84639358008ed2ae7-Paper.pdf,2002
How the Poverty of the Stimulus,"Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax. 1 The Language Acquisition Problem For both artificial systems and non-human animals, learning the syntax of natural languages is a notoriously hard problem. All healthy human infants, in contrast, learn any of the approximately 6000 human languages rapidly, accurately and spon taneously. Any explanation of how they accomplish this difficult task must specify the (innate) inductive bias that human infants bring to bear, and the input data that is available to them. Traditionally, the inductive bias is termed - somewhat un fortunately - ""Universal Grammar"", and the input data ""primary linguistic data"". Over the last 30 years or so, a ",NLP,04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf,2002
Classifying Patterns of Visual Motion -,"We report a system that classifies and can learn to classify patterns of visualmotionon-line. Thecompletesystemisdescribedbythedynam- ics of its physical network architectures. The combination of the fol- lowingpropertiesmakes the system novel: Firstly, the front-endof the systemconsistsofanaVLSIopticalflowchipthatcollectivelycomputes 2-Dglobal visualmotionin real-time[1]. Secondly,the complexityof the classification task is significantly reduced by mapping the continu- ousmotiontrajectoriestosequencesof’motionevents’. Andthirdly,all the network structuresare simple and with the exceptionof the optical flowchipbasedonaWinner-Take-All(WTA)architecture. Wedemon- strate the application of the proposed generic system for a contactless man-machineinterfacethatallowstowritelettersbyvisualmotion. Re- gardingthelowcomplexityofthesystem,itsrobustnessandthealready existingfront-end,acompleteaVLSIsystem-on-chipimplementationis realistic,allowingvariousapplicationsinmobileelectronicdevices. 1 Intr",Computer Vision,046ddf96c233a273fd390c3d0b1a9aa4-Paper.pdf,2002
Fast Transformation-Invariant Factor Analysis,"Dimensionalityreductiontechniquessuchasprincipalcomponentanaly- sisandfactoranalysisareusedtodiscoveralinearmappingbetweenhigh dimensional data samples and points in a lower dimensional subspace. In [6], Jojic and Frey introduced mixture of transformation-invariant component analyzers (MTCA) that can account for global transforma- tionssuchastranslationsandrotations,performclusteringandlearnlo- calappearancedeformationsbydimensionalityreduction.However,due toenormouscomputationalrequirementsoftheEMalgorithmforlearn- ingthemodel,O( (cid:4) (cid:1) ) where (cid:4) is thedimensionalityofadatasample, MTCAwasnotpracticalformostapplications.Inthispaper,wedemon- stratehowfastFouriertransformscanreducethecomputationtotheor- derof (cid:4) log (cid:4) . Withthisspeedup,weshowtheeffectivenessofMTCA in various applications - tracking, video textures, clustering video se- quences,objectrecognition,andobjectdetectioninimages. 1 Introduction Dimensionalityreductiontechniquessuchasprincipalcomponentan",Optimization & Theoretical ML,05a70454516ecd9194c293b0e415777f-Paper.pdf,2002
Graph-Driven Features Extraction from,"Wepresentanalgorithmtoextractfeaturesfromhigh-dimensionalgene expressionprofiles, basedonthe knowledgeofa graphwhichlinks to- gether genes known to participate to successive reactions in metabolic pathways. Motivatedbytheintuitionthatbiologicallyrelevantfeatures arelikelyto exhibitsmoothnesswithrespecttothegraphtopology,the algorithm involves encoding the graph and the set of expression pro- filesintokernelfunctions,andperformingageneralizedformofcanoni- calcorrelationanalysisinthecorrespondingreproduciblekernelHilbert spaces. Functionpredictionexperimentsforthegenesoftheyeast S.Cerevisiae validatethis approachbyshowingaconsistentincreasein performance whenastate-of-the-artclassifierusesthevectoroffeaturesinsteadofthe originalexpressionprofiletopredictthefunctionalclassofagene. 1 Introduction Microarraytechnology(DNAchips)isquicklybecomingamajordataproviderinthepost- genomicsera,enablingthemonitoringofthequantityofmessengerRNApresentinacell for several thousandsgenes simultaneously. By",Optimization & Theoretical ML,074177d3eb6371e32c16c55a3b8f706b-Paper.pdf,2002
Spikernels:,"Inner-productoperators,oftenreferredtoaskernelsinstatisticallearning,de- fine a mapping from some input space into a feature space. The focus of thispaperistheconstructionofbiologically-motivatedkernelsforcorticalac- tivities.Thekernelswederive,termedSpikernels,mapspikecountsequences intoanabstractvectorspaceinwhichwecanperformvariouspredictiontasks. WediscussindetailthederivationofSpikernelsanddescribeanefficiental- gorithmforcomputingtheirvalueonanytwosequencesofneuralpopulation spikecounts. Wedemonstratethemeritsofourmodelingapproachusingthe Spikernelandvariousstandardkernelsforthetaskofpredictinghandmove- mentvelocitiesfromcorticalrecordings. Inallofourexperimentsalltheker- nelswetestedoutperformthestandardscalarproductusedinregressionwith theSpikernelconsistentlyachievingthebestperformance. 1 Introduction Neuronalactivityinprimarymotorcortex(MI)duringmulti-jointarmreachingmovementsin2- Dand3-D[1,2]anddrawingmovements[3]hasbeenusedextensivelyasatestbedforgaining understandingofneur",Computer Vision,06d5ae105ea1bea4d800bc96491876e9-Paper.pdf,2002
Spike Timing-Dependent Plasticity,"Address-event representation (AER), originally proposed as a means to communicatesparse neural events betweenneuromorphicchips, has proven efficient in implementing large-scale networks with arbitrary, configurablesynaptic connectivity. In this work, we furtherextendthe functionalityofAERtoimplementarbitrary,configurablesynapticplas- ticity in the address domain. As proof of concept, we implementa bi- ologically inspired form of spike timing-dependent plasticity (STDP) based on relative timing of events in an AER framework. Experimen- talresults fromananalogVLSI integrate-and-firenetworkdemonstrate address domainlearningina task that requiresneuronstogroupcorre- latedinputs. 1 Introduction Ithasbeensuggestedthatthebrain’simpressivefunctionalityresultsfrommassivelypar- allel processingusing simple andefficient computationalelements [1]. Developmentsin neuromorphicengineeringand address-event representation (AER) have provided an in- frastructuresuitable for emulatinglarge-scale neural s",Computer Vision,0a87257e5308197df43230edf4ad1dae-Paper.pdf,2002
Efficient Learning Equilibrium *,"We introduce efficient learning equilibrium (ELE), a normative ap proach to learning in non cooperative settings. In ELE, the learn ing algorithms themselves are required to be in equilibrium. In addition, the learning algorithms arrive at a desired value after polynomial time, and deviations from a prescribed ELE become ir rational after polynomial time. We prove the existence of an ELE in the perfect monitoring setting, where the desired value is the expected payoff in a Nash equilibrium. We also show that an ELE does not always exist in the imperfect monitoring case. Yet, it exists in the special case of common-interest games. Finally, we extend our results to general stochastic games. 1 Introduction Reinforcement learning in the context of multi-agent interaction has attracted the attention of researchers in cognitive psychology, experimental economics, machine learning, artificial intelligence, and related fields for quite some time [8, 4]. Much of this work uses repeated games [3",Optimization & Theoretical ML,0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf,2002
Using Tarjan’s Red Rule for Fast Dependency,"We focus on the problem of efficient learning of dependency trees. It is well-known that given the pairwise mutual information coefficients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time. However, for large data-sets it is the construc- tionofthecorrelationmatrixthatdominatestherunningtime. Wehave developedanewspanning-treealgorithmwhichiscapableofexploiting partialknowledgeaboutedgeweights. Thepartialknowledgewemain- tain is a probabilistic confidence interval on the coefficients, which we derive by examining just a small sample of the data. The algorithm is able to flag the need to shrink an interval, which translates to inspec- tion of more data for the particular attribute pair. Experimental results showrunningtimethatisnear-constantinthenumberofrecords, with- outsignificantlossinaccuracyofthegeneratedtrees. Interestingly,our spanning-treealgorithmisbasedsolelyonTarjan’sred-edgerule,which isgenerallyconsideredaguaranteedrecipeforbadperf",Optimization & Theoretical ML,0f46c64b74a6c964c674853a89796c8e-Paper.pdf,2002
Approximate Inference and,"Side-chain prediction is an important subtask in the protein-folding problem. We show that finding a minimal energy side-chain con figuration is equivalent to performing inference in an undirected graphical model. The graphical model is relatively sparse yet has many cycles. We used this equivalence to assess the performance of approximate inference algorithms in a real-world setting. Specifi cally we compared belief propagation (BP), generalized BP (GBP) and naive mean field (MF). In cases where exact inference was possible, max-product BP al ways found the global minimum of the energy (except in few cases where it failed to converge), while other approximation algorithms of similar complexity did not. In the full protein data set, max product BP always found a lower energy configuration than the other algorithms, including a widely used protein-folding software (SCWRL). 1 Introduction Inference in graphical models scales exponentially with the number of variables. Since many real-wor",Optimization & Theoretical ML,100d9f30ca54b18d14821dc88fea0631-Paper.pdf,2002
Learning a Forward Model of a Reflex,"Wedevelopasystemstheoreticaltreatmentofabehaviouralsystemthat interactswithitsenvironmentinaclosedloopsituationsuchthatitsmo- toractionsinfluenceits sensorinputs. Thesimplestformofafeedback isareflex. Reflexesoccuralways“toolate”; i.e.,only after a(unpleas- ant,painful,dangerous)reflex-elicitingsensoreventhasoccurred. This definesanobjectiveproblemwhichcanbesolvedifanothersensorinput exists which can predict the primary reflex and can generate an earlier reaction. In contrast to previous approaches, our linear learning algo- rithmallowsforananalyticalproofthatthissystemlearnstoapplyfeed- forwardcontrolwiththeresultthatslowfeedbackloopsarereplacedby their equivalent feed-forward controller creating a forward model. In otherwords,learningturnsthereactivesystemintoapro-activesystem. Bymeansofarobotimplementationwedemonstratetheapplicabilityof thetheoreticalresultswhichcanbeusedinavarietyofdifferentareasin physicsandengineering. 1 Introduction Feedbackloopsare prevalentin animal behaviour,",Computer Vision,10907813b97e249163587e6246612e21-Paper.pdf,2002
Information Regularization with Partially,"Classification with partially labeled data requiresusing a largenumber ofunlabeledexamples(oranestimatedmarginal P(x)),tofurthercon- straintheconditionalP(yjx)beyondafewavailablelabeledexamples. We formulatearegularizationapproachtolinkingthemarginalandthe conditional in a general way. The regularization penalty measures the informationthat is implied about the labels over coveringregions. No parametricassumptionsarerequiredandtheapproachremainstractable evenforcontinuousmarginaldensitiesP(x). Wedevelopalgorithmsfor solvingtheregularizationproblemforfinitecovers,establishalimiting differentialequation,andexemplifythebehaviorofthenewregulariza- tionapproachinsimplecases. 1 Introduction Many modernclassification problems are rife with unlabeledexamples. To benefit from suchexamples,wemustexploiteitherimplicitlyorexplicitlythelinkbetweenthemarginal densityP(x)overexamplesxandtheconditionalP(yjx)representingthedecisionbound- aryforthelabels y. Highdensityregionsorclustersinthedata,forexamp",Computer Vision,1145a30ff80745b56fb0cecf65305017-Paper.pdf,2002
Intrinsic Dimension Estimation Using Packing,"Weproposeanewalgorithmtoestimatetheintrinsicdimensionofdata sets. The method is based on geometric properties of the data and re- quiresneitherparametricassumptionsonthedatageneratingmodelnor input parameters to set. The method is compared to a similar, widely- used algorithm from the same family of geometric techniques. Experi- mentsshowthatourmethodismorerobustintermsofthedatagenerating distributionandmorereliableinthepresenceofnoise. 1 Introduction High-dimensionaldatasetshaveseveralunfortunatepropertiesthatmakethemhardtoan- alyze. Thephenomenonthatthecomputationalandstatisticalefficiencyofstatisticaltech- niquesdegraderapidlywiththedimensionisoftenreferredtoasthe“curseofdimension- ality”. One particular characteristic of high-dimensional spaces is that as the volumes of constantdiameterneighborhoodsbecomelarge,exponentiallymanypointsareneededfor reliabledensityestimation.Anotherimportantproblemisthatasthedatadimensiongrows, sophisticateddatastructuresconstructedtospeedupnearestneig",Optimization & Theoretical ML,1177967c7957072da3dc1db4ceb30e7a-Paper.pdf,2002
Mismatch String Kernels for SVM Protein,"Weintroduceaclassofstringkernels, calledmismatchkernels,foruse with support vector machines (SVMs) in a discriminative approach to theproteinclassificationproblem. Thesekernelsmeasuresequencesim- ilarity based on shared occurrences of (cid:1) -length subsequences, counted withupto (cid:2) mismatches, anddonotrelyonanygenerativemodelfor thepositivetrainingsequences.Wecomputethekernelsefficientlyusing a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVMclassifierperformsaswellastheFisherkernel, themostsuccess- fulmethodforremotehomologydetection,whileachievingconsiderable computationalsavings. 1 Introduction Afundamentalproblemincomputationalbiologyistheclassificationofproteinsintofunc- tional and structural classes based on homology (evolutionary similarity) of protein se- quence data. Known methods for protein classification and homology detection include pairwisesequencealignment[1,2,3],profilesforpr",Optimization & Theoretical ML,12b1e42dc0746f22cf361267de07073f-Paper.pdf,2002
Adaptive Caching by Refetching,"Weareconstructingcachingpoliciesthathave13-20%lowermissrates than the best of twelve baseline policies overa largevarietyof request streams.Thisrepresentsanimprovementof49–63%overLeastRecently Used, themostcommonlyimplementedpolicy. We achievethis notby designingaspecificnewpolicybutbyusingon-lineMachineLearning algorithmsto dynamicallyshift betweenthestandardpoliciesbasedon their observed miss rates. A thorough experimental evaluation of our techniques is given, as well as a discussion of what makes caching an interestingon-linelearningproblem. 1 Introduction Cachingisubiquitousinoperatingsystems.Itisusefulwheneverwehaveasmall,fastmain memory and a larger, slower secondary memory. In file system caching, the secondary memoryisaharddriveoranetworkedstorageserverwhileinwebcachingthesecondary memory is the Internet. The goal of caching is to keep within the smaller memory data objects (files, web pages, etc.) from the larger memory which are likely to be accessed againinthenearfuture.Sin",Optimization & Theoretical ML,130f1a8e9e102707f3f91b010f151b0b-Paper.pdf,2002
Timing and Partial Observability in the,"Accordingtoaseriesofinfluentialmodels,dopamine(DA)neuronssig- nalrewardpredictionerrorusingatemporal-difference(TD)algorithm. Weaddressaproblemnotconvincinglysolvedintheseaccounts:howto maintainarepresentationofcuesthatpredictdelayedconsequences.Our newmodelusesaTDrulegroundedinpartiallyobservablesemi-Markov processes,aformalismthatcapturestwolargelyneglectedfeaturesofDA experiments:hiddenstateandtemporalvariability.Previousmodelspre- dictedrewardsusingatappeddelaylinerepresentationofsensoryinputs; wereplacethiswithamoreactiveprocessofinferenceabouttheunder- lying state of the world. The DA system can then learn to map these inferredstates to rewardpredictionsusingTD. Thenew modelcanex- plainpreviouslyvexingdataontheresponsesofDAneuronsintheface of temporal variability. By combining statistical model-based learning with a physiologically grounded TD theory, it also brings into contact withphysiologysomeinsightsaboutbehaviorthathadpreviouslybeen confinedtomoreabstractpsychologicalmodels",Computer Vision,13111c20aee51aeb480ecbd988cd8cc9-Paper.pdf,2002
Multiple Cause Vector Quantization,"We proposea modelthatcanlearnparts-basedrepresentationsofhigh- dimensionaldata. Ourkeyassumptionisthatthedimensionsofthedata canbeseparatedintoseveraldisjointsubsets, orfactors, whichtakeon valuesindependentlyofeachother. Weassumeeachfactorhasasmall number of discrete states, and model it using a vector quantizer. The selectedstates ofeachfactorrepresentthe multiplecausesoftheinput. Given a set of training examples, our model learns the association of datadimensionswithfactors,aswellasthestatesofeachVQ.Inference and learning are carried out efficiently via variational algorithms. We presentapplicationsofthismodeltoproblemsinimagedecomposition, collaborativefiltering,andtextclassification. 1 Introduction Manycollectionsofdataexhibitacommonunderlyingstructure:theyconsistofanumber ofpartsorfactors,eachofwhichhasasmallnumberofdiscrete states. Forexample,ina collectionoffacialimages,everyimagecontainseyes,anose,andamouth(exceptunder occlusion),each of whichhas a rangeof differentappearances",Computer Vision,1368ba1ab6ed38bb1f26f36673739d54-Paper.pdf,2002
Unsupervised Color Constancy,"In[1]weintroducedalinearstatistical modelof jointcolorchanges in imagesduetovariationinlightingandcertainnon-geometriccamerapa- rameters.Wedidthisbymeasuringthemappingsofcolorsinoneimage ofa scenetocolorsinanotherimageofthesamesceneunderdifferent lighting conditions. Here we increase the flexibility of this color flow model by allowing flow coefficients to vary according to a low order polynomialoverthe image. This allows us to better fit smoothlyvary- inglightingconditionsaswellascurvedsurfaceswithoutendowingour modelwithtoomuchcapacity. Weshowresultsonimagematchingand shadowremovalanddetection. 1 Introduction Thenumberofpossibleimagesofanobjectorscene,evenwhentakenfromasingleview- pointwitha fixedcamera, is verylarge. Lightsources, shadows, cameraaperture,expo- suretime,transducernon-linearities,andcameraprocessing(suchasauto-gain-controland colorbalancing)canallaffectthefinalimageofascene. Theseeffectshaveasignificant impact onthe images obtainedwith cameras andhenceon imageprocessi",Computer Vision,149815eb972b3c370dee3b89d645ae14-Paper.pdf,2002
Value-Directed Compression of POMDPs,"Weexaminetheproblemofgeneratingstate-spacecompressionsofPOMDPsina waythatminimallyimpactsdecisionquality.Weanalyzetheimpactofcompres- sionsondecisionquality,observingthatcompressionsthatallowaccuratepolicy evaluation (prediction of expected future reward) willnot affect decision qual- ity.Wederiveasetofsufficientconditionsthatensureaccuratepredictioninthis respect,illustrateinterestingmathematicalpropertiestheseconferonlosslesslin- earcompressions,andusethesetoderiveaniterativeprocedureforfindinggood linearlossycompressions. Wealsoelaborateonhowstructuredrepresentations ofaPOMDPcanbeusedtofindsuchcompressions. 1 Introduction PartiallyobservableMarkovdecisionprocesses(POMDPs)providearichframeworkfor modeling a wide range of sequential decision problems in the presence of uncertainty. Unfortunately,theapplicationofPOMDPstorealworldproblemsremainslimiteddueto the intractability of current solution algorithms, in large part because of the exponential growthofstatespaceswiththenumberofrelev",Optimization & Theoretical ML,14ea0d5b0cf49525d1866cb1e95ada5d-Paper.pdf,2002
Constraint Classification for Multiclass,"The constraint classification framework captures many flavors of mul- ticlass classification including winner-take-all multiclass classification, multilabel classification and ranking. We present a meta-algorithmfor learninginthisframeworkthatlearnsviaasinglelinearclassifierinhigh dimension.Wediscussdistributionindependentaswellasmargin-based generalization bounds and present empirical and theoretical evidence showing that constraint classification benefits overexisting methods of multiclassclassification. 1 Introduction Multiclassclassificationis acentralprobleminmachinelearning,asapplicationsthatre- quireadiscriminationamongseveralclassesareubiquitous.Inmachinelearning,thesein- cludehandwrittencharacterrecognition[LS97,LBD (cid:2) 89],part-of-speechtagging[Bri94, EZR01],speechrecognition[Jel98]andtextcategorization[ADW94,DKR97]. While binaryclassification is well understood,relatively little is knownaboutmulticlass classification. Indeed, the most common approach to multiclass classi",Optimization & Theoretical ML,16026d60ff9b54410b3435b403afd226-Paper.pdf,2002
Neural Decoding of Cursor Motion Using a Kalman Filter,"Thedirectneuralcontrolofexternaldevicessuchascomputerdisplays orprostheticlimbsrequirestheaccuratedecodingofneuralactivityrep- resentingcontinuousmovement. Wedevelopareal-timecontrolsystem using the spiking activity of approximately 40 neurons recorded with an electrode array implanted in the arm area of primary motor cortex. In contrast to previous work, we develop a control-theoretic approach that explicitly models the motion of the hand and the probabilistic re- lationshipbetweenthis motionand themean firingrates of the cells in 70 (cid:7)(cid:9) (cid:8) bins. Wefocusonarealisticcursorcontroltaskinwhichthesub- jectmustmoveacursorto“hit”randomlyplacedtargetsonacomputer monitor. Encodingand decodingof the neural data is achieved with a Kalman filter which has a number of advantages over previous linear filtering techniques. In particular, the Kalman filter reconstructions of hand trajectories in off-line experiments are more accurate than previ- ouslyreportedresultsandthemodelprovides",Optimization & Theoretical ML,169779d3852b32ce8b1a1724dbf5217d-Paper.pdf,2002
On the Dirichlet Prior and Bayesian,"A common objective in learning a model from data is to recover its network structure, while the model parameters are of minor in terest. For example, we may wish to recover regulatory networks from high-throughput data sources. In this paper we examine how Bayesian regularization using a product of independent Dirichlet priors over the model parameters affects the learned model struc ture in a domain with discrete variables. We show that a small scale parameter - often interpreted as ""equivalent sample size"" or ""prior strength"" - leads to a strong regularization of the model structure (sparse graph) given a sufficiently large data set. In par ticular, the empty graph is obtained in the limit of a vanishing scale parameter. This is diametrically opposite to what one may expect in this limit, namely the complete graph from an (unregularized) maximum likelihood estimate. Since the prior affects the parame ters as expected, the scale parameter balances a trade-off between regularizing the ",Optimization & Theoretical ML,1819932ff5cf474f4f19e7c7024640c2-Paper.pdf,2002
Scaling of Probability-Based Optimization,"Population-based Incremental Learning is shown require very sen sitive scaling of its learning rate. The learning rate must scale with the system size in a problem-dependent way. This is shown in two problems: the needle-in-a haystack, in which the learning rate must vanish exponentially in the system size, and in a smooth function in which the learning rate must vanish like the square root of the system size. Two methods are proposed for removing this sensitiv ity. A learning dynamics which obeys detailed balance is shown to give consistent performance over the entire range of learning rates. An analog of mutation is shown to require a learning rate which scales as the inverse system size, but is problem independent. 1 Introduction There has been much recent work using probability models to search in optimization problems. The probability model generates candidate solutions to the optimization problem. It is updated so that the solutions generated should improve over time. Usually, th",Optimization & Theoretical ML,1943102704f8f8f3302c2b730728e023-Paper.pdf,2002
Forward-Decoding Kernel-Based,"Forward decoding kernel machines (FDKM) combine large-margin clas sifiers with hidden Markov models (HMM) for maximum a posteriori (MAP) adaptive sequence estimation. State transitions in the sequence are conditioned on observed data using a kernel-based probability model trained with a recursive scheme that deals effectively with noisy and par tially labeled data. Training over very large data sets is accomplished us ing a sparse probabilistic support vector machine (SVM) model based on quadratic entropy, and an on-line stochastic steepest descent algorithm. For speaker-independent continuous phone recognition, FDKM trained over 177 ,080 samples of the TlMIT database achieves 80.6% recognition accuracy over the full test set, without use of a prior phonetic language model. 1 Introduction Sequence estimation is at the core of many problems in pattern recognition, most notably speech and language processing. Recognizing dynamic patterns in sequential data requires a set of tools very di",Computer Vision,1a3f91fead97497b1a96d6104ad339f6-Paper.pdf,2002
Optoelectronic Implementation of a,"An optoelectronicimplementationof a spiking neuronmodel based on the FitzHugh-Nagumo equations is presented. A tunable semiconduc- tor laser source and a spectral filter providea nonlinear mapping from drivervoltagetodetectedsignal. Linearelectronicfeedbackcompletes theimplementation,whichallows eitherelectronicoropticalinputsig- nals. Experimental results for a single system and numeric results of modelinteractionconfirmthatimportantfeaturesofspikingneuralmod- elscanbeimplementedthroughthisapproach. 1 Introduction Biologically-inspired computation paradigms take different levels of abstraction when modeling neural dynamics. The production of action potentials or spikes has been ab- stractedawayinmanyrate-basedneurodynamicmodels,butrecentlythisfeaturehasgained renewed interest [1, 2]. A computationalparadigm that takes into account the timing of spikes (instead of spike rates only) might be more efficient for signal representationand processing,especiallyatshorttimewindows[3,4,5]. Opti",Computer Vision,1c6a0198177bfcc9bd93f6aab94aad3c-Paper.pdf,2002
Margin-Based Algorithms,"Inthiswork,westudyaninformationfilteringmodelwheretherelevance labels associated to a sequenceof featurevectorsare realizationsof an unknownprobabilisticlinearfunction. Buildingontheanalysisofare- strictedversionofourmodel,wederiveageneralfilteringrulebasedon themarginofaridgeregressionestimator. Whileourrulemayobserve thelabelofavectoronlybyclassfyingthevectorasrelevant,experiments on a real-world documentfiltering problem show that the performance of our rule is close to that of the on-line classifier which is allowed to observealllabels. Theseempiricalresultsarecomplementedbyatheo- reticalanalysiswhereweconsiderarandomizedvariantofourruleand provethatitsexpectednumberofmistakesisnevermuchlargerthanthat oftheoptimalfilteringrulewhichknowsthehiddenlinearmodel. 1 Introduction Systems able to filter out unwanted pieces of information are of crucial importance for several applications. Consider a stream of discrete data that are individually labelled as “relevant”or“nonrelevant”according",Optimization & Theoretical ML,1e8c391abfde9abea82d75a2d60278d4-Paper.pdf,2002
Half-Lives of EigenFlows for Spectral Clustering,"Using a Markov chain perspective of spectral clustering we present an algorithmtoautomaticallyfindthenumberofstableclustersinadataset. TheMarkovchain’sbehaviourischaracterizedbythespectralproperties ofthematrixoftransitionprobabilities,fromwhichwederiveeigenflows alongwiththeirhalflives. Aneigenflowdescribestheflowofprobabil- ity mass due to the Markov chain, and it is characterized by its eigen- value, or equivalently, by the halflife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigenflow and infi- nite half-life. The keyinsight in this paperis that bottlenecksbetween weakly coupled clusters can be identified by computing the sensitivity oftheeigenflow’shalflifetovariationsintheedgeweights. Wepropose anovelEIGENCUTS algorithmtoperformclusteringthatremovesthese identifiedbottlenecksinaniterativefashion. 1 Introduction Weconsiderpartitioningaweightedundirectedgraph—correspondingtoagivendataset— intoasetofdiscreteclusters. Ideally,thevertices(i.e.",Optimization & Theoretical ML,2056d8c1dec3d12cbce646b348d189d1-Paper.pdf,2002
The RA Scanner: Prediction of Rheumatoid,"WedescribetheRAscanner,anovelsystemfortheexaminationofpa- tients suffering fromrheumatoid arthritis. TheRA scanneris basedon a novel laser-based imaging technique which is sensitive to the optical characteristics of finger joint tissue. Based on the laser images, finger joints are classified according to whether the inflammatory status has improved or worsened. To perform the classification task, various lin- earandkernel-basedsystemswereimplementedandtheirperformances werecompared. Specialemphasis wasputonmeasurestoreliablyper- form parameter tuningand evaluation, since only a very small data set was available. Basedonthe resultspresentedin this paper, it was con- cludedthattheRAscannerpermitsareliableclassificationofpatholog- ical finger joints, thus paving the way for a further development from prototypetoproductstage. 1 Introduction Rheumatoidarthritis(RA)isthemostcommoninflammatoryarthropathywith1–2%ofthe populationbeingaffected.Thischronic,mostlyprogressivediseaseoftenleadstoearl",Computer Vision,211c1e0b83b9c69fa9c4bdede203c1e3-Paper.pdf,2002
Optimality of Reinforcement Learning,"There are several reinforcement learning algorithms that yield ap proximate solutions for the problem of policy evaluation when the value function is represented with a linear function approximator. In this paper we show that each of the solutions is optimal with respect to a specific objective function. Moreover, we characterise the different solutions as images of the optimal exact value func tion under different projection operations. The results presented here will be useful for comparing the algorithms in terms of the error they achieve relative to the error of the optimal approximate solution. 1 Introduction In large domains the determination of an optimal value function via a tabular rep resentation is no longer feasible with respect to time and memory considerations. Therefore, reinforcement learning (RL) algorithms are combined with linear func tion approximation schemes. However, the different RL algorithms, that all achieve the same optimal solution in the tabular case, conv",Reinforcement Learning,228bbc2f87caeb21bb7f6949fddcb91d-Paper.pdf,2002
Evidence Optimization Techniques,"Anessentialstepinunderstandingthefunctionofsensorynervoussys- tems is to characterize as accurately as possible the stimulus-response function (SRF) of the neurons that relay and process sensory informa- tion. One increasingly common experimental approach is to present a rapidlyvaryingcomplexstimulusto theanimal whilerecordingthe re- sponses of one or more neurons, and then to directly estimate a func- tional transformationof the input that accounts for the neuronalfiring. Theestimationtechniquesusuallyemployed,suchasWienerfilteringor othercorrelation-basedestimationoftheWienerorVolterrakernels,are equivalenttomaximumlikelihoodestimationinaGaussian-output-noise regressionmodel.WeexploretheuseofBayesianevidence-optimization techniquestoconditiontheseestimates.Weshowthatbylearninghyper- parametersthatcontrolthesmoothnessandsparsityofthetransferfunc- tionitispossibletoimprovedramaticallythequalityofSRFestimates, asmeasuredbytheirsuccessinpredictingresponsestonovelinput. 1 Introduction A c",Optimization & Theoretical ML,229754d7799160502a143a72f6789927-Paper.pdf,2002
Binary Coding in Auditory Cortex,"Cortical neurons have been reported to use both rate and temporal codes. Here we describe a novel mode in which each neuron generates exactly 0 or 1 action potentials, but not more, in response to a stimulus. We used cell-attached recording, which ensured single-unit isolation, to record responses in rat auditory cortex to brief tone pips. Surprisingly, the majority of neurons exhibited binary behavior with few multi-spike responses; several dramatic examples consisted of exactly one spike on 100% of trials, with no trial-to-trial variability in spike count. Many neurons were tuned to stimulus frequency. Since individual trials yielded at most one spike for most neurons, the information about stimulus frequency was encoded in the population, and would not have been accessible to later stages of processing that only had access to the activity of a single unit. These binary units allow a more efficient population code than is possible with conventional rate coding units, and are consiste",Computer Vision,2321994d85d661d792223f647000c65f-Paper.pdf,2002
Learning Attractor Landscapes for,"Manycontrolproblemstakeplaceincontinuousstate-actionspaces, e.g., as in manipulator robotics, where the control objective is of- tendeflnedasflndingadesiredtrajectorythatreachesaparticular goalstate. Whilereinforcementlearningofiersatheoreticalframe- worktolearnsuchcontrolpoliciesfromscratch,itsapplicabilityto higher dimensional continuous state-action spaces remains rather limited to date. Instead of learning from scratch, in this paper we suggest to learn a desired complex control policy by transforming an existing simple canonical control policy. For this purpose, we represent canonical policies in terms of difierential equations with well-deflned attractor properties. By nonlinearly transforming the canonicalattractordynamicsusingtechniquesfromnonparametric regression, almost arbitrary new nonlinear policies can be gener- ated without losing the stability properties of the canonical sys- tem. We demonstrate our techniques in the context of learning a set of movement skills for a hu",Computer Vision,23c97e9cb93576e45d2feaf00d0e8502-Paper.pdf,2002
Combining Dimensions and Features in,"Thispaperdevelopsanewrepresentationalmodelofsimilaritydata thatcombinescontinuousdimensionswithdiscretefeatures. Anal- gorithm capable oflearningthese representations is described, and a Bayesian model selection approach for choosing the appropriate number of dimensions and features is developed. The approach is demonstrated on a classic data set that considers the similarities between the numbers 0 through 9. 1 Introduction A central problem for cognitive science is to understand the way people mentally representstimuli. Onewidely usedapproach forderivingrepresentations from data is to base them on measures of stimulus similarity (see Shepard 1974). Similarity is naturally understood as a measure of the degree to which the consequences of onestimulusgeneralizetoanother, andmaybemeasuredusinganumberofexperi- mental methodologies, including ratings scales, confusion probabilities, or grouping or sorting tasks. For a domain with n stimuli, similarity data take the form of an n n matrix, ",Optimization & Theoretical ML,243facb29564e7b448834a7c9d901201-Paper.pdf,2002
Bayesian Monte Carlo,"We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the in- corporation of prior knowledge, such as smoothness of the integrand, intotheestimation. Inasimpleproblemweshowthatthisoutperforms anyclassicalimportancesamplingmethod. Wealsoattemptmorechal- lengingmultidimensionalintegralsinvolvedincomputingmarginallike- lihoods of statistical models (a.k.a. partition functions and model evi- dences). We find that Bayesian Monte Carlo outperformed Annealed ImportanceSampling, althoughfor very high dimensionalproblemsor problemswithmassivemultimodalityBMCmaybelessadequate. One advantageoftheBayesianapproachtoMonteCarloisthatsamplescan bedrawnfromanydistribution. Thisallowsforthepossibilityofactive designofsamplepointssoastomaximiseinformationgain. 1 Introduction Inferenceinmostinterestingmachinelearningalgorithmsisnotcomputationallytractable, andis solvedusingapproximations. This is particularlytrue forBayesian model",Optimization & Theoretical ML,24917db15c4e37e421866448c9ab23d8-Paper.pdf,2002
A Model for Learning Variance Components of,"WepresentahierarchicalBayesianmodelforlearningefficientcodesof higher-orderstructure in natural images. The model, a non-lineargen- eralizationofindependentcomponentanalysis,replacesthestandardas- sumptionof independenceforthe joint distributionofcoefficients with adistributionthatisadaptedtothevariancestructureofthecoefficients of an efficient image basis. This offers a novel description of higher- orderimagestructureandprovidesawaytolearncoarse-coded,sparse- distributed representations of abstract image properties such as object location,scale,andtexture. 1 Introduction One of the major challenges in vision is how to derive from the retinal representation higher-orderrepresentationsthatdescribepropertiesofsurfaces,objects,andscenes.Phys- iologicalstudiesofthevisualsystemhavecharacterizedawiderangeofresponseproper- ties, beginningwith, forexample, simple cells and complexcells. These, however,offer only limited insight into how higher-orderproperties of images might be represented or ",Computer Vision,2596a54cdbb555cfd09cd5d991da0f55-Paper.pdf,2002
Effective Dimension and Generalization of,"We investigate the generalization performance of some learning prob- lems in Hilbert function Spaces. We introduce a concept of scale- sensitiveeffectivedatadimension,andshowthatitcharacterizesthecon- vergencerateoftheunderlyinglearningproblem.Usingthisconcept,we cannaturallyextendresultsforparametricestimationproblemsinfinite dimensionalspacestonon-parametrickernellearningmethods. Wede- riveupperboundsonthegeneralizationperformanceandshowthatthe resultingconvergentratesareoptimalundervariouscircumstances. 1 Introduction The goal of supervised learning is to predict an unobservedoutput value (cid:0) based on an observedinputvector (cid:1) . This requiresus to estimate a functionalrelationship (cid:0)(cid:3) (cid:2)(cid:5) (cid:4)(cid:7) (cid:6) (cid:1)(cid:9) (cid:8) fromasetoftrainingexamples.Usuallythequalityofthepredictor (cid:4)(cid:7) (cid:6) (cid:1)(cid:10) (cid:8) canbemeasured byalossfunction (cid:11) (cid:6)(cid:12)(cid:4)(cid:7) (cid:6) (cid:1)(cid:10) (cid:8)(cid:14) (cid:13",Optimization & Theoretical ML,25db67c5657914454081c6a18e93d6dd-Paper.pdf,2002
Nash Propagation for Loopy Graphical Games,"We introduce NashProp, an iterative and local message-passing algo- rithmfor computingNash equilibriain multi-playergamesrepresented byarbitraryundirectedgraphs. Weprovideaformalanalysisandexper- imental evidence demonstrating that NashProp performs well on large graphicalgameswithmanyloops,oftenconverginginjustadozenitera- tionsongraphswithhundredsofnodes. NashProp generalizes the tree algorithm of (Kearns et al. 2001), and can be viewed as similar in spirit to belief propagation in probabilis- tic inference, and thus complements the recent work of (Vickrey and Koller2002),whoexploredajunctiontreeapproach. Thus,asforprob- abilistic inference, we have at least two promisinggeneral-purposeap- proachestoequilibriacomputationingraphs. 1 Introduction There has been considerable recent interest in representational and algorithmic issues arising in multi-player game theory. One example is the recent work on graphical games (Kearns et al. 2001) (abbreviated KLS in the sequel). Here a multi-pl",Optimization & Theoretical ML,26505e0494662534f633586941b77d0c-Paper.pdf,2002
Neuromorphic Bistable VLSI Synapses with,"Wepresentanalogneuromorphiccircuitsforimplementingbistablesyn- apseswithspike-timing-dependentplasticity(STDP)properties.Inthese typesofsynapses,theshort-termdynamicsofthesynapticefficaciesare governed by the relative timing of the pre- and post-synaptic spikes, while on long time scales the efficacies tend asymptotically to either a potentiatedstateortoadepressedone. WefabricatedaprototypeVLSI chip containing a network of integrate and fire neurons interconnected viabistableSTDPsynapses. Testresultsfromthischipdemonstratethe synapse’s STDP learning properties, and its long-termbistable charac- teristics. 1 Introduction Most artificial neural network algorithms based on Hebbian learning use correlations of meanrate signalstoincreasethesynapticefficaciesbetweenconnectedneurons. Topre- ventuncontrolledgrowthofsynapticefficacies,thesealgorithmsusuallyincorporatealso weight normalization constraints, that are often not biophysically realistic. Recently an alternativeclassofcompetitiveHebbi",Computer Vision,273448411df1962cba1db6c05b3213c9-Paper.pdf,2002
Source Separation with a Sensor Array Using,"Sourceseparationisanimportantproblemattheintersectionofseveral fields, including machine learning, signal processing, and speech tech- nology. Here we describe new separation algorithms which are based onprobabilisticgraphicalmodelswithlatentvariables. Incontrastwith existing methods, these algorithms exploit detailed models to describe source properties. They also use subband filtering ideas to model the reverberantenvironment, andemployanexplicitmodelforbackground andsensornoise. Weleveragevariationaltechniquestokeepthecompu- tationalcomplexityperEMiterationlinearinthenumberofframes. 1 TheSourceSeparationProblem Fig. 1 illustrates the problem of source separation with a sensor array. In this problem, signals from K independent sources are received by each of L (cid:21) K sensors. The task is to extract the sources from the sensor signals. It is a difficult task, partly because the receivedsignalsaredistortedversionsoftheoriginals. Therearetwotypesofdistortions. Thefirsttypearisesfrom",Signal Processing,2952351097998ac1240cb2ab7333a3d2-Paper.pdf,2002
Dopamine Induced Bistability Enhances,"Single unit activity in the striatum of awake monkeys shows a marked dependence on the expected reward that a behavior will elicit. We present a computational model of spiny neurons, the principal neurons of the striatum, to assess the hypothesis that di rect neuromodulatory effects of dopamine through the activation of D 1 receptors mediate the reward dependency of spiny neuron activity. Dopamine release results in the amplification of key ion currents, leading to the emergence of bistability, which not only modulates the peak firing rate but also introduces a temporal and state dependence of the model's response, thus improving the de tectability of temporally correlated inputs. 1 Introduction The classic notion of the basal ganglia as being involved in purely motor processing has expanded over the years to include sensory and cognitive functions. A sur prising new finding is that much of this activity shows a motivational component. For instance, striatal activity related to visual ",Computer Vision,2a34abd6ebbd7fcf5a4421229c946c0a-Paper.pdf,2002
An Information Theoretic Approach to the,"Apopulationofneuronstypicallyexhibitsabroaddiversityofresponses tosensoryinputs. Theintuitivenotionoffunctionalclassificationisthat cellscanbeclusteredsothatmostofthediversityiscapturedbytheiden- tity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, with- outanyneedtointroduceametriconthespaceofstimuliorresponses. Appliedtotheretinalganglioncellsofthesalamander,thisapproachre- covers classical results, but also provides clear evidence for subclasses beyondthoseidentifiedpreviously. Further,wefindthateachofthegan- glioncellsisfunctionallyunique,andthatevenwithinthesamesubclass onlyafewspikesareneededtoreliablydistinguishbetweencells. 1 Introduction Neuronsexhibitanenormousvarietyofshapesandmolecularcompositions.Alreadyinhis classicalwork,Cajal[1]recognizedthattheshapesofcellscanbeclassified,andheiden- tifiedmanyofthecelltypesthatwerecognizetoday. Suchclassificationisfundamentally important,becauseitimpli",Computer Vision,2a8a812400df8963b2e2ac0ed01b07b8-Paper.pdf,2002
An Asynchronous Hidden Markov Model,"This paper presents a novel Hidden Markov Model architecture to model the joint probability of pairs of asynchronous sequences de scribing the same event. It is based on two other Markovian models, namely Asynchronous Input/Output Hidden Markov Models and Pair Hidden Markov Models. An EM algorithm to train the model is presented, as well as a Viterbi decoder that can be used to ob tain the optimal state sequence as well as the alignment between the two sequences. The model has been tested on an audio-visual speech recognition task using the M2VTS database and yielded robust performances under various noise conditions. 1 Introduction Hidden Markov Models (HMMs) are statistical tools that have been used success fully in the last 30 years to model difficult tasks such as speech recognition [6) or biological sequence analysis [4). They are very well suited to handle discrete of con tinuous sequences of varying sizes. Moreover, an efficient training algorithm (EM) is available, as well as a",Optimization & Theoretical ML,2b45c629e577731c4df84fc34f936a89-Paper.pdf,2002
Shape Recipes: Scene Representations that Refer,"The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be verycomplex,conventionallyrequiringhighdimensionalrepresentations whicharehardtoestimateandstore.Weproposealow-dimensionalrep- resentation, called a scene recipe, that relies on the image itself to de- scribethecomplexsceneconfigurations. Shaperecipesareanexample: these are the regression coefficients that predict the bandpassed shape from image data. We describe the benefits of this representation, and showtwousesillustratingtheirproperties: (1)weimprovestereoshape estimatesbylearningshaperecipesatlowresolutionandapplyingthem atfullresolution;(2)Shaperecipesimplicitlycontaininformationabout lightingandmaterialsandweusethemformaterialsegmentation. 1 Introduction Fromimages, wewant toestimate variouslow-levelscenepropertiessuchas shape, ma- terial, albedoormotion. Forsuchanestimationtask, therepresentationofthequantities tobeestimatedcanbecritical. T",Computer Vision,2b8eba3cb0d0f1d761cb74d94a5ace36-Paper.pdf,2002
Real-time Particle Filters,"Particlefilters estimatethestate ofdynamicalsystemsfromsensorinfor- mation. Inmanyrealtimeapplicationsofparticlefilters,however,sensor informationarrivesatasignificantlyhigherratethantheupdaterateofthe filter. Theprevalentapproachtodealingwithsuchsituationsistoupdate theparticlefilterasoftenaspossibleandtodiscardsensorinformationthat cannotbeprocessedintime.Inthispaperwepresentreal-timeparticlefil- ters,whichmakeuseofallsensorinformationevenwhenthefilterupdate rateisbelowtheupdaterateofthesensors. Thisisachievedbyrepresent- ingposteriorsasmixturesofsamplesets,whereeachmixturecomponent integratesoneobservationarrivingduringafilterupdate. Theweightsof themixturecomponentsaresetsoastominimizetheapproximationerror introducedbythemixturerepresentation. Thereby,ourapproachfocuses computationalresources(samples)onvaluablesensorinformation.Exper- iments using data collected with a mobile robot show that our approach yieldsstrongimprovementsoverotherapproaches. 1 Introduction Duetotheirsample-b",Computer Vision,2d2ca7eedf739ef4c3800713ec482e1a-Paper.pdf,2002
Critical Lines in Symmetry of Mixture Models,"Weshowtheexistenceofcriticalpointsaslinesforthelikelihoodfunc- tionofmixture-typemodels. Theyaregivenbyembeddingofacritical point for models with less components. A sufficient condition that the critical line gives local maxima or saddle points is also derived. Based onthisfact,acomponent-splitmethodisproposedforamixtureofGaus- siancomponents,anditseffectivenessisverifiedthroughexperiments. 1 Introduction Thelikelihoodfunctionofamixturemodeloftenhasacomplexshapesothatcalculation of an estimator can be difficult, whether the maximum likelihood or Bayesian approach is used. In the maximum likelihood estimation, convergence of the EM algorithm to the globalmaximumisnotguaranteed,whileitisastandardmethod. Investigationofthelike- lihoodfunctionformixturemodelsisimportanttodevelopeffectivemethodsforlearning. Thispaperdiscussesthecriticalpointsofthelikelihoodfunctionformixture-typemodels byanalyzingtheirhierarchicalsymmetricstructure.Asgeneralizationof[1],weshowthat, givenacriticalpointofthel",Optimization & Theoretical ML,2d3acd3e240c61820625fff66a19938f-Paper.pdf,2002
Manifold Parzen Windows,"Thesimilaritybetweenobjectsisafundamentalelementofmanylearn- ingalgorithms. Mostnon-parametricmethodstakethissimilaritytobe fixed,butmuchrecentworkhasshowntheadvantagesoflearningit,in particular to exploit the local invariances in the data or to capture the possiblynon-linearmanifoldonwhichmostofthedatalies.Wepropose anewnon-parametrickerneldensityestimationmethodwhichcaptures thelocalstructureofanunderlyingmanifoldthroughtheleadingeigen- vectorsofregularizedlocalcovariancematrices. Experimentsindensity estimationshowsignificantimprovementswithrespecttoParzendensity estimators. ThedensityestimatorscanalsobeusedwithinBayesclassi- fiers,yieldingclassificationratessimilartoSVMsandmuchsuperiorto theParzenclassifier. 1 Introduction In[1],whileattemptingtobetterunderstandandbridgethegapbetweenthegoodperfor- manceofthepopularSupportVectorMachinesandthemoretraditionalK-NN(KNearest Neighbors) for classification problems, we had suggested a modified Nearest-Neighbor algorithm. This algorithm,whi",Optimization & Theoretical ML,2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf,2002
Parametric Mixture Models for,"Weproposeprobabilisticgenerativemodels,calledparametricmix- ture models (PMMs), for multiclass, multi-labeled text categoriza- tion problem. Conventionally, the binary classi(cid:12)cation approach has been employed, in which whether or not text belongs to a cat- egory is judged by the binary classi(cid:12)er for every category. In con- trast,ourapproachcansimultaneouslydetectmultiplecategoriesof textusingPMMs. Wederivee(cid:14)cientlearningandpredictionalgo- rithmsforPMMs. Wealsoempiricallyshowthatourmethodcould signi(cid:12)cantlyoutperformtheconventionalbinarymethodswhenap- plied to multi-labeled text categorization using real World Wide Web pages. 1 Introduction Recently, as the number of online documents has been rapidly increasing, auto- matic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Since a document often belongs to multiple categories, the task of text categorization is generally de(cid:12)ned as assigning o",NLP,3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf,2002
Transductive and Inductive Methods for,"Gaussianprocessregressionallowsasimpleanalyticaltreatmentofex- actBayesianinferenceandhasbeenfoundtoprovidegoodperformance, yetscalesbadlywiththenumberoftrainingdata. Inthispaperwecom- pare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian commit- tee machine. Furthermore we provide theoretical insight into some of ourexperimentalresults. Wefoundthatsubsetofrepresentersmethods can give good and particularly fast predictions for data sets with high andmediumnoiselevels. Oncomplexlownoisedatasets,theBayesian committeemachineachievessignificantlybetteraccuracy,yetatahigher computationalcost. 1 Introduction Gaussianprocessregression (GPR) hasdemonstratedexcellentperformance in anumber ofapplications. OneunpleasantaspectofGPRisitsscalingbehaviorwiththesizeofthe training data set N. In direct implementations, training time increases as O (cid:1) N3",Optimization & Theoretical ML,329e6581efbc90bd92a1f22c4ba2103d-Paper.pdf,2002
Adapting Codes and Embeddings for Polychotomies,"Inthispaperweconsiderformulationsofmulti-classproblemsbasedon ageneralizednotionofamarginandusingoutputcoding.Thisincludes, butis notrestrictedto, standardmulti-classSVM formulations. Differ- ently from many previous approacheswe learn the code as well as the embedding function. We illustrate how this can lead to a formulation thatallowsforsolvingawiderrangeofproblemswithforinstancemany classes or even “missing classes”. To keep our optimization problems tractable we propose an algorithm capable of solving them using two- classclassifiers,similarinspirittoBoosting. 1 Introduction Thetheoryofpatternrecognitionisprimarilyconcernedwiththecaseofbinaryclassifica- tion,i.e.ofassigningexamplestooneoftwocategories,suchthattheexpectednumberof misassignmentsisminimal. Whilstthisscenarioisratherwellunderstood,theoreticallyas wellasempirically,itisnotdirectlyapplicabletomanypracticallyrelevantscenarios,the mostprominentbeingthecaseofmorethantwopossibleoutcomes. Severallearningtechniquesnaturallyge",NLP,32e05616c8ed659463f9af00b142dd6f-Paper.pdf,2002
Topographic Map Formation by Silicon,"We describe a self-configuring neuromorphic chip that uses a model of activity-dependent axon remodeling to automatically wire topographic maps based solely on input correlations. Axons are guided by growth cones, which are modeled in analog VLSI for the first time. Growth cones migrate up neurotropin gradients, which are represented by charge diffusing in transistor channels. Virtual axons move by rerouting address-events. We refined an initially gross topographic projection by simulating retinal wave input. 1 Neuromorphic Systems Neuromorphic engineers are attempting to match the computational efficiency of biological systems by morphing neurocircuitry into silicon circuits [1]. One of the most detailed implementations to date is the silicon retina described in [2]. This chip comprises thirteen different cell types, each of which must be individually and painstakingly wired. While this circuit-level approach has been very successful in sensory systems, it is less helpful when modelin",Computer Vision,3323fe11e9595c09af38fe67567a9394-Paper.pdf,2002
Analysis of Information in Speech based,"We propose analysis of information in speech using three sources - language (phone), speaker and channeL Information in speech is measured as mutual information between the source and the set of features extracted from speech signaL We assume that distribu tion of features can be modeled using Gaussian distribution. The mutual information is computed using the results of analysis of variability in speech. We observe similarity in the results of phone variability and phone information, and show that the results of the proposed analysis have more meaningful interpretations than the analysis of variability. 1 Introduction Speech signal carries information about the linguistic message, the speaker, the communication channeL In the previous work [1, 2], we proposed analysis of in formation in speech as analysis of variability in a set of features extracted from the speech signal. The variability was measured as covariance of the features, and analysis was performed using using multivariate ",NLP,333ac5d90817d69113471fbb6e531bee-Paper.pdf,2002
Discriminative Binaural Sound Localization,Timedifferenceofarrival(TDOA)iscommonlyusedtoestimatetheaz- imuth of a source in a microphone array. The most common methods to estimate TDOA are based on finding extrema in generalized cross- correlationwaveforms. In this paper we apply microphonearray tech- niques to a manikin head. By considering the entire cross-correlation waveformweachieveazimuthpredictionaccuracythatexceedsextrema locating methods. We do so by quantizing the azimuthal angle and treatingthe predictionproblemas a multiclass categorizationtask. We demonstrate the merits of our approach by evaluating the various ap- proachesonSony’sAIBOrobot. 1 Introduction Inthispaperwedescribeandevaluateseveralalgorithmstoperformsoundlocalizationin acommercialentertainmentrobot. Thephysicalsystembeinginvestigatediscomposedof amanikinheadequippedwithatwomicrophonesandplacedonamanikinbody.Thistype of systems is commonlyused to model sound localization in biological systems and the algorithmsusedtoanalyzethesignalareusuallyinspiredfr,Computer Vision,350db081a661525235354dd3e19b8c05-Paper.pdf,2002
Fractional Belief Propagation,"Weconsiderloopybeliefpropagationforapproximateinferenceinprob- abilisticgraphicalmodels. Alimitationofthestandardalgorithmisthat clique marginals are computed as if there were no loops in the graph. Toovercomethislimitation,weintroducefractionalbeliefpropagation. Fractional belief propagationis formulated in terms of a family of ap- proximate free energies, which includes the Bethe free energy and the naivemean-fieldfreeasspecialcases. Usingthelinearresponsecorrec- tionofthecliquemarginals,thescaleparameterscanbetuned. Simula- tionresultsillustratethepotentialmeritsoftheapproach. 1 Introduction Probabilistic graphical models are powerful tools for learning and reasoning in domains withuncertainty.Unfortunately,inferenceinlarge,complexgraphicalmodelsiscomputa- tionallyintractable. Therefore,approximateinferencemethodsareneeded. Basically,one candistinguishbetweentotypesofmethods,stochasticsamplingmethodsanddeterminis- ticmethods.OneofmethodsinthelatterclassisPearl’sloopybeliefpropagatio",Optimization & Theoretical ML,35936504a37d53e03abdfbc7318d9ec7-Paper.pdf,2002
Hidden Markov Model of Cortical Synaptic,"Cortical synaptic plasticity depends on the relative timing of pre- and postsynapticspikesandalsoonthetemporalpatternofpresynapticspikes andofpostsynapticspikes. Westudythehypothesisthatcorticalsynap- tic plasticity does not associate individual spikes, but rather whole fir- ing episodes, and depends only on when these episodes start and how longtheylast,butaslittleaspossibleonthetimingofindividualspikes. Here we present the mathematical backgroundfor such a study. Stan- dardmethodsfromhiddenMarkovmodelsareusedtodefinewhat“fir- ingepisodes”are.Estimatingtheprobabilityofbeinginsuchanepisode requiresnotonlytheknowledgeofpastspikes,butalsooffuturespikes. We show how to construct a causal learning rule, which depends only onpastspikes,butassociatespre-andpostsynapticfiringepisodesasif it also knew future spikes. We also show that this learning rule agrees withsomefeaturesofsynapticplasticityinsuperficiallayersofratvisual cortex(FroemkeandDan,Nature416:433,2002). 1 Introduction Cortical syn",NLP,38651c4450f87348fcbe1f992746a954-Paper.pdf,2002
Stability-Based Model Selection,"Modelselectionislinkedto modelassessment,whichistheproblemof comparingdifferentmodels,ormodelparameters,foraspecificlearning task. For supervisedlearning,the standardpracticaltechniqueis cross- validation,whichisnotapplicableforsemi-supervisedandunsupervised settings. In this paper, a new model assessment scheme is introduced which is basedon a notionof stability.The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems. In the experimental part, the performanceof the stability measure is studiedfor modelorderse- lectionincomparisontostandardtechniquesinthisarea. 1 Introduction Oneofthefundamentalproblemsoflearningtheoryismodelassessment:Givenaspecific dataset,howcanonepracticallymeasurethegeneralizationperformanceofamodeltrained tothedata.Insupervisedlearning,thestandardtechniqueiscross-validation.Itconsistsin usingonlyasubsetofthedatafortraining,andthentestingontheremainingdatainorderto estimatethe",Optimization & Theoretical ML,37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf,2002
Automatic Alignment of Local Representations,"Wepresentanautomaticalignmentprocedurewhichmapsthedisparate internalrepresentationslearnedbyseverallocaldimensionalityreduction expertsintoasingle,coherentglobalcoordinatesystemfortheoriginal data space. Our algorithm can be applied to any set of experts, each of which produces a low-dimensional local representation of a high- dimensional input. Unlike recent efforts to coordinate such models by modifyingtheirobjectivefunctions[1,2],ouralgorithmisinvokedafter training and applies an efficient eigensolverto post-process the trained models.Thepost-processinghasnolocaloptimaandthesizeofthesys- temitmustsolvescaleswiththenumberoflocalmodelsratherthanthe numberoforiginaldatapoints,makingitmoreefficientthanmodel-free algorithmssuchasIsomap[3]orLLE[4]. 1 Introduction: Local vs. GlobalDimensionalityReduction Beyonddensitymodelling,animportantgoalofunsupervisedlearningistodiscovercom- pact,informativerepresentationsofhigh-dimensionaldata.Ifthedatalieonasmoothlow dimensionalmanifold,thenanexcell",Optimization & Theoretical ML,3a1dd98341fafc1dfe9bcf36360e6b84-Paper.pdf,2002
Field-Programmable Learning Arrays,"This paper introduces the Field-Programmable Learning Array, a new paradigm for rapid prototyping of learning primitives and machine- learningalgorithmsinsilicon. TheFPLAisamixed-signalcounterpart totheall-digitalField-ProgrammableGateArrayinthatitenablesrapid prototypingofalgorithmsin hardware. Unlikethe FPGA, the FPLA is targeted directly for machine learning by providinglocal, parallel, on- line analog learning using floating-gate MOS synapse transistors. We present a prototype FPLA chip comprising an array of reconfigurable computational blocks and local interconnect. We demonstrate the via- bility of this architectureby mappingseveral learning circuits onto the prototypechip. 1 Introduction Implementingmachine-learningalgorithmsinVLSIisalogicalsteptowardenablingreal- timeormobileapplicationsofthesealgorithms[1].Severalmachine-learningarchitectures suchas neuralnetworks and Bayesnets mapnaturallytoVLSI,becauseeachuses many simpleelementsinparallelandcomputesusingonlylocalinformatio",Computer Vision,3dd9424294b0292b6e89ea2bba2e1144-Paper.pdf,2002
Boosting Density Estimation,"Severalauthorshavesuggestedviewingboostingasagradientdescentsearchfor agoodfitinfunctionspace. Weapplygradient-basedboostingmethodologyto theunsupervisedlearningproblemofdensityestimation. Weshowconvergence propertiesofthealgorithmandprovethatastrengthofweaklearnabilityprop- ertyappliestothisproblemaswell. Weillustratethepotentialofthisapproach throughexperimentswithboostingBayesiannetworkstolearndensitymodels. 1 Introduction Boosting is a method for incrementally building linear combinations of “weak” models, to generate a “strong” predictive model. Given data (cid:0)(cid:2) (cid:1)(cid:4) (cid:3)(cid:6) (cid:5)(cid:2) (cid:7)(cid:3)(cid:9)(cid:8)(cid:11) (cid:10) , a basis (or dictionary) of weak learners (cid:12) and a loss function (cid:13) , a boosting algorithm sequentially finds models (cid:14) (cid:10)(cid:2) (cid:15) (cid:14)(cid:17) (cid:16) (cid:15)(cid:19) (cid:18)(cid:9)(cid:18)(cid:20)(cid:18)(cid:22) (cid:21) (cid:12) and constants (cid:23) (cid:10)(cid:2) (cid:15) (cid:",Optimization & Theoretical ML,3de568f8597b94bda53149c7d7f5958c-Paper.pdf,2002
Support Vector Machines for,"This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristically. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharma ceutical data set and on applications in automated image indexing and document categorization. 1 Introduction Multiple-instance learning (MIL) [4] is a generalization of supervised classification in which training class labels are associated with sets of patterns, or bags, instead of individual patterns. While every pattern may possess an associated true label, it is assumed that pattern labels are only indirectly accessible through labels attached to bags. The law of inheritance ",Optimization & Theoretical ML,3e6260b81898beacda3d16db379ed329-Paper.pdf,2002
Bias-Optimal Incremental Problem Solving,"Givenisaproblemsequenceandaprobabilitydistribution(thebias)on programs computing solutioncandidates. We present an optimallyfast wayofincrementallysolvingeachtaskinthesequence. Bias shiftsare computedbyprogramprefixesthatmodifythedistributionontheirsuf- fixesbyreusingsuccessfulcodeforprevioustasks(storedinnon-modifi- ablememory).Notestedprogramgetsmoreruntimethanitsprobability timesthetotalsearchtime.Inillustrativeexperiments,oursbecomesthe firstgeneralsystemtolearnauniversalsolverforarbitrary (cid:0) diskTow- ers of Hanoi tasks (minimal solutionsize (cid:1)(cid:3) (cid:2)(cid:5) (cid:4)(cid:7) (cid:6) ). It demonstrates the advantagesofincrementallearningbyprofitingfrompreviouslysolved, simplertasksinvolvingsamplesofasimplecontextfreelanguage. 1 Brief Introduction to OptimalUniversalSearch Consideranasymptoticallyoptimalmethodfortaskswithquicklyverifiablesolutions: Method1.1(LSEARCH) Viewthe (cid:0) -thbinarystring (cid:8)(cid:10) (cid:9)(cid:3) (cid:11) (cid:6) (cid:11)(cid:12) (cid:",Optimization & Theoretical ML,40b5f25a228570053bc64a043c3f1833-Paper.pdf,2002
Rate Distortion Function in the Spin Glass State:,"Weappliedstatisticalmechanicstoaninverseproblemoflinearmapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a fidelity criterion, is derived. Numerical study shows that sparse constructions of the model providesuboptimalcompressions. 1 Introduction Manyinformation-sciencestudiesareverysimilartothoseofstatisticalphysics. Statistical physicsandinformationsciencemayhavebeenexpectedtobedirectedtowardscommon objectives since Shannon formulated an information theory based on the concept of en- tropy. However,envisaginghowthisactuallyhappenedwouldhavebeendifficult;thatthe physicsofdisorderedsystems,andspinglasstheoryinparticular,atitsmaturitynaturally includessomeimportantaspectsofinformationsciences,thusreunitingthetwodisciplines. Thiscross-disciplinaryfieldcanthusbeexpectedtodevelop",Optimization & Theoretical ML,41e7637e7b6a9f27a98b84d3a185c7c0-Paper.pdf,2002
Adaptive Nonlinear System Identification,"Echo state networks (ESN) are a novel approach to recurrent neu ral network training. An ESN consists of a large, fixed, recurrent ""reservoir"" network, from which the desired output is obtained by training suitable output connection weights. Determination of op timal output weights becomes a linear, uniquely solvable task of MSE minimization. This article reviews the basic ideas and de scribes an online adaptation scheme based on the RLS algorithm known from adaptive linear systems. As an example, a 10-th or der NARMA system is adaptively identified. The known benefits of the RLS algorithms carryover from linear systems to nonlinear ones; specifically, the convergence rate and misadjustment can be determined at design time. 1 Introduction It is fair to say that difficulties with existing algorithms have so far precluded su pervised training techniques for recurrent neural networks (RNNs) from widespread use. Echo state networks (ESNs) provide a novel and easier to manage approach to su",Optimization & Theoretical ML,426f990b332ef8193a61cc90516c1245-Paper.pdf,2002
Real Time Voice Processing with Audiovisual,"Wehaveimplementedarealtimefrontendfordetectingvoicedspeech and estimating its fundamental frequency. The front end performs the signalprocessingforvoice-drivenagentsthatattendtothepitchcontours ofhumanspeechandprovidecontinuousaudiovisualfeedback. Theal- gorithmweuseforpitchtrackinghasseveraldistinguishingfeatures: it makesnouseofFFTsorautocorrelationatthepitchperiod;itupdatesthe pitchincrementallyonasample-by-samplebasis;itavoidspeakpicking anddoesnotrequireinterpolationintimeorfrequencytoobtainhighres- olutionestimates;anditworksreliablyoverafouroctaverange,inreal time, without the need for postprocessingto producesmoothcontours. Thealgorithmis basedontwo simpleideasin neuralcomputation: the introductionofapurposefulnonlinearity,andtheerrorsignalofaleast squaresfit.Thepitchtrackerisusedintworealtimemultimediaapplica- tions: avoice-to-MIDIplayerthatsynthesizeselectronicmusicfromvo- calizedmelodies,andanaudiovisualKaraokemachinewithmultimodal feedback. Bothapplicationsrunonalaptopanddi",Computer Vision,43351f7bf9a215be70c2c2caa7555002-Paper.pdf,2002
Visual Development Aids the Acquisition of,"We considerthehypothesisthatsystems learningaspectsofvisualper- ceptionmaybenefitfromtheuseofsuitablydesigneddevelopmentalpro- gressionsduringtraining. Fourmodelsweretrainedtoestimatemotion velocitiesinsequencesofvisualimages. Threeofthemodelswere“de- velopmentalmodels”inthesensethatthenatureoftheirinputchanged during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improvedas training progressed. One model used a coarse-to-multiscale develop- mental progression (i.e. it received coarse-scale motion features early in training and finer-scale features were added to its input as training progressed), another model used a fine-to-multiscale progression, and the third model used a randomprogression. The final model was non- developmentalinthesensethatthenatureofitsinputremainedthesame throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offe",Computer Vision,443dec3062d0286986e21dc0631734c9-Paper.pdf,2002
On the Complexity of Learning,"Weinvestigatedatabasedproceduresforselectingthekernelwhenlearn- ing with Support Vector Machines. We provide generalization error boundsbyestimatingtheRademachercomplexitiesofthecorresponding functionclasses.Inparticularweobtainacomplexityboundforfunction classesinducedbykernelswithgiveneigenvectors,i.e.,weallowtovary the spectrumandkeepthe eigenvectorsfix. This boundis onlya loga- rithmic factorbiggerthan the complexityof the functionclass induced by a single kernel. However, optimizing the margin over such classes leadstooverfitting. Wethusproposeasuitablewayofconstrainingthe class. We use an efficientalgorithmto solve theresultingoptimization problem,presentpreliminaryexperimentalresults,andcomparethemto analignment-basedapproach. 1 Introduction EversincetheintroductionoftheSupportVectorMachine(SVM)algorithm,thequestion ofchoosingthekernelhasbeenconsideredascrucial. Indeed,thesuccessofSVMcanbe attributedtothejointuseofarobustclassificationprocedure(largemarginhyperplane)and ofaconve",Optimization & Theoretical ML,46a558d97954d0692411c861cf78ef79-Paper.pdf,2002
An Impossibility Theorem for Clustering,"Although the study of clustering is centered around an intuitively compelling goal, it has been very di(cid:14)cult to develop a uni(cid:12)ed framework for reasoning about it at a technical level, and pro- foundly diverse approaches to clustering abound in the research community. Here we suggesta formal perspective on the di(cid:14)culty in (cid:12)nding such a uni(cid:12)cation, in the form of an impossibility theo- rem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these prop- erties expose some of the interesting (and unavoidable) trade-o(cid:11)s atworkinwell-studiedclusteringtechniquessuchassingle-linkage, sum-of-pairs, k-means, and k-median. 1 Introduction Clustering is a notion that arisesnaturally in many (cid:12)elds; wheneverone has a het- erogeneousset of objects, it is natural to seek methods for groupingthem together based on an underlying measure of similarity. A standard approach is to represent t",Optimization & Theoretical ML,43e4e6a6f341e00671e123714de019a8-Paper.pdf,2002
Artefactual Structure from Least Squares,"Weconsidertheproblemofillusoryorartefactualstructurefromthevi- sualisation of high-dimensionalstructureless data. In particular we ex- aminetheroleofthedistancemetricintheuseoftopographicmappings basedonthestatisticalfieldofmultidimensionalscaling. Weshowthat theuseofasquaredEuclideanmetric(i.e.theSSTRESS measure)gives rise to an annular structure when the input data is drawn from a high- dimensionalisotropicdistribution,andweprovideatheoreticaljustifica- tionforthisobservation. 1 Introduction Thediscoveryofmeaningfulpatternsandrelationshipsfromlargeamountsofmultivariate dataisasignificantandchallengingproblemwithclosetiestothefieldsofpatternrecog- nition and machine learning, and important applications in the areas of data mining and knowledgediscoveryindatabases(KDD). Formanyreal-worldhigh-dimensionaldatasets (suchas collectionsofimages,ormulti- channel recordingsof biomedicalsignals) there will generallybe strong correlations be- tween neighbouring observations, and thus we expect t",Optimization & Theoretical ML,487d4c6a324446b3fa45b30cfcee5337-Paper.pdf,2002
Adaptation and Unsupervised Learning,"Adaptationis aubiquitousneuralandpsychologicalphenomenon,with a wealth of instantiations and implications. Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theoryofonlyonemainvariety. Inthispaper,westudyadaptationfrom theperspectiveoffactoranalysis, aparadigmatictechniqueofunsuper- visedlearning. Weusefactoranalysistore-interpretastandardviewof adaptation,andapplyournewmodeltosomerecentdataonadaptation inthedomainoffacediscrimination. 1 Introduction Adaptationisoneofthefirstfactswithwhichneophyteneuroscientistsandpsychologists arepresented.Essentiallyallsensoryandcentralsystemsshowadaptationatawidevariety oftemporalscales, andto a widevarietyofaspects oftheir informationalmilieu. Adap- tationis aproduct(orpossiblyby-product)ofmanyneuralmechanisms,fromshort-term synapticfacilitationanddepression,1andspike-rateadaptation,28throughsynapticremod- eling27andwaybeyond.Adaptationhasbeendescribedasthepsychophysicist’selectrode, sinceitcanbeuseda",NLP,49c0b9d84c2a16fcaf9d25694fda75e1-Paper.pdf,2002
Learning in Zero-Sum Team Markov Games,"We present a new method for learning good strategies in zero-sum Markovgamesin which each side is composedof multiple agents col- laboratingagainstanopposingteamofagents. Ourmethodrequiresfull observabilityand communicationduringlearning, but the learnedpoli- ciescanbeexecutedinadistributedmanner. Thevaluefunctionisrep- resentedasafactoredlineararchitectureanditsstructuredeterminesthe necessarycomputationalresourcesandcommunicationbandwidth. This approachpermitsatradeoffbetweensimplerepresentationswithlittleor nocommunicationbetweenagentsandcomplex,computationallyinten- siverepresentationswithextensivecoordinationbetweenagents. Thus, we provide a principled means of using approximation to combat the exponentialblowupinthejointactionspaceoftheparticipants. Theap- proachisdemonstratedwithanexamplethatshowstheefficiencygains overnaiveenumeration. 1 Introduction The Markov game framework has received increased attention as a rigorous model for defininganddeterminingoptimalbehaviorinmultiag",Reinforcement Learning,4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf,2002
Convergence Properties of some,"vVe analyzetheconvergencepropertiesofthreespike-triggereddata analysis techniques. All of our results are obtained in the set ting ofa (possiblymultidimensional) linear-nonlinear (LN) cascade model for stimulus-driven neural activity. Westart by givingexact rate ofconvergence results for the common spike-triggered average (STA) technique. Next, we analyze a spike-triggered covariance method, variants ofwhichhavebeenrecentlyexploitedsuccessfully byBialek, Simoncelli, and colleagues. Thesefirst two methods suf fer from extraneous conditions on their convergence; therefore, we introduce an estimator for the LN model parameters which is de signed to be consistent under general conditions. We provide an algorithm for the computation ofthis estimator and derive its rate of convergence. We close with a brief discussion of the efficiency of these estimators and an application to data recorded from the primary motor cortex of awake, behaving primates. 1 Introduction Systems-level neuroscientist",Optimization & Theoretical ML,4aecfbe5d21e3f7912bf8eb29124423a-Paper.pdf,2002
Learning to Take Concurrent Actions,"We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. Weintroducethreetypesofparallelterminationschemes{all,any andcontinue{andtheoreticallyandexperimentallycomparethem. 1 Introduction We investigate a general framework for modeling concurrent actions. The notion of concurrentactionisformalizedinageneralway, tocapturebothsituationswherea single agent can execute multiple parallel processes, as well as the multi-agent case where many agents act in parallel. Concurrency clearly allows agents to achieve goals more quickly: in making breakfast, we interleave making toast and co(cid:11)ee with other activities such as getting milk; in driving, we search for road signs while controlling the wheel, accelerator and brakes. Most previous work on concurrency has focused on parallelizing primitive (unit step) actions. Reiter developed axioms for concurrent planni",Optimization & Theoretical ML,4b4edc2630fe75800ddc29a7b4070add-Paper.pdf,2002
Handling Missing Data with Variational,Missingdataiscommoninreal-worlddatasetsandisaproblemformany estimationtechniques.WehavedevelopedavariationalBayesianmethod toperformIndependentComponentAnalysis(ICA)onhigh-dimensional datacontainingmissingentries.Missingdataarehandlednaturallyinthe Bayesianframeworkbyintegratingthegenerativedensitymodel. Mod- elingthedistributionsoftheindependentsourceswithmixtureofGaus- siansallowssourcestobeestimatedwithdifferentkurtosisandskewness. The variational Bayesian method automatically determines the dimen- sionality of the data and yields an accurate density model for the ob- serveddatawithoutoverfittingproblems. Thisallowsdirectprobability estimation of missing values in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data. 1 Introduction Datadensityestimationisanimportantstepinmanymachinelearningproblems.Oftenwe arefacedwithdatacontainingincompleteentries. Thedatamaybemissingduetomea- surementorrecordingfailure. Anotherfrequentcau,Computer Vision,4cb811134b9d39fc3104bd06ce75abad-Paper.pdf,2002
Ranking with Large Margin Principle: Two,"We discuss the problem of ranking k instances with the use of a ""large margin"" principle. We introduce two main approaches: the first is the ""fixed margin"" policy in which the margin of the closest neighboring classes is being maximized - which turns out to be a direct generaliza tion of SVM to ranking learning. The second approach allows for k - 1 different margins where the sum of margins is maximized. This approach is shown to reduce to lI-SVM when the number of classes k = 2. Both approaches are optimal in size of 21 where I is the total number of training examples. Experiments performed on visual classification and ""collab orative filtering"" show that both approaches outperform existing ordinal regression algorithms applied for ranking and multi-class SVM applied to general multi-class classification. 1 Introduction In this paper we investigate the problem of inductive learning from the point of view of predicting variables of ordinal scale [3, 7,5], a setting referred to as ranki",Optimization & Theoretical ML,51de85ddd068f0bc787691d356176df9-Paper.pdf,2002
A Bilinear Model for Sparse Coding,"Recentalgorithmsforsparsecodingandindependentcomponentanaly- sis(ICA)havedemonstratedhowlocalizedfeaturescanbelearnedfrom natural images. However, theseapproaches donottakeimagetransfor- mations into account. As a result, they produce image codes that are redundantbecausethesamefeatureislearnedatmultiplelocations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between im- agefeaturesandtheirtransformations,thebilinearapproachhelpsreduce redundancy in the image code and provides a basis for transformation- invariantvision.Wepresentresultsdemonstratingbilinearsparsecoding of natural images. We also explore an extension of the model that can capturespatialrelationshipsbetweentheindependentfeaturesofanob- ject,therebyprovidinganewframeworkforparts-basedobjectrecogni- tion. 1 Introduction Algorithms for redundancy reduction and efficient coding have been the subject of con- siderable attention in recent years",Optimization & Theoretical ML,5249ee8e0cff02ad6b4cc0ee0e50b7d1-Paper.pdf,2002
Data-Dependent Bounds for Bayesian,"We consider Bayesian mixture approaches, where a predictor is constructedbyformingaweightedaverageofhypothesesfromsome space of functions. While such procedures are known to lead to optimalpredictorsinseveralcases,wheresu–cientlyaccurateprior information is available, it has not been clear how they perform when some of the prior assumptions are violated. In this paper we establish data-dependent bounds for such procedures, extending previous randomized approaches such as the Gibbs algorithm to a fully Bayesian setting. The flnite-sample guarantees established in this work enable the utilization of Bayesian mixture approaches in agnostic settings, where the usual assumptions of the Bayesian paradigmfailtohold. Moreover,theboundsderivedcanbedirectly applied to non-Bayesian mixture approaches such as Bagging and Boosting. 1 Introduction and Motivation The standard approach to Computational Learning Theory is usually formulated within the so-called frequentist approach to Statistics. Withi",Optimization & Theoretical ML,53ed35c74a2ec275b837374f04396c03-Paper.pdf,2002
Clustering with the Fisher Score,"RecentlytheFisherscore(ortheFisherkernel)isincreasinglyusedasa featureextractorforclassificationproblems.TheFisherscoreisavector ofparameterderivativesofloglikelihoodofaprobabilisticmodel. This paper gives a theoretical analysis about how class information is pre- servedin the space of the Fisher score, which turns out that the Fisher scoreconsistsofafewimportantdimensionswithclassinformationand manynuisancedimensions.WhenweperformclusteringwiththeFisher score,K-Meanstypemethodsareobviouslyinappropriatebecausethey makeuseofalldimensions.Sowewilldevelopanovelbutsimpleclus- teringalgorithmspecializedfortheFisherscore,whichcanexploitim- portantdimensions.Thisalgorithmissuccessfullytestedinexperiments withartificialdataandrealdata(aminoacidsequences). 1 Introduction Clustering is widely used in exploratory analysis for various kinds of data [6]. Among them, discrete data such as biological sequences [2] are especially challenging, because efficientclusteringalgorithmse.g.K-Means[6]cannotbe",Optimization & Theoretical ML,47810f956e3d8fb8a32fb276448b464d-Paper.pdf,2002
Information Diffusion Kernels,"A new family of kernels for statistical learning is introduced that ex- ploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold defined by the Fisher informa- tionmetric,informationdiffusionkernelsgeneralizetheGaussiankernel ofEuclideanspace, andprovideanaturalwayof combininggenerative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for thenewkernelsareprovedusingspectraltheoryindifferentialgeometry, andexperimentalresultsarepresentedfortextclassification. 1 Introduction The use of kernels is of increasing importance in machine learning. When “kernelized,” simplelearningalgorithmscanbecomesophisticatedtoolsfortacklingnonlineardataanal- ysisproblems.Researchinthisareacontinuestoprogressrapidly,withmostoftheactivity focusedontheunderlyinglearningalgorithmsratherthanonthekernelsthe",Optimization & Theoretical ML,5938b4d054136e5d59ada6ec9c295d7a-Paper.pdf,2002
Boosted Dyadic Kernel Discriminants,"We introduce a novel learning algorithm for binary classi(cid:12)cation with hyperplane discriminants based on pairs of training points from oppositeclasses(dyadic hypercuts). This algorithmis further extended to nonlinear discriminants using kernel functions satisfy- ingMercer’sconditions. Anensembleofsimpledyadichypercutsis learnedincrementallybymeansofacon(cid:12)dence-ratedversionofAd- aBoost,which providesa sound strategyfor searchingthroughthe (cid:12)nite set of hypercut hypotheses. In experiments with real-world datasets from the UCI repository, the generalization performance of the hypercut classi(cid:12)ers was found to be comparable to that of SVMs and k-NN classi(cid:12)ers. Furthermore, the computational cost of classi(cid:12)cation (at run time) was found to be similar to, or bet- ter than, that of SVM. Similarly to SVMs, boosted dyadic kernel discriminants tend to maximize the margin (via AdaBoost). In contrast to SVMs, however, we o(cid:11)er an on-line and incremental ",Optimization & Theoretical ML,598920e11d1eb2a49501d59fce5ecbb7-Paper.pdf,2002
Derivative observations in Gaussian Process,"Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observationsinanempiricalmodel. This is ofparticularimportancein identificationofnonlineardynamicsystemsfromexperimentaldata.1)It allowsustocombinederivativeinformation,andassociateduncertainty with normal function observations into the learning and inference pro- cess. This derivativeinformationcanbein theformofpriorsspecified byanexpertoridentifiedfromperturbationdataclosetoequilibrium.2) It allows a seamless fusion of multiple local linear modelsin a consis- tent manner,inferringconsistent modelsand ensuringthat integrability constraints are met. 3) It improves dramatically the computational ef- ficiencyofGaussianprocessmodelsfordynamicsystemidentification, bysummarisinglargequantitiesofnear-equilibriumdatabyahandfulof linearisations,reducingthetrainingsetsize–traditionallyaproblemfor Gaussianprocessmodels. 1 Introduction In many applications which ",Optimization & Theoretical ML,5b8e4fd39d9786228649a8a8bec4e008-Paper.pdf,2002
Global Versus Local Methods,"Recentlyproposedalgorithmsfornonlineardimensionalityreductionfall broadlyintotwocategorieswhichhavedifferentadvantagesanddisad- vantages:global(Isomap[1]),andlocal(LocallyLinearEmbedding[2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previ- ouslybeenexclusiveadvantagesoflocalmethods: computationalspar- sityandtheabilitytoinvertconformalmaps. 1 Introduction Inthis paperwe discuss the problemofnonlineardimensionalityreduction(NLDR): the taskofrecoveringmeaningfullow-dimensionalstructureshiddeninhigh-dimensionaldata. Anexamplemightbeaset ofpixelimagesofanindividual’sfaceobservedunderdiffer- entposeandlightingconditions;thetaskistoidentifytheunderlyingvariables(posean- gles, directionoflight, etc.) givenonlythe high-dimensionalpixelimagedata. Inmany cases of interest, the observeddata are foundto lie on an embeddedsubmanifoldof the high-dimensionalspace.Thedegreesoffreedomalongthissubmanifoldcorrespondtothe",Optimization & Theoretical ML,5d6646aad9bcc0be55b2c82f69750387-Paper.pdf,2002
Learning Graphical Models,"We presenta class of algorithmsfor learningthe structureof graphical models from data. The algorithms are based on a measure known as the kernel generalized variance (KGV), which essentially allows us to treat all variables on an equal footing as Gaussians in a feature space obtainedfromMercerkernels. Thusweareabletolearnhybridgraphs involvingdiscreteandcontinuousvariablesofarbitrarytype.Weexplore the computational properties of our approach, showing how to use the kerneltricktocomputetherelevantstatisticsinlineartime.Weillustrate ourframeworkwithexperimentsinvolvingdiscreteandcontinuousdata. 1 Introduction Graphicalmodelsareacompactandefficientwayofrepresentingajointprobabilitydistri- butionofasetofvariables. Inrecentyears,therehasbeenagrowinginterestinlearning thestructureofgraphicalmodelsdirectlyfromdata,eitherinthedirectedcase[1,2,3,4] or the undirectedcase [5]. Current algorithms deal reasonablywell with models involv- ing discrete variablesor Gaussian variables having onlylimited",Optimization & Theoretical ML,5f6371c9126149517d9ba475def53139-Paper.pdf,2002
Stochastic Neighbor Embedding,"We describeaprobabilisticapproachtothetask ofplacingobjects, de- scribedbyhigh-dimensionalvectorsorbypairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centeredon each object in the high-dimensionalspace and the densities underthis Gaussian (or the givendissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leiblerdivergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlikeotherdimensionalityreductionmethods,thisprobabilistic frameworkmakesiteasytorepresenteachobjectbyamixtureofwidely separatedlow-dimensionalimages. Thisallowsambiguousobjects,like thedocumentcountvectorfortheword“bank”,tohaveve",Optimization & Theoretical ML,6150ccc6069bea6b5716254057a194ef-Paper.pdf,2002
Learning in Spiking Neural Assemblies,"We consider a statistical framework for learning in a class of net- works of spiking neurons. Our aim is to show how optimal local learningrulescanbereadilyderivedoncetheneuraldynamicsand desired functionality of the neural assembly have been specifled, in contrast to other models which assume (sub-optimal) learning rules. Withinthisframeworkwederivelocalrulesforlearningtem- poral sequences in a model of spiking neurons and demonstrate its superior performance to correlation (Hebbian) based approaches. We further show how to include mechanisms such as synaptic de- pression and outline how the framework is readily extensible to learninginnetworksofhighlycomplexspikingneurons. Astochas- ticquantalvesiclereleasemechanismisconsideredandimplications on the complexity of learning discussed. 1 Introduction Models of individual neurons range from simple rate based approaches to spik- ing models and further detailed descriptions of protein dynamics within the cell[9, 10, 13, 6, 12]. As the expe",Optimization & Theoretical ML,619205da514e83f869515c782a328d3c-Paper.pdf,2002
A Model for Real-Time Computation in Generic,"A key challenge for neural modeling is to explain how a continuous streamofmulti-modalinputfromarapidlychangingenvironmentcanbe processedbystereotypicalrecurrentcircuitsofintegrate-and-fireneurons in real-time. We propose a new computational model that is based on principles of high dimensional dynamical systems in combination with statisticallearningtheory. Itcanbeimplementedongenericevolvedor foundrecurrentcircuitry. 1 Introduction Diverse real-time information processing tasks are carried out by neural microcircuits in thecerebralcortexwhoseanatomicalandphysiologicalstructureisquitesimilarinmany brainareasandspecies.Howeveramodelthatcouldexplainthepotentiallyuniversalcom- putational capabilities of such recurrent circuits of neurons has been missing. Common modelsfortheorganizationofcomputations,suchasforexampleTuringmachinesorat- tractorneuralnetworks,arenotsuitablesincecorticalmicrocircuitscarryoutcomputations on continuous streams of inputs. Often there is no time to wait until a",Computer Vision,6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf,2002
Categorization Under Complexity: A Unified,"We present an account ofhuman concept learning-thatis, learning of categories from examples-basedontheprincipleofminimumdescrip tion length (MDL). In support ofthis theory, we tested a wide range oftwo-dimensional concept types, including both regular (simple) and highlyirregular(complex)structures, andfoundtheMDLtheoryto give agood account ofsubjects' performance. This suggests thatthe intrin siccomplexityofaconcept(thatis,itsdescription-length)systematically influencesitsleamability. 1- The Structure ofCategories Anumberofdifferentprincipleshavebeen advancedto explainthe mannerinwhichhu manslearnto categorizeobjects. Ithasbeenvariouslysuggestedthattheunderlyingprin ciplemightbethesimilaritystructureofobjects [1],themanipulabilityofdecision bound~ aries [2], orBayesianinference [3][4]. While manyofthese theories are mathematically well-grounded and have been successful in explaining arange ofexperimentalfindings, theyhave commonly onlybeentested on anarrow collectionofconcept types si",NLP,635440afdfc39fe37995fed127d7df4f-Paper.pdf,2002
Learning with Multiple Labels,"In this paper, we study a special kind of learning problem in which each training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one. Such a problem can occur, e.g., in an information retrieval setting where a set of words is associated with an image, or if classes labels are organized hierarchically. We propose a novel discriminative approach for handling the ambiguity of class labels in the training examples. The experiments with the proposed approach over five different UCI datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label. In contrast, naIve methods degrade rapidly as more ambiguity is introduced into the labels. 1 Introduction Supervised and unsupervised learning problems have been extensively studied in the machine learning literature. In superv",NLP,653ac11ca60b3e021a8c609c7198acfc-Paper.pdf,2002
Dyadic Classification Trees,"Classificationtreesareoneofthemostpopulartypesofclassifiers,with ease of implementation and interpretation being among their attractive features. Despite the widespread use of classification trees, theoretical analysisoftheirperformanceisscarce.Inthispaper,weshowthatanew family of classification trees, called dyadicclassification trees (DCTs), are near optimal (in a minimax sense) for a very broad range of clas- sification problems. This demonstratesthat other schemes(e.g., neural networks, support vectormachines) cannotperformsignificantly better thanDCTs in manycases. We also showthat this nearoptimalperfor- manceisattainedwithlinear(inthenumberoftrainingdata)complexity growingand pruningalgorithms. Moreover,the performanceof DCTs on benchmark datasets compares favorably to that of standard CART, which is generallymore computationallyintensive and which does not possesssimilarnearoptimalityproperties. Ouranalysisstemsfromthe- oreticalresultsonstructuralriskminimization,onwhichtheprun",Optimization & Theoretical ML,6832a7b24bc06775d02b7406880b93fc-Paper.pdf,2002
Branching Law for Axons,"What determines the caliber of axonal branches? We pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays, while keeping arbor volume to a minimum. We show that for a general cost function the optimal diameters of mother (do) and daughter (d], d 2) branches at a bifurcation obey a b ranc hm· g 1a w: d 0v +2 =d ]v +2 + d 2v +2 . The den""v atIOn re ll' es on t h e fact that the conduction speed scales with the axon diameter to the power V (v = 1 for myelinated axons and V = 0.5 for non myelinated axons). We test the branching law on the available experimental data and find a reasonable agreement. 1 Introduction Multi-cellular organisms have solved the problem of efficient transport of nutrients and communication between their body parts by evolving spectacular networks: trees, blood vessels, bronchs, and neuronal arbors. These networks consist of segments bifurcating into thinner and thinner branches. Understanding of branching in transport networ",Computer Vision,66e8ba8216a1e152d72653d99a4f03ab-Paper.pdf,2002
ynamic Causal Learning,"Current psychological theories of human causal learning and judgment focus primarily on long-run predictions: two by estimating parameters ofa causal Bayes nets (though for different parameterizations), and a third through structural learning. This paper focuses on people's short-run behavior by examining dynamical versions of these three theories, and comparing their predictionsto areal-world dataset. 1 Introduction Currently active quantitative models of human causal judgment for single (and sometimes multiple) causes include conditional j}JJ [8], powerPC [1], and Bayesian network structure learning [4], [9]. All of these theories have some normative justification, and all can be understood rationally in terms oflearning causal Bayes nets. The first two theories assume a parameterization for a Bayes net, and then perform maximum likelihood parameter estimation. Each has been the target of numerous psychological studies (both confirming and disconfirming) over the past ten years. The ",NLP,697e382cfd25b07a3e62275d3ee132b3-Paper.pdf,2002
shorter argument and much tighter than previous margin bounds.,No abstract found,Optimization & Theoretical ML,68d309812548887400e375eaa036d2f1-Paper.pdf,2002
Maximally Informative Dimensions: Analyzing,No abstract found,Optimization & Theoretical ML,69dafe8b58066478aea48f3d0f384820-Paper.pdf,2002
Fast Exact Inference with a Factored Model for,"Wepresentanovelgenerativemodelfornaturallanguagetreestructures inwhichsemantic(lexicaldependency)andsyntactic(PCFG)structures are scored with separate models. This factorization provides concep- tual simplicity, straightforward opportunities for separately improving thecomponentmodels,andalevelofperformancecomparabletosimi- lar,non-factoredmodels.Mostimportantly,unlikeothermodernparsing models,thefactoredmodeladmitsanextremelyeffectiveA*parsingal- gorithm,whichenablesefficient,exactinference. 1 Introduction Syntacticstructurehasstandardlybeendescribedintermsofcategories(phrasallabelsand word classes), with little mention of particular words. This is possible, since, with the exceptionofcertaincommonfunctionwords,theacceptablesyntacticconfigurationsofa languagearelargelyindependentoftheparticularwordsthatfilloutasentence.Conversely, for resolving the important attachment ambiguities of modifiers and arguments, lexical preferencesareknowntobeveryeffective.Additionally,methodsbasedonlyonk",NLP,6c97cd07663b099253bc569fe8d342bb-Paper.pdf,2002
One-Class LP Classifier for Dissimilarity,"Problemsinwhichabnormalornovelsituationsshouldbedetectedcan be approached by describing the domain of the class of typical exam- ples. These applications come from the areas of machine diagnostics, fault detection, illness identification or, in principle, refer to any prob- lemwherelittleknowledgeisavailableoutsidethetypicalclass. Inthis paperweexplainwhyproximitiesarenaturalrepresentationsfordomain descriptorsandweproposeasimpleone-classclassifierfordissimilarity representations. Bytheuseoflinearprogramminganefficientone-class descriptioncanbefound,basedonasmallnumberofprototypeobjects. Thisclassifiercanbemade(1)morerobustbytransformingthedissimi- laritiesand(2)cheapertocomputebyusingareducedrepresentationset. Finally,acomparisontoacomparableone-classclassifierbyCampbell andBennettisgiven. 1 Introduction Theproblemofdescribingaclassoradomainhasrecentlygainedalotofattention,sinceit canbeidentifiedinmanyapplications. Theareaofinterestcoversalltheproblems,where thespecifiedtargetshavetob",Optimization & Theoretical ML,6e5025ccc7d638ae4e724da8938450a6-Paper.pdf,2002
Morton-Style Factorial Coding of Color in,"We introduce the notion of Morton-stylefactorial coding and illustrate howitmayhelpunderstandinformationintegrationandperceptualcod- ing in the brain. We show that by focusing on average responses one maymisstheexistenceoffactorialcodingmechanismsthatbecomeonly apparent when analyzing spike count histograms. We show evidence suggestingthattheclassical/non-classicalreceptivefieldorganizationin thecortexeffectivelyenforcesthedevelopmentofMorton-stylefactorial codes. Thismayprovidesomecuestohelpunderstandperceptualcod- ing in the brain and to develop new unsupervised learning algorithms. WhilemethodslikeICA(Bell&Sejnowski,1997)developindependent codes,inMorton-stylecodingthegoalistomaketwoormoreexternal aspectsoftheworldbecomeindependentwhenconditioningoninternal representations. InthispaperweintroducethenotionofMorton-stylefactorialcodingandillustratehowit mayhelpanalyzeinformationintegrationandperceptualorganizationinthebrain. Inthe neurosciences factorial codes are often studied in the",Neuroscience,6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf,2002
Regularized Greedy Importance Sampling,"Greedyimportancesamplingisanunbiasedestimationtechniquethatre- ducesthevarianceofstandardimportancesamplingbyexplicitlysearch- ing for modes in the estimation objective. Previous work has demon- strated the feasibility of implementingthis methodand provedthat the techniqueis unbiasedin both discreteand continuousdomains. In this paper we present a reformulation of greedy importance sampling that eliminatesthefreeparametersfromtheoriginalestimator,andintroduces anewregularizationstrategythatfurtherreducesvariancewithoutcom- promisingunbiasedness.Theresultingestimatorisshowntobeeffective for difficult estimation problems arising in Markov random field infer- ence. In particular, improvements are achieved over standard MCMC estimatorswhenthedistributionhasmultiplepeakedmodes. 1 Introduction Many inference problems in graphical models can be cast as determining the expected value of a random variable of interest, (cid:2) , given observations drawn according to a tar- getdistribution (cid:3",Optimization & Theoretical ML,71560ce98c8250ce57a6a970c9991a5f-Paper.pdf,2002
Fast Kernels for String and Tree Matching,"In this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time, thus obviating dynarrtic programming with quadratic time complexity. Furthermore, prediction cost in many cases can be reduced to linear cost in the length of the se quence to be classified, regardless of the number of support vectors. This improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner. 1 Introduction Many problems in machine learning require the classifier to work with a set of discrete ex amples. Common examples include biological sequence analysis where data is represented as strings [4] and Natural Language Processing (NLP) where the data is in the form a parse tree [3]. In order to apply kernel methods one defines a measure of similarity between discrete structures via a feature map ¢ : X ----+ Jek. Here X is the set of discrete structures (eg. the set of all parse trees of a language) and JeK i",NLP,743394beff4b1282ba735e5e3723ed74-Paper.pdf,2002
Expected and Unexpected Uncertainty:,"Inference and adaptation in noisy and changing, rich sensory environ- mentsarerifewithavarietyofspecificsortsofvariability.Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reportedbydifferent(notablyneuromodulatory)systems. Here, we re- fine our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic)terms of expected uncertainty, and advocatea theory fornorepinephrineintermsof unexpecteduncertainty. We suggestthat norepinephrinereportstheradicaldivergenceofbottom-upinputsfrom prevailingtop-downinterpretations,toinfluenceinferenceandplasticity. Weillustratethisproposalusinganadaptivefactoranalysismodel. 1 Introduction Animals negotiatingrich environmentsare faced with a set of hugelycomplexinference andlearningproblems,involvingmanyformsofvariability.Theycanbe unsurewhichcon- textpresentlypertains,cuescanbesystematicallymoreorless reliable,andrelationship",NLP,758a06618c69880a6cee5314ee42d52f-Paper.pdf,2002
Feature Selection and Classiflcation on,"We investigate the problem of learning a classiflcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column ob- jects may belong to difierent sets, and the entries in the matrix expresstherelationshipsbetweenthem. Weinterpretthematrixel- ementsasbeingproducedbyanunknownkernelwhichoperateson objectpairsandweshowthat-undermildassumptions-theseker- nels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classi- fler which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantagethatitallowstheanalysisofmatriceswhicharenotpos- itivedeflnite, andnotevensymmetricorsquare. Wethenconsider thecasethatrowobjectsareinterpretedasfeatures. Wesuggestan additionalconstraint, whichimposessparsenessontherowobjects and sh",Optimization & Theoretical ML,761c7920f470038d4c8a619c79eddd62-Paper.pdf,2002
Learning Semantic Similarity,"The standard representation of text documents as bags of words suffers from well known limitations, mostly due to its inability to exploit semantic similarity between terms. Attempts to incorpo rate some notion of term similarity include latent semantic index ing [8], the use of semantic networks [9], and probabilistic methods [5]. In this paper we propose two methods for inferring such sim ilarity from a corpus. The first one defines word-similarity based on document-similarity and viceversa, giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure. The second method models semantic relations by means of a diffusion process on a graph defined by lexicon and co-occurrence information. Both approaches produce valid kernel functions parametrised by a real number. The paper shows how the alignment measure can be used to successfully perform model selection over this parameter. Combined with the use of support vector machines we obtain pos",NLP,778609db5dc7e1a8315717a9cdd8fd6f-Paper.pdf,2002
How Linear are Auditory Cortical Responses?,"By comparison to some other sensory cortices, the functional proper- ties ofcells inthe primaryauditorycortexare notyetwell understood. Recent attemptsto obtaina generalizeddescriptionofauditorycortical responses have often relied upon characterization of the spectrotempo- ral receptive field (STRF), which amounts to a model of the stimulus- responsefunction(SRF)thatislinearinthespectrogramofthestimulus. Howwellcansuchamodelaccountforneuralresponsesattheveryfirst stagesofauditorycorticalprocessing? Toanswerthisquestion,wede- velopanovelmethodologyforevaluatingthefractionofstimulus-related responsepowerina populationthatcanbecapturedbya giventypeof SRFmodel.Weusethistechniquetoshowthat,inthethalamo-recipient layers of primary auditory cortex, STRF models account for no more than40%ofthestimulus-relatedpowerinneuralresponses. 1 Introduction A number of recent studies have suggested that spectrotemporal receptive field (STRF) models [1, 2], whichare linearin the stimulus spectrogram, can ",Computer Vision,7b4773c039d539af17c883eb9283dd14-Paper.pdf,2002
Modeling Midazolam's Effect on the,"The benz.odiaze:pine '~1idazolam causes dense,but temporary~ anterograde amnesia, similar to that produced by-hippocampal damage~Does the actionofM'idazola:mon the hippocanlpus cause less storage, or less accurate storage,.ofinformationinepisodic. long-term menlory?-\rVe used a sinlple variant oftheREJv1. JD.odel [18] to fit data collected. by IIirsbnla.n~Fisher, .IIenthorn,Arndt} and Passa.nnante [9] on the effects ofMidazola.m, study time~ and normative \vQrd...frequenc:y on both yes-no and remember-k.novv recognition m.emory. That a: simple strength.'model fit well \\tas cont.rary to the expectations of 'flirshman etaLMore important,within the Bayesianbased R.EM modeling frame\vork, the data were consistentw'ith the view that Midazolam causes less accurate storage~ rather than less storage, of infornlation in episodic mcm.ory.. 1 Introduction 'Danlage to the hippocampus (and nearby regions), often caused by lesiclns 1 leaves normal cognitive function intact in the short term., inclu",NLP,716e1b8c6cd17b771da77391355749f3-Paper.pdf,2002
Incremental Gaussian Processes,"In this paper, we consider Tipping’s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity oftheRVMsolutionwillensurethatthenumberofbasisfunctionsand thereby the computational complexity is kept low. We also introduce a mean field approachto theintractableclassification modelthat is ex- pected to give a very good approximation to exact Bayesian inference and contains the Laplace approximationas a special case. We test the algorithmsontwolargedatasetswith (103 104)examples. There- O (cid:0) sults indicatethat Bayesian learningoflargedatasets, e.g.the MNIST databaseisrealistic. 1 Introduction Tipping’srelevancevectormachine(RVM)bothachievesasparsesolutionlikethesupport vectormachine(SVM)[2,3]andtheprobabilisticpredictionsofBayesiankernelmachines based upona Gaussian process (GP) priors over functions[4, 5, 6",Optimization & Theoretical ML,7bc1ec1d9c3426357e69acd5bf320061-Paper.pdf,2002
Dynamic Bayesian Networks with,"The application of latent/hidden variable Dynamic Bayesian Net- worksisconstrainedbythecomplexityofmarginalisingoverlatent variables. For this reason either small latent dimensions or Gaus- sian latent conditional tables linearly dependent on past states are typicallyconsideredinorderthatinferenceistractable. Wesuggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisa- tion has the advantage of tractable inference even for highly com- plexnon-linear/non-Gaussianvisibleconditionalprobabilitytables. This approach enables the consideration of highly complex latent dynamics whilst retaining the bene(cid:12)ts of a tractable probabilistic model. 1 Introduction Dynamic Bayesian Networks are a powerful framework for temporal data models withwidespreadapplicationintimeseriesanalysis[10,2,5]. Atimeseriesoflength T isasequenceofobservationvectorsV =fv(1);v(2);:::;v(T)g,wherev (t)repre- i sentsthestateofvisibleva",Computer Vision,7cc532d783a7461f227a5da8ea80bfe1-Paper.pdf,2002
Geoffrey J. Gordon,"We introduce the Generalized2 Linear2 Model, a statistical estima tor which combines features of nonlinear regression and factor anal ysis. A (GL)2M approximately decomposes a rectangular matrix X into a simpler representation j(g(A)h(B)). Here A and Bare low-rank matrices, while j, g, and h are link functions. (GL)2Ms include many useful models as special cases, including principal components analysis, exponential-family peA, the infomax formu lation of independent components analysis, linear regression, and generalized linear models. They also include new and interesting special cases, one of which we describe below. We also present an iterative procedure which optimizes the parameters of a (GL)2M. This procedure reduces to well-known algorithms for some of the special cases listed above; for other special cases, it is new. 1 Introduction Let the m x n matrix X contain an independent sample from some unknown distri bution. Each column of X represents a training example, and each row ",Optimization & Theoretical ML,7dc1c7653ac42a05642a667959c12239-Paper.pdf,2002
Spectro-Temporal Receptive Fields of,"Howdocorticalneuronsrepresenttheacousticenvironment?Thisques- tionisoftenaddressedbyprobingwithsimplestimulisuchasclicksor tonepips. Suchstimulihavetheadvantageofyieldingeasilyinterpreted answers,buthavethedisadvantagethattheymayfailtouncovercomplex orhigher-orderneuronalresponseproperties. Hereweadoptanalternativeapproach,probingneuronalresponseswith complexacousticstimuli,includinganimalvocalizationsandmusic.We haveusedinvivowholecellmethodsintheratauditorycortextorecord subthreshold membrane potential fluctuations elicited by these stimuli. Whole cell recording reveals the total synaptic input to a neuron from alltheotherneuronsinthecircuit,insteadofjustitsoutput—asparsebi- naryspiketrain—asinconventionalsingleunitphysiologicalrecordings. Wholecellrecordingthusprovidesamuchrichersourceofinformation abouttheneuron’sresponse. Many neurons responded robustly and reliably to the complex stimuli in our ensemble. Here we analyze the linear component—the spectro- temporal receptivefield(ST",Computer Vision,7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf,2002
Bayesian Models of Inductive Generalization,"We argue that human inductive generalization is best explained in a Bayesian framework, rather than by traditional models based on simi- laritycomputations. WegobeyondpreviousworkonBayesianconcept learning by introducing an unsupervised method for constructing flex- ible hypothesis spaces, and we propose a version of the Bayesian Oc- cam’s razor that trades off priors and likelihoods to prevent under- or over-generalizationin these flexible spaces. We analyze two published datasetsoninductivereasoningaswellastheresultsofanewbehavioral studythatwehavecarriedout. 1 Introduction The problemof inductivereasoning— in particular, how we can generalize after seeing onlyoneorafewspecificexamplesofanovelconcept—hastroubledphilosophers,psy- chologists,andcomputerscientistssincetheearlydaysoftheirdisciplines.Computational approaches to inductive generalization range from simple heuristics based on similarity matching to complex statistical models [5]. Here we consider where human inference falls ",Optimization & Theoretical ML,80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf,2002
Dynamical Constraints on Computing,"If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex. 1 Introduction Several models of neural computation use the precise timing of spikes to encode information. For example, Abeles et al. have proposed synchronous volleys of spikes (synfire chains) as a candidate for representing information in the cortex [1]. More recently, Maass has demonstrated how spike timing in general, not merely synfire chains, can be utilized to perform nonlinear computations [6]. For any of these schemes to function, the timing of the spikes must ",Optimization & Theoretical ML,806fec5af7f5b48b8a31a003e171f3fb-Paper.pdf,2002
Bayesian Estimation of Time-Frequency,"TheBayesianparadigmprovidesanaturalandeffectivemeansofexploit- ing prior knowledge concerning the time-frequency structure of sound signalssuchasspeechandmusic—somethingwhichhasoftenbeenover- lookedintraditionalaudiosignalprocessingapproaches.Here,aftercon- structingaBayesianmodelandpriordistributionscapableoftakinginto account the time-frequencycharacteristics of typical audio waveforms, weapplyMarkovchainMonteCarlomethodsinordertosamplefromthe resultant posteriordistributionof interest. We present speechenhance- mentresultswhichcomparefavourablyinobjectivetermswithstandard time-varyingfilteringtechniques(andinseveralcasesyieldsuperiorper- formance, both objectively and subjectively); moreover, in contrast to such methods, our results are obtained without an assumption of prior knowledgeofthenoisepower. 1 Introduction Natural sounds can be meaningfully represented as a superposition of translated and frequency-modulatedversions of simple functions (atoms). As a result, so-called time-",Computer Vision,81b3833e2504647f9d794f7d7b9bf341-Paper.pdf,2002
Real-Time Monitoring of Complex Industrial,"This paper discusses the application of particle filtering algorithms to faultdiagnosisin complexindustrialprocesses. We considertwo ubiq- uitous processes: an industrial dryer and a level tank. For these appli- cations, we compared three particle filtering variants: standard parti- cle filtering, Rao-Blackwellised particle filtering and a versionof Rao- Blackwellised particle filtering that does one-step look-ahead to select goodsamplingregions. Weshowthattheoverheadoftheextraprocess- ingperparticleofthemoresophisticatedmethodsismorethancompen- satedbythedecreaseinerrorandvariance. 1 Introduction Real-time monitoring is important in many areas such as robot navigation or diagnosis ofcomplexsystems[1,2]. Thispaperconsidersonlinemonitoringofcomplexindustrial processes.Theprocesseshaveanumberofdiscretestates,correspondingtodifferentcom- binationsoffaultsorregionsofqualitativelydifferentdynamics.Thedynamicscanbevery differentbasedonthediscretestates.Evenifthereareveryfewdiscretestates,exa",Optimization & Theoretical ML,86e78499eeb33fb9cac16b7555b50767-Paper.pdf,2002
Retinal Processing Emulation in a,"A bio-inspired model for an analog programmable array processor (APAP), based on studies on the vertebrate retina, has permitted the realization of complex programmable spatio-temporal dynam- ics in VLSI. This model mimics the way in which images are pro- cessed in the visual pathway, rendering a feasible alternative for the implementation of early vision applications in standard tech- nologies. A prototype chip has been designed and fabricated in a 0.5(cid:22)m standard CMOS process. Computing power per area and power consumption is amongst the highest reported for a single chip. Design challenges, trade-o(cid:11)s and some experimental results are presented in this paper. 1 Introduction The conventional role of analog circuits in mixed-signal VLSI is providing the I/O interface to the digital core of the chip |which realizes all the signal processing. However,thisapproachmaynotbeoptimumfortheprocessingofmulti-dimensional sensorysignals, such as those found in vision applications. Whe",Computer Vision,88a839f2f6f1427879fc33ee4acf4f66-Paper.pdf,2002
Bayesian Image Super-Resolution,"The extraction of a single high-quality image from a set of low resolution images is an important problem which arises in fields such as remote sensing, surveillance, medical imaging and the ex traction of still images from video. Typical approaches are based on the use of cross-correlation to register the images followed by the inversion of the transformation from the unknown high reso lution image to the observed low resolution images, using regular ization to resolve the ill-posed nature of the inversion process. In this paper we develop a Bayesian treatment of the super-resolution problem in which the likelihood function for the image registra tion parameters is based on a marginalization over the unknown high-resolution image. This approach allows us to estimate the unknown point spread function, and is rendered tractable through the introduction of a Gaussian process prior over images. Results indicate a significant improvement over techniques based on MAP (maximum a-posteriori) ",Computer Vision,88bfcf02e7f554f9e9ea350b699bc6a7-Paper.pdf,2002
Charting a Manifold,"Weconstructanonlinearmappingfromahigh-dimensionalsamplespace to a low-dimensional vector space, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled. Themappingpreserveslocalgeometricrelationsinthemanifoldandis pseudo-invertible. Weshowhowtoestimatetheintrinsicdimensionality of the manifold from samples, decompose the sample data into locally linear low-dimensional patches, merge these patches into a single low- dimensionalcoordinatesystem, andcomputeforwardandreversemap- pingsbetweenthesampleandcoordinatespaces.Theobjectivefunctions areconvexandtheirsolutionsaregiveninclosedform. 1 Nonlineardimensionalityreduction(NLDR)bycharting Charting is the problem of assigning a low-dimensional coordinate system to data points in a high-dimensional sample space. It is presumed that the data lies on or near a low- dimensionalmanifoldembeddedinthesamplespace,andthatthereexistsa1-to-1smooth nonlineartransformbetweenthemanifoldandalow-dimensionalvector",Unknown,8929c70f8d710e412d38da624b21c3c8-Paper.pdf,2002
A Minimal Intervention Principle for,"Behavioral goals are achieved reliably and repeatedly with movements rarelyreproducibleintheirdetail.Hereweofferanexplanation:weshow thatnotonlyarevariabilityandgoalachievementcompatible,butindeed thatallowingvariability inredundantdimensions is theoptimal control strategyinthefaceofuncertainty.Theoptimalfeedbackcontrollawsfor typicalmotortasksobeya“minimalintervention”principle: deviations fromtheaveragetrajectoryareonlycorrectedwhentheyinterfere with thetaskgoals. Theresultingbehaviorexhibitstask-constrainedvariabil- ity, as well as synergetic coupling among actuators—which is another unexplainedempiricalphenomenon. 1 Introduction Both the difficulty and the fascination of the motor coordination problem lie in the ap- parent conflict between two fundamental properties of the motor system: the ability to accomplishitsgoalreliablyandrepeatedly,andthefactthatitdoessowithvariablemove- ments[1].Moreprecisely,trial-to-trialfluctuationsinindividualdegreesoffreedomareon averagelargerthanfluc",Unknown,8c5f6ecd29a0eb234459190ca51c16dd-Paper.pdf,2002
